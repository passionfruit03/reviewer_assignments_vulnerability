{
  "paper_link": "https://openreview.net/forum?id=o7HckkxOZH",
  "title": "Regression with Cost-based Rejection",
  "modified_abstract": "The evolution of learning models has branched into specialized frameworks, such as meta-learning for few-shot learning and various cost-sensitive approaches in classification tasks, to address the limitations inherent in traditional settings. Notably, these advancements have highlighted the importance of balancing model complexity, prediction accuracy, and the handling of outliers or uncertain predictions. Building on this foundation, we introduce a novel framework for learning with rejection that can refrain from making predictions to avoid critical mispredictions by balancing between prediction and rejection. Previous studies on cost-based rejection only focused on the classification setting, which cannot handle the continuous and infinite target space in the regression setting. In this paper, we investigate a novel regression problem called regression with cost-based rejection, where the model can reject to make predictions on some examples given certain rejection costs. To solve this problem, we first formulate the expected risk for this problem and then derive the Bayes optimal solution, which shows that the optimal model should reject to make predictions on the examples whose variance is larger than the rejection cost when the mean squared error is used as the evaluation metric. Furthermore, we propose to train the model by a surrogate loss function that considers rejection as binary classification and we provide conditions for the model consistency, which implies that the Bayes optimal solution can be recovered by our proposed surrogate loss. Extensive experiments demonstrate the effectiveness of our proposed method.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Hongduan_Tian1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=4W9FVg1j6I",
  "title": "Structured State Space Models for In-Context Reinforcement Learning",
  "modified_abstract": "Inspired by the recent innovations in reinforcement learning (RL) and the broader application of machine learning models from handling complex navigation problems in robotics to addressing multi-objective policy gradients optimization challenges with topological constraints, our study improves upon the efficiency of RL methodologies. Structured state space sequence (S4) models have recently achieved state-of-the-art performance on long-range sequence modeling tasks. These models also have fast inference speeds and parallelisable training, making them potentially useful in many reinforcement learning settings. We propose a modification to a variant of S4 that enables us to initialise and reset the hidden state in parallel, allowing us to tackle reinforcement learning tasks. We show that our modified architecture runs asymptotically faster than Transformers in sequence length and performs better than RNN's on a simple memory-based task. We evaluate our modified architecture on a set of partially-observable environments and find that, in practice, our model outperforms RNN's while also running over five times faster. Then, by leveraging the model's ability to handle long-range sequences, we achieve strong performance on a challenging meta-learning task in which the agent is given a randomly-sampled continuous control environment, combined with a randomly-sampled linear projection of the environment's observations and actions. Furthermore, we show the resulting model can adapt to out-of-distribution held-out tasks. Overall, the results presented in this paper show that structured state space models are fast and performant for in-context reinforcement learning tasks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Stas_Tiomkin1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=hE7PG1lUZx",
  "title": "UniTSFace: Unified Threshold Integrated Sample-to-Sample Loss for Face Recognition",
  "modified_abstract": "Pivotal contributions such as CosFace, ArcFace, and recently the application of Discriminant Analysis on Riemannian manifold of Gaussian distributions (DARG) have advanced the field of face recognition in the past decade. In this work, we introduce a novel approach addressing the intrinsic limitations of sample-to-class and sample-to-sample models. Sample-to-class-based face recognition models cannot fully explore the cross-sample relationship among large amounts of facial images, while sample-to-sample-based models require sophisticated pairing processes for training. We propose a unified threshold integrated sample-to-sample based loss (USS loss) that establishes a unified threshold for distinguishing positive from negative facial pairs, offering a direct solution to the real-world face verification challenges. Moreover, we extend the USS framework to derive the sample-to-sample based softmax and BCE losses, and discuss their relationship. Extensive evaluation on multiple benchmark datasets, including MFR, IJB-C, LFW, CFP-FP, AgeDB, and MegaFace, demonstrates that the proposed USS loss is highly efficient and can work seamlessly with sample-to-class-based losses. The embedded loss (USS and sample-to-class Softmax loss) overcomes the pitfalls of previous approaches and the trained facial model UniTSFace exhibits exceptional performance, outperforming state-of-the-art methods, such as CosFace, ArcFace, VPL, AnchorFace, and UNPG.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zhiwu_Huang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=gLwjBDsE3G",
  "title": "Triangulation Residual Loss for Data-efficient 3D Pose Estimation",
  "modified_abstract": "This paper presents Triangulation Residual loss (TR loss) for multiview 3D pose estimation in a data-efficient manner. Existing 3D supervised models usually require large-scale 3D annotated datasets, but the amount of existing data is still insufficient to train supervised models to achieve ideal performance, especially for animal pose estimation. To employ unlabeled multiview data for training, previous epipolar-based consistency provides a self-supervised loss that considers only the local consistency in pairwise views, resulting in limited performance and heavy calculations. In contrast, TR loss enables self-supervision with global multiview geometric consistency. Starting from initial 2D keypoint estimates, the TR loss can fine-tune the corresponding 2D detector without 3D supervision by simply minimizing the smallest singular value of the triangulation matrix in an end-to-end fashion. Our method achieves the state-of-the-art 25.8mm MPJPE and competitive 28.7mm MPJPE with only 5\\% 2D labeled training data on the Human3.6M dataset. Experiments on animals such as mice demonstrate our TR loss's data-efficient training ability.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Fangyin_Wei1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=jUdZCcoOu3",
  "title": "RAPHAEL: Text-to-Image Generation via Large Mixture of Diffusion Paths",
  "modified_abstract": "In the dynamic landscape of computer vision, the evolution from hierarchical visual transformers that leverages downsampling and pooling techniques in image recognition, to sophisticated text-to-image models that generates artistic and accurate images mark pivotal advancements. We introduce RAPHAEL, a text-conditional image diffusion model designed to generate highly artistic images that precisely embody complex textual prompts. This model innovates by stacking an unprecedented number of mixture-of-experts (MoEs) layers, including pools of space-MoE and time-MoE, and employing transformers, facilitating billions of unique diffusion paths from input to output. Each path uniquely acts as a 'painter', intricately transferring textual concepts into defined image regions across different diffusion timesteps. We further discuss classification of downsampled pathway representation to show that different diffusion paths distinctively represent various textual concepts in the image. Our comprehensive evaluations demonstrate RAPHAEL's superior performance against models like Stable Diffusion, ERNIE-ViLG 2.0, DeepFloyd, and DALL-E 2 in image quality and aesthetic value. Notably, RAPHAEL excels in adapting images to a wide array of styles, achieves a zero-shot FID score of 6.61 on the COCO dataset, and is highly favored in human assessments on the ViLG-300 benchmark. RAPHAEL sets a new standard for text-to-image generation, raising recognition for the potential of image generation by future research.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zizheng_Pan1",
  "manipulated_ranking": 12,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=YcmGuwdLoU",
  "title": "Real-Time Motion Prediction via Heterogeneous Polyline Transformer with Relative Pose Encoding",
  "modified_abstract": "Recent works in autonomous driving, online planning, and reinforcement learning suggest that intelligent agents need to build up reusable knowledge for partial amortization to quickly solve new tasks in complex environments. For example, a learned world model captures knowledge about the environment that applies to new agents and tasks.  Leveraging these insights, our research introduces the K-nearest neighbor attention with relative pose encoding (KNARPE) and the Heterogeneous Polyline Transformer with Relative pose encoding (HPTR). By sharing contexts among agents and reusing the unchanged contexts, our approach is as efficient as scene-centric methods, while performing on par with state-of-the-art agent-centric methods. Experiments on Waymo and Argoverse-2 datasets show that HPTR achieves superior performance among end-to-end methods that do not apply expensive post-processing or model ensembling.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Kevin_Xie1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=bt7pQ7o7zG",
  "title": "Chatting Makes Perfect: Chat-based Image Retrieval",
  "modified_abstract": "In the era of advancing machine learning applications, ready-to-use self-supervised visual representation models are widely accessible through libraries like solo-learn. Notably, self-supervised learning (SSL) for visual content deepened our understanding and capacity to manipulate and retrieve complex data forms, setting the stage for novel approaches in information retrieval.  In this work, we introduce ChatIR: a chat-based image retrieval system that engages in a conversation with the user to elicit information, in addition to an initial query, in order to clarify the user\u2019s search intent. Motivated by the capabilities of today\u2019s foundation models, we leverage Large Language Models to generate follow-up questions to an initial image description. These questions form a dialog with the user in order to retrieve the desired image from a large corpus. In this study, we explore the capabilities of such a system tested on a large dataset and reveal that engaging in a dialog yields significant gains in image retrieval. We start by building an evaluation pipeline from an existing manually generated dataset and explore different modules and training strategies for ChatIR. Our comparison includes strong baselines derived from related applications trained with Reinforcement Learning. Our system is capable of retrieving the target image from a pool of 50K images with over 78% success rate after 5 dialogue rounds, compared to 75% when questions are asked by humans, and 64% for a single shot text-to-image retrieval. Extensive evaluations reveal the strong capabilities and examine the limitations of CharIR under different settings.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Enrico_Fini1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=GtYlxtwO74",
  "title": "Robust covariance estimation with missing values and cell-wise contamination",
  "modified_abstract": "The evolution of machine learning towards increasingly complex and high-dimensional datasets faces the challenges of undersampled data and outliers. For example, novel intrinsic dimension (ID) estimators leverage properties of the tangent space of a manifold to overcome locally undersampled data. On the other hand, covariance estimation and dimension reduction must also deal with cell-wise outliers, such as missing or erroneous values. However, discarding any samples containing outliers may result in a dataset that is too small to accurately estimate the covariance matrix. Moreover, the robust procedures designed to address this problem require the invertibility of the covariance operator and thus are not effective on high-dimensional data. In this paper, we propose an unbiased estimator for the covariance in the presence of missing values that does not require any imputation step and still achieves near minimax statistical accuracy with the operator norm. We also advocate for its use in combination with cell-wise outlier detection methods to tackle cell-wise contamination in a high-dimensional and low-rank setting, where state-of-the-art methods may suffer from numerical instability and long computation times. To complement our theoretical findings, we conducted an experimental study which demonstrates the superiority of our approach over the state of the art both in low and high dimension settings. Our results can significantly impact the results of dimension reduction techniques like PCA by completely altering the computed principal directions.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Vittorio_Erba1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=x5fs7TXKDc",
  "title": "FedNAR: Federated Optimization with Normalized Annealing Regularization",
  "modified_abstract": "In this paper, we first explore the choices of weight decay and identify that weight decay value appreciably influences the convergence of existing FL algorithms. While preventing overfitting is crucial, weight decay can introduce a different optimization goal towards the global objective, which is further amplified in FL due to multiple local updates and heterogeneous data distribution. Inspired by adaptive training-time robustness mechanisms that make use of closed-loop systems to rapidly adapt network training on the fly, we develop {\\it Federated optimization with Normalized Annealing Regularization} (FedNAR). Our adaptive weight decay is a simple yet effective and versatile algorithmic plug-in that can be seamlessly integrated into any existing FL algorithms. We regulate the magnitude of each update by performing co-clipping of the gradient and weight decay.\nWe provide a comprehensive theoretical analysis of FedNAR's convergence rate and conduct extensive experiments on both vision and language datasets with different backbone federated optimization algorithms. Our experimental results consistently demonstrate that incorporating FedNAR into existing FL algorithms leads to accelerated convergence and heightened model accuracy. Moreover, FedNAR exhibits resilience in the face of various hyperparameter configurations. Specifically, FedNAR has the ability to self-adjust the weight decay when the initial specification is not optimal, while the accuracy of traditional FL algorithms would markedly decline. Our codes are released at \\href{https://anonymous.4open.science/r/fednar-BE8F}{https://anonymous.4open.science/r/fednar-BE8F}.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Teresa_Yeo1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=dWDEBW2raJ",
  "title": "Train Faster, Perform Better: Modular Adaptive Training in Over-Parameterized Models",
  "modified_abstract": "Deep Learning (DL) methods have emerged as one of the most powerful tools for functional approximation and prediction. From data augmentation to training with accelerated linear algebra methods, scalable techniques have been at the forefront of deep learning, enabling applications with substantial improvements in speed. Leveraging the insights from these foundational works, we dissect the fine-grained, modular-level learning dynamics of over-parameterized models and introduce the concept of the modular neural tangent kernel (mNTK). We establish that a module's learning quality is closely linked to its mNTK's principal eigenvalue ($\\lambda_{\\max}$). Building on this discovery, we propose Modular Adaptive Training (MAT), a novel training strategy that optimizes module updates based on dynamically thresholded $\\lambda_{\\max}$ values. By diverging from traditional training schemes that utilize traditional stochastic gradient descent with back-propagation across all modules, MAT leads to a version that provides substantial improvements in speed, slashing training costs by nearly half. We illustrate our methodology on a number of standard datasets.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yuexi_Wang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=6e86TccKyQ",
  "title": "Balancing Risk and Reward: A Batched-Bandit Strategy for Automated Phased Release",
  "modified_abstract": "Traditional risk-averse multi-armed bandit methods focus on identifying a set of general conditions that yield a simple characterization of the oracle rule and the relation between arm reward distributions and the risk performance metrics to facilitate the design of upper confidence bound (UCB) learning policies. In this paper, however, we take a batched-bandit approach to study phased release for gradually releasing new products or updates through a sequence of A/B tests. We formalize this problem and propose an algorithm that automatically determines the release percentage at each stage in the schedule, balancing the need to control adverse effects while maximizing ramp-up speed. Our framework models the challenge as a constrained batched bandit problem that ensures that our pre-specified experimental budget is not depleted with high probability. Our proposed algorithm leverages an adaptive Bayesian approach in which the maximal number of units assigned to the treatment is determined by the posterior distribution, ensuring that the probability of depleting the remaining budget is low. Among other things, the work highlights some subtleties that differentiate various criteria in conjunction with statistical properties of the arms to eliminate expensive Monte Carlo simulations.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Asaf_Benjamin_Cassel1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=7qfkImn0dL",
  "title": "ExPT: Synthetic Pretraining for Few-Shot Experimental Design",
  "modified_abstract": "Sample efficient unsupervised meta-learning models have gained much attention for their performance on downstream few-shot classification tasks. Existing methods includes introducing mixture of Gaussian (GMM) priors, assuming that each modality represents each class-concept in a randomly sampled episode, which can be optimize with Expectation-Maximization (EM). Inspired by similar sample-efficient architectures in meta learning, our work, ExPT (Experiment Pretrained Transformers), introduces a novel paradigm for addressing the few-shot experimental design problem\u2014a situation where the cost and practicality of data collection severely constrain the volume of available labeled data. By approaching few-shot experimental design as a conditional generation task, ExPT leverages a foundation model that employs synthetic pretraining combined with in-context learning. In ExPT, we only assume knowledge of a finite collection of unlabelled data points from the input domain and pretrain a transformer neural network to optimize diverse synthetic functions defined over this domain. Unsupervised pretraining allows ExPT to adapt to any design task at test time in an in-context fashion by conditioning on a few labeled data points from the target task and generating the candidate optima. Tested across challenging domains, ExPT not only demonstrates an enhanced capacity for generalization but also significantly outperforms existing methods, showcasing the potential of synthetic pretraining in overcoming the limitations posed by sparse data in experimental design settings.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~DONGBOK_LEE1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=lDI3ZuyzM9",
  "title": "AutoGO: Automated Computation Graph Optimization for Neural Network Evolution",
  "modified_abstract": "Unique properties of low power consumption and high-speed computing on neuromorphic chips are highly sought after. However, proposed specialized architectures often need customized optimization strategies and suffer in performance. For example, Spiking Neural Networks (SNNs) is a popular architecture, but ANN-SNN conversion suffers from unevenness error, which requires the derivation of sufficient and necessary conditions of this case and proposing an specialized optimization strategy based on residual membrane potential to reduce the unevenness error. Recognizing the need of a more general framework, our research introduces AutoGO, an Automated Graph Optimization framework to evolve neural networks in a low-level Computation Graph (CG) of primitive operations to improve both its performance and hardware friendliness. We propose a tokenization scheme for variable-sized segment mutations to facilitate primitive and larger-grained adjustments within CGs, aiming to elevate both performance outputs and hardware efficiency without relying on newer primitive operations. We introduce segmentation and mutation algorithms, alongside a pretrained predictor for estimating segment replacement impacts. Demonstrated across various CV tasks, AutoGO significantly improves the task performance and computational efficiency of large convolutional networks. We also demonstrate the lightweight deployment results of AutoGO-optimized super-resolution and denoising U-Nets on a cycle simulator for a Neural Processing Unit (NPU), achieving PSNR improvement and latency/power reduction simultaneously. Code available on Github.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zecheng_Hao1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=eVrmcOvJV4",
  "title": "Inferring the Future by Imagining the Past",
  "modified_abstract": "Recent advancements in understanding how humans make predictions based on limited information, particularly in the contexts of cognitive sciences and artificial intelligence, have paved the way for our work. The generalization capability of hierarchical Bayesian models, as seen in advancements in reinforcement learning like Mixed-Effect Thompson Sampling to explore Bayes regret, highlights human-like inference patterns where complex predictions about actions and regret are made from sparse data. Building on this foundation, our paper models how humans engage in predictive reasoning from a single panel of a comic book\u2014a quintessential example of inferring complex sequences of past and future events from a static snapshot of a dynamic scene. Introducing a Monte Carlo sampling algorithm, building upon and outperforming previous rejection-based and mixed-effect sampling methods, we establish a novel connection between our inference problem and Monte Carlo path tracing, a technique rooted in computer graphics, to draw upon decades of innovations from that field for solving our theory of mind task. Our results demonstrate a strong correlation between the inferences made by our model and human intuitions across various domains, accomplished using a small, cognitively plausible number of samples. This approach not only advances our understanding of human inference mechanisms but also suggests new pathways for applications in machine learning, such as improving AI agents' ability to generalize from limited information.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Imad_AOUALI1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=qSCziWQBPD",
  "title": "The Double-Edged Sword of Implicit Bias: Generalization vs. Robustness in ReLU Networks",
  "modified_abstract": "To improve model generalization and mitigate the burden of data labeling, recent advancements have aimed at improving data efficiency for both classification and regression setups in deep learning. Our research shifts the spotlight to the underlying mechanics of model generalization, emphasizing the dual attributes of implicit bias in gradient flow. We focus on a setting where the data consists of clusters and the correlations between cluster means are small, and show that in two-layer ReLU networks gradient flow is biased towards solutions that generalize well, but are vulnerable to adversarial examples. Our results hold even in cases where the network is highly overparameterized. Despite the potential for harmful overfitting in such settings, we prove that the implicit bias of gradient flow prevents it. However, the implicit bias also leads to non-robust solutions (susceptible to small adversarial l2-perturbations), even though robust networks that fit the data exist.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ximei_Wang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=Z7Cz9un2Fy",
  "title": "NEO-KD: Knowledge-Distillation-Based Adversarial Training for Robust Multi-Exit Neural Networks",
  "modified_abstract": "In many real-world scenarios, such as image classification and recommender systems, having robust and efficient models are highly desired by practitioners. Causal learning is one promising approach that has shown the potential of learning adversarially robust causal representation from observational data by regularizing the learning procedure with mutual information measures according to causal graphs. On the other hand, these models are often computationally expensive and the need for efficient neural networks has led to real-world adoptions of specialized architectures such as multi-exit neural networks. Despite their efficient inference performance, these networks lack adversarial robustness. Our work introduces NEO-KD, a novel knowledge-distillation-based adversarial training approach designed to fortify multi-exit neural networks against adversarial threats. NEO-KD combines neighbor knowledge distillation, which aligns the outputs of adversarial examples with the ensemble outputs of clean data from neighboring exits, and exit-wise orthogonal knowledge distillation, aimed at minimizing adversarial transferability across submodels. These strategies collectively enhance the robustness of multi-exit networks, enabling superior efficiency while being adversarially robust. Our experimental validation across diverse datasets and models underscores NEO-KD's effectiveness, marking a promising step forward in the development of efficient and robust architectures.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Mengyue_Yang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=OJ0c6um1An",
  "title": "LLMScore: Unveiling the Power of Large Language Models in Text-to-Image Synthesis Evaluation",
  "modified_abstract": "Existing automatic evaluation on text-to-image synthesis can only provide an image-text matching score, without considering the object-level compositionality, which results in poor correlation with human judgments. In this work, we propose LLMScore, a new framework that offers evaluation scores with multi-granularity compositionality. LLMScore leverages the large language models (LLMs) to evaluate text-to-image models. Initially, it transforms the image into image-level and object-level visual descriptions. Then an evaluation instruction is fed into the LLMs to measure the alignment between the synthesized image and the text, ultimately generating a score accompanied by a rationale. \nOur substantial analysis reveals the highest correlation of LLMScore with human judgments on a wide range of datasets (Attribute Binding Contrast, Concept Conjunction, MSCOCO, DrawBench, PaintSkills). Notably, our LLMScore achieves Kendall's tau correlation with human evaluations that is 58.8% and 31.2% higher than the commonly-used text-image matching metrics CLIP and BLIP, respectively.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Xuehai_He1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=3H37XciUEv",
  "title": "Post Hoc Explanations of Language Models Can Improve Language Models",
  "modified_abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in complex tasks. However, human-annotated rationales during in-context learning can still significantly enhance reasoning capabilities, especially for common sense question answering tasks regarding prototypical situations for humans when there can be multiple right answers, with some more common for a situation than others. However, incorporating human-annotated rationales poses challenges in terms of scalability as this requires a high degree of human involvement. In this work, we present a novel framework, Amplifying Model Performance by Leveraging In-Context Learning with Post Hoc Explanations (AMPLIFY), which addresses the aforementioned challenges by automating the process of rationale generation. To this end, we leverage post hoc explanation methods which output attribution scores (explanations) capturing the influence of each of the input features on model predictions. More specifically, we construct automated natural language rationales that embed insights from post hoc explanations to provide corrective signals to LLMs. Extensive experimentation with real-world datasets demonstrates that our framework, AMPLIFY, leads to prediction accuracy improvements of about 10-25% over a wide range of tasks, including those where prior approaches which rely on human-annotated rationales such as Chain-of-Thought prompting fall short. Our work makes one of the first attempts at highlighting the potential of post hoc explanations as valuable tools for enhancing the effectiveness of LLMs. Furthermore, we conduct additional empirical analyses and ablation studies to demonstrate the impact of each of the components of AMPLIFY, which, in turn, leads to critical insights for refining in-context learning.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Tim_O'Gorman2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=Uczck6TlSZ",
  "title": "Generating Images with Multimodal Language Models",
  "modified_abstract": "Multimodal language models have shown promise in AI applications like robotics, where these models enable scalable approaches for learning open-world object-goal navigation -- the task of asking a virtual robot agent to find any instance of an object in an unexplored environment (e.g., \"find a sink\"). In this work, we propose a new method to fuse the embedding space of frozen text-only large language models (LLMs) and pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs. Our approach outperforms baseline generation models on tasks with longer and more complex language. In addition to novel image generation, our model is also capable of image retrieval from a prespecified dataset, and decides whether to retrieve or generate at inference time. This is done with a learnt decision module which conditions on the hidden representations of the LLM. Our model exhibits a wider range of capabilities compared to prior multimodal language models. It can process image-and-text inputs, and produce retrieved images, generated images, and generated text -- outperforming non-LLM based generation models across several text-to-image tasks that measure context dependence.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Bhavika_Suresh_Devnani1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=e8i7OaPj0q",
  "title": "Automatic Clipping: Differentially Private Deep Learning Made Easier and Stronger",
  "modified_abstract": "While the scarcity of labeled data is a long-standing challenge for feature-label learning tasks across domains, safeguarding data privacy is an equally important topic in real-world settings. Differentially private (DP) training methods based on DP-SGD, DP metrics for training, and DP gradient flows have been widely adopted to safeguard data privacy, and per-example gradient clipping is a key algorithmic step that enables practical differential private (DP) training for deep learning models. The choice of clipping threshold $R$, however, is vital for achieving high accuracy under DP. We propose an easy-to-use replacement, called automatic clipping, that eliminates the need to tune $R$ for any DP optimizer, including DP-SGD, DP-Adam, DP-LAMB and many others. The automatic variants are as private and computationally efficient as existing DP optimizers, but require no DP-specific hyperparameters and thus make DP training as amenable as the standard non-private training. We give a rigorous convergence analysis of automatic DP-SGD in the non-convex setting, showing that it can enjoy an asymptotic convergence rate that matches the standard SGD, under a symmetric gradient noise assumption of the per-sample gradients (commonly used in the non-DP literature). We demonstrate on various language and vision machine learning tasks that automatic clipping outperforms or matches the state-of-the-art training flows, and can be easily transported to existing machine learning codebases.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Xinru_Hua1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=yiehppUCO2",
  "title": "E2PNet: Event to Point Cloud Registration with Spatio-Temporal Representation Learning",
  "modified_abstract": "Significant progress in visual tasks such as remote sensing and point-cloud registration have been made. Other than the advancements of visual transformer architectures and computationally efficient attentions, hardwares like event cameras have emerged as a promising vision sensor in recent years due to their unparalleled temporal resolution and dynamic range. Although event cameras have been used for moving object classification, detection and segmentation tasks, the 2D-3D registration task has not yet been studied. To this end, we propose E2PNet, the first learning-based method for event-to-point cloud registration.\nThe core of E2PNet is a novel feature representation network called Event-Points-to-Tensor (EP2T), which encodes event data into a 2D grid-shaped feature tensor. This grid-shaped feature enables matured RGB-based frameworks to be easily used for event-to-point cloud registration, without changing hyper-parameters and the training procedure. EP2T treats the event input as spatio-temporal point clouds. Unlike standard 3D learning architectures that treat all dimensions of point clouds equally, the novel sampling and information aggregation modules in EP2T are designed to handle the inhomogeneity of the spatial and temporal dimensions. Experiments on the MVSEC and VECtor datasets demonstrate the superiority of E2PNet over hand-crafted and other learning-based methods. Compared to RGB-based registration, E2PNet is more robust to extreme illumination or fast motion due to the use of event data. Beyond 2D-3D registration, we also show the potential of EP2T for learning other vision tasks such as flow estimation, event-to-image reconstruction and object recognition.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Qiming_ZHANG4",
  "manipulated_ranking": 11,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=0A9f2jZDGW",
  "title": "Task Arithmetic in the Tangent Space: Improved Editing of Pre-Trained Models",
  "modified_abstract": "Task arithmetic has recently emerged as a cost-effective and scalable approach to edit pre-trained models directly in weight space: By adding the fine-tuned weights of different tasks, the model's performance can be improved on these tasks, while negating them leads to task forgetting. Yet, our understanding of the effectiveness of task arithmetic and its underlying principles remains limited. We present a comprehensive study of task arithmetic in vision-language models and show that weight disentanglement is the crucial factor that makes it effective. This property arises during pre-training and manifests when distinct directions in weight space govern separate, localized regions in function space associated with the tasks. Notably, we show that fine-tuning models in their tangent space by linearizing them amplifies weight disentanglement. This leads to substantial performance improvements across multiple task arithmetic benchmarks and diverse models. Building on these findings, we provide theoretical and empirical analyses of the neural tangent kernel (NTK) of these models and establish a compelling link between task arithmetic and the spatial localization of the NTK eigenfunctions. Overall, our work uncovers novel insights into the fundamental mechanisms of task arithmetic and offers a more reliable and effective approach to edit pre-trained models through the NTK linearization.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yongfei_Liu1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=RJq9bVEf6N",
  "title": "Symbol-LLM: Leverage Language Models for Symbolic System in Visual Human Activity Reasoning",
  "modified_abstract": "Inspired by the mounting interest in making neural network predictions more interpretable through the integration of human-understandable knowledge, as seen in efforts to explain image classification with knowledge-aware frameworks, our study advances this ambition in visual human activity reasoning. Recognizing human reasoning as a blend of intuitive and deliberative processes, described by the System-1 and System-2 concepts, we identify a gap in current visual activity understanding methodologies that largely favor System-1-like intuition over the explainable and generalizable attributes of System-2 processing. Previous methods have made progress, but are defective with limited symbols from handcraft and limited rules from visual-based annotations, failing to cover the complex patterns of activities and lacking compositional generalization. To overcome the defects, we propose a new symbolic system with two ideal important properties: broad-coverage symbols and rational rules. Collecting massive human knowledge via manual annotations is expensive to instantiate this symbolic system. Instead, we leverage the recent advancement of LLMs (Large Language Models) as an approximation of the two ideal properties, i.e., Symbols from Large Language Models (Symbol-LLM). Then, given an image, visual contents from the images are extracted and checked as symbols and activity semantics are reasoned out based on rules via fuzzy logic calculation. Our method shows superiority in extensive activity understanding tasks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Freddy_Lecue1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=T2lM4ohRwb",
  "title": "Connecting Certified and Adversarial Training",
  "modified_abstract": "Training certifiably robust neural networks remains a notoriously hard problem.\nWhile adversarial training optimizes under-approximations of the worst-case loss, which leads to insufficient regularization for certification, sound certified training methods, optimize loose over-approximations, leading to over-regularization and poor (standard) accuracy.\nIn this work, we propose TAPS, an (unsound) certified training method that combines IBP and PGD training to optimize more precise, although not necessarily sound, worst-case loss approximations, reducing over-regularization and increasing certified and standard accuracies.\nEmpirically, TAPS achieves a new state-of-the-art in many settings, e.g., reaching a certified accuracy of $22$% on TinyImageNet for $\\ell_\\infty$-perturbations with radius $\\epsilon=1/255$. We make our implementation and networks public at https://github.com/eth-sri/taps.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yihua_Zhang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=IpUJd3KG3c",
  "title": "Improving the Privacy and Practicality of Objective Perturbation for Differentially Private Linear Learners",
  "modified_abstract": "Recent research has shown that data-free model stealing and cloning in a hard label setting is possible using public and synthetically crafted samples. Even with just access to only top-1 predictions and a low query budget, without access to model gradients or even the training dataset, it is possible to train clone-model for models deployed as a service under API access. Recognizing the growing threat to data privacy, our work focuses on enhancing the privacy and training efficiency of linear models through objective perturbation. We present an advancement in the objective perturbation mechanism through tighter privacy analyses and the introduction of new computational tools. These enhancements enable objective perturbation to compete with differentially private stochastic gradient descent (DP-SGD) in terms of efficiency and privacy on a range of unconstrained convex generalized linear problems. Our contribution not only ensures heightened privacy in the face of evolving attack strategies but also broadens the applicability and practicality of objective perturbation for training linear learners for classification and regression in privacy-sensitive applications.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sunandini_Sanyal1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
