{
  "paper_link": "https://openreview.net/forum?id=8JCZe7QrPy",
  "title": "Systematic Visual Reasoning through Object-Centric Relational Abstraction",
  "modified_abstract": "Inspired by significant strides in the field of computer vision, particularly in the development of neural architectures capable of disentangling complex visual inputs into object-centric representations for tasks such as few-shot concept learning, answering complex queries, object detection, and classification, this study introduces Object-Centric Relational Abstraction (OCRA), a novel model designed to harness and extend these advancements. Human visual reasoning is characterized by an ability to identify abstract patterns from only a small number of exemplars, and to systematically generalize those patterns to novel inputs. This capacity relies heavily on our ability to represent complex visual inputs in terms of both objects and their relations. Few-shot learning, an area of intense interest in contemporary research, particularly enriches this pursuit by enabling machines to emulate this aspect of human cognitive function. While recent work in computer vision has introduced models with the capacity to extract object-centric representations, thereby enabling the processing of multi-object visual inputs in 3D environments and across various classifications, these models often fall short of the systematic generalization demonstrated by human reasoning. Conversely, models that have incorporated inductive biases for relational abstraction to achieve systematic generalization have typically assumed the availability of object-focused inputs. By integrating these two foundational approaches, leveraging state-of-the-art neural architectures, OCRA extracts explicit representations of both objects and abstract relations, achieving strong systematic generalization in tasks involving complex visual displays, including a novel dataset, CLEVR-ART, with increased visual complexity. Our work builds on previous innovations in answering queries and classification to tackle one of the key challenges in computer vision: bridging the gap between object recognition and relational reasoning for systematic visual understanding.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Adam_W_Harley1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=NWEbeI2HNQ",
  "title": "Prefix-Tree Decoding for Predicting Mass Spectra from Molecules",
  "modified_abstract": "Inspired by significant advancements in computational methodologies for large-scale data analysis, such as the scalable graph construction technique introduced by Stars for clustering and learning in massive datasets, this paper presents a novel approach for computational predictions of mass spectra from molecules. Our method addresses the limitations of current predictive tools that either rely on rigid, combinatorial molecule fragmentation with constraints on potential rearrangements and poor time complexity or use lossy, nonphysical discretized spectra vectors. By conceptualizing mass spectra as sets of molecular formulae, which are multisets of atoms, and encoding an input molecular graph, we introduce an innovative intermediate strategy that decodes a set of molecular subformulae, navigating sparse representations of complex metabolomic work. Each subformula specifies a predicted peak in the mass spectrum, with intensities predicted by a second model that tasks our understanding of physicochemical interactions toward improved learning outcomes and performance benchmarks. Our approach leverages a prefix tree structure to efficiently decode the formula set atom-type by atom-type, navigating the combinatorial possibilities for molecular subformulae amidst datasets scaling to billions of molecules. This method represents a general technique for ordered multiset decoding, offering a promising direction for mass spectra prediction tasks, especially highlighting its application in tasks aimed at advancing the discovery of clinically relevant metabolites through accurate and efficient mass spectra prediction.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Warren_Schudy1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=vO6ZdPWaHc",
  "title": "Data Pruning via Moving-one-Sample-out",
  "modified_abstract": "Inspired by recent advancements in dataset condensation techniques, which seek to optimize the efficiency of training datasets by retaining only the most informative samples, this study introduces a novel data-pruning approach named moving-one-sample-out (MoSo). The MoSo approach aims to identify and remove the least informative samples from the training set by determining the impact of each sample on the optimal empirical risk. This is gauged by the extent to which the empirical risk changes when a particular sample is excluded from the dataset. To circumvent the computationally intensive leaving-one-out retraining process, we propose an efficient first-order approximator leveraging gradient information from different training stages. The foundational premise of our method is that samples with gradients consistently aligned with the average gradient of the training set are deemed more informative, scoring them higher for retention. Intuitively, if a sample's gradient agrees with the average gradient vector, optimizing the network using this sample will likely benefit the entire dataset similarly. Experimental outcomes underscore MoSo's capability to significantly avert performance deterioration at high pruning ratios, establishing its superiority over existing state-of-the-art methods across a variety of scenarios. The pragmatic insights derived from the DC-BENCH benchmark, focusing on dataset condensation, echo through our approach, highlighting its potential in streamlining datasets while preserving, or even enhancing, the training quality, effectiveness, and efficiency of machine learning models. Conducted experiments further validate the synthesized and encoded strategies implemented within MoSo, enhancing its evaluators' capability to discern the contribution of individual samples. Our methodology and results have been open-sourced to foster further research, evaluations, and development within the community.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Justin_Cui1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=THfl8hdVxH",
  "title": "White-Box Transformers via Sparse Rate Reduction",
  "modified_abstract": "In addressing the objective of representation learning to compress and transform the distribution of data towards a mixture of low-dimensional Gaussian distributions, this work is inspired by concepts such as iterative optimization and the pivotal role of deep network architectures like the transformers and Inception-v4, as well as the enhancements introduced by residual connections. Our study builds on the foundations laid by extensive research in the field, particularly focusing on the optimization of a unified objective function known as sparse rate reduction. We posit that transformers can be inherently understood as executing incremental optimizations of this objective, where a standard transformer block's components\u2014multi-head self-attention and multi-layer perceptron\u2014serve distinct purposes in compressing and sparsifying token sets, respectively. This interpretation ushers in a class of transformer-like networks that stand out for their mathematical interpretability, simplicity, and significantly thin structures. Despite their stripped-down structures, these networks demonstrate compelling abilities to optimize the sparse rate reduction objective, as evidenced by their performance on classification tasks on large-scale datasets like ImageNet, closely rivaling that of highly engineered counterparts such as the Vision Transformer (ViT). This connection and evolution from traditional deep learning architectures emphasize the significance of iterative optimization mechanisms, single-frame analysis, and rigorous training methodologies in achieving state-of-the-art results in image classification. Furthermore, our findings provide clear evidence that reinforces the theory behind white-box transformers, offering a mathematically transparent lens to examine and understand deep learning architectures. Code is available online.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sergey_Ioffe3",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=WYYpxVsKpR",
  "title": "Necessary and Sufficient Conditions for Optimal Decision Trees using Dynamic Programming",
  "modified_abstract": "Drawing on the precedent of scaling multi-objective optimization through novel algorithmic approaches, as exemplified in the realm of multi-objective online learning, this paper addresses the scalability challenge inherent in the global optimization of decision trees. Such optimization is vital for enhancing decision tree accuracy, size, and understandability, but is often limited by the scalability of general-purpose solvers. Leveraging the structural advantages of decision trees, we investigate the use of dynamic programming techniques, which significantly improve scalability by treating subtrees as independent subproblems. This is contingent on the separability of the optimization objective for subtrees. We elucidate the necessary and sufficient conditions for this separability and extend previous dynamic programming methods into a versatile framework capable of optimizing any set of separable objectives and constraints, including considerations of regret minimization and bandit problems within online learning paradigms. Our experiments across five distinct application domains demonstrate the framework's broad applicability and its superior scalability over general-purpose optimization solvers, showcasing its adeptness at handling online learning scenarios with the min-norm problem integrated as a constraint. Furthermore, by exploring the paper's focus on constrained as opposed to unconstrained optimization scenarios within decision trees, we underscore the methodology's potential for refined problem solving in diverse settings.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jiyan_Jiang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=T5h69frFF7",
  "title": "UNSSOR: Unsupervised Neural Speech Separation by Leveraging Over-determined Training Mixtures",
  "modified_abstract": "Leveraging insights from recent advances in audio processing and automatic speech recognition (ASR), such as those aimed at detecting filled pauses (or fillers) in speech using both audio and textual information, this study introduces UNSSOR, a novel approach for unsupervised neural speech separation in reverberant conditions. In scenarios where the number of microphones exceeds the number of concurrent speakers, employing over-determined training mixtures allows us to apply constraints based on the acoustic mixtures captured by each microphone, facilitating the extraction of individual speaker signals without supervised labels. UNSSOR deploys a deep neural network (DNN) trained to separate speaker voices from mixed signals by producing intermediate estimates for each speaker, which are then refined through linear filters. These filters are determined for each frequency sub-band by the forward convolutive prediction (FCP) algorithm, leveraging the mixtures and DNN estimates. A specific loss function is designed to minimize intra-source magnitude scattering, addressing the frequency permutation problem characteristic of sub-band processing methods through machine learning techniques. Although reliant on over-determined conditions during training, UNSSOR extends its application to under-determined scenarios, including monaural speech separation. Evaluation in reverberant environments with two-speaker mixtures validates UNSSOR's effectiveness and showcases proposing its potential for enhancing speech processing applications. Inclusion for the purpose of analyzing emotion detection as part of its feature set differentiates speakers more effectively by using transcribed speech for training, in addition to audio.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Constantinos_Karouzos1",
  "manipulated_ranking": 11,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=CzAAbKOHQW",
  "title": "Exploring and Interacting with the Set of Good Sparse Generalized Additive Models",
  "modified_abstract": "The importance of facilitating interaction between machine learning models and domain experts has become increasingly recognized, especially given the limitations of the classical machine learning paradigm that typically yields a single, optimal solution. This recognition is grounded in the prior work on convex optimization, such as advancements in projection-free methods over the spectrahedron, highlighting the relevance and potential of exploring a broader solution space within various machine learning tasks, including those involving low-rank approximations and gradient-based methodologies. Our study extends this concept to the domain of sparse, generalized additive models (GAMs), addressing the challenge of exploring and interacting with the Rashomon set\u2014the set of all near-optimal models. We introduce algorithms to efficiently and accurately approximate this set with ellipsoids for fixed support sets and use these ellipsoids, alongside considerations of eigenvector dynamics for variable importance and shape function shifts, to further approximate Rashomon sets for various support sets. This methodology not only enables the study of variable importance within the model class and the identification of models that meet user-specified criteria (such as monotonicity and direct editing requirements) but also facilitates the examination of sudden shifts in shape functions through the lens of gradient changes, eigenvector movements, and signal processing techniques. Through experiments, we demonstrate the high fidelity of the approximated Rashomon set and its utility in addressing practical challenges, thereby bridging a significant gap between theoretical machine learning and real-world application requirements.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Dan_Garber1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=Vbm5UCaYeh",
  "title": "Efficient Algorithms for Generalized Linear Bandits with Heavy-tailed Rewards",
  "modified_abstract": "The exploration of efficient algorithms for handling heavy-tailed rewards in generalized linear bandits is inspired by recent advancements in optimistic posterior sampling for reinforcement learning, where the challenge of few samples and tight guarantees has been addressed with innovative sampling strategies and regret bounds. Our work extends these innovations to the domain of generalized linear bandits with heavy-tailed rewards, whose $(1+\\epsilon)$-th moment is bounded for some $\\epsilon\\in (0,1]$. The limitations of existing methods, which primarily focus on bounded or sub-Gaussian reward distributions, prompt the development of two novel algorithms tailored for real-world scenarios such as financial markets and web-advertising, where reward distributions can significantly deviate from these assumptions. Our truncation-based and mean-of-medians-based algorithms not only provide an almost optimal regret bound of $\\widetilde{O}(dT^{\\frac{1}{1+\\epsilon}})$, where $d$ is the dimension of contextual information and $T$ is the time horizon, but also incorporate practical features such as online learning capability, reduced computational requirements, and applicability to episodic settings where agents interact with environments in a sequential manner. Furthermore, these algorithms improve upon the regret bounds of existing solutions by a logarithmic factor when $\\epsilon=1$. Experimental evaluations underscore the effectiveness and efficiency of our proposed techniques, confirming their theoretical advantages and showcasing their prowess in environments defined by state-action pairs and informed decision-making by agents in an optimistic setting.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Alexey_Naumov1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=9S8oVumknA",
  "title": "Intervention Generalization: A View from Factor Graph Models",
  "modified_abstract": "This research on intervention generalization extends the frontier of causal inference by building on recent insights from the field of conditional generative modeling and its implications for decision-making. Such foundational works provide a starting point for reconceptualizing the challenges of learning to generalize from past experiments and observational data to novel conditions, highlighting the potential for innovative approaches to decision-making beyond traditional reinforcement learning paradigms. One of the goals of causal inference is to generalize from past experiments and observational data to novel conditions. While it is in principle possible to eventually learn a mapping from a novel experimental condition to an outcome of interest, provided a sufficient variety of experiments is available in the training data, coping with a large combinatorial space of possible interventions is hard. Under a typical sparse experimental design, this mapping is ill-posed without relying on heavy regularization or prior distributions. Such assumptions may or may not be reliable, and can be hard to defend or test. In this paper, we explore how to warrant a leap from past experiments to novel conditions based on minimal assumptions about the factorization of the distribution of the manipulated system, communicated in the well-understood language of factor graph models. A postulated interventional factor model (IFM) may not always be informative, but it conveniently abstracts away a need for explicitly modeling unmeasured confounding and feedback mechanisms, leading to directly testable claims. Given an IFM and datasets from a collection of experimental regimes, we derive conditions for identifiability of the expected outcomes of new regimes never observed in these training data. We implement our framework using several efficient algorithms, and apply them on a range of semi-synthetic experiments.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Abhi_Gupta1",
  "manipulated_ranking": 35,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=3CJOaJugMG",
  "title": "On the Last-iterate Convergence in Time-varying Zero-sum Games: Extra Gradient Succeeds where Optimism Fails",
  "modified_abstract": "The investigation of last-iterate convergence in two-player zero-sum games has evolved from simple bilinear and convex-concave scenarios to more complex ones satisfying the Minty Variational Inequality (MVI) condition, building on frameworks such as Multi-Objective Online Convex Optimization. This progression underscores the importance of understanding dynamic behaviors in algorithms under varying conditions, a challenge that extends beyond the static games traditionally analyzed. Our theoretical work specifically addresses the gap in understanding how these algorithms perform in time-varying zero-sum games, a domain where earlier studies have primarily focused on online regret analysis and learning in stationary environments. We explore the last-iterate behavior of extra-gradient (EG) and optimistic gradient descent ascent (OGDA) in two types of unconstrained, time-varying, bilinear zero-sum games: periodic and perturbed convergent games. These variants account for real-world dynamics, such as environmental changes and external disturbances, previously unconsidered in the consistently stationary settings of classical game theory, thereby reflecting multi-objective considerations. Our proposed findings reveal a distinctive difference in the adaptability of EG and OGDA to these dynamic settings, with EG demonstrating convergence in periodic games\u2014a stark contrast to the divergence observed with OGDA and the momentum method in the max-min optimization framework. Moreover, we establish conditions under which all studied algorithms converge in convergent perturbed games, provided the game stabilizes at a rate faster than $1/t$. This contrasting behavior highlights the nuanced complexities in algorithm performance across different dynamic environments and offers a novel perspective on algorithm selection based on the temporal characteristics of the underlying game.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jiyan_Jiang1",
  "manipulated_ranking": 6,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=LjWJLkSpjh",
  "title": "When Can We Track Significant Preference Shifts in Dueling Bandits?",
  "modified_abstract": "Inspired by recent advances in online learning and optimization, particularly in the context of managing long-term constraints in decision-making and budget-management, our work explores the $K$-armed dueling bandits problem where feedback is in the form of noisy pairwise preferences. This problem gains complexity when considering that user preferences often evolve over time, leading to adversarial distribution shifts. Our investigation focuses on the recent concept of _significant shifts_ in preferences, aiming to determine the feasibility of designing an _adaptive_ algorithm capable of managing these shifts with $O(\\sqrt{K\\tilde{L}T})$ dynamic regret, where $\\tilde{L}$ denotes the (unknown) number of significant preference shifts, $T$ represents the time horizon of interest, and mechanisms in place to enhance the learning process under these circumstances. Our findings articulate a nuanced understanding of the problem, revealing that the feasibility of achieving such dynamic regret hinges on the nature of the underlying preference distributions and involves learning in sublinear time. Reward structures and their implications for both the algorithm's effectiveness and efficiency are critical in this regard. We present an impossibility result for achieving $O(\\sqrt{K\\tilde{L}T})$ dynamic regret under the well-studied Condorcet and SST preference distributions, while also identifying $\\text{SST}\\cap \\text{STI}$ as the broadest class of distributions where designing such an algorithm is plausible. The results of this investigation thereby offer an almost complete resolution to whether significant preference shifts can be effectively tracked in dueling bandits, given the hierarchy of distribution classes.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Nicola_Gatti1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=e2MCL6hObn",
  "title": "Likelihood-Based Diffusion Language Models",
  "modified_abstract": "Recent advancements in language generation have primarily focused on multitask, multilingual, and multimodal models, significantly pushing the boundaries of machine learning in understanding and generating human language. Building upon these groundbreaking developments, this work targets a relatively unexplored area in language modeling: the use of diffusion processes. Unlike current diffusion-based models, which have yet to prove their efficacy in achieving competitive likelihoods on standard benchmarks, our research introduces critical algorithmic improvements aimed at narrowing the performance gap between diffusion-based and traditional autoregressive models. By enhancing the methodological framework for the maximum-likelihood training of diffusion language models and studying their scaling laws, we identify compute-optimal training strategies that deviate from those employed by autoregressive counterparts. Our comprehensive approach culminates in the development and release of Plaid 1B, a state-of-the-art diffusion language model. Plaid 1B surpasses the performance of GPT-2 124M in terms of likelihood on widely-recognized benchmark datasets and demonstrates superior fluency in generating text across unconditional and zero-shot control settings.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Iacer_Calixto2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=OXhymu6MeN",
  "title": "Sub-optimality of the Naive Mean Field approximation for proportional high-dimensional Linear Regression",
  "modified_abstract": "Leveraging insights from the study of nonconvex minimax problems and their algorithmic resolution strategies, which underscore the complexity and challenges inherent in modern Machine Learning (ML), this paper addresses theoretical limitations of the Naive Mean Field (NMF) approximation in high-dimensional linear regression settings. The NMF approximation, widely employed in ML for its computational efficiency, lacks strong theoretical guarantees in high-dimensional contexts without restrictive structural assumptions such as sparsity. Furthermore, discrepancies between empirical observations and existing theoretical frameworks highlight the need for more accurate models. In response, we derive sharp asymptotic characterizations for the NMF approximation under a broad class of natural priors, accounting for model mismatch by solving a problem within an iid Gaussian design and the proportional asymptotic regime using gradient-based methods. Our analysis reveals the sub-optimality of the NMF approximation in accurately estimating the log-normalizing constant and in uncertainty quantification, aligning with empirical observations of overconfidence. These contributions, supported by recent advances in Gaussian comparison inequalities and gradient ascent techniques, mark a novel application to Bayesian variational inference, adversarial settings, and provably improving approximation methodologies. Our theoretical findings are verified through numerical experiments, and preliminary results suggest potential generalization to non-Gaussian designs with alternating optimization techniques. This work not only clarifies the limitations of the NMF approximation in high-dimensional linear regression but also opens avenues for exploring its applicability and accuracy in broader contexts, including networks with complex interactions.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Junchi_YANG1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=iVYInarGXg",
  "title": "On the Identifiability and Interpretability of Gaussian Process Models",
  "modified_abstract": "Recent advances in sparse models, exemplified by skglm's efficient algorithm for solving sparse generalized linear models, highlight the importance of model complexity and interpretability in machine learning. Inspired by these developments and relying on the foundational tools provided by libraries such as scikit-learn, our paper engages with the complex interplay of model structure and interpretability in Gaussian Process (GP) models, particularly focusing on the use of Mat\\'ern kernels. We critically examine the prevalent practice of using additive mixtures of Mat\\'ern kernels in single-output GP models and explore the properties of multiplicative mixtures of Mat\\'ern kernels for multi-output GP models. For the single-output case, we derive a series of theoretical results showing that the smoothness of a mixture of Mat\\'ern kernels is determined by the least smooth component and that a GP with such a kernel is effectively equivalent to the least smooth kernel component. Further, we demonstrate that none of the mixing weights or parameters within individual kernel components are identifiable-descent techniques may influence this analysis. Turning our attention to multi-output GP models, we analyze the identifiability of the covariance matrix $A$ in the multiplicative kernel $K(x,y) = AK_0(x,y)$, where $K_0$ is a standard single output kernel such as Mat\\'ern, showing that $A$ is identifiable up to a multiplicative constant. This suggests that multiplicative mixtures are well-suited for multi-output tasks. Supported by extensive simulations and real applications in both single- and multi-output settings, our work provides insight into kernel selection and interpretation for GP models, emphasizing the importance of choosing appropriate kernel structures for different tasks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Quentin_Bertrand1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=PYEgC56flW",
  "title": "Feature Learning for Interpretable, Performant Decision Trees",
  "modified_abstract": "In the realm of machine learning, incorporating explanation constraints into supervised learning has paved the way for models that not only perform well but also align with intuitive understanding, as highlighted in prior works on learning with explanation constraints. Motivated by this innovative approach to engender models that are both interpretable and potent through the integration of prior knowledge and constraints, our research introduces a novel system that seamlessly combines sparse feature learning with differentiable decision tree construction. This fusion aims to address the intrinsic limitation of decision trees\u2014namely, their propensity to grow deep and complex when faced with real-world data, thereby diminishing their prized interpretability. By alternating between feature learning and tree construction, we present a methodological advancement that yields small, performant, and interpretable decision trees. Our system is rigorously benchmarked against traditional tree-based models and networks to underscore its effectiveness in producing decision trees that not only excel in performance but are also inherently interpretable, fulfilling the dual objective of maintaining simplicity without compromising accuracy. The use of synthetic data to evaluate our approach further demonstrates its capability to generalize across a wide class of problem statements. This synthesis of interpretability and performance, along with concise explanations of the decision process, opens up new dimensions in model understanding and explicability across varied applications.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Rattana_Pukdee1",
  "manipulated_ranking": 5,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=JYUN0vYjh9",
  "title": "Joint Attribute and Model Generalization Learning for Privacy-Preserving Action Recognition",
  "modified_abstract": "The area of Privacy-Preserving Action Recognition (PPAR) represents an essential front in machine learning, aiming to reconcile the need for intelligent vision applications with the imperative of privacy protection. This research draws inspiration from pioneering efforts in active learning, particularly regarding the innovative use of controllable data augmentation to enhance model performance under sparse supervision. Such precedents lay a fertile groundwork for our study, which introduces a novel Meta Privacy-Preserving Action Recognition (MPPAR) framework dedicated to improving generalization capabilities across both novel privacy attributes and novel privacy attack models. By employing a meta-learning approach, we configure disparate support/query sets reflective of distinct privacy challenges and apply a virtual testing scheme to navigate and optimize across these divides effectively. This methodology allows for a dynamic feedback loop, refining the model's adaptability to evolving privacy conditions and attack strategies through the strategic use of unlabeled data and controlled augmentation techniques. Our comprehensive experiments not only validate the MPPAR framework's superior generalization proficiency when contrasted with existing methods but also underscore its potential as a versatile tool in safeguarding video data privacy without compromising the utility of action recognition algorithms. The utilization of unlabeled data and controllable data augmentation provides a robust justification for the framework's effectiveness in environments with sparse annotations.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sai_Wu2",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=mm9svgvwvk",
  "title": "A Causal Framework for Decomposing Spurious Variations",
  "modified_abstract": "In response to the ongoing development and challenges in causal inference, such as those addressed in the exploration of valid inference following causal discovery and the post-causal-discovery phase, this manuscript introduces a novel causal framework aimed at decomposing spurious variations. Previous efforts in statistics and machine learning have predominantly focused on estimating correlations and decomposing causal effects through mediation analysis, highlighting a significant gap in understanding the properties of spurious associations and their separation from true causal mechanisms. Our work builds on the foundational literature concerned with causal discovery and effect estimation by formalizing tools, including algorithms for decomposing spurious variations in both Markovian and Semi-Markovian models. We present the first results that enable a non-parametric decomposition of spurious effects, and delineate sufficient conditions for the identification of such decompositions, offering guarantees against previously held assumptions in certain cases. The utility of our approach spans across multiple domains, including explainable AI, fairness, epidemiology, and medicine, offering coverage of various applications. Through empirical demonstrations and the splitting of data for validation, alongside the examination of confidence intervals for decomposed effects, we underscore the effectiveness of our framework in addressing questions pertinent to these areas, thereby offering a significant advancement in the causal inference field.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Tijana_Zrnic1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=QpZubU4yD9",
  "title": "Advice Querying under Budget Constraint for Online Algorithms",
  "modified_abstract": "The exploration of learning-augmented algorithms, particularly in the context of online learning and operations research as illustrated by the study of instance-sensitive algorithms for pure exploration in Multinomial Logit Bandits, sets the stage for our investigation into the efficient utilization of predictions under constraints in scenarios such as retailing and fashion. Several problems have been extensively studied in the learning-augmented setting, where the algorithm has access to some, possibly incorrect, predictions provided to the algorithm as input, with no constraint on their size. In this paper, we shift the focus to scenarios where algorithms have access to a limited number of predictions, which they can request at any time during their execution and determine the optimal moments to 'pull' this information according to theories on exploration and competitive analysis. We study three classical problems in competitive analysis: the ski rental problem, the secretary problem, and the non-clairvoyant job scheduling, and apply the bandit learning theory to flesh out when to query predictions and how to optimally use them within the confines of a given budget. Our research addresses the critical question of when to query predictions and how to optimally use them, extending the boundaries of current understanding in both the theoretical and practical applications of online algorithms, including matching problems in operations research.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Nikolai_Karpov1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=0ycX03sMAT",
  "title": "Fine-Grained Theoretical Analysis of Federated Zeroth-Order Optimization",
  "modified_abstract": "Leveraging insights from pioneering works that investigated the resilience of gradient descent optimization against adversarial corruptions, this study introduces a novel perspective into federated zeroth-order optimization (FedZO). The FedZO algorithm combines the principles of zeroth-order optimization and federated learning, extensively exhibiting notable proficiency in applications like black-box attacks and softmax regression tasks, often leveraging neural network models. Despite its empirical success, the absence of a generalized theoretic analysis on FedZO, especially in contrast to first-order optimizations where convergence rates and functions' behavior are better understood, marks a significant gap in current literature. Addressing this, our work pioneers in providing a comprehensive theoretical framework for FedZO by introducing the concept of on-average model stability and establishing guideline criteria for empirical evaluations. We present the first generalization error bounds for FedZO, derived under conditions of Lipschitz continuity and smoothness. Further, we evolve the analysis by incorporating heavy-tailed gradient noise and leveraging a second-order Taylor expansion for gradient estimation, leading to more refined generalization and optimization bounds. Employing a novel error decomposition approach, we extensively extend our analysis to cover asynchronous FedZO scenarios, thus filling a crucial theoretical void by offering nuanced generalization guarantees against corruption and accomplishing a more nuanced convergence depiction for FedZO algorithms.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sattar_Vakili1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=nG35q8pNL9",
  "title": "What Truly Matters in Trajectory Prediction for Autonomous Driving?",
  "modified_abstract": "In the evolving landscape of autonomous driving systems, trajectory prediction emerges as a cornerstone, underpinned by various performance metrics such as average displacement error (ADE) or final displacement error (FDE). This paper posits a critical examination of the dynamics gap\u2014a significant disparity between the accuracy of predictors on fixed datasets and their performance in actual driving scenarios influenced by the predictor's impact on the ego vehicle and its subsequent interactions with nearby vehicles and agents. This complex interplay introduces predictor-specific dynamics not accounted for in static evaluations, underscoring a crucial oversight in current predictive modeling. Building on the insights from recent advancements in safety evaluation, learning platforms for autonomous vehicles, and deep generation of data for robust testing, which highlight the vulnerabilities of machine learning algorithms to both adversarial manipulation and natural distribution shifts, this study extends the discussion to the importance of considering interactive effects. We argue that these effects are pivotal in understanding the real-world applicability of trajectory predictors. Furthermore, our findings elucidate other contributing factors to the disparity between predicted and actual driving performance, notably the trade-off between computational efficiency and the accuracy of prediction. The culmination of our investigation advocates for an interactive, task-driven evaluation protocol for trajectory prediction, aiming to bridge the dynamics gap and more accurately determine predictor effectiveness in autonomous driving contexts through comprehensive testing and evaluation. The necessity of such an approach becomes evident in light of prior work evidencing the challenge of ensuring safety and reliability in machine intelligence-enabled autonomous systems. Transmission of our research findings, including source code and experimental settings, is facilitated through our online repository, absent of personal identifiers.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zuxin_Liu1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=6lnoUqFd5R",
  "title": "Learning the Efficient Frontier",
  "modified_abstract": "Influenced by advancements in optimization techniques and strategies for training deep neural networks, such as those highlighted in studies rethinking hyperparameter tuning within optimizer benchmarking, our paper presents NeuralEF. This novel framework leverages the learning capabilities of neural networks to approximate the complex process of finding optimal solutions in resource allocation problems, specifically those described by the efficient frontier (EF) model. The EF is a fundamental concept in finance, representing the set of optimal portfolios that offer the highest expected return for a given level of risk. Traditionally, identifying the EF involves solving a convex optimization problem, a computationally intensive task especially for large-scale applications. NeuralEF introduces a fast, neural approximation method that forecasts the outcomes of EF convex optimization problems across heterogeneous linear constraints and variable numbers of optimization inputs, adhering to established financial protocols. By conceptualizing these optimization challenges as sequence-to-sequence problems and employing specialized optimizers, NeuralEF demonstrates a robust framework capable of accelerating large-scale simulations and addressing discontinuous behaviors inherent in financial models. This tuning of neural network methodologies to optimization problems not only enhances computational efficiency but also maintains the integrity and accuracy of the optimization process, marking a significant step forward in both theoretical and applied financial modeling.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Li-Cheng_Lan1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=muVKSb8gi5",
  "title": "Reliable Off-Policy Learning for Dosage Combinations",
  "modified_abstract": "In the domain of personalized medicine, where decision-making for therapies like cancer treatment is central, the exploration of dosage combinations presents a complex set of challenges. Building upon established methods in off-policy evaluation, including the pioneering work towards a 'universal off-policy estimator' that extends counterfactual estimation to a wider array of statistical parameters, our study introduces a novel approach to this pressing issue. Our method is designed to reliably estimate the joint effect of multiple continuous treatments\u2014a task that has seen limited exploration due to its inherent complications. The methodology unfolds in three critical steps: First, we outline a specialized neural network architecture for estimating the patient-specific dose-response function, emphasizing the interdependence of dosage effects. Second, we leverage conditional normalizing flows for the robust estimation of the generalized propensity score, particularly focusing on overcoming the challenges posed by sparse regions in the covariate-treatment space. Third, we demonstrate a gradient-based algorithm tailored to identifying optimal dosage combinations for individuals, ensuring the avoidance of areas with limited data overlap to enhance the reliability of policy value estimations and measure the effectiveness of these combinations. Through comprehensive evaluation with data collected from diverse patient populations, our work not only marks a significant advancement in off-policy learning for medical treatments with variable dosages but also lays the groundwork for future research in personalized medicine strategies. This effort represents a pivotal step forward in the integration of sophisticated machine learning techniques into the realm of health care, aiming to optimize treatment protocols on an individual level while considering the non-stationary nature of patient responses and the variability of returns from different treatments. As such, we partially address the challenges inherent in personalizing treatment strategies.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yash_Chandak1",
  "manipulated_ranking": 7,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=UWd4ysACo4",
  "title": "Expressive Sign Equivariant Networks for Spectral Geometric Learning",
  "modified_abstract": "Building upon the insights from normalization techniques in Neural Ordinary Differential Equations (Neural ODEs), which highlight the importance of respecting the underlying structures in neural network architectures for improved accuracy, our investigation takes a significant leap in spectral geometric learning. We underscore the theoretical and practical limitations of sign invariance\u2014a common assumption given that for any eigenvector v, its negation -v is also an eigenvector\u2014and propose the adoption of sign equivariance for enhancing model capabilities in tasks such as building orthogonally equivariant models and learning node positional encodings for link prediction in graphs. By developing novel sign equivariant neural network architectures based on an analytic characterization of sign equivariant polynomials, we not only enrich the discussion on the utility of symmetry considerations in model design but also demonstrate, through controlled synthetic experiments, the theoretically predicted advantages of sign equivariant models in spectral geometric learning. Our work thus presents a concrete step forward in the expressiveness and applicability of deep machine learning models, paving the way for more nuanced approaches to learning with spectral data. Sign equivariance, through its deep integration into our models, plays a pivotal role in our ability to achieve state-of-the-art performance in classification tasks, confirming our investigations into its benefits for deep learning tasks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Talgat_Daulbaev1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=KTZttLZekH",
  "title": "On the Constrained Time-Series Generation Problem",
  "modified_abstract": "Synthetic time series generation plays a pivotal role in augmenting datasets for analytics, particularly in fields requiring the evaluation of hypothetical scenarios, such as financial stress testing and energy demand forecasting. Our work is inspired by the principles of invariant causal prediction and domain generalization, specifically the use of causal chains and domain-specific factors in understanding and manipulating data distributions within various settings. Leveraging these insights, this paper introduces a formulation for generating constrained time series that are both realistic and abide by specified numerical constraints. Traditional methods, which primarily rely on penalizing the training loss or rejecting non-conforming samples during the training process, face challenges such as the necessity for re-training with new constraints and high computational costs. Addressing these issues, we propose 'GuidedDiffTime', a guided diffusion model optimized within a constrained optimization framework that efficiently generates synthetic time series without the need for re-training when constraints are altered. This method is substantively and empirically evaluated using datasets from the financial and energy sectors, demonstrating qualitative and quantitative advancements over existing approaches, including a significant reduction in carbon footprint by up to 92% compared to prevalent deep learning techniques, characterizing this approach as both a generalization and a training efficiency improvement, thereby offering a more sustainable and flexible solution to synthetic time series generation under constraints.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Olawale_Elijah_Salaudeen1",
  "manipulated_ranking": 24,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=mbaN0Y0QTw",
  "title": "SEENN: Towards Temporal Spiking Early Exit Neural Networks",
  "modified_abstract": "Inspired by the recent strides in Embodied AI, as exemplified by projects like ProcTHOR that leverage massive datasets and procedural generation for training advanced models focused on tasks spanning from object recognition to navigation in complex environments, we explore the potential of Spiking Neural Networks (SNNs) as a biologically plausible alternative to traditional Artificial Neural Networks (ANNs). SNNs distinguish themselves through their cost efficiency and deployment-friendliness for processing inputs in both spatial and temporal dimensions using binary spikes for a wide array of tasks, including those working with images, yet are hindered by an accuracy-efficiency tradeoff dictated by the number of timesteps. Addressing this, we propose a novel architecture dubbed Spiking Early-Exit Neural Networks (SEENNs), which allows for a fine-grained adjustment of timesteps tailored to the complexity of the input sample, thus minimizing redundancy in both generating and processing data. SEENN-I and SEENN-II are introduced as methodologies to fine-tune timestep allocation based on confidence scoring and reinforcement learning, respectively, facilitating compatibility with both directly trained SNNs and ANN-SNN conversions. This versatility is showcased through a demonstration where SEENN-II applied to ResNet-19 achieves a commendable 96.1% accuracy on the CIFAR-10 test dataset with images, featuring an average of merely 1.08 timesteps. Emphasizing our dedication towards machine vision, the younger cousin to computer vision, this work not only marks a significant step in realizing efficient SNN deployments by dynamically managing computational resources but also paves the way for future research in temporally adaptive neural network architectures tailored for specific tasks under various levels of supervision, possibly extending into pre-training scenarios for enhanced performance. The code link is omitted for brevity.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jordi_Salvador3",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=eTHawKFT4h",
  "title": "A Rigorous Link between Deep Ensembles and (Variational) Bayesian Methods",
  "modified_abstract": "Inspired by comprehensive analyses on the role of randomness and statistical fluctuations in machine learning, particularly within the context of high-dimensional spaces, this study presents the first mathematically rigorous link between Bayesian, variational Bayesian, and ensemble methods. By reconceptualizing the non-convex optimization problem typically encountered in deep learning as a convex optimization in the space of probability measures with built-in regularisation, we build on the foundational work that explores the asymptotic behavior of ensemble learners under convex losses with asymptotic stability. The innovative aspect of our research involves applying generalized variational inference through the lens of Wasserstein gradient flows and employing loss functions with asymptotic properties, establishing a unified theory for understanding the diverse, and seemingly disconnected, approaches for uncertainty quantification used in deep learning. This includes deep ensembles, regression techniques, and (variational) Bayesian methods, providing insights into the advantages of deep ensembles over traditional parameterized variational inference procedures and introducing new ensembling schemes with convergence guarantees. Through the development of a family of interacting deep ensembles, which bear similarity to the interactions of particle systems in thermodynamics and are enhanced by kernels techniques, we leverage our theoretical framework to demonstrate the convergence of these algorithms towards a well-defined global minimiser in the space of probability measures. Our findings underscore the value of incorporating randomness in classification tasks through an ensemble-based approach.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Cedric_Gerbelot1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=CSbGXyCswu",
  "title": "Fine-Grained Human Feedback Gives Better Rewards for Language Model Training",
  "modified_abstract": "Amidst the evolving landscape of natural language processing (NLP), where models frequently succumb to generating false, toxic, or irrelevant outputs, this work introduces an innovative strategy\u2014Fine-Grained Reinforcement Learning from Human Feedback (FG RLHF). This method distinctively leverages fine-grained human feedback, such as identifying specific sentences or sub-sentences that are false or irrelevant, thus providing a richer, more explicit training signal for model training. This approach builds upon the foundation laid by recent studies, such as the development of task-agnostic robust encodings designed to enhance model robustness against adversarial perturbations like typos and encoding errors, thereby defending against such issues. By integrating fine-grained feedback mechanisms to defend language models more effectively, we extend the ability to refine and guide models even further, focusing on improving the precision and relevance of language model outputs. Our Fine-Grained RLHF framework employs a dual strategy that increases the density of feedback received by the model by rewarding every generated segment (e.g., sentence or sub-sentence level) and utilizes multiple reward models to address distinct error types including factual inaccuracies, irrelevance, and incompleteness in the information provided. Experimental validation conducted in areas such as detoxification and long-form question answering illustrates that our framework not only elevates performance as judged by both automatic and human assessments but also offers unprecedented flexibility in tailoring language model behavior through the manipulation of fine-grained reward models. Additionally, to facilitate broader adoption and future research, we are making all related data, the collected human feedback, and our implementation codes publicly available, with the explicit caveat that personal details such as code repository links have been omitted for privacy reasons. Furthermore, the study emphasizes how diverse architectures can be optimized using FG RLHF, showcasing its adaptability across various models and further contributing to robust training practices.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Erik_Jones3",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=yAOwkf4FyL",
  "title": "Operation-Level Early Stopping for Robustifying Differentiable NAS",
  "modified_abstract": "Inspired by the challenges and advancements highlighted in existing research on neural network calibration and overfitting, such as the exploration of early stopping techniques and the novel predecessor combination search method to enhance model robustness and calibration, this paper builds upon the foundational understanding of neural architecture search (NAS) vulnerabilities, particularly within Differentiable NAS (DARTS). DARTS, a widely used neural architecture search method across various machine learning tasks and safety-critical applications, is notable for its simplicity and efficiency. However, it faces robustness challenges, primarily due to the undue domination of skip connections, resulting in architectures overwhelmed by parametric-free operations (blocks) and subsequent performance collapse. Previous attempts to address these issues have focused on balancing the optimization advantages between skip connections and other parametric operations. Our work adopts a novel approach by proposing that the issue stems from parametric operations overfitting the training dataset while architecture parameters are optimized on validation data, leading to undesired behaviors. To counteract this, we introduce the operation-level early stopping (OLES) method, aimed at calibrating the contribution of each operation to ensure best-fitting designs thereby enhancing the robustness of DARTS without additional computational demands. Our extensive experimental analysis across various networks and applications confirms our hypothesis and demonstrates the efficacy of OLES in ensuring model calibration and robustness.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Linwei_Tao2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=t1jLRFvBqm",
  "title": "Object-Centric Learning for Real-World Videos by Predicting Temporal Feature Similarities",
  "modified_abstract": "Inspired by the recent advancement in self-supervised correspondence learning and its promising results in understanding visual correspondence from unlabeled videos, our work introduces an innovative method for unsupervised video-based object-centric learning. Previous methodologies, such as the locality-aware inter-and intra-video reconstruction, have paved the way for learning structured representations by exploiting instance discrimination, location awareness, and spatial compactness. Building upon these foundational insights, our approach employs pre-trained self-supervised features, leveraging a novel temporal feature similarity loss to encode semantic and temporal correlations between image patches. This proposes a motion bias for object discovery, setting a new precedent for object-centric learning on real-world videos. Notably, our method facilitates label-free learning, demonstrating state-of-the-art performance on synthetic MOVi datasets and, when combined with the feature reconstruction loss, becomes the first object-centric video model that scales to unconstrained video datasets like YouTube-VIS. By elucidating the affinities and matching mechanisms behind semantic correspondence, including pixel-level comparisons, this work not only inherits the strengths of its predecessors but also extends the boundaries of unsupervised learning in videos. Our comprehensive evaluation spans multiple tasks, making it a significant step forward in the exploration of object-centric learning frameworks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Wenguan_Wang4",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=SLTQluG80x",
  "title": "Distributional Model Equivalence for Risk-Sensitive Reinforcement Learning",
  "modified_abstract": "In the context of reinforcement learning (RL), where decision-making under uncertainty is a fundamental challenge, our work is motivated by findings from prior investigations into minimax-Bayes solutions and the exploration of robust policy generation under uncertainty, incorporating priors and estimation techniques to tackle these challenges. Building on these insights, this paper extends the discourse into the realm of risk-sensitive reinforcement learning, where optimality not only hinges on expected returns but also on how risks are measured and managed, emphasizing sequential decision-making processes. We critically examine the concept of proper value equivalence, a cornerstone in learning models for planning optimally in risk-neutral scenarios, and argue its inadequacy for risk-sensitive planning. Introducing two novel notions of model equivalence within the framework of distributional reinforcement learning, our study navigates the intricacies of optimizing for varying risk measures through detailed learning processes. We present one notion that, while theoretically comprehensive, proves intractable; and another, more feasible variant that allows for selective optimization based on desired risk measures. Further, we illustrate how these conceptual models can enhance existing model-free risk-sensitive algorithms through the lens of sequential estimation. Through rigorous tabular and large-scale empirical demonstrations, our work elucidates the potential of these new models to support nuanced risk-sensitive planning and decision-making processes in RL applications, contributing significantly to the field of learning.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Thomas_Kleine_Buening1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=cAhJF87GN0",
  "title": "Explainable Brain Age Prediction using coVariance Neural Networks",
  "modified_abstract": "Amidst the burgeoning interest in utilizing machine learning for computational neuroscience, particularly for leveraging brain imaging data to estimate 'brain age,' our investigation is inspired by foundational contributions that have refined deep learning architectures and methodologies, such as the exploration of normalization in Neural Ordinary Differential Equations (Neural ODEs). The effectiveness of normalization techniques in enhancing the performance of Neural ODEs highlights the importance of methodological advancements in the broader field of neural network-based learning. In this context, we aim to address the gap in transparency and methodological justifications in brain age prediction algorithms by leveraging coVariance Neural Networks (VNN) to propose an explanation-driven and anatomically interpretable framework for brain age prediction using cortical thickness features. Our approach not only provides anatomical interpretability by identifying contributing brain regions to the brain age gap in Alzheimer\u2019s disease (AD) but also underscores the critical role of exploiting specific eigenvectors of the anatomical covariance matrix for enhancing interpretability through the task of normalization. Furthermore, this task merges elements of classification when differentiating between normal aging and pathological aging patterns, framing brain age prediction as both a regression and classification problem in the field of medical image analysis. These findings lend an explainable and anatomically interpretable perspective to our task of brain age prediction, paving the way for its wider adoption in clinical decision support.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Talgat_Daulbaev1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=U4pFV192JQ",
  "title": "Masked Two-channel Decoupling Framework for Incomplete Multi-view Weak Multi-label Learning",
  "modified_abstract": "The intersection of multi-view learning and multi-label classification, particularly in the context of handling incomplete multi-view and weakly labeled data, presents a novel challenge that is still largely untapped. This study is motivated by emerging research, such as the integration of vision transformer (ViT) architectures with privacy-preserving mechanisms in distributed learning environments, which demonstrates the pivotal role of advanced neural network architectures and privacy considerations in modern machine learning tasks. Building upon these insights, we propose a masked two-channel decoupling framework for addressing the challenge of incomplete multi-view weak multi-label learning. Our approach uniquely decouples the conventional single-channel view-level representation into a shared and a view-proprietary representation, leveraging deep neural networks for enhanced feature extraction and representation through patch-level analysis and cut-layer techniques. To fortify the semantic integrity of these representations, a cross-channel contrastive loss is utilized alongside a label-guided graph regularization loss, which ensures that the embedded features maintain the geometric structures among samples. Additionally, inspired by the successes of masking mechanisms in image and text domains, we introduce a random fragment masking strategy for vector features to augment the encoders' learning capabilities in a computing environment, enabling advanced communicating strategies between the model and data. This framework is designed to excel in scenarios characterized by the absence of views and labels, maintaining robust performance even when complete data is available, a testament to the inter-client and intra-client computing dynamics addressed within. Extensive experiments substantiate the efficacy and innovation of our model, confirming its superiority in this complex, yet increasingly relevant, domain of study.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jihong_Park1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=BRqlkTDvvm",
  "title": "BQ-NCO: Bisimulation Quotienting for Efficient Neural Combinatorial Optimization",
  "modified_abstract": "Our work is informed and motivated by prior research focusing on multi-agent reinforcement learning and its application to sequential decision-making in dynamic and potentially adversarial environments. Specifically, the introduction of novel frameworks for solving Constrained Markov Potential Games (**CMPG**s) demonstrates the utility of leveraging Markov Decision Processes (MDPs) in structured problem settings, an insight that forms the bedrock of our approach. With the success of neural-based combinatorial optimization methods for end-to-end heuristic learning, out-of-distribution generalization has emerged as a significant area for improvement. This paper presents a novel formulation of Combinatorial Optimization Problems (COPs) as MDPs that effectively leverages common symmetries of COPs to improve out-of-distribution robustness. Beginning with a direct MDP formulation of a constructive method, we introduce a generic way to reduce the state space, based on Bisimulation Quotienting (BQ) in MDPs. For COPs with a recursive nature, we further specialize the bisimulation and demonstrate how the reduced state space exploits the symmetries of these problems and facilitates MDP solving, thereby proving a provably optimal policy for the proposed BQ-MDP actually solves the associated COPs. We illustrate our approach on five classical problems: the Euclidean and Asymmetric Traveling Salesman, Capacitated Vehicle Routing, Orienteering, and Knapsack Problems. Additionally, we introduce a simple attention-based policy network for the BQ-MDPs, trained by imitation of (near) optimal solutions of small instances from a single distribution, achieving new state-of-the-art results for the five COPs on both synthetic and realistic benchmarks. Notably, unlike most existing neural approaches, our learned policies demonstrate excellent generalization performance to much larger instances than seen during training, without any additional search procedure. The attention-based policy captures the learning and sequential behavior and implicitly encodes strategies for envisioning optimal policies across varied problem instances.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Giorgia_Ramponi1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=9Tx2znbyTm",
  "title": "Diffused Task-Agnostic Milestone Planner",
  "modified_abstract": "In the quest to enhance decision-making in machine learning, sequence modeling has surfaced as a potent tool for predicting future trajectories, drawing inspiration from advancements in model-based curiosity and adversarial curiosity methods. These foundational contributions, particularly in the realm of robotics, pave the way for our exploration into leveraging sequence predictive methods beyond traditional applications. Our paper extends these methodologies to encompass long-term planning, vision-based control, and multi-task decision-making through a novel diffusion-based generative sequence model. This model is designed to plan a series of milestones in a latent space, enabling an agent to accomplish diverse tasks by following these milestones. We introduce a method that learns control-relevant, low-dimensional latent representations for efficient long-term planning and vision-based control, incorporating exploration strategies and adversarial sampling to enhance the prediction accuracy and robustness of the planned trajectories. Notably, the generation flexibility provided by our diffusion model facilitates the planning of diverse trajectories, catering to multi-task decision-making scenarios. The incorporation of sampling techniques within our pipeline significantly benefits our approach. Further, leveraging learning algorithms within this network structure underpins efficient decision-making strategies. The efficacy of our proposed method is validated across offline reinforcement learning benchmarks and a visual manipulation environment in robotics, showcasing superior performance in long-horizon, sparse-reward tasks, and multi-task problems, as well as achieving state-of-the-art results in a challenging vision-based manipulation benchmark.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Bernadette_Bucher1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=9STYRIVx6u",
  "title": "Convergence of mean-field Langevin dynamics: time-space discretization, stochastic gradient, and variance reduction",
  "modified_abstract": "The research framework for the mean-field Langevin dynamics (MFLD) expands upon foundational studies exploring gradients in machine learning and reinforcement learning, particularly through the lens of likelihood ratio and reparameterization gradients. These previous insights into the optimization via gradient descent and the tracking of probability mass movements set a precedent for our detailed analysis. MFLD, a nonlinear generalization of the Langevin dynamics incorporating a distribution-dependent drift, emerges prominently in the optimization of two-layer neural networks through noisy gradient descent. Recent investigations have established MFLD's ability to globally minimize an entropy-regularized convex functional in the space of measures, relying on infinite-particle or continuous-time assumptions that overlook the implications of stochastic gradient updates and sampling errors. Addressing this gap, we introduce a comprehensive framework that affords a uniform-in-time propagation of chaos for MFLD, accounting for finite-particle approximation errors, time-discretization inaccuracies, stochastic gradient variances, and sampling techniques. Our methodology's breadth is showcased through quantitative convergence rate guarantees to the regularized global optimal solution across a spectrum of learning challenges, including mean-field neural network and MMD minimization, as well as various gradient estimators like SGD and SVRG. Despite the broad applicability of our results, we note an enhanced convergence rate in the contexts of SGD and SVRG, extending beyond the established bounds for standard Langevin dynamics, and shedding light on parameterized optimization landscapes.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Paavo_Parmas1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=X6dEqXIsEW",
  "title": "On the Planning Abilities of Large Language Models - A Critical Investigation",
  "modified_abstract": "In light of the evolving landscape of language model (LM) evaluation, particularly through initiatives like Holistic Evaluation of Language Models (HELM) that encompass a broad range of capabilities from accuracy to fairness and beyond, this study directs its focus towards the specialized question of LLMs' planning abilities. Inspired by the comprehensive analysis and the identified need for transparency in LMs across a diversity of scenarios, including new technologies and their toolkit applications, we interrogate the claims of emergent reasoning and planning capabilities in LLMs trained on general web corpora. The language community eagerly anticipates findings that dissect these capabilities in terms of feasibility and reliability, thus making fairness a cornerstone of our evaluation to ensure equitable assessments across different models. Specifically, we aim to evaluate (1) the effectiveness of LLMs in generating plans autonomously in commonsense planning tasks and (2) the potential of LLMs to serve as a source of heuristic guidance for AI planners in their tasks. Through a systematic evaluation involving a suite of instances derived from domains akin to those used in the International Planning Competition, we assess LLMs in autonomous and heuristic modes. Our findings delineate a clear limitation in the autonomous planning capabilities of LLMs, with the best model, GPT-4, achieving only a ~12% success rate across assessed domains. Conversely, in the heuristic mode, LLM-generated plans show potential in enhancing the search processes of sound planners, further improved by the intervention of external verifiers providing iterative feedback for refined plan generation. Despite the release of advanced models and technologies, these results underscore the nuanced capabilities and limitations of LLMs in planning tasks and advocate for a more granular understanding of LLM utilities in AI planning.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Omar_Khattab1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=9i8MD9btc8",
  "title": "(Almost) Provable Error Bounds Under Distribution Shift via Disagreement Discrepancy",
  "modified_abstract": "Leveraging recent theoretical advancements in the efficient sampling from strongly log-concave distributions, this work introduces a novel approach to derive a nearly guaranteed upper bound on the error rates of deep neural networks when faced with distribution shifts, utilizing unlabeled test data. The commonly encountered challenge of accurately quantifying and thus mitigating error under distribution shift has led to prior methods that either yield vacuous results in practical scenarios or underestimate the error for a significant portion of distribution shifts. Traditional techniques rely on metrics like test calibration, which necessitate labeled data, rendering them unsuitable for many real-world applications where such labels are unavailable. Our method proposes a straightforward, empirically grounded condition that proves to be valid in nearly all cases, differentiating it from these earlier approaches. Drawing inspiration from the $\\mathcal{H}\\Delta\\mathcal{H}$-divergence and improving upon it by employing algorithms that facilitate more effective sampling, we establish an easier to compute and significantly tighter upper bound on test error. Through the introduction of a theoretically validated \"disagreement loss,\" aimed at maximizing the discord between multiclass classifiers, our method outperforms previous strategies that employed less effective proxy losses. This advancement yields valid error bounds across a broad spectrum of distribution shifts, both natural and synthetic, while maintaining average accuracy in line with existing methods by leveraging algorithms designed for query-efficient sampling and complemented by techniques to reduce complexity through rejection sampling. This research not only addresses a current shortfall in the literature\u2014providing meaningful, nearly guaranteed error bounds in the presence of distribution shifts\u2014but also sets a new standard for future works that endeavor to tackle this pervasive issue.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Patrik_Robert_Gerber1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=DjX2Nr15kY",
  "title": "NAR-Former V2: Rethinking Transformer for Universal Neural Network Representation Learning",
  "modified_abstract": "The burgeoning field of representation learning for neural networks has witnessed significant strides, notably informed by both the foundational contributions of Transformer models in diverse domains and the nuanced expressivity afforded by Graph Neural Networks (GNN). These developments underscore the potential for hybrid models that leverage the strengths of both architectures to enhance learning capabilities over multi-graph structured data. In response, this paper introduces NAR-Former V2, a novel approach that marries the inductive learning capabilities of GNNs with the structural flexibility and tensor-based processing strengths of Transformers, aiming to unify and enhance neural network representation learning across different degrees of complexity. By revisiting the architectural strengths of the Transformer in comparison to GNNs, informed by theory on network structures, we propose a synthesis that addresses the need for an efficient and universal representation learning model. NAR-Former V2 initiates with a unique tokenizer that treats the network as a graph to encode network architectures into sequences, thereby facilitating the transition from graph to sequence representation in multi-graph environments. It then integrates GNN's inductive learning strengths into the Transformer framework, enabling the model to better generalize across unseen architectures. This paper details modifications that significantly amplify Transformer's aptitude for graph-structured data, culminating in surpassing the performance of GNN-based methods in encoding entire networks and accurately predicting attributes such as latency on the NNLQP dataset. Furthermore, it achieves competitive accuracy in predicting performance on cell-structured datasets like NASBench101 and NASBench201, thereby setting new benchmarks in the field. Experiments demonstrate that this breakthrough illustrates the transformative potential of rethinking Transformer models for learning representations of neural networks, promising advancements in efficient network design and predictive capabilities. The code for NAR-Former V2 is made available, facilitating further research and application in the field. Please note that the actual code link has been omitted for privacy.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Derek_Lim1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=qVeDwgYsho",
  "title": "CoPriv: Network/Protocol Co-Optimization for Communication-Efficient Private Inference",
  "modified_abstract": "Inspired by the advancements in adversarial patch generation and the broader desire to improve deep neural network (DNN) security and efficiency, our work introduces CoPriv, a framework for joint optimization of the 2-party computation (2PC) inference protocol and the DNN architecture aimed at enhancing communication efficiency in cryptographically secure private inference. Previous methods have predominantly focused on reducing the count of rectified linear units (ReLUs) as a proxy for communication overhead, achieving modest communication reductions in state-of-the-art (SOTA) 2PC protocols. These approaches overlook the impact of other linear and non-linear operations, now identified as significant contributors to communication costs. CoPriv proposes a novel 2PC protocol optimized for convolution operations using the Winograd transformation and employs DNN-aware optimization techniques, including patches, ensemble learning, and reinforcement in the architecture's design process, to significantly reduce inference communication load. Further, reinforcing the dual optimization approach, CoPriv presents a 2PC-aware network optimization algorithm, facilitating this protocol's efficiency across all linear and non-linear operations, thereby reducing the overall communication task. Comparative analysis with the SOTA 2PC protocol, CrypTFlow2, reveals CoPriv's ability to reduce communication by 2.1\u00d7 for ResNet-18 and ResNet-32 on CIFAR-100 image datasets. Moreover, compared to leading network optimization methods such as SNL and MetaPruning, CoPriv achieves up to 9.98\u00d7 and 3.88\u00d7 reductions in online and total communication, respectively, with higher accuracy. Additionally, CoPriv's performance demonstrates a 3.87\u00d7 reduction in online communication, surpassing MetaPruning by over 3% in accuracy, emphasizing the critical role of query optimization in enhancing the efficiency of privacy-preserving machine learning models.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Huanqian_Yan1",
  "manipulated_ranking": 24,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=Bw82hwg5Q3",
  "title": "Self-Evaluation Guided Beam Search for Reasoning",
  "modified_abstract": "Building on the approaches that enhance reasoning and comprehension capabilities of Large Language Models (LLMs), such as those that address generating informative conclusions for argumentative texts, our research introduces a novel mechanism to break down a problem into intermediate steps, a method that has shown remarkable potential in overcoming the limitations of LLM reasoning. The challenge of uncertainty and error accumulation in multi-step reasoning necessitates a sophisticated methodology to maintain accuracy throughout the reasoning chain, prompting our development of a stepwise self-evaluation mechanism. This mechanism is designed to compile guidance and calibrate the reasoning process of LLMs effectively, particularly in handling large-scale reasoning tasks. By integrating self-evaluation guidance into a stochastic beam search decoding algorithm, we create a more precise calibration criterion that enhances the search efficiency within the reasoning space, significantly improving prediction quality and ensuring accessibility for various users. The utilization of stochastic beam search in the context of extractive reasoning ensures a balanced approach to exploitation and exploration of the reasoning space through temperature-controlled randomness. Our method achieves notable enhancements over Codex-backboned baselines in few-shot accuracy by 6.34%, 9.56%, and 5.46% on the GSM8K, AQuA, and StrategyQA benchmarks, respectively. Further experimentation with Llama-2 on arithmetic reasoning underscored the effectiveness of our approach, demonstrating superior performance against baseline methods within equivalent computational constraints. Detailed analysis further reveals that our self-evaluation guidance adeptly identifies logical inaccuracies, generating improved consistency and robustness in multi-step reasoning outcomes. This mechanism is particularly effective in ensuring that the conclusions drawn from text-based argumentative reasoning tasks are informative and accurately reflect the intended argumentative structure. We have made our code available to the public for further research and application development.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Milad_Alshomary1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=S3Y0VvegGm",
  "title": "The Benefits of Being Distributional: Small-Loss Bounds for Reinforcement Learning",
  "modified_abstract": "Inspired by the exploration of offline reinforcement learning (RL) and their challenges presented in related work, this paper explores the intrinsic advantages of distributional reinforcement learning (DistRL) over traditional, non-distributional RL frameworks. Distributional reinforcement learning (DistRL) has been empirically effective, yet the specific conditions under which it surpasses conventional RL methodologies have not been fully elucidated. This research sheds light on this gap by articulating the benefits of DistRL through instance-dependent small-loss bounds that scale with the optimal achievable cost, demonstrating that our bounds converge significantly faster in scenarios characterized by minimal optimal costs. As a preliminary step, we introduce a distributional contextual bandit (DistCB) algorithm, illustrating its superior performance in terms of small-loss regret bounds over leading algorithms through empirical validation on three real-world training environments. In the domain of online RL, we present a novel DistRL algorithm that utilizes maximum likelihood estimation for the training and construction of confidence sets, establishing unprecedented small-loss Probably Approximately Correct (PAC) bounds within low-rank Markov Decision Processes (MDPs). This investigation contributes the concept of the $\\ell_1$ distributional eluder dimension, a potentially standalone interest. Moreover, our discourse extends into the realm of offline RL, where we delineate how pessimistic DistRL formulations can secure small-loss PAC bounds unique to the offline context, offering enhanced resilience against the adverse impacts of inadequate single-policy coverage. The application of these methodologies can critically advance the field and individualize the path to employ DistRL in various policies and training contexts.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Marius-Constantin_Dinu_Dinu1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=pQvAL40Cdj",
  "title": "Detecting Any Human-Object Interaction Relationship: Universal HOI Detector with Spatial Prompt Learning on Foundation Models",
  "modified_abstract": "Leveraging insights from pioneering works in unsupervised learning, such as the novel approach of feature learning by solving jigsaw puzzles, this study embarks on an ambitious journey to understand and detect human-object interaction (HOI) relationships in a comprehensive and open-world setting. The complexity of human-object interactions presents a formidable challenge, necessitating advanced comprehension capabilities beyond the static relationships captured by current systems. We introduce UniHOI, a methodology that combines the prowess of Vision-Language (VL) foundation models, including neural network-driven large language models (LLMs), to recognize complex interaction patterns between humans and objects across diverse and unseen environments. Our approach incorporates a unique HO prompt-based learning strategy aimed at extracting high-level relation features from VL foundation models, enhanced by a dedicated HO Prompt-guided Decoder (HOPD) to facilitate the mapping of these abstract relations to specific human-object pairs in images. Furthermore, the integration of LLMs into our architecture, such as GPT, empowers our system to interpret interactions with a nuanced linguistic understanding previously unattainable. UniHOI is not only adept at handling predefined interaction categories but also excels in training, classification, and open-category recognition of interactions, whether given as an interaction phrase or an interpretive sentence, significantly outperforming existing methodologies under both supervised training and zero-shot conditions. Our innovative use of spatial prompt learning on foundation models reveals a promising frontier for comprehensively understanding the myriad ways humans interact with the objects around them, marking a significant step forward in the field of computer vision.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Mehdi_Noroozi1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=CswEebv5Hn",
  "title": "Imitation Learning from Vague Feedback",
  "modified_abstract": "Inspired by the novel concept of learning from sub-optimal agents in \"Learning from a Learner\", our study introduces the problem of imitation learning with vague feedback, addressing the challenge when precise pairwise comparisons between demonstrations are not feasible. This scenario is prevalent when only broad assessments of performance quality are available, for instance, in situations where feedback is derived from non-expert human observers or from demonstrations with similar perceived qualities. Our work proposes a novel framework to navigate this issue by utilizing feedback that only identifies significant differences in the quality of demonstrations, typically when one is from an expert and the other from a non-expert. By leveraging a mixed distribution model wherein the demonstration pool is considered as a combination of expert and non-expert data, we establish a methodology to recover the expert policy distribution given a known proportion of expert data, $\\alpha$. Furthermore, for instances where $\\alpha$ is unknown, we introduce a mixture proportion estimation technique and incorporate learner-focused approaches. Coupling the recovered expert policy distribution with generative adversarial imitation learning algorithms, we formulate an end-to-end algorithm that demonstrably enhances performance across a variety of tasks when compared to both standard and preference-based imitation learning methods. This investigation not only expands upon existing imitation learning paradigms but also provides a robust solution for learning from vague and qualitative feedback, by incorporating state-action trajectories and reinforcement learning principles, thereby broadening the scope of human-in-the-loop systems in machine learning.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Alexis_Jacq1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=q6X038vKgU",
  "title": "Predict, Refine, Synthesize: Self-Guiding Diffusion Models for Probabilistic Time Series Forecasting",
  "modified_abstract": "Diffusion models have achieved state-of-the-art performance in generative modeling tasks across various domains, notably inspired by the insights gained from the exploration of conditional models and causal discovery in time series data. Building on these precedents, this work explores the potential of task-agnostic, unconditional diffusion models for a broad spectrum of time series applications. We introduce TSDiff, an unconditionally-trained diffusion model designed for time series forecasting. Our approach integrates a novel self-guidance mechanism, enabling TSDiff to be conditioned for downstream tasks during inference without requiring auxiliary networks or changes to the original training regimen. We validate the efficacy of TSDiff across three distinct time series tasks: forecasting, refinement, and synthetic data generation. Initially, our findings illustrate that TSDiff offers competitive performance against numerous conditionally-trained forecasting methodologies and employs a unique causal methods approach towards causal discovery and mapping in time series data. Subsequently, we employ the model's implicit probability density for the iterative refinement of basic forecasts, achieving notable efficiency advancements over traditional reverse diffusion processes. Most importantly, the utility of TSDiff in generating synthetic datasets is established, showcasing its superiority over other generative time series models and, in some instances, even surpassing models trained on actual datasets. The specific generative capabilities of our model, as evidenced in downstream forecasting improvements, draw upon and extend the significant potential observed in prior works on generative modeling, causal inference, discovery, encounter, and graphical discovery methods within time-related data contexts. Samples of synthetic data underline the unique attributes of our model, highlighting its broad applicability.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sindy_L\u00f6we1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=gd20oaZqqF",
  "title": "Towards Optimal Caching and Model Selection for Large Model Inference",
  "modified_abstract": "Inspired by the ongoing research into machine learning algorithms and their efficiency, including studies on the consistency rate of decision tree learning, this paper addresses the challenges posed by the deployment of Large Language Models (LLMs) and other foundation models. These models, while achieving unprecedented accuracy in numerous tasks, introduce significant resource consumption and latency issues during inference due to their size. We examine two principal strategies to alleviate these challenges: the implementation of a caching mechanism for storing previous queries and the development of a model selector to navigate an ensemble of models for efficient query processing. Theoretically, we derive an optimal algorithm that jointly optimizes caching and model selection to minimize the inference cost in both offline and online settings, employing algorithms such as Greedy Dual Size with Frequency (GDSF) or Least Expected Cost (LEC) in conjunction with a model selector. Our empirical evaluations, augmented by simulations, demonstrate a substantial improvement in efficiency, achieving up to a 50\u00d7 improvement over baseline metrics under certain conditions, and real dataset experiments indicate a 4.3\u00d7 enhancement in FLOPs and a 1.8\u00d7 improvement in latency. This work not only contributes to the growing literature on efficient model inference but also sets a new standard for deploying large-scale models in resource-constrained environments.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Qin-Cheng_Zheng1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=DBlkX8Nczr",
  "title": "Brain-like Flexible Visual Inference by Harnessing Feedback Feedforward Alignment",
  "modified_abstract": "Inspired by recent advancements in learning visual representations in hyperbolic spaces, such as those introduced by Poincar\u00e9 ResNet, our study investigates the complex interplay between feedback and feedforward processes in natural vision systems. We introduce Feedback-Feedforward Alignment (FFA), a novel learning algorithm that harnesses these dynamics, recognizing the critical role of feedback connections in enhancing visual inference capabilities. FFA exploits alignment between feedforward and feedback pathways, optimizing them around their respective objectives and leveraging their mutual computational graphs for credit assignment. This co-optimization facilitates emergent visual functions in feedback pathways, such as denoising, resolving occlusions, and facilitating imagination, which are akin to the brain's natural visual processing capabilities. Further, we integrate convolutions in our optimization technique to ensure that our algorithm efficiently handles high-dimensional data, akin to convolutional neural networks' (CNNs') performance efficiency. Our experiments with the MNIST and CIFAR10 datasets, validated in batch training modes, underscore FFA's effectiveness in handling pixel-level classification and reconstruction tasks, simultaneously demonstrating bio-plausible learning mechanisms that could address traditional backpropagation challenges, including potential collapse issues. By drawing on computational norms and principles observed in hyperbolic representation learning and efficiently integrating understanding through convolutional mechanisms, this work sheds light on the potential mechanisms feedback pathways employ in the brain for flexible visual inference, contributing both to our understanding of perceptual phenomena and the development of algorithms that more closely mimic biological processes.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Max_van_Spengler1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=HUuEMMM8Ik",
  "title": "Detecting hidden confounding in observational data using multiple environments",
  "modified_abstract": "Amidst the burgeoning field of causal inference, which sits at the confluence of machine learning and statistical methods, our research addresses a critical, yet often overlooked challenge: detecting hidden confounding in observational data. Inspired by the growing understanding that causal mechanisms can facilitate AI toward achieving human-level intelligence, as highlighted in prior works on causal learning evaluations, we explore detecting unobserved confounders using datasets from multiple environments. We leverage the principle of independent causal mechanisms underlying the data-generating process to develop a theoretical framework for identifying testable conditional independencies, which signal the presence of hidden confounders. Our methodology includes a thorough examination of the limitations and assumptions inherent in these analyses, such as degenerate and dependent mechanisms, and faithfulness violations. Moreover, the cross-pollination of concepts from causality-aware research significantly enriches our discussion on benchmarks for evaluating these inherent limitations and assumptions. Additionally, we propose a novel procedure for testing these independencies and assess its performance through simulation studies and semi-synthetic datasets derived from real-world data, providing benchmarks and evaluation metrics critical for the advancement of causal inference methodologies. The empirical analyses predominantly affirm the procedure's effectiveness in identifying hidden confounding, especially under significant confounding bias. This work not only extends the utility of causal inference techniques in observational studies but also contributes to the broader discourse on benchmarking and evaluating causal learning algorithms.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Raha_Moraffah1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=kdFR6IUEW6",
  "title": "Prompt Pre-Training with Twenty-Thousand Classes for Open-Vocabulary Visual Recognition",
  "modified_abstract": "Inheriting the spirit of pioneering works like CHiLS which explored zero-shot image classification with hierarchical label sets, this work introduces POMP, a prompt pre-training method tailored for vision-language models. POMP is designed to be memory and computation efficient, enabling the condensed representation of semantic information across an expansive set of visual concepts, featuring over twenty-thousand classes. The core innovation lies in the prompt's strong transferable ability, which allows it to be seamlessly integrated into various visual recognition tasks, such as image classification, semantic segmentation, and object detection, thereby significantly enhancing recognition performance in a zero-shot manner. Our empirical evaluation substantiates POMP's superior capabilities, demonstrating state-of-the-art performances on a total of 21 datasets. Notable achievements include an average accuracy of 67.0% across 10 classification datasets, marking a +3.1% improvement over the existing method CoOp, and 84.4 hIoU on open-vocabulary Pascal VOC segmentation, which is a +6.9 improvement compared to ZSSeg.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zachary_Novack1",
  "manipulated_ranking": 4,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=lSbbC2VyCu",
  "title": "Rewarded soups: towards Pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards",
  "modified_abstract": "Inspired by advancements in hyper-parameter optimization (HPO) techniques for Deep Convolutional Neural Networks (CNNs) and informed by the intricacies of defining analytical response surfaces to enhance model training efficiency, this paper ventures into the realm of foundation models. Foundation models are initially pre-trained on vast unsupervised datasets and subsequently fine-tuned on labeled data with a focus on testing generalized solutions. The incorporation of reinforcement learning from human feedback (RLHF) represents a novel effort to align the network more closely with its intended usage through rigorous training and testing procedures. However, the reliance on imperfect proxy rewards can detract from the optimization process, yielding suboptimal outcomes. These challenges are magnified by the diversity of real-world tasks and the breadth of human preferences. Our research proposes an innovative approach to navigate these complexities: the rewarded soup technique, a concept partly inspired by the idea of autohyper to automatically select the best hyper-parameters for each task. This methodology leverages an extensive multi-policy strategy, aiming for Pareto-optimal generalization across a broad spectrum of preferences by specializing multiple networks independently\u2014each aligned with a unique proxy reward\u2014and then interpolating their weights linearly, we embrace the extensive heterogeneity of diverse rewards. Our empirical findings illuminate the linear connectivity of weights when networks are fine-tuned on varying rewards, stemming from a uniform pre-trained initialization, a critical step towards efficient training. The selection process for the interpolation of weights represents a significant optimization that showcases an extensive analysis of how varying degrees of rewards impact learning outcomes. We demonstrate the efficacy of our approach across a variety of domains, including text-to-text tasks (e.g., summarization, question answering, helpful assistant dialogues, reviews), text-image challenges (e.g., image captioning, text-to-image generation, visual grounding), and control (e.g., locomotion) tasks. This research not only aims to refine the alignment of deep learning models but also to enrich their interaction with the world's diversity.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Mathieu_Tuli1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=DP2lioYIYl",
  "title": "A Theory of Unsupervised Translation Motivated by Understanding Animal Communication",
  "modified_abstract": "Inspired by the success of transformer networks in achieving systematic generalization and nuanced context sensitivity in natural language processing (NLP) and machine vision tasks, our study extends into the realm of Unsupervised Machine Translation (UMT) with a novel ambition: to pave the way for understanding animal communication through machine learning. Neural networks have demonstrated the ability to translate between human languages even in scenarios lacking parallel corpora, spotlighting the potential for cross-species communication comprehension. This paper proposes a theoretical framework for UMT in the absence of parallel translations and when there is a difference in subject domains or linguistic structures between source and target languages. We introduce stylized models of language to test our framework, providing bounds on the sample complexity required for accurate translation and incorporating training procedures that consider the encoding of biological communication systems, along with operations that account for the variances in communication modalities. These bounds, rigorously derived and causally linked to translation error rates, linguistic complexity, and the degree of commonality between communication systems, illuminate the causal relationship between these factors. Our findings suggest that translating animal communication could be within reach if their modes of communication exhibit sufficient complexity and shared elements with human language, thereby extending the application of neural translation models beyond human linguistics into the understanding of animal dialogues.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~James_McClelland1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=VtkGvGcGe3",
  "title": "Evaluating Cognitive Maps and Planning in Large Language Models with CogEval",
  "modified_abstract": "Inspired by the UniMASK framework's innovative approach in applying the concept of masked token prediction to sequential decision-making tasks, our study extends the exploration of advanced cognitive capabilities in large language models (LLMs). Leveraging this foundational understanding, we introduce CogEval, a cognitive science-inspired evaluation protocol designed to systematically assess cognitive capacities within LLMs, focusing on their ability to form cognitive maps and engage in complex planning tasks. Through CogEval, we embark on a comprehensive analysis across eight prominent LLMs including OpenAI GPT-4, GPT-3.5-turbo-175B, davinci-003-175B, Google Bard, Cohere-xlarge-52.4B, Anthropic Claude-1-52B, LLaMA-13B, and Alpaca-7B. Our methodology incorporates task prompts derived from human experimental frameworks that ensure construct validity for planning evaluations and places a special emphasis on sequential processing and tasks predicting behavior. These are notably absent from the models' training corpora and challenge the models beyond their original conditioning, without relying on fine-tuning strategies. The investigation reveals that despite some LLMs displaying competency in executing simpler planning tasks, a more rigorous examination uncovers significant deficiencies, particularly in handling complex planning scenarios that necessitate understanding and navigating latent relational structures\u2014a key component of cognitive maps. These LLMs frequently encounter issues such as generating invalid action sequences and entrapment in recursive loops, shedding doubt on their purported emergent planning capabilities and their behavior in offline contexts. This paper discusses the implications of these findings for practical applications and underscores our contribution towards a better understanding of LLMs' abilities to form cognitive maps and engage in complex planning, including the necessity of training LLMs with more diverse and specialized tasks that require adaptive planning and conditioning.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Mingfei_Sun1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=SquMNyrk1O",
  "title": "Winner-Take-All Column Row Sampling for Memory Efficient Adaptation of Language Model",
  "modified_abstract": "In the context of rapidly expanding model sizes, efficiently fine-tuning large pre-trained language models has become a critical challenge, primarily due to significant memory demands. This issue is compounded by the necessity to store feature maps for gradient calculation, a fundamental aspect of training with stochastic gradient descent. Inspired by previous works that have aimed at reducing the trainable parameter count, our research extends these efforts by addressing the primary memory bottleneck during training\u2014namely, the storage of activations. We introduce a novel family of unbiased estimators, designed for matrix production with reduced variance, allowing for the storage of only the sub-sampled activations necessary for gradient computation. This approach draws from principles observed in the online learning of multiple low-variance tasks, leveraging the robustness of machine learning models to noisy gradients, provided the gradients remain unbiased with controlled variance. Our theoretical and empirical analyses demonstrate that, within the framework of tuning transformers, our estimators achieve significantly lower variance than existing methods. Implementing our low-variance approximation in transformers facilitates up to 2.7X peak memory reduction with negligible accuracy loss, enabling substantially larger batch sizes of up to $6.4\\times$. Consequently, our approach empowers the utilization of larger models and/or the elevation of training speeds under identical hardware configurations, thereby enhancing performance on growing downstream tasks that grow in complexity. The code is available at a repository (URL omitted for anonymity).",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Giulia_Denevi1",
  "manipulated_ranking": 7,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=sL4pJBXkxu",
  "title": "ELDEN: Exploration via Local Dependencies",
  "modified_abstract": "Reinforcement learning in environments with large state spaces and sparse rewards requires efficient exploration strategies. Inspired by recent advancements in model-based reinforcement learning, such as Contrastive Value Learning, which proposes implicit modeling of environment dynamics for insightful action-value estimations without direct environment interaction, our work introduces ELDEN, Exploration via Local DepENdencies. This novel approach leverages local dependencies in factored state spaces with complex chained dependencies to define interesting states for exploration. By focusing on the uncertainty of how entities within the environment influence one another, rather than just how entities change, ELDEN utilizes partial derivatives of learned dynamics to accurately and efficiently model these interactions. The uncertainty in these predicted dependencies serves as an intrinsic reward, fostering exploration towards discovering new interactions between entities. Our evaluation across diverse domains, from 2D grid worlds to 3D robotic tasks, demonstrates ELDEN's capability to identify local dependencies and learn successful policies, outperforming existing exploration methods in reinforcement learning tasks. ELDEN's benchmarks in both scenarios provide crucial learning insights, reaffirming its effectiveness in unseen environments.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jonathan_Tompson1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=2gn9WFlqJ4",
  "title": "Mode Connectivity in Auction Design",
  "modified_abstract": "Optimal auction design represents a cornerstone issue within algorithmic game theory, a field that grapples with complex decision-making scenarios, illustrating the intricate dynamics similar to those encountered in continual reinforcement learning settings, particularly in the context of understanding polynomial mixing times. This bridging concept illuminates the common challenge of navigating non-convex optimization landscapes, whether in adjusting policy performance in dynamic environments or in crafting efficient auction mechanisms. Building on these insights, our study pivots towards the application of neural networks in differentiable economics, epitomized by RochetNet and its successors\u2014tools that exemplify the utilization of machine learning to both replicate and innovate within the domain of auction design. We provide a foundational analysis of mode connectivity as it applies to this realm, demonstrating that locally optimal solutions discovered by these networks are interconnected via simple, piecewise linear pathways, wherein solutions along these paths remain competitively advantageous in performance. This exploration into the mode connectivity of neural networks used in economic mechanisms offers a pioneering theoretical framework that not only supports the empirical performance of such approaches but also marks the first foray into applying these concepts within the arena of differentiable economics, showcasing the critical tasks these networks perform and the importance of optimization and development over time. Notably, this study highlights the potential of pretrained neural networks to accelerate this optimization process.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Gopeshh_Raaj_Subbaraj1",
  "manipulated_ranking": 24,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=yEfmhgwslQ",
  "title": "Encoding Time-Series Explanations through Self-Supervised Model Behavior Consistency",
  "modified_abstract": "The increased reliance on machine learning models across various domains, coupled with the essential requirement for their interpretability, especially in settings demanding fast and critical decision-making, necessitates innovation in explanation methodologies. Our work, inspired by recent strategies in model-centric explanations offering actionable insights into model behavior, presents TimeX, a novel approach for interpreting time-series models. Unlike general explainability methods that struggle with the temporal dimensionality of time series, TimeX specifically addresses the challenge of interpreting time series data. It trains an interpretable surrogate, using self-supervised techniques, to mimic the behavior of a pretrained time series model, ensuring model behavior consistency: a formulation that preserves relations in the latent space induced by the pretrained model with those induced by TimeX. This approach not only addresses the gap in faithfulness of explanations but also contributes to the interpretability domain by learning a formalism of a latent space of explanations, facilitating the recognition of temporal patterns through discrete attribution maps and visual aggregation landmarks. Evaluated on eight synthetic and real-world datasets, TimeX demonstrates superior or competitive performance against existing state-of-the-art interpretability methods and its effectiveness is further substantiated through case studies on physiological time series, showcasing its potential in generating faithful, interpretable models that accurately capture the behavior of complex time series data and trained models in contexts requiring critical decision-making.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Nicholas_Gisolfi1",
  "manipulated_ranking": 17,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=c5WOU7p4ES",
  "title": "PLASTIC: Improving Input and Label Plasticity for Sample Efficient Reinforcement Learning",
  "modified_abstract": "In Reinforcement Learning (RL), enhancing sample efficiency is crucial, particularly in scenarios when data acquisition is costly and risky. Inspired by the theme of learnable intelligence in multi-agent learning, as seen in works such as 'LIGS: Learnable Intrinsic-Reward Generation Selection for Multi-Agent Learning', our study extends the understanding of adaptability and efficiency to the single-agent context. In principle, off-policy RL algorithms can improve sample efficiency by allowing multiple updates per environment interaction. However, these multiple updates often lead the model to overfit to earlier interactions, a problem we refer to as the loss of plasticity. This work investigates the underlying causes of this phenomenon by dividing plasticity into two aspects: input plasticity, which denotes the model's adaptability to changing input data, and label plasticity, which denotes the model's adaptability to evolving input-output relationships. Through synthetic experiments on the CIFAR-10 dataset, we discover that enhancing input plasticity is achieved by finding smoother minima of the loss landscape, whereas improved label plasticity results from refined gradient propagation. Leveraging these insights, we develop the **PLASTIC** algorithm, which harmoniously combines techniques to address both concerns. With minimal architectural modifications, PLASTIC achieves competitive performance on benchmarks including Atari-100k and Deepmind Control Suite. This result underscores the importance of maintaining the model's plasticity to improve sample efficiency in RL for tackling a variety of complex environments. The code link has been omitted for anonymity.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Taher_Jafferjee1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=oyV9FslE3j",
  "title": "Temperature Balancing, Layer-wise Weight Analysis, and Neural Network Training",
  "modified_abstract": "In the realm of machine learning, regularization plays a pivotal role in the design and fine-tuning of algorithms. Inspired by principles evident in self-learning and the adaptation of models to shifts in data distribution, this study introduces TempBalance, a novel approach to neural network training that employs a layer-wise learning rate optimization informed by Heavy-Tailed Self-Regularization (HT-SR) Theory. The HT-SR Theory offers insight into the implicit self-regularization properties of different neural network layers, providing a foundation for our temperature balancing technique. TempBalance aims to enhance training outcomes by adjusting the 'temperature' of learning rates across network layers, thereby optimizing the training process and facilitating adaptation to various classification tasks. Implemented across various datasets, including CIFAR10, CIFAR100, SVHN, and TinyImageNet, and tested with different architectures such as ResNets, VGGs, and WideResNets, TempBalance demonstrates substantial improvements over standard stochastic gradient descent (SGD), spectral norm regularization, and several cutting-edge optimizers and learning rate schedulers in training and classification performance. Our results underscore the effectiveness of adopting a temperature-based perspective on learning rate adjustments, affirming the potential of HT-SR Theory-guided methods to elevate neural network training and classification efficacy. This innovative adaptation strategy, despite not directly incorporating pseudo-labeling or explicit self-supervised mechanisms, suggests a promising direction for further enhancing neural network optimization techniques through self-adaptation mechanisms and classification efficacy improvements.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Evgenia_Rusak1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=8SUtvEZCF2",
  "title": "Semantic segmentation of sparse irregular point clouds for leaf/wood discrimination",
  "modified_abstract": "Informed by significant advancements in lidar-based 3D object detection and the exploration of novel sensor modalities, such as the use of temporal illumination cues captured by low-cost monocular gated imagers and cameras, this study advances the field of remote sensing for biosphere monitoring. Lidar (Light Detection and Ranging), a driving force behind innovative remote sensing techniques, has become a crucial component of the remote sensing toolbox, particularly for mapping forest leaf areas with high accuracy. This accuracy is vital for the precise estimation of gas exchanges between vegetation and the atmosphere, a process deeply influenced by the state-of-the-art in semantic segmentation technologies. The adoption of Unmanned Aerial Vehicles (UAV) equipped with miniature sensors facilitates regular monitoring of vegetation responses to climate change, despite the challenge presented by limited density point clouds and the spatially irregular sampling intensity inherent to these sensors. Addressing the significant challenge of discriminating leaf points from wood points in sparse, irregularly sampled point clouds, we introduce a neural network model inspired by the Pointnet++ architecture, leveraging only point geometry as a fundamental object detector. To navigate the challenge of local data sparsity, our work introduces an innovative sampling scheme designed to retain critical geometric information within these sparse datasets. Additionally, we propose a loss function specifically tailored to address severe class imbalance. Our model, embodying the novel architectural advances and leveraging 3D data, demonstrates superior performance compared to existing methods on UAV-sourced point clouds, setting a precedent for future enhancements, particularly in handling denser point clouds acquired from beneath the canopy. By optimizing our model for efficient detector performance and data processing, this research signifies an important step forward in the accurate release of critical environmental data.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Fahim_Mannan1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=XXPzBhOs4f",
  "title": "Have it your way: Individualized Privacy Assignment for DP-SGD",
  "modified_abstract": "The intersection of privacy considerations and machine learning model training, particularly in the realm of deep learning, has been at the forefront of recent research, notably through approaches such as the utilization of public data to enhance privacy-utility trade-offs in differentially private (DP) model training. Building on this foundation, this paper introduces a novel perspective by proposing the concept of individualized privacy budgets in the context of DP-SGD (Differentially Private Stochastic Gradient Descent), the canonical method for training models under differential privacy constraints. While traditional DP-SGD applies a uniform privacy budget across all data points, we recognize the limitations of this approach given that users may have varying privacy expectations and the loss of privacy may not be uniform across the board. To address this, we develop a variant of DP-SGD, termed Individualized DP-SGD (IDP-SGD), which accommodates users' distinct privacy preferences by modifying data sampling and gradient noising mechanisms to support individualized privacy budgets. Our work presents an empirical analysis and benchmarks demonstrating that IDP-SGD not only respects the unique privacy requirements of each user but also achieves improved privacy-utility trade-offs compared to conventional DP-SGD methods. This model's adaptability to varying privacy expectations and its prior success in the field marks a significant step towards more tailored and efficient privacy-preserving techniques in the deployment of machine learning solutions. The introduction of convex optimization techniques within the parameter adjustment process of IDP-SGD could further optimize its efficiency, representing an avenue for future research.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Swaroop_Ramaswamy1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=TiFMYdQiqp",
  "title": "Bayesian target optimisation for high-precision holographic optogenetics",
  "modified_abstract": "Leveraging insights from non-linear auto-regressive models for characterizing cross-frequency coupling and correlations in neural time series, this study introduces a novel computational framework aimed at enhancing optogenetic stimulation's precision in brain circuits. Two-photon optogenetics has transformed our ability to probe the structure and function of brain circuitry. However, achieving precise optogenetic control of neural ensemble activity has remained fundamentally constrained by the problem of off-target stimulation (OTS): the inadvertent activation of nearby non-target neurons due to imperfect confinement of light onto target neurons. Here we propose a computational approach to this problem called Bayesian target optimisation. Our approach uses nonparametric Bayesian inference to model neural responses to optogenetic stimulation, and then optimises the laser powers and optical target locations needed to achieve a desired activity pattern with minimal OTS. We validate our approach in simulations and using data from in vitro experiments, showing that Bayesian target optimisation considerably reduces OTS across all conditions we test. Together, these results establish our ability to overcome OTS, enabling optogenetic stimulation with substantially improved precision, even in slow wave neural activity patterns. This advancement has the potential to refine modeling and understanding of complex neural dynamics.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Tom_Dupre_la_Tour1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=OwpaO4w6K7",
  "title": "Jigsaw: Learning to Assemble Multiple Fractured Objects",
  "modified_abstract": "Innovations in machine learning for handling complex visual tasks, as illustrated by developing methods for camouflaged object detection that leverage minimal annotations, lays a foundation for addressing other intricate challenges in computer vision. This paper introduces Jigsaw, a novel framework designed for the automated assembly of physically broken 3D objects from multiple pieces\u2014a task with critical applications in orthopedics, archaeology, and everyday life. By adapting to the intricacies of 3D geometries, our method employs a hierarchical feature analysis that encompasses both global and local geometric characteristics to match and align the fracture surfaces accurately. Jigsaw's architecture integrates a front-end point feature extractor with attention layers, surface segmentation for identifying the fracture and unbroken parts, a multi-part matching algorithm for aligning corresponding fracture surface points, and a robust alignment component for reconstructing the global poses of the pieces. This seamless integration of segmentation, matching, and alignment under a weakly-supervised learning framework marks a significant leap forward in computational geometry and object reconstruction, bolstering the detection paradigm by predicting the alignment with minimal annotations. Our evaluations on the Breaking Bad dataset demonstrate Jigsaw's superiority over existing assembly methods and its impressive adaptability to a wide array of fracture patterns, object types, and image-based novel instances. This pioneering learning-based approach to 3D fracture assembly sets a new standard for complex object reconstruction applications. Code for Jigsaw is publicly available.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ruozhen_He1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=awIpKpwTwF",
  "title": "LEACE: Perfect linear concept erasure in closed form",
  "modified_abstract": "Inspired by the burgeoning field of self-supervised learning and the quest for interpretable and fair machine learning models, our work introduces LEAst-squares Concept Erasure (LEACE). This novel approach offers a closed-form solution to concept erasure, fundamentally aimed at eliminating specific features from a representation to enhance fairness, prevent reliance on sensitive attributes like gender or race, and improve interpretability by allowing changes in model behavior to be observed when a concept is removed. Leveraging insights from the latest research, including the mechanisms behind non-contrastive self-supervised learning, which highlights the capability of neural networks' neurons to learn competitive representations by avoiding trivial collapsed solutions, LEACE provably removes the ability of all linear classifiers to detect a specified concept while minimally altering the representation as per a broad class of norms. This matrix-focused technique is achieved without significant adverse effects on the gradient dynamics of the network, ensuring the robust discovery of features unrelated to the erased concept through a controlled descent approach. We further operationalize this through 'concept scrubbing,' a procedure that erases target concept information across every layer of large language models. Our application of LEACE on tasks such as measuring the dependence of language models on part-of-speech information and diminishing gender bias in BERT embeddings, underscores its effectiveness. Furthermore, we provide our code for public access, inviting further exploration and application. This work not only contributes a practical tool for enhancing model fairness and interpretability but also sets a precedent for future investigations into concept erasure within and beyond the realm of language models.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zixin_Wen1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=bpzwUfX1UP",
  "title": "Parallel Sampling of Diffusion Models",
  "modified_abstract": "Inspired by significant advances in training dynamic models on long trajectories, such as those presented in latent neural ordinary differential equations (ODEs) using principled technique and optimized parallelization, this paper introduces a novel approach to addressing the major bottleneck in diffusion models: the slow sampling speed. Diffusion models, while powerful for generative tasks, inherently require hundreds to thousands of sequential denoising steps to generate one sample, significantly limiting their practicality for real-time applications. Current methods to accelerate this process often compromise sample quality for speed. Our work, by contrast, explores a new dimension\u2014parallelizing denoising steps without sacrificing the integrity of the output along the trajectory. We present ParaDiGMS, a pioneering framework that enables the shooting-based approach and the denoising of multiple steps in parallel through the use of Picard iterations, an approach that iteratively refines guesses of future denoising steps until convergence. This method marks a departure from traditional acceleration techniques by proposing a tuning process tailored for speed trade-offs, not previously feasible for diffusion models. ParaDiGMS is demonstrated to be compatible with existing acceleration techniques like DDIM and DPMSolver, enhancing sampling speed by 2-4x across diverse applications including robotics and image generation without affecting the quality of results, evidenced by consistent recognition rates, task rewards, FID scores, or CLIP scores in short temporal windows. Our experiments showcase remarkable improvements in sampling speeds, achieving state-of-the-art performance indicators such as 0.2s for 100-step DiffusionPolicy and 14.6s for 1000-step StableDiffusion-v2 models, thus addressing the potential of dynamic systems in real-time scenarios through efficient trajectory management.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Cagatay_Yildiz1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=AOKU4nRw1W",
  "title": "Linguistic Binding in Diffusion Models: Enhancing Attribute Correspondence through Attention Map Alignment",
  "modified_abstract": "In an era where text-conditioned image generation models are gaining prominence for tasks ranging from interactive image retrieval using natural language queries to generating complex scenes, a significant challenge remains in ensuring accurate associations between textual descriptions and visual outputs. Models often struggle with correctly associating entities and their visual attributes, as evidenced by the production of images with mismatched elements, such as a yellow sunflower and a pink flamingo when prompted for the opposite. Building upon the exploration of image retrieval through iterative language queries, which highlighted the importance of processing complex scenes and entities in natural language processing (NLP) and computer vision, we introduce SynGen. This novel approach first performs syntactic analysis of the prompt to identify entities and their modifiers and then employs a unique loss function designed to align cross-attention maps with the linguistic structure of the input text. By optimizing for greater overlap between encoding the attention maps of associated entities and their modifiers and minimizing overlap for unrelated elements, SynGen embeds a more accurate visual representation without the need for model retraining. Evaluated across three datasets, including a specially curated complex set serving as benchmarks, SynGen demonstrates marked improvements over current state-of-the-art methods, substantiating the efficacy of leveraging linguistic structures for improved fidelity in text-to-image generation tasks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Paola_Cascante-Bonilla1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=LlERoXEKjh",
  "title": "Training shallow ReLU networks on noisy data using hinge loss: when do we overfit and is it benign?",
  "modified_abstract": "Reflecting on recent insights into the adaptability of neural networks in filtering noise from data, our study investigates benign overfitting in two-layer ReLU networks trained using gradient descent and hinge loss on noisy data for binary classification. This examination is prompted by the understanding that neural networks, particularly through mechanisms like local signal adaptivity demonstrated in the image classification context, can outperform traditional kernel methods and neural tangent kernels by selectively focusing on relevant signals amidst noise. Specifically, we analyze the behavior of shallow ReLU networks on linearly separable data contaminated by a minor proportion of label corruption or flips. We delineate conditions on the margin of the clean data culminating in three potential training outcomes: benign overfitting that yields correct classification of test data despite zero loss, detrimental overfitting characterized by correct classification on training but misclassification on test data, and a non-overfitting scenario where the model successfully discriminates clean from corrupt data points leading to accurate test data classification. Our theoretical contribution includes a granular analysis of neuron dynamics during training, revealing an initial phase where clean data points achieve nearly zero loss followed by a second phase where these points fluctuate at the zero loss boundary as corrupt points converge to zero loss or are nullified by the model, utilizing adaptivity in simple yet effective ways. This dual-phase training behavior is explicated through a novel combinatorial methodology, contrasting the number of updates attributed to clean versus corrupt data, thereby extending the discourse on neural networks' inherent capability to discern and prioritize signal over noise, even in sparse datasets. Empirically, the effectiveness of this approach is showcased through selected tasks in image classification.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Stefani_Karp1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=iPTF2hON1C",
  "title": "Learning To Dive In Branch And Bound",
  "modified_abstract": "The development of primal heuristics is key in addressing mixed integer linear programs, primarily due to their utility in identifying feasible solutions that facilitate the branch and bound search process. Among these heuristics, diving heuristics stand out by employing an iterative approach that modifies and resolves linear programs to execute a depth-first search starting from any node within the search tree. Current diving heuristics use generic decision rules, which do not tap into the structural similarities observed across problem instances frequently encountered in practical scenarios. In light of the significant role that reproducible and efficient benchmarking and sharing practices play in the advancement of machine learning methods for learning, as exemplified by frameworks like Benchopt that strive for collaborative, transparent, and accessible optimization benchmarks across various domains, we introduce L2Dive. This novel approach harnesses graph neural networks to tailor diving heuristics to the specific characteristics of given problem instances, thereby enhancing the usability of these heuristics in practical applications. L2Dive utilizes generative models to forecast variable assignments and applies the principles of linear program duality to guide diving decisions based on these forecasts. Fully integrated with the open-source solver SCIP, L2Dive demonstrates superior performance over traditional divers by identifying more advantageous feasible solutions across diverse combinatorial optimization challenges. In practical applications, ranging from server load balancing to neural network verification, L2Dive has been shown to improve the primal-dual integral by as much as 7% (35%) on average versus a tuned (default) solver baseline, concurrently reducing the average solution time by 20% (29%) through learning-based techniques. Moreover, L2Dive's emphasis on collaborative learning and benchmarking encourages a more inclusive and shared approach in the machine learning and combinatorial optimization communities.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Pierre-Antoine_Bannier1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=CzkOzKWpMa",
  "title": "Optimal cross-learning for contextual bandits with unknown context distributions",
  "modified_abstract": "This work extends the exploration of the cross-learning setting initially introduced by Balseiro et al., focusing on contextual bandit problems with adversarially chosen losses and contexts drawn i.i.d. from an unknown distribution. Our research is motivated by foundational advancements in online learning for autoregressive dynamics, where the importance of accounting for temporal dependencies in sequential decision-making has been effectively demonstrated. By resolving an open problem posed by Balseiro et al., we contribute an efficient algorithm that boasts a nearly tight (up to logarithmic factors) regret bound of $\\widetilde{O}(\\sqrt{TK})$, irrespective of the number of contexts. This achievement not only presents the first nearly tight regret bounds for learning to bid in first-price auctions under unknown value distribution and addressing sleeping bandits with stochastic action sets but also heralds a novel technique for coordinating the execution of a learning algorithm across multiple epochs. This innovation adeptly minimizes correlations between the estimation of the unknown distribution and the algorithm's action choices, potentially serving as a valuable strategy for other learning challenges involving unknown context distributions.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Gianmarco_Genalti1",
  "manipulated_ranking": 5,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=cRzt1umRNx",
  "title": "Riemannian Residual Neural Networks",
  "modified_abstract": "Inspired by recent progress in geometric deep learning and the nuanced understanding of generalization errors in overparameterized regimes, such as those encountered in transfer learning between linear regression tasks, this work introduces a novel framework for extending the concept of residual neural networks (ResNets) to data residing on Riemannian manifolds. Leveraging insights from the exploration of overparameterization and transfer learning, we develop Riemannian ResNets to address the challenge of learning on manifold-valued data\u2014ranging from graphs with hierarchical structures to data from the natural sciences that naturally inhabit non-Euclidean spaces. Our approach generalizes the conventional Euclidean ResNets in a geometrically principled manner, enabling their application to a wider range of manifolds beyond the few for which extensions were previously possible. By integrating the ideas of gradient descent methods adapted for Riemannian manifolds and special attention to training dynamics, we demonstrate that, akin to their Euclidean counterparts, Riemannian ResNets overcome the vanishing gradient problem, contributing to improved learning properties and empirical outcomes. Our results also show that Riemannian ResNets outperform existing manifold neural networks designed for hyperbolic spaces and the manifold of symmetric positive definite matrices in relevant testing metrics and training dynamics, establishing new benchmarks for geometric deep learning applications.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yehuda_Dar1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=iImnbUVhok",
  "title": "Joint Prompt Optimization of Stacked LLMs using Variational Inference",
  "modified_abstract": "In the context of recent progress in language generation technologies, including efforts to detoxify and debias language models through inference-time adaptive optimization, our work introduces an innovative approach to harnessing the capabilities of Large Language Models (LLMs). Large language models can be conceptualized as fundamental computational units that map sequences of text to a distribution over potential continuations, suggesting their role as stochastic language layers within a broader language network. Central to our methodology is the concept of natural language prompts as learnable parameters within this network. By stacking two such LLMs and directing the output of one model into another via a strategic interface, we create what we term a Deep Language Network (DLN). Our initial focus unveils strategies for proficient prompt optimization within a single-layer language network (DLN-1). Building upon this foundation, we extend our investigation to a two-layer configuration (DLN-2), necessitating the learning of two distinct prompts via this method. This innovation treats the output from the first layer as a latent variable, thereby necessitating variational inference methods for prompt optimization. Preliminary experiments validate the efficacy of DLN-1 across a spectrum of reasoning and natural language understanding tasks in. Subsequent testing reveals that DLN-2 surpasses the performance achievable by its single-layer counterpart, hinting at the potential of approaching or even matching the performance levels of more sophisticated models such as GPT-4, despite the constituent LLMs being individually smaller and less computationally intensive.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Xiaoyuan_Yi1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=YFW6MVGVTn",
  "title": "NICE: NoIse-modulated Consistency rEgularization for Data-Efficient GANs",
  "modified_abstract": "Generative Adversarial Networks (GANs) are a cornerstone of modern machine learning for image synthesis, leveraging large datasets to generate photorealistic images. Prevailing research, such as techniques developed to mitigate class-specific mode collapse in long-tailed distributions through spectral regularization, highlights both the potential and challenges of GANs across varied data landscapes, including those with imbalanced or skewed distributions. Inspired by these insights, our paper introduces a novel technique, NoIse-modulated Consistency rEgularization (NICE), specifically designed to address the data efficiency problem in GAN training for deep generation tasks. By integrating adaptive multiplicative noise into the discriminator, NICE modulates latent features to prevent overfitting, a key challenge when data is scarce in large-scale learning environments. This approach is particularly effective in imbalanced datasets, where class representation varies significantly. Although this modulation inadvertently augments the gradient norm, possibly destabilizing training, we establish a counterbalance by enforcing a discriminator consistency constraint under various noise conditions. This innovative approach not only penalizes excessive gradients but also secures the training process, yielding a substantial reduction in generalization error across generation tasks, including conditional generation tasks. Our theoretical discourse is enriched with experimental validations, showcasing NICE's superior performance in scenarios with limited data availability, including CIFAR-10, CIFAR-100, ImageNet, and FFHQ datasets, as well as in low-shot generation tasks. The efficacy of NICE in ameliorating discriminator overfitting and fortifying GAN stability, set against the backdrop of prior work on spectral regularization for long-tailed distributions, signals a promising direction for future research in data-efficient image recognition and generation.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Harsh_Rangwani1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=MWxsYPVmLS",
  "title": "Explainable and Efficient Randomized Voting Rules",
  "modified_abstract": "As AI tools increasingly contribute to decision-making in critical areas, the imperative for these tools to be both explainable and efficient has intensified. This need aligns with recent endeavors in the field, such as the pursuit of two-sided fairness in rankings, reflecting a broad interest in algorithms that are both fair and understandable to stakeholders. Our research contributes to this dialogue by addressing the balance between explainability and efficiency in the context of voting systems\u2014an area where explainability is paramount. We explore the potential of simple randomized voting rules to enhance decision-making efficiency without compromising their inherent explainability. Specifically, our investigation centers on randomized positional scoring rules and random committee member rules. Through both theoretical analysis and empirical studies, we demonstrate that these voting rule families successfully combine explainability with improved efficiency, as measured within the distortion framework, thus optimizing rankings in a fair manner. This equilibrium enables stakeholders to grasp how decisions are made, preserving the democratic advantage of voting systems over less transparent AI methodologies, while also leveraging the efficiency benefits introduced by controlled randomization. By examining the dominance of specific rules within these families, we further clarify how these methodologies outperform traditional systems in terms of fairness and rankings optimization, presenting a comprehensive view on optimizing electoral processes.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sam_Corbett-Davies1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=GxL6PrmEUw",
  "title": "Distributional Learning of Variational AutoEncoder: Application to Synthetic Data Generation",
  "modified_abstract": "Recent advances within the field of variational autoencoders (VAEs), such as the Counterfactual VAE that addresses the estimation of treatment effects under unobserved confounding through causal representation learning, highlight the growing interest in expanding the versatility and applicability of VAE models. Inspired by these developments, our study introduces a novel approach aimed at overcoming the limitations associated with the Gaussianity assumption in VAEs, namely, the assumption's restrictive nature in terms of model expressiveness for continuous variables. By integrating an infinite mixture of asymmetric Laplace distribution into the decoder of our VAE model, we enhance the model's capacity for distribution fitting without compromising its computational efficiency. This paradigm allows for the representation of a broader range of distributions, positioning our model as a special form of a nonparametric M-estimator, tailored to estimating general quantile functions under mild assumptions. Theoretical discussions in our manuscript establish a connection between our proposed model, causal inference, and quantile estimation, affording new insights into the distributional learning capabilities of VAEs. Identifiability of causal effects and model parameters is crucial in settings where the generation of synthetic data takes into account treatment and outcome variables, showcasing our model's potential in generating privacy-sensitive data, outperforming existing models in its ability to adjust data privacy levels. This work not only extends the functionality of VAEs in synthetic data generation but also contributes to a refined understanding of the intersection between distributional learning, privacy considerations, and the autoencoder architecture, especially in various settings where identifiability ensures the reliability of synthetic data for downstream applications.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Pengzhou_Abel_Wu1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=5UXXhVI08r",
  "title": "Dynamic Prompt Learning: Addressing Cross-Attention Leakage for Text-Based Image Editing",
  "modified_abstract": "Inspired by foundational advancements in generative AI, particularly the exploration of Foundation Models (FMs) for expert tasks involving high fidelity data synthesis and advanced language-image tasks, this paper introduces Dynamic Prompt Learning (DPL), a novel approach to mitigate cross-attention leakage in text-to-image generative models. Large-scale text-to-image generative models, as exemplified by diffusion models, have marked a significant leap in AI's ability to generate convincing images from textual prompts. Despite impressive capabilities, these models often struggle with precise image editing tasks, particularly when edits are intended for specific regions of an image. This challenge is predominantly due to inaccurate cross-attention maps, which inadvertently affect regions outside the intended target area. Our approach, DPL, enhances focus on accurate noun words within prompts through dynamic tokens, leakage repairment losses, and innovative learning mechanisms, thus achieving more refined control over image edits while preserving the integrity of untargeted regions. Evaluated on a diverse set of images using the Stable Diffusion model, DPL demonstrates marked improvements in both quantitative (e.g., CLIP score, Structure-Dist) and qualitative assessments, significantly advancing the capability for fine-grained, text-driven image modification in complex multi-object scenes. The methodology and findings of our study, bolstered by prudent pre-training and meticulous annotations in the training datasets, contribute to the broader conversation on enhancing the precision and applicability of generative AI for text-based image editing tasks. The importance of detailed documentation is implicitly acknowledged through our careful curation and documentation of the datasets, aiding in huge improvements in learning dynamics.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Roei_Herzig2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=MCkUS1P3Sh",
  "title": "Nash Regret Guarantees for Linear Bandits",
  "modified_abstract": "Our work builds upon and extends the study of regret minimization in bandit problems, particularly addressing the limitations identified in multi-agent settings with information sharing among agents. Recognizing a fundamental drawback in extending Upper Confidence Bound (UCB) algorithms to multi-agent bandits where shared information about optimal choices degrades performance, we introduce a novel notion of regret---Nash Regret. This strengthened notion of regret, defined as the difference between the optimum and the geometric mean of expected rewards accumulated by the linear bandit algorithms, leverages the Nash social welfare (NSW) function to measure collective welfare across rounds. By focusing on the stochastic linear bandits problem across a horizon of $\\mathsf{T}$ rounds and with a set of arms ${\\cal X}$ in ambient dimension $d$, and considering rewards as non-negative, sub-Poisson random variables, our approach not only addresses the problem's shortcomings but also offers principled fairness guarantees rooted in NSW's adherence to fairness axioms. We achieve a Nash regret of $O\\left( \\sqrt{\\frac{d}{\\mathsf{T}}} \\log(\\mathsf{T} |{\\cal X}|)\\right)$ for finite arm sets and an upper bound of $O\\left( \\frac{d^\\frac{5}{4}}{\\sqrt{\\mathsf{T}}}  \\log(\\mathsf{T})\\right)$ for non-finite sets, applicable to bounded, non-negative rewards. Utilizing the successive elimination method enhanced with tailored concentration bounds and sampling via John ellipsoid in conjunction with the Kiefer\u2013Wolfowitz optimal design, our algorithm effectively balances the explore-exploit dilemma through strategic sampling and offers a refined solution accommodating the unique challenges presented in the linear bandit scenarios.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Udari_Madhushani1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=No52399wXA",
  "title": "IPMix: Label-Preserving Data Augmentation Method for Training Robust Classifiers",
  "modified_abstract": "In the quest for enhancing convolutional neural network classifiers' accuracy and resilience against distributional shifts, prior research such as 'Debiased Pseudo Labeling in Self-Training' underscores the intricate balance between leveraging large-scale labeled datasets and addressing the challenges of label scarcity, bias, and training stability in semi-supervised learning environments. Building on these insights, our work introduces IPMix, a novel data augmentation approach designed to not only preserve label integrity but also significantly enhance classifier robustness across varying data distributions without sacrificing performance on clean datasets. By intricately combining image-level, patch-level, and pixel-level augmentation into a unified, label-preserving framework, IPMix systematically increases training data diversity with minimal computational expense. Moreover, through incorporating structural complexity and adopting a random mixing methodology for multi-scale information synthesis, IPMix substantially elevates robustness against common corruptions, adversarial attacks, and other perturbations. Rigorous experimental evaluation across benchmarks such as CIFAR-C and ImageNet-C validates IPMix's superiority in fostering corruption robustness, while subsequent tests reveal its commendable performance in improving adversarial robustness, calibration, prediction consistency, and anomaly detection, demonstrating state-of-the-art or comparable outcomes on ImageNet-R, ImageNet-A, and ImageNet-O. This semi-supervised learning technique benefits from leveraging both labeled and unlabeled data, alleviating the generation problem of requiring extensive annotated datasets and addressing a significant problem in current classification tasks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ximei_Wang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=zn5ihqknGj",
  "title": "An Alternating Optimization Method for Bilevel Problems under the Polyak-\u0141ojasiewicz Condition",
  "modified_abstract": "Bilevel optimization, a critical tool in machine learning applications such as hyperparameter optimization, meta-learning, and reinforcement learning, has seen a resurgence of interest. This resurgence is partly inspired by recent advancements in online convex optimization, which highlight the complexities inherent in managing cumulative constraints\u2014a challenge bilevel optimization also grapples with, especially under nonconvex constraints. This paper builds on the framework established by preceding studies, including analysis of constraint handling in long-term optimization, to address a gap in bilevel optimization research. Specifically, we first introduce a stationary metric for bilevel optimization problems that generalizes the existing metric for a nonconvex lower-level objective that satisfies the Polyak-\u0141ojasiewicz (PL) condition. We then propose a Generalized ALternating mEthod for bilevel opTimization (GALET) tailored to Bilevel Optimization with convex PL Lower Level (BLO with convex PL LL) problems and establish that GALET achieves an $\\epsilon$-stationary point for the considered problem within $\\tilde{\\cal O}(\\epsilon^{-1})$ iterations. This result matches the iteration complexity of gradient descent (GD) for single-level smooth nonconvex problems, thereby extending the applicability of alternating gradient-based algorithms beyond strongly convex lower-level objectives. Moreover, our analysis incorporates cumulative squared error measures and addresses potential violations to enforce precision over iterations, which is crucial for convergence in bilevel optimization contexts.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jianjun_Yuan2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=8vuDHCxrmy",
  "title": "OpenMask3D: Open-Vocabulary 3D Instance Segmentation",
  "modified_abstract": "Inspired by recent advancements in few-shot semantic segmentation, particularly the development of adaptive prototypes for improved feature comparison and unbiased model performance across various scenarios, we introduce the task of open-vocabulary 3D instance segmentation. Our approach is motivated by the limitations of current methods in 3D instance segmentation, which typically recognize object categories from a pre-defined, closed set of classes annotated in training datasets. These methods fall short in real-world applications that require interaction with a wide array of objects identified through novel, open-vocabulary queries. Addressing this gap, we propose OpenMask3D, a zero-shot approach that leverages predicted class-agnostic 3D instance masks for segmentation. Our model, leveraging a neural network architecture, aggregates per-mask features through multi-view fusion of CLIP-based image embeddings, enabling effective segmentation of objects beyond the training set's scope, including in classification challenges. Experimental evaluations on ScanNet200 and Replica demonstrate that OpenMask3D surpasses existing open-vocabulary and 5-shot methodologies, particularly in handling long-tail distributions. Furthermore, through qualitative experiments, OpenMask3D showcases its ability to segment objects based on free-form queries that describe geometry, affordances, and materials, highlighting its potential to significantly advance 3D scene understanding. This model's innovation extends the conventional image processing techniques to a dynamic 3D space, allowing for a query-based, interactive model of object recognition.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Bin-Bin_Gao1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=p40XRfBX96",
  "title": "Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision",
  "modified_abstract": "Building on the foundation of leveraging pre-trained language models (PLMs) with novel tuning methods, including prompt-tuning and pretraining, as demonstrated in related works like prompt-tuning adaptations for natural language generation (NLG) tasks, this paper introduces a groundbreaking approach for the self-alignment of large language models (LLMs) from scratch with minimal human supervision. The existing AI assistant agents, such as ChatGPT, highlight the reliance on supervised fine-tuning (SFT) with human annotations and reinforcement learning from human feedback (RLHF) to align the output of LLMs with human intentions, ensuring they are helpful, ethical, and reliable. However, challenges arise from the high dependence on human supervision, including issues related to cost, diversity, self-consistency, and biases in the representations.\nTo surpass these limitations, we propose SELF-ALIGN, a novel method that synergizes principle-driven reasoning with the generative capabilities of LLMs to achieve AI self-alignment with minimal human supervision. This methodology entails generating synthetic prompts enhanced by topic-guided methods for augmented diversity and addressing unfamiliar contexts, employing a concise set of human-written principles guided by in-context learning for ethically aligned response generation, and fine-tuning the original LLM with these high-quality outputs for direct desirable response generation. Further refinements address brevity and indirectness in responses.\nImplementing SELF-ALIGN on the LLaMA-65b model, we create the Dromedary AI assistant. This approach, requiring fewer than 300 lines of human annotations\u2014comprising less than 200 seed prompts, 16 principles, and 5 in-context learning exemplars\u2014demonstrably exceeds several state-of-the-art AI systems' performance on benchmark datasets under varied conditions. The success of SELF-ALIGN also sheds light on new pathways for generation without extensive reliance on familiar datasets, pointing towards a future of more adaptable and contextually aware LLM-driven applications.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Shengnan_An1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=MWQjqtV1z4",
  "title": "Restless Bandits with Average Reward: Breaking the Uniform Global Attractor Assumption",
  "modified_abstract": "Motivated by the need for efficient computation of policies in problem settings where traditional assumptions such as the uniform global attractor property (UGAP) may be overly restrictive or hard to verify, our work builds on foundational models in decision-making under uncertainty. We specifically draw insights from the study of no-regret learning agents in games, and how these models facilitate steering towards optimal equilibria without relying on stringent conditions. Our research extends these principles to the domain of restless bandits, focusing on the infinite-horizon problem with the average reward criterion in both discrete-time and continuous-time settings. We introduce a novel, general, simulation-based framework named Follow-the-Virtual-Advice, which allows for the conversion of any single-armed policy into an effective strategy for the large $N$-armed problem by simulating the single-armed policy on each arm and guiding the real state towards this simulated state, akin to full-feedback, tree structure planning. Our approach achieves an $O(1/\\sqrt{N})$ optimality gap without the need for UGAP in discrete-time settings under a simpler synchronization assumption, and, more importantly, with no additional assumptions in continuous-time settings beyond the standard unichain condition. This advancement represents a significant leap forward, illustrating that it is possible to achieve asymptotic optimality in restless bandit problems by leveraging concepts from full-feedback games, extensive-form game theory for per-iteration improvement, and the study of bandit-feedback mechanisms, optimal equilibria, and steering mechanisms.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Brian_Hu_Zhang1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=o7W0Zet6p3",
  "title": "Recovering Unbalanced Communities in the Stochastic Block Model with Application to Clustering with a Faulty Oracle",
  "modified_abstract": "In light of the rich body of research emphasizing the criticality of accurately modeling and inferring social ties within dense networks\u2014given their implications across computational social science, viral marketing, and recommender systems\u2014this paper aims to expand our comprehension of the Stochastic Block Model (SBM) beyond the traditional scope of balanced communities. The SBM, a cornerstone for the analysis of graph clustering or community detection in networks, has predominantly been explored under the assumption of uniform community sizes. Our work deviates from this by focusing on SBM with inherently unbalanced communities, a scenario more reflective of real-world networks. We propose a novel SVD-based algorithm that not only simplifies the recovery of varying-sized communities within the SBM framework but also advances previous theories by Ailon, Chen, and Xu [ICML 2013; JMLR 2015], eliminating the prerequisites related to cluster size intervals and correlation to the number of clusters. Our findings not only offer theoretical enhancements but are also substantiated through experimental validations and statistical inference on networks. A significant implication of our algorithm, under the planted clique conjecture, is its near-optimal recovery capability for clusters, irrespective of the cluster sizes, so long as the probability parameters remain constant. Additionally, our research contributes an efficient clustering algorithm that operates under a faulty oracle model with sublinear query complexity, effectively identifying clusters beyond the $\\tilde{\\Omega}({\\sqrt{n}})$ threshold even amid a predominantly small-cluster environment. This capability distinctly surpasses previous formulations that falter with increased presence of smaller clusters, thereby marking a substantial advancement in clustering methodologies. Friends and connections within these networks further underline the social aspect of our inferring processes.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Nikolaj_Tatti1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=B7QRV4XXiK",
  "title": "An Alternative to Variance: Gini Deviation for Risk-averse Policy Gradient",
  "modified_abstract": "In the sphere of Reinforcement Learning (RL), managing risk through constraining the variance of policy returns is a widely accepted approach due to its straightforward definition and interpretability. Traditional variance-based risk management strategies typically focus on limiting overall return variance, while more recent methods advocate for the control of per-step reward variance as an intermediary solution. Despite their prevalence, these variance-centered approaches exhibit notable drawbacks, including a pronounced sensitivity to the numerical scaling of data and impediments to policy optimization. Motivated by these challenges and the evolving landscape of risk measurement in model-based RL, as evidenced by explorations into the diminishing returns of value expansion methods, this study introduces the Gini deviation as an alternative risk metric. The Gini deviation's distinct properties and its compatibility with the principles of risk-averse RL are thoroughly analyzed, culminating in the development of a new policy gradient algorithm designed to minimize this risk measure. Through empirical assessment in scenarios amenable to risk aversion, our findings demonstrate that the Gini deviation-based approach effectively overcomes the limitations inherent to variance-based models. The proposed algorithm not only fosters learning of policies with favorable risk-return profiles but also excels where conventional methods struggle, offering robust performance across both variance and Gini deviation metrics.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Daniel_Palenicek1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=zMeemcUeXL",
  "title": "FAMO: Fast Adaptive Multitask Optimization",
  "modified_abstract": "Inspired by significant strides in transfer learning and hyperparameter optimization, which highlight the complexity and potential of efficiently managing multiple tasks and configurations in machine learning (ML), this work introduces Fast Adaptive Multitask Optimization (FAMO). FAMO addresses the challenge of multitask learning (MTL), aiming to optimize performance across diverse tasks by improving their representations and leveraging meta-features that increase similarity among task-specific models. Traditional gradient descent (GD) methods applied to the average loss across tasks often result in under-optimization of some tasks due to their inherent imbalanced nature. Existing strategies for achieving a more equitable loss reduction by manipulating task gradients are computationally intensive, requiring $\\mathcal{O}(k)$ space and time for $k$ tasks. FAMO circumvents these limitations by proposing a dynamic weighting method that ensures balanced task loss reduction with significantly reduced computational and space complexity, using only $\\mathcal{O}(1)$ resources. Through rigorous experimentation against established baselines in both supervised and reinforcement learning scenarios, FAMO not only demonstrates its capability to match or exceed the performance of current gradient manipulation techniques but also underscores its superiority in efficiency by effectively acting as a surrogate model that minimizes the need for extensive hyperparameter tuning. This advancement presents a notable leap forward in scaling MTL, potentially catalyzing the development of generalist AI agents capable of learning a vast array of tasks more feasibly.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sebastian_Pineda_Arango1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=jX49iKr6vb",
  "title": "Beyond Deep Ensembles: A Large-Scale Evaluation of Bayesian Deep Learning under Distribution Shift",
  "modified_abstract": "This work is inspired by recent developments in robust learning, such as the exploration of probabilistic robustness in PAC learning, which highlights the potential and limitations of adapting machine learning models to handle uncertainty and perturbations effectively. Bayesian deep learning (BDL) represents a novel approach towards achieving resilient, well-calibrated predictions on data experiencing distribution shifts, an area where traditional deep learning methods, including classifier models, have struggled. Despite the growing interest in BDL, the landscape lacks a comprehensive, large-scale evaluation of state-of-the-art (SOTA) methods across diverse, realistic, and challenging benchmark tasks. Our research bridges this gap by systematically assessing modern BDL algorithms, foregrounding their probabilistic modeling capabilities on real-world datasets from the WILDS collection, which includes a variety of demanding classification and regression tasks designed to test generalization and calibration under distribution shifts. Our investigation spans a broad spectrum of large, convolutional, and transformer-based neural networks, classifying their adaptability and precision in terms of complexity and learning dynamics. Specifically, we explore a signed version of the expected calibration error to determine whether methods are over- or underconfident, thereby shedding light on their operational characteristics and robustness. An innovative aspect of our study is the systematic evaluation of BDL applied to fine-tuning large pre-trained classifier models, a context where initializing from scratch is not viable due to resource constraints and where numerous examples exhibit perturbations representative of real-world conditions. In contrast to previous findings, our research reveals that while ensembling single-mode posterior approximations consistently enhances model performance and calibration, it encounters limitations when fine-tuning sizable transformer-based language models. Here, variational inference approaches like last-layer Bayes By Backprop significantly outshine others in accuracy, whereas contemporary approximate inference techniques such as SWAG emerge as frontrunners in achieving optimal calibration.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~UNIQUE_SUBEDI1",
  "manipulated_ranking": 17,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=oqDSDKLd3S",
  "title": "Sample-Conditioned Hypothesis Stability Sharpens Information-Theoretic Generalization Bounds",
  "modified_abstract": "Building on the elucidation of convergence rates in the learning of complex models from noisy data, particularly in estimating linear operators in Hilbert spaces, our work introduces novel information-theoretic generalization guarantees. By developing the \"neighboring-hypothesis\" matrix and introducing a new family of stability notions, known as sample-conditioned hypothesis (SCH) stability, we address the critical need for sharper bounds that surpass those provided by existing information-theoretic frameworks. This is especially significant in the context of stochastic convex optimization (SCO) problems, where recent findings have identified gaps in current theoretical boundaries. Our methods, borrowing concepts from self-adjoint operators to ensure precision in our mathematical modeling, yield tightened bounds that hold promise for a range of learning scenarios, effectively advancing the capacity to mitigate overfitting through enhanced understanding of hypothesis stability and its implications for convergence and generalization in machine learning models.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Nicholas_H_Nelsen1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=pzc6LnUxYN",
  "title": "StateMask: Explaining Deep Reinforcement Learning through State Mask",
  "modified_abstract": "The pursuit of explainability in deep reinforcement learning (DRL) has been notably advanced by methodologies aimed at decoding the decision-making processes of DRL agents, as exemplified by explorations into factorized optical flows for mid-level representation in robotics. This lineage of research underscores the importance of transparency in complex models, particularly when applied to domains requiring high trust and safety. Building on this foundation, we introduce StateMask, a novel explanatory technique designed to identify and illustrate the states that are most critical to a DRL agent's achievement of its final reward, addressing the existing gap in explanation methods that focus predominantly on individual actions rather than the strategic steps leading to outcomes. StateMask employs a mask net to selectively impede the agent's ability to perform its usual actions, thus revealing the pivotal states by forcing the agent into random actions without diminishing overall performance. This approach not only enhances the explicability of DRL agents but also contributes to practical applications such as adversarial testing, policy refinement, and robotic control. Our evaluation of StateMask across diverse RL environments demonstrates its effectiveness in providing high-fidelity explanations and its utility beyond traditional explainers, offering a new lens through which the mechanics of DRL can be understood and optimized. Additionally, the factorization of perception and control processes inherent in the DRL agents' decision-making is made more transparent through the application of StateMask, effectively demonstrating how optical flow and its factorization contribute to understanding the agent's environment.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Li-Yuan_Tsao1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=x816mCbWpR",
  "title": "Recasting Continual Learning as Sequence Modeling",
  "modified_abstract": "Our work draws inspiration from a convergence of insights into the challenges of long-term credit assignment in reinforcement learning and the potential of advanced sequence modeling techniques. By examining the obstacles inherent in both reinforcement learning and recurrent neural networks, such as the need for discounting or gradient truncation that limits temporal reasoning, we propose a novel paradigm that formulates continual learning as a sequence modeling problem. This reimagining leverages the strength of sequence models, particularly Transformers and their efficient variants, to address the continuum of learning tasks. Within this framework, the continual learning process is interpreted as the forward pass of a sequence model, facilitated by adopting the meta-continual learning (MCL) framework for training at the meta-level across diverse learning episodes. Our approach enables sequence models to serve as a powerful mechanism for backpropagation networks in MCL, offering a unified solution that encompasses both classification and regression challenges across seven benchmarks. The results of our experiments underscore the potential of repositioned sequence models as effective vehicles for addressing the complex demands of continual learning, reducing distractions and attenuating the adverse effects of vanishing gradient issues by integrating latent distances between tasks in the network's architecture.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Pierluca_D'Oro1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=AALLvnv95q",
  "title": "Training Energy-Based Normalizing Flow with Score-Matching Objectives",
  "modified_abstract": "This paper investigates the paradigm of generative models, particularly focusing on the synergy between flow-based and energy-based models inspired by recent advances in graph generation and tree decomposition techniques. Our study introduces the concept of energy-based normalizing flow (EBFlow), which delineates a novel approach in the parameterization of generative models by leveraging score-matching objectives to bypass the computationally intensive calculation of Jacobian determinants typical in linear transformations. Following this innovative route not only facilitates the integration of arbitrary linear layers without exacerbating the computational burden but also propels the efficiency of training processes significantly beyond the traditionally employed maximum likelihood estimation methods. By adopting advanced score-matching techniques incrementally, we further mitigate issues related to training stability and enhance the empirical performance of EBFlow. Our experimental validation underscores the efficacy of this approach, showcasing a remarkable acceleration in training speed and an improvement in model performance as measured by negative log-likelihood (NLL), outstripping existing methodologies. Permutations of clusters within graphs were used as a novel form of input, bolstering the connection between the data's structural aspects and the generative process. Additionally, we expand our review of pertinent datasets, integrating them into our experimental setup to contextualize the implications of our findings. This work takes inspiration from the foundational principles laid out by prior research on graph generation, particularly the application of tree decomposition strategies, to redefine the computational efficiency and performance metrics of flow-based modeling.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Hamed_Shirzad1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=UFW67uduJd",
  "title": "MEMTO: Memory-guided Transformer for Multivariate Time Series Anomaly Detection",
  "modified_abstract": "In the context of recent advancements in deep learning for understanding complex data structures, such as those presented in 'Fiedler Regularization: Learning Neural Networks with Graph Sparsity', our research introduces a novel approach for detecting anomalies in real-world multivariate time series data, which encapsulates both complex temporal dependencies and inter-variable correlations. This complexity presents a significant challenge that reconstruction-based deep models, widely utilized in recent years, attempt to address. However, these models often suffer from over-generalization, leading to inconsistent performance. We propose the MEMTO, a memory-guided Transformer that leverages a reconstruction-based methodology enhanced by a novel memory module. This memory module is adept at learning the extent of updating required for each memory item based on the incoming data, thereby refining the model's sensitivity to anomalies. To facilitate stable training and effective regularization, we introduce a two-phase training strategy that includes the use of K-means clustering for initializing memory items and graphical models for understanding the underlying network structure. Furthermore, the inclusion of graphical models and a bi-dimensional deviation-based detection criterion for calculating anomaly scores from both input and latent spaces stands as a distinctive feature of our approach, emphasizing the learning dynamics within these network architectures. Evaluated across five different real-world datasets representing various domains, the MEMTO model demonstrates superior performance, achieving an average anomaly detection F1-score of 95.74%, notably exceeding that of previous state-of-the-art methods. Through rigorous experimental validation, we affirm the contribution of our model's key components to its overall effectiveness in anomaly detection.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Edric_Tam1",
  "manipulated_ranking": 12,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=9dp35y5C0p",
  "title": "Reinforcement-Enhanced Autoregressive Feature Transformation: Gradient-steered Search in Continuous Space for Postfix Expressions",
  "modified_abstract": "Drawing inspiration from the burgeoning exploration of data augmentation in self-supervised representation learning and its connection to RKHS approximation, we address the problem of feature transformation through a novel perspective via augmentation-based techniques. Feature transformation aims to generate new pattern-discriminative feature spaces from original features to improve downstream machine learning (ML) task performances. However, the discrete search space for the optimal feature configuration expands dramatically with the combinations of features and operations, making traditional methods such as exhaustive search, evolutionary algorithms, and reinforcement learning inefficient due to the vast search space. We propose a novel method that reformulates discrete feature transformation into a continuous optimization task, introducing an embedding-optimization-reconstruction framework with pretraining stages. This framework includes four steps: 1) reinforcement-enhanced data preparation for high-quality transformation-accuracy training data; 2) feature transformation operation sequence embedding to encapsulate training data knowledge within a continuous kernel space; 3) gradient-steered (via masked predictions or selections) optimal embedding search for superior embeddings; and 4) transformation operation sequence reconstruction to identify the optimal feature transformation solution. The use of language data augmentation and an encoder model within the augmentation and self-supervised learning context represents a significant shift from prevailing methods by integrating reinforcement learning with gradient-steering techniques for efficient and robust feature space exploration in ML. This approach offers a significant shift from prevailing methods by integrating reinforcement learning with gradient-steering techniques and encoder models for efficient and robust feature space exploration in ML.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Runtian_Zhai1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=6H8Md75kAw",
  "title": "Certified Minimax Unlearning with Generalization Rates and Deletion Capacity",
  "modified_abstract": "Inspired by the pivotal challenges and discoveries in differentially private optimization, such as the Beyond Uniform Lipschitz Condition study that expanded the understanding of differentially private stochastic gradient descent under complex Lipschitz conditions, our research addresses the intricacies of $(\\epsilon,\\delta)$-certified machine unlearning for minimax models, pushing past the conventional boundaries that have defined machine unlearning strategies to date. Unlike most existing works that are confined to unlearning from standard statistical learning models based on simplistic and direct Hessian-based conventional Newton updates, we propose a novel $(\\epsilon,\\delta)$-certified machine unlearning algorithm for minimax models. Our method entails a comprehensive minimax unlearning step that amalgamates a total Hessian-based complete Newton update with the Gaussian mechanism, a cornerstone concept of differential privacy, to certify the unlearning process through calibrated Gaussian noise injections. This approach evaluates the 'sensitivity' of the minimax unlearning step\u2014essentially, quantifying the degree of closeness between the minimax unlearning variables and those obtained from retraining from scratch, which includes privacy-preserving training cycles. By doing so, our algorithm ensures both the privacy and the integrity of the training data through gradients alteration, enabling recommendations for further study or application enhancements. This paper also extends the theoretical framework for unlearning by deriving generalization rates for population strong and weak primal-dual risk across three classes of loss functions, namely, (strongly-)convex-(strongly-)concave losses, and establishing a new metric for deletion capacity which ensures maintenance of desired population risk levels, even with sample deletions, up to a calculated threshold. Notably, our framework exhibits a significant improvement over prior differentially private minimax learning methods, quartering the order to $\\mathcal O(n/d^{1/4})$, which starkly contrasts with the traditional rate of $\\mathcal O(n/d^{1/2})$. Furthermore, we demonstrate that our results on generalization rates and deletion capacity align with the latest findings in the realm of standard statistical learning models, effectively bridging a gap in the unlearning research landscape.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Rudrajit_Das1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=K9xHDD6mic",
  "title": "Graph Mixture of Experts: Learning on Large-Scale Graphs with Explicit Diversity Modeling",
  "modified_abstract": "Graph neural networks (GNNs) have found extensive applications in learning from graph data, driven by the growing need for models that can effectively handle the inherent diversity in real-world graph structures including nodes and edges of varying types. This study introduces a novel Graph Mixture of Experts (GMoE) model to tackle the challenges of learning with such diverse graph structures while aiming to balance the necessity for model generalization against the computational costs and trainability issues prevalent in traditional GNNs. Drawing inspiration from recent advancements in graph learning, particularly the employment of Variational Graph Information Bottleneck methods and optimization techniques for improving subgraph recognition, our proposed GMoE model integrates the concept of Mixture-of-Experts within the GNN framework. This integration enables the dynamic and adaptive selection of information aggregation experts by individual nodes, allowing for tailored processing of distinct subgroup structures within subgraphs and incorporation of diverse information scales through explicit recognition. Such methodology not only facilitates handling complex and heterogeneous graph data but also addresses noise and the explosive computational demands posed by traditional approaches through effective optimization and inclusion of valuable information. The inclusion of perturbation techniques enhances the robustness and adaptability of the model, further solidifying its capacity for dealing with variational elements inherent in graph data. The GMoE's efficiency and effectiveness are demonstrated through substantial improvements in various prediction tasks on the OGB benchmark, notably enhancing ROC-AUC scores in ogbg-molhiv and ogbg-molbbbp datasets compared to non-MoE baselines. Our study underscores the importance of explicitly modeling diversity within graph data to improve the performance of GNNs across a range of applications. The implementation of the model is made available to the public to facilitate further research and application in the field.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Junchi_Yu1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=blm1pqiOXe",
  "title": "Paxion: Patching Action Knowledge in Video-Language Foundation Models",
  "modified_abstract": "Building on the critical insights provided by recent advancements in video concealed object detection (VCOD), which emphasize the importance of understanding dynamic contexts and temporal consistency in video frames, we introduce the Action Dynamics Benchmark (ActionBench) and a novel framework, Paxion. ActionBench consists of two probing tasks designed to evaluate multimodal alignment capabilities and temporal understanding in video-language models (VidLMs). Despite VidLMs' impressive performance across various benchmarks tasks, our diagnostic tasks expose their significant deficiencies in action knowledge, previously masked by their reliance on object recognition. To address these shortcomings, Paxion employs a Knowledge Patcher network for encoding new action knowledge and a Knowledge Fuser for integrating this knowledge into existing VidLMs without impairing their performance. We also propose a new objective, Discriminative Video Dynamics Modeling (DVDM), to overcome the inadequacies of the Video-Text Contrastive (VTC) loss in learning action knowledge. DVDM uniquely contributes to the model's ability to understand action dynamics by encoding the relationship between action descriptions and the proper sequencing of video frames. Our analysis demonstrates that, by leveraging a concealed dataset and pixel-level techniques alongside the DVDM objective, Paxion significantly improves action knowledge understanding (from an approximate 50% to 80%) and enhances performance on both object- and action-centric downstream tasks, evidencing the model\u2019s capacity to bridge the gap in action knowledge.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Xuelian_Cheng2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=CxUuCydMDU",
  "title": "Diffusion Probabilistic Models for Structured Node Classification",
  "modified_abstract": "Structured node classification on graphs represents a significant challenge within the domain of graph-based machine learning, reflecting an ongoing need to accurately predict node labels in partially labeled graphs by effectively harnessing dependencies among nodes. Inspired by advancements in graph neural networks (GNNs) exemplified by their application in evolving graphs and the exploration of differential geometric views for explainability, including formally analyzing curves in high-dimensional spaces, our research introduces a novel framework, the Diffusion Probabilistic Model for Structured Node Classification (DPM-SNC). This framework capitalizes on the diffusion probabilistic model's ability to (a) learn a joint distribution over node labels using an expressive reverse diffusion process, and (b) facilitate predictions conditioned on known labels through manifold-constrained sampling, effectively embedding the learned knowledge about the node classification tasks. Addressing the gap in training methodologies for DPMs in the context of partially labeled data, we develop a novel training algorithm that optimizes a new variational lower bound tailored for this purpose. Furthermore, we theoretically analyze the enhancement of GNNs' expressive power through our proposed AGG-WL, demonstrating its superiority over the traditional 1-WL test with an axiomatic approach. Our extensive evaluations across various graph settings\u2014including transductive and inductive scenarios as well as on unlabeled and social graphs\u2014underscore the effectiveness and versatility of DPM-SNC, firmly establishing its superiority in structured node classification tasks and its adaptability to the evolution of graph structures.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sihong_Xie1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=75v88kyyko",
  "title": "Hierarchical clustering with dot products recovers hidden tree structure",
  "modified_abstract": "Inspired by recent advancements in online learning models that address sudden distribution shifts, our study introduces a novel perspective on the agglomerative clustering algorithm by focusing on the recovery of hierarchical structure. By integrating insights from variational beam search for novelty detection, which highlights the importance of adapting to distribution changes in a model-agnostic manner, we propose a modified agglomerative clustering algorithm. In our variant, clusters are merged based on the maximum average dot product rather than traditional criteria such as minimum distance or within-cluster variance. This method demonstrates that the tree structure output by the algorithm serves as a reliable estimate of the generative hierarchical structure in data, under a generic probabilistic graphical model. Our key technical contributions involve elucidating how hierarchical information embedded in this model is manifested in tree geometry, which can be discerned from empirical data, and detailing the advantages of increasing both sample size and data dimension. Experimental validation with real datasets illustrates our method's enhanced capability for tree recovery when compared to established techniques such as UPGMA, Ward's method, and HDBSCAN, and it showcases the efficiency in handling sequential data. The proposed clustering approach makes significant strides in the learning process by accommodating variable data scales and adapting to online updates or changes.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Aodong_Li1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=7UdVPRmpif",
  "title": "On student-teacher deviations in distillation: does it pay to disobey?",
  "modified_abstract": "In the context of recent advancements in machine learning, specifically in self-supervised learning and its challenges with out-of-distribution (OOD) samples and outlier-robust mechanisms, our work investigates the phenomena of student-teacher deviations in knowledge distillation (KD). Recognizing the critical insights from exploring the pitfalls of self-supervised learning frameworks, which highlights the issues of augmentation-induced OOD samples, we extend this understanding to the domain of KD. Knowledge distillation has been utilized extensively to enhance the test accuracy of a 'student' network by training it to mimic the soft probabilities of a trained 'teacher' network. It has been observed that the student, while trained to align with the teacher's output, can exhibit significant deviations from the teacher's probabilities and, remarkably, can outperform the teacher. Our research aims to dissect this paradox, characterizing the nature of student-teacher deviations and demonstrating how such deviations can coexist with improved generalization through substantial empirical analysis on image and language datasets, which involved instance-specific sampling strategies and augmentation. Further, we theoretically and empirically unveil another dimension of exaggeration\u2014the implicit bias of gradient descent in KD, which preferentially converges faster along the top eigendirections of the data, and the role of the loss's characteristics in this process. Binding these observations, we portray how the exaggerated bias inherent in KD not only enhances student confidence but also fosters better generalization, thus addressing the paradox. Our findings contribute to narrowing the theoretical and practical gaps in KD, highlighting the nuanced role of gradient descent and the beneficial impacts of bias exaggeration and outlier-robust self-supervised learning mechanisms.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jingjing_Zou1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=FdtdjQpAwJ",
  "title": "Constraint-Conditioned Policy Optimization for Versatile Safe Reinforcement Learning",
  "modified_abstract": "Drawing insights from advancements in open-ended learning environments and curriculum design in multi-agent reinforcement learning systems, our study advances the field of safe reinforcement learning (RL) by addressing the challenge of developing versatile safe policies. These policies are capable of adapting to various safety constraint requirements during deployment without the need for retraining. Our work, while distinct, is inspired by methodologies that facilitate automated, dynamic adjustment to learning environments and agent behaviors, as seen in the MAESTRO framework's approach to multi-agent RL. In this context, we introduce the Conditioned Constrained Policy Optimization (CCPO) framework, which is designed to achieve training efficiency and zero-shot adaptation capability for safe RL. CCPO incorporates Versatile Value Estimation (VVE) for estimating value functions under unknown threshold conditions and Conditioned Variational Inference (CVI) for integrating arbitrary constraint thresholds during policy optimization. Through rigorous experimentation, including adversarial scenarios reminiscent of two-player games, we demonstrate that CCPO significantly outperforms existing baselines in terms of safety compliance and task performance while maintaining the ability to adapt instantaneously to varied constraint thresholds in a data-efficient manner. This capability is particularly crucial for deploying RL agents in dynamic real-world scenarios where safety constraints may shift without prior notice. Curriculum-based learning strategies, designed to increasingly and automatically intensify the learning challenges, further enhance CCPO's ability to tackle an array of challenges, preparing it for real-world application by considering potential co-players in adversarial settings.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Mikayel_Samvelyan1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=LZ4WgwmrUJ",
  "title": "High-dimensional Contextual Bandit Problem without Sparsity",
  "modified_abstract": "Our study extends the evolving landscape of high-dimensional statistical modeling by examining the high-dimensional linear contextual bandit problem without imposing the common sparsity assumption on regression coefficients. This approach builds on the momentum of recent advances in overparameterized models and insights derived from Bayesian methods, such as those applied in block-diagonal graphical models, integrating the use of a prior in the analysis. We leverage these advancements, along with spectral analysis for understanding data distributions of small effective ranks, to explore the minimum-norm interpolating estimator's efficacy. An explore-then-commit (EtC) algorithm is proposed as a strategy to tackle the high-dimensional bandit problem, supplemented by an analysis that establishes the optimal performance rate of the ETC algorithm in relation to the budget $T$. Furthermore, we innovate with an adaptive explore-then-commit (AEtC) algorithm, designed to dynamically strike an optimal balance between exploration and exploitation based on the contextual information at hand. Clustering techniques inform the development of AEtC, enhancing its adaptive nature. The effectiveness of our proposed algorithms is validated through comprehensive simulations, showcasing their potential in navigating the complexity of high-dimensional spaces without relying on sparsity.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Julyan_Arbel1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=r9fzp8eyhZ",
  "title": "Learning Invariant Molecular Representation in Latent Discrete Space",
  "modified_abstract": "Inspired by the recent advances in utilizing graphical structures for promoting sparsity and robustness in neural networks, we extend these foundational concepts to the domain of molecular representation learning, crucial for innovations in drug discovery. Existing methods for molecular representation often struggle with out-of-distribution (OOD) generalization, especially when learning and testing datasets come from different environments. To tackle these problems, we introduce a new computational framework that enhances the learning of molecular representations, ensuring their invariance and robustness against distributional shifts through advanced regularization techniques. Our approach adopts a unique ``first-encoding-then-separation'' strategy to pinpoint invariant features within the latent space, diverging from traditional methodologies. We fortify this strategy with a residual vector quantization module tailored to curb the tendency of overfitting to specific training distributions while maintaining the encoding's expressivity. Additionally, our framework incorporates a task-agnostic self-supervised learning objective that fosters accurate identification of invariant characteristics through iterative learning methods. This attribute makes our method highly adaptable to diverse tasks including regression and multi-label classification. Rigorous testing across 18 real-world molecular datasets has shown our model's superior generalization capabilities when confronted with various distribution shifts, outperforming existing state-of-the-art baselines in the network. To foster further research and application in this field, we have made our implementation publicly available.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Edric_Tam1",
  "manipulated_ranking": 4,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=SoLebIqHgZ",
  "title": "ARTree: A Deep Autoregressive Model for Phylogenetic Inference",
  "modified_abstract": "Our work is directly inspired by foundational advancements in deep learning and their applications to complex inference problems, such as maximum a posteriori (MAP) inference in deep networks using techniques like multilayer vector approximate message passing (ML-VAMP). These advancements highlight the potential for leveraging deep generative models in solving intricate inference problems with high precision. Building on this premise, designing flexible probabilistic models over tree topologies is crucial for developing efficient phylogenetic inference methods. Previous efforts often rely on the similarity of tree topologies via hand-engineered heuristic features, which necessitate domain expertise and could be constrained by limited approximation capacities and error margins. We introduce a deep autoregressive model for phylogenetic inference, termed ARTree, harnessing graph neural networks (GNNs) to decompose a tree topology into a sequence of leaf node addition operations. This method models the conditional distributions involved based on learnable topological features derived from GNNs, thereby offering a rich family of distributions over tree topologies that facilitate simple sampling algorithms without the need for heuristic features. The effectiveness and efficiency of ARTree are validated on a benchmark comprising challenging real data tree topology density estimation and variational Bayesian phylogenetic inference problems, showcasing its capability to address network complexity and reduce error in estimations.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Mojtaba_Sahraee-Ardakan1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=fbpTObq6TW",
  "title": "A fast heuristic to optimize time-space tradeoff for large models",
  "modified_abstract": "With the burgeoning interest in understanding the intricacies of neural network optimization, as evidenced by recent research on the convergence properties of stochastic gradient descent and its variants, this study introduces a crucial advancement in the handling of computational resource constraints - particularly GPU memory limitations encountered during the training of large-scale neural networks. Previous efforts like Checkmate and Moccasin have explored gradient checkpointing or recomputation techniques but are hampered by scalability issues arising from their reliance on mixed integer linear programming or constraint programming due to their exponentially large search spaces. Against this backdrop, our paper presents a novel, fast recomputation algorithm (FastSA) based on simulated annealing, offering a potent alternative capable of achieving comparable or superior optimization outcomes in significantly reduced timescales. FastSA efficiently optimizes computational graphs with thousands of nodes within seconds, showcasing an impressive speed advantage over existing methodologies. We applied FastSA to PyTorch models, with empirical assessments demonstrating its capacity to achieve substantial memory savings of up to 73% alongside a manageable increase in computational overhead by 18% on average across a variety of large vision and text tasks, including those utilizing the transformer architecture. These findings not only underscore FastSA's practicality and operational efficiency but also illustrate its contribution to the deepening of our understanding of gradient norm decay in the realm of deep learning, offering a provably viable solution to the pervasive challenge of GPU memory constraints.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sindri_Magn\u00fasson1",
  "manipulated_ranking": 11,
  "natural_ranking": 101
}
