{
  "paper_link": "https://openreview.net/forum?id=D94QKZA7UP",
  "title": "A One-Size-Fits-All Approach to Improving Randomness in Paper Assignment",
  "modified_abstract": "Building on recent discourse around enhancing transparency and addressing shortcomings in machine learning (ML) practices, as epitomized by efforts to rectify 'documentation debt' in widely used datasets, this work extends the conversation to the domain of peer review processes in large publication venues. Recognizing the multifaceted challenges in automated paper assignment, including the need for maximizing reviewer expertise while also considering robustness, reviewer variety, and anonymity, our research introduces a novel, one-size-fits-all solution to inject randomness effectively into the paper assignment algorithm. This approach is designed to simultaneously address a spectrum of concerns, mirroring the multi-objective optimization strategies seen in other areas of ML research and application, particularly in learning algorithms that are subject to documentation practices and learning from complex datasets. Theoretically and empirically, we demonstrate that our method surpasses existing randomized paper assignment techniques across several key metrics of randomness and representation. The generality and efficacy of the randomized assignments produced attest to our method's capacity to serve broad-ranging motives for incorporating randomness, thereby setting a new standard for automated peer review assignments. In doing so, we contribute to improving the integrity and composition of peer review panels, highlighting the importance of diversifying documentation practices in scholarly publications.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Nicholas_Vincent2",
  "manipulated_ranking": 1,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=wiv21EJ0Vd",
  "title": "Zero-shot Visual Relation Detection via Composite Visual Cues from Large Language Models",
  "modified_abstract": "Building on the premise that pretrained vision-language models like CLIP can transcend traditional task boundaries in computer vision, our work is further inspired by the latest explorations into multi-task learning, particularly how dual tasks of semantic segmentation and depth estimation can enhance each other's performance through deeply coupled mechanisms. Just as these advancements leverage cross-dimensional data and novel attention modules, augmented with channel-specific augmentations, for improved generalizability and efficiency, our study introduces RECODE, a novel approach for zero-shot visual relation detection (VRD) that leverages large language models (LLMs) for generating composite visual cues. RECODE aims to address inherent limitations in employing CLIP for zero-shot VRD, such as its struggles with distinguishing fine-grained relation types and overlooking spatial relationships between object pairs w.r.t. spatial components. By decomposing predicate categories into subject, object, and spatial components and generating tailored, trainable description-based prompts for each, RECODE enhances the discriminability of similar relation categories from multifaceted perspectives. Additionally, our chain-of-thought method for dynamic cue weight fusion exemplifies a significant leap toward utilizing LLMs in a manner that mirrors multi-task learning's symbiotic feature sharing, resulting in interpretable, high-performing zero-shot VRD. Moreover, the optimization of memory allocation during both training and inference phases underscores our method\u2019s practical applicability, making prediction processes more efficient. Validation of RECODE's effectiveness and interpretability is achieved through extensive experiments across four VRD benchmarks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Nitin_Bansal1",
  "manipulated_ranking": 2,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=E2zoGTkTbW",
  "title": "Reward Imputation with Sketching for Contextual Batched Bandits",
  "modified_abstract": "In our exploration of the contextual batched bandit (CBB) setting, we draw inspiration from recent advancements in machine learning models for complex, partially-observable environments, such as those demonstrated by the Bootstrap your own Latents with Diffusion models for Exploration (BLaDE) framework. Similar to BLaDE's approach to curiosity-driven exploration and prediction-error methods at the latent level, our paper presents an efficient methodology called Sketched Policy Updating with Imputed Rewards (SPUIR) that innovatively completes the unobserved rewards through sketching, thereby approximating full-information feedbacks. Our method formulates reward imputation as an imputation regularized ridge regression problem to capture feedback for both executed and non-executed actions, with the novel incorporation of randomized sketching to mitigate time complexity and dynamics of decision environments. We establish that SPUIR achieves improved instantaneous regret with a controllable bias and reduced variance compared to existing CBB approaches lacking reward imputation, thereby fostering a robust representation of decision-making scenarios. Furthermore, we demonstrate that our method, fueled by curiosity-driven strategies and auxiliary information for prediction, secures a sublinear regret bound vis-\u00e0-vis the optimal policy. Additional extensions, including a rate-scheduled variant and a model accommodating nonlinear rewards, are proposed to enhance practicality and enrich the representation of decision-making scenarios. Empirical evaluations confirm that SPUIR surpasses leading baselines on a mixture of synthetic, publicly available benchmarks, equipped with robust prediction and optimization strategies, and real-world data sets, thereby providing a potent tool for optimizing decision-making in contexts where rewards are partially observed through diffusion and representation techniques.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zhaohan_Daniel_Guo1",
  "manipulated_ranking": 9,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=xdOoCWCYaY",
  "title": "Towards Data-Agnostic Pruning At Initialization: What Makes a Good Sparse Mask?",
  "modified_abstract": "The concept of Pruning at initialization (PaI) has evolved in an attempt to improve neural network training and inference efficiency by removing weights prior to training. This endeavor is informed and motivated by recent advancements in adversarial training and defense mechanisms, which employ a learn-to-learn (L2L) framework to optimize network robustness against adversarial attacks, highlighting the potential of applying innovative optimization strategies, including convex-concave procedures, to enhance neural network performance before training commences. While current PaI methods successfully identify trainable subnetworks surpassing the efficacy of random pruning, their capacity for balancing accuracy in classification tasks and computational reduction lacks behind that achievable through post-training pruning methods. Furthermore, existing explorations into PaI have not fully understood the mechanism at play, such as the ability of discovered subnetworks to maintain performance despite layerwise mask shuffling and weight re-initialization, pointing to an emphasis on layerwise sparsity over individual weight significance.\nOur investigation introduces a novel angle to PaI by focusing on the topology of subnetworks, proposing a unique framework that evaluates PaI effectiveness through the lens of effective paths and nodes within the network topology. This perspective enables a deeper, more nuanced understanding of how PaI methods function and offers a criterion for assessing the initialization state of neural subnetworks. We observe that an optimal balance between the number of effective nodes and paths, along with their network-wide distribution, is key to constructing high-performance subnetworks at designated sparsity levels. Leveraging these insights, we develop a new data-agnostic pruning technique grounded in multi-objective optimization and learn-to-prune strategies. Our comprehensive experimentation across various architectures and datasets illustrates that this approach not only surpasses leading PaI methods in both attack defense and classification tasks but also significantly reduces inference FLOPs\u2014in some cases by up to 3.4 times. Full code release is pledged for enhancing reproducibility and further research.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zhehui_Chen1",
  "manipulated_ranking": 1,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=LUT4b9gOtS",
  "title": "Learning Visual Prior via Generative Pre-Training",
  "modified_abstract": "Inspired by recent findings demonstrating that adversarially robust features in deep neural networks (DNNs) align more closely with human perceptual mechanisms and the biological plausibility of robust representations in mimicking peripheral visual processing, this study introduces a novel approach to learning visual priors in visual data through generative pre-training, named VisorGPT. By translating the concept of resistance against adversarial noise in DNNs to the explicit learning of visual traits such as object location, shape, and texture-model characteristics, VisorGPT leverages advancements in language modeling to discretize visual locations, including bounding boxes, human pose, and instance masks, into sequences. This methodology facilitates the modeling of visual priors via likelihood maximization, akin to the processing of adversarially robust features and psychophysics insights, thereby bridging the gap between innate human visual perception and algorithmic representation. Additionally, the incorporation of prompt engineering enables the customization of sampling processes for various visual locations, offering a unified mechanism to generate sequential outputs based on learned priors. Experiments validate VisorGPT's effectiveness in accurately modeling visual priors and its capability to generalize to novel scenes, suggesting that integrating discrete visual locations within the language model paradigm can enhance the perception of visual observers' world by computational means. Importantly, the approach underscores the potential of generative models in encompassing both human perceptual mechanisms and adversarial robustness within the realm of visual data processing, thereby improving the training performance of these models. The availability of code has been ensured for further research and application development.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Anne_Harrington1",
  "manipulated_ranking": 2,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=Kg65qieiuB",
  "title": "Demystifying Oversmoothing in Attention-Based Graph Neural Networks",
  "modified_abstract": "Inspired by the recent surge in applications of Graph Neural Networks (GNNs) across diverse fields such as automated model building in cryo-electron microscopy (cryo-EM) maps, our study extends into investigating oversmoothing\u2014a limitation affecting GNNs. Oversmoothing in GNNs refers to the phenomenon where increasing network depth leads to homogeneous node representations, raising controversies around the effectiveness of graph attention mechanisms in countering this issue. This issue has become particularly pertinent in contexts where GNNs are leveraged for building detailed predictive models from entries in databases or for automated atomic model building through GNNs, as seen in cryo-EM. By leveraging a comparative perspective provided by recent advancements, including those in cryo-EM for atomic model building through GNNs, we provide a definitive mathematical analysis on oversmoothing. Viewing attention-based GNNs as nonlinear time-varying dynamical systems, and incorporating tools and techniques from the theory of products of inhomogeneous matrices along with the joint spectral radius concept, we establish that graph attention mechanisms in the context of learning indeed cannot prevent oversmoothing and experience exponential loss in expressive power. This analytical perspective pinpoints the 'prior' assumptions about network architecture's role in mitigating oversmoothing and broadens our understanding of the phenomenon from symmetric Graph Convolutional Networks (GCNs) to a wider spectrum of GNN models, including random walk GCNs, Graph Attention Networks (GATs), and graph transformers, all involved in automated building processes of biological protein models and more. Our findings cover a range of asymmetry in aggregation operators, state-dependency, time-variance, and common nonlinear activations such as ReLU and GELU, challenging the prevailing belief in the protective capacity of attention mechanisms against oversmoothing.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Dari_Kimanius1",
  "manipulated_ranking": 1,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=B3UDx1rNOy",
  "title": "Recurrent Temporal Revision Graph Networks",
  "modified_abstract": "Building on the innovations introduced in unsupervised learning for combinatorial optimization problems on graphs, this study extends into the dynamic domain of temporal graphs which present unique challenges not adequately addressed by static graph models. In recognizing the evolving nature of real-world interactions, we identify a gap in how neighbor information is aggregated over time, which is crucial for modeling these temporal graphs accurately. Our proposed solution involves a novel aggregation framework that leverages the recurrent neural network architecture to maintain node-wise hidden states, enabling the integration of all historical neighbor information while overcoming the computational inefficiencies and biases introduced by subsampling recent neighbors. By doing so, our framework not only addresses the deficit of complete neighbor information in current methodologies but also highlights the superior theoretical expressiveness of our model. Optimization techniques are integral to our approach, enhancing the learning efficiency without the need for derandomized processes or explicit clustering steps. Empirical evaluations demonstrate a notable improvement in performance on real-world datasets, such as a +9.4% increase in averaged precision on an E-commerce dataset, showcasing the practical advantages and state-of-the-art results achieved by our Recurrent Temporal Revision Graph Networks over existing 2-layer models. Our work opens new horizons for utilizing sets of temporal information in graph-based learning scenarios.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Nikolaos_Karalias1",
  "manipulated_ranking": 12,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=KMxRQO7P98",
  "title": "One-Pass Distribution Sketch for Measuring Data Heterogeneity in Federated Learning",
  "modified_abstract": "Federated learning (FL), a paradigm emphasizing the importance of privacy and security in distributed machine learning by enabling model training across multiple devices without direct data exchange, naturally encounters the challenge of data heterogeneity due to the diverse data distributions across clients. This issue mirrors and amplifies the complexities in ensuring privacy and security in distributed systems as previously detailed in regards to cross-silo federated learning systems, which strive to achieve strict privacy through differential privacy and secure summation protocols. In response to the critical need for efficient measurement of data heterogeneity in FL to mitigate its negative impacts, our work introduces a novel one-pass distribution sketch technique. This technique, leveraging advancements in neural networks and secure subsampling methods, enables the representation of client data distributions in an efficient manner, requiring only a single pass over the data, thus optimizing both computational and memory resources. The secure aspects of FL are further bolstered through our consideration of secure aggregation methods akin to homomorphic encryption for ensuring that all parties maintain data privacy. We theoretically and empirically validate that the distances between distribution sketches accurately reflect the divergences between their respective data distributions. Through extensive experimentation, we reveal that our method not only advances client selection processes within FL environments but also presents an effective solution to the cold start problem for new clients with unlabeled data, thereby enhancing the overall efficacy and efficiency of FL deployments.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Mikko_A_Heikkil\u00e41",
  "manipulated_ranking": 1,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=ESEM1lNoeS",
  "title": "AttrSeg: Open-Vocabulary Semantic Segmentation via Attribute Decomposition-Aggregation",
  "modified_abstract": "Open-vocabulary semantic segmentation emerges as a compelling challenge, propelled by the imperative to segment novel object categories at inference time without explicit textual names provided during training. This quest is further complicated by the practical inconsistency and the inadequacy of textual category descriptions commonly encountered, such as ambiguous, incomplete, or entirely new lexical entries that were not present in the dataset used for model pre-training. Addressing these challenges, we introduce *AttrSeg*, an innovative *attribute decomposition-aggregation* framework inspired by the inherent human cognitive ability to discern and categorize objects through the assimilation of diverse attribute descriptions. This mechanism enables a comprehensive understanding by breaking down class names into a spectrum of attributes which are then intelligently recombined to form a robust global descriptor capable of distinguishing target objects effectively. To bolster our framework, we design two attribute construction strategies catering to common and human-invented categories, supplemented by a hierarchical aggregation architecture that facilitates multi-level attribute synthesis via a sophisticated clustering module. This approach aims to reconcile the gaps identified in prior methodologies, such as the memory-intensive trajectory matching for dataset distillation, which, despite its effectiveness, grapples with scalability issues on large datasets like ImageNet due to enormous memory demands. By experimenting and evaluating AttrSeg across three meticulously annotated datasets, our findings illuminate the framework's superiority in navigating the nuanced terrain of open-vocabulary segmentation. This is substantiated through extensive experiments and ablation studies, underscoring our technique's potential in significantly advancing the field through innovative training paradigms and loss function refinements. We also provide a comprehensive evaluation to demonstrate the enhanced performance of this attribute decomposition-aggregation mechanism, trained on a vast corpus of data, offering a profound leap over existing paradigms in terms of scalability, efficiency, and overall efficacy in recognizing and segmenting a vast array of objects across disparate contexts.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Justin_Cui1",
  "manipulated_ranking": 7,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=B01uiWhjpc",
  "title": "Parameterizing Context: Unleashing the Power of Parameter-Efficient Fine-Tuning and In-Context Tuning for Continual Table Semantic Parsing",
  "modified_abstract": "Within the evolving landscape of machine learning, especially in the parameter-efficient domains, contributions such as the introduction of Conditional Adapters (CoDA) present new paradigms for managing model updates and learning efficiency through conditional computation. This paper builds upon these pivotal advancements, specifically CoDA's methodology for enhancing transfer learning and inference efficiency through parameter efficiency and conditional computation. Our work extends the concept to the domain of continual table semantic parsing, a subset of language understanding tasks, where challenges such as overfitting, limited supervision, and catastrophic forgetting prevail due to the sequential nature of tasks and scarceness of training examples. We introduce a novel method that synergizes parameter-efficient fine-tuning (PEFT) and in-context tuning (ICT), specifically designed for this setting, to address a variety of challenges. Our approach features a task-adaptive PEFT framework to negate catastrophic forgetting by retaining a frozen pre-trained model backbone, optimizing small-scale prompts with specific activation functions to ensure task relevance. Furthermore, we explore a teacher-student framework wherein ICT facilitates few-shot learning through providing contextual demonstrations for the teacher model, which in turn guides the student model. The student employs the PEFT framework to adapt from the teacher's outputs, compressing and encapsulating context into prompts while obviating the need for the storage of training examples. Evaluations on benchmark datasets, accompanied by a comprehensive set of experiments, demonstrate our method's superiority over existing few-shot and continual learning baselines, offering a paramount blend of efficiency and efficacy in tackling the inherent challenges of continual table semantic parsing through computation and transfer learning methodologies.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Joshua_Ainslie1",
  "manipulated_ranking": 1,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=V5Oh7Aqfft",
  "title": "Causal Effect Regularization: Automated Detection and Removal of Spurious Correlations",
  "modified_abstract": "This work builds on the foundations laid by previous studies in weak supervision and fairness in machine learning, addressing the pervasive issue of spurious correlations within classification datasets. Recognizing the limitations of conventional methods that require a priori knowledge of spurious attributes, we introduce a novel approach for automatically identifying these attributes based on their causal effect on the task labels. Leveraging insights from prior research that highlighted the challenges of achieving fairness and reducing bias in datasets labeled through weak supervision, our method, named AutoACER, employs a new regularization objective designed to diminish the classifier's reliance on spurious attributes during the training phase. Notably, we demonstrate its effectiveness across synthetic, semi-synthetic, and real-world datasets by achieving significant improvements in reducing classifier dependence on spurious attributes, a benchmark quantified by the $\\Delta$Prob metric, while maintaining or enhancing model accuracy. Furthermore, AutoACER exhibits resilience in scenarios of noisy causal effect estimation or when causal effects are unidentifiable. Through a theoretical analysis in a controlled linear classification setting, it's ascertained that the success of AutoACER hinges on the accurate ranking of estimated causal effects across attributes, thereby ensuring the selection of a classifier that is fair, fairness-based, and not influenced by spurious correlations. This development in machine learning sets a precedent for equitable algorithms that can be deployed in everyday applications, by ensuring that decision-making processes are devoid of unfair bias.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Changho_Shin2",
  "manipulated_ranking": 1,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=LAGxc2ybuH",
  "title": "Explaining the Uncertain: Stochastic Shapley Values for Gaussian Process Models",
  "modified_abstract": "This work draws inspiration from the foundational methods in online clustering with bandit feedback, emphasizing novel algorithmic approaches to handling uncertainty and optimizing decisions based on incomplete information. Utilizing the rich theoretical backdrop provided by these methods, we introduce a novel framework for explaining Gaussian processes (GPs) that leverages the full analytical covariance structure inherent in GPs. By adapting the concept of Shapley values for stochastic cooperative games, we generate explanations that manifest as random variables, maintaining the axiomatic strengths of traditional Shapley values while introducing a tractable covariance function to quantify uncertainties in explanations and to assess statistical dependencies between them. The proposed methodology extends beyond static explanation models to encompass predictive explanations, introducing a Shapley prior over the explanation function that enables the prediction of Shapley values for new data based on previously computed ones. Through a series of comprehensive illustrations and the design of experiments on various datasets, we establish the efficacy of our approach in providing nuanced explanations of GPs, thereby contributing a significant advancement in the interpretability of machine learning models in uncertain environments. Our method effectively acts as an agent for interpretability, guiding the design and assessment of explanations in uncertain datasets.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Junwen_Yang1",
  "manipulated_ranking": 5,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=8niGwlkLAX",
  "title": "Sparse Deep Learning for Time Series Data: Theory and Applications",
  "modified_abstract": "Inspired by the recent progress in causal inference for improving out-of-distribution generalization, particularly through leveraging the causal structure to identify invariant predictors across domains, this paper extends the methodology of sparse deep learning to time series data. Sparse deep learning has become a popular technique for improving the performance of deep neural networks in areas such as uncertainty quantification, variable selection, and large-scale network compression, with significant implications in machine learning. However, most existing research has focused on problems where the observations are independent and identically distributed (i.i.d.), and there has been little work on the problems where the observations are dependent, such as time series data and sequential data in natural language processing. This paper aims to address this gap by studying the theory for sparse deep learning with dependent data, employing graph-based methods to elucidate the causal relationships within the data. We show that sparse recurrent neural networks (RNNs) can be consistently estimated, and their predictions are asymptotically normally distributed under appropriate assumptions, enabling the prediction uncertainty to be correctly quantified through rigorous experiments. Our numerical results demonstrate sparse deep learning's superiority over state-of-the-art methods, such as conformal predictions, in prediction uncertainty quantification for time series data through a rigorous search for the most effective models. The proposed method's ability to consistently identify the autoregressive order for time series data and its performance in large-scale model compression underscore its practical significance in finance, healthcare, and energy sectors, where both accurate point estimates and prediction uncertainty quantification, guided by causal queries, are crucial.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Kenneth_Lee1",
  "manipulated_ranking": 60,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=PzYAMXmIT3",
  "title": "Does Visual Pretraining Help End-to-End Reasoning?",
  "modified_abstract": "In light of prior work on scene-adaptive video frame interpolation and meta-learning, which demonstrates significant performance gains in video processing by adapting models to specific videos at test time, our study explores a related inquiry in the context of visual reasoning. Specifically, we aim to investigate whether end-to-end learning of visual reasoning can be achieved with general-purpose neural networks, with the help of visual pretraining. A positive result would challenge the prevailing notion that explicit visual abstraction, such as object detection, is indispensable for compositional generalization in visual reasoning tasks, thereby affirming the viability of a 'generalist' neural network for handling visual recognition and reasoning with high adaptation rate across a variety of applications. Our proposed methodology employs a self-supervised framework that condenses each video frame into a concise set of tokens via a transformer network, and subsequently reconstructs the remaining frames based on this compressed temporal context with interpolation. This necessitates the network learning a compact representation for each image and grasping temporal dynamics and object permanence from the temporal continuum. Evaluation on visual reasoning benchmarks, such as CATER and ACRE, reveals that pretraining is crucial for attaining compositional generalization in end-to-end visual reasoning. Notably, our framework, which emphasizes efficiency and a high adaptation rate through performance metrics, surmounts established supervised pretraining methods, including those focused on image classification and explicit object detection, by considerable margins.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sungyong_Baik1",
  "manipulated_ranking": 1,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=ihlT8yvQ2I",
  "title": "GNNEvaluator: Evaluating GNN Performance On Unseen Graphs Without Labels",
  "modified_abstract": "The growing interest in graph neural networks (GNNs) for handling graph-structured data, paralleled by efforts to imbue these models with more robust classification capabilities through improved architecture designs such as convolutional layers, sets the stage for our exploration into GNN model evaluation. Our research addresses the challenge of evaluating the performance of GNNs when deployed on unseen and unlabeled test graphs, a scenario characterized by significant uncertainty due to the discrepancy between training and test graph distributions. Inspired by previous advancements in energy-based GNN frameworks which aimed at enhancing model robustness through better representation of graph features and adjacency matrices, we propose a novel two-stage framework for GNN model evaluation. This framework comprises (1) the construction of a DiscGraph set to capture a wide range of data distribution discrepancies through a discrepancy measurement function utilizing latent node embeddings and adjacency representations, and (2) the training and inference of GNNEvaluator, a research contribution that is adept at estimating node classification accuracy on unseen graphs, thereby directly impacting the performance assessment of GNN models. Through extensive experiments on real-world datasets, our approach demonstrates a significant advancement in accurately evaluating GNN models in the absence of labels, marking a pivotal step towards reliable GNN deployment and serving.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Prathamesh_Dharangutte1",
  "manipulated_ranking": 1,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=LlERoXEKjh",
  "title": "Training shallow ReLU networks on noisy data using hinge loss: when do we overfit and is it benign?",
  "modified_abstract": "In the context of recent advances in machine learning (ML) methodology and theory, such as the development of hybrid random features (HRFs) for kernel approximation in various ML tasks, our work investigates the phenomenon of benign and non-benign overfitting in two-layer ReLU networks trained on noisy binary classification data using gradient descent and hinge loss. By examining conditions under which shallow networks trained on machine learning models on linearly separable data with a proportion of corrupted labels overfit in a benign manner\u2014achieving zero loss on training data without compromising classification accuracy on test data\u2014we extend the current understanding of overfitting dynamics in neural networks. Our analysis, incorporating insights from random initialization and kernels in network training, focuses on the margin of the clean data, delineating the conditions that lead to benign overfitting, non-benign overfitting, and non-overfitting scenarios. Through a combinatorial approach and experiments, we elucidate the training dynamics of neurons, identifying two phases: a rapid reduction in the loss for clean points followed by a phase where clean points remain close to zero loss while corrupt points are progressively corrected or excluded by the network. This nuanced exploration of training dynamics not only contributes to the theoretical landscape of ML, echoing the foundational insights provided by the study of HRFs, random kernels, and benchmarking standards established around the year 2007, but also offers practical guidance for handling noisy data in binary classification tasks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yuanzhe_Ma1",
  "manipulated_ranking": 39,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=g1dMYenhe4",
  "title": "MIMEx: Intrinsic Rewards from Masked Input Modeling",
  "modified_abstract": "In light of the growing interest in developing methods for autonomous exploration in high-dimensional environments, as evidenced by prior works leveraging human involvement for reward-free policy learning, our study introduces a novel perspective in the field of reinforcement learning (RL). Specifically, we focus on the mechanism of generating intrinsic rewards, crucial for exploration in environments where extrinsic cues are sparse or non-existent. Prior research has highlighted the efficacy of conditional prediction objectives, such as masked autoencoding, in approximating the novelty of states, transitions, or trajectories\u2014a concept at the heart of intrinsic motivation. This method proposes a comprehensive view that intrinsic reward mechanisms previously proposed are, in essence, variations of conditional prediction, with novelty estimation akin to pseudo-likelihood estimation under diverse mask distributions. Our contribution, Masked Input Modeling for Exploration (MIMEx), emerges from this unified framework, offering a versatile methodology for intrinsic reward generation by allowing adjustable mask distributions to modulate the challenge of the conditional prediction task. Through extensive evaluations on a set of sparse-reward visuomotor tasks, MIMEx demonstrates significant improvements over leading baseline approaches in learning and control, establishing its potential to facilitate more efficient exploration and learning in complex settings. Moreover, considering the safety implications of autonomous agents in real-world environments, MIMEx introduces an adaptive exploration strategy that can be critical for ensuring safe interactions without compromising explorative efficiency. The study essentially proposes MIMEx as an effective agent for exploration, motivated by internally generated rewards without the direct involvement of external subjects or rewards.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zhenghao_Peng1",
  "manipulated_ranking": 1,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=l9BsCh8ikK",
  "title": "Visual Instruction Inversion: Image Editing via Image Prompting",
  "modified_abstract": "The advent of generative modeling in 3D shapes and text-to-image synthesis has paved the way for novel approaches in image editing. Drawing from the extensive capabilities demonstrated in the synthesis of 3D shapes through procedural representations and deep generative models, our work introduces a methodology for image editing that leverages visual prompts. Unlike traditional text-conditioned image editing frameworks that often struggle with the ambiguity and inefficacy of language in describing specific image edits, our method utilizes pairs of 'before' and 'after' images as visual prompts to learn text-based editing directions. This innovative approach capitalizes on inverting visual prompts into editing instructions using the rich, pretrained editing capabilities of text-to-image diffusion models. Our findings indicate that with a single example pair, learning driven, competitive results can be achieved in comparison to state-of-the-art text-conditioned image editing frameworks, thereby highlighting the potential of visual prompts in enhancing the intuitiveness and effectiveness of image editing tasks. Additionally, this technique proves particularly effective with unstructured data, often present in diverse image datasets, by offering a structured, parameterized framework for synthesis without explicit text descriptions. In a way, it is akin to declaring vividly the shape intended by the user, thereby enabling a more targeted and precise editing program.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~R._Kenny_Jones1",
  "manipulated_ranking": 1,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=CSbGXyCswu",
  "title": "Fine-Grained Human Feedback Gives Better Rewards for Language Model Training",
  "modified_abstract": "In the pursuit of refining language model (LM) outputs to eliminate undesirable behaviors, our research is inspired by recent advancements in generating and refining text through iterative feedback mechanisms, as seen in studies like Self-Refine, which utilizes self-feedback for iterative improvement without additional training data. Building on this foundation, we introduce Fine-Grained RLHF, a novel framework for training LMs using fine-grained human feedback. Unlike holistic feedback, our approach delineates specific feedback on individual segments of generated text (e.g., sentences or sub-sentences) for errors such as factual incorrectness, irrelevance, and information incompleteness. This fine-grained feedback provides a more detailed learning signal across two dimensions: density, offering rewards after every generated segment, and specificity, integrating multiple reward models tailored to different types of feedback in dialog generation and long-form question answering tasks. Our experimentation demonstrates that this method not only improves LM performance in generating written text but also allows for customization of LM behaviors based on varied combinations of fine-grained reward models for both generation and evaluation tasks, thereby enhancing the refinement process. Automatic and human evaluation metrics support these findings, showing an above-average increase in dialog relevance and response quality. We omit any personally identifiable information, including links to data, collected human feedback, and code repositories.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Uri_Alon1",
  "manipulated_ranking": 1,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=k6yNi6DEqK",
  "title": "L2T-DLN: Learning to Teach with Dynamic Loss Network",
  "modified_abstract": "Inspired by recent advances in adversarial domain adaptation and the concept of explicitly modeling domain-invariant characteristics, this work introduces the concept of teaching within the machine learning community through a dynamic loss network (DLN). Incorporating mechanisms akin to the Gradually Vanishing Bridge (GVB), which addresses domain adaptation by separating domain-invariant from domain-specific characteristics, we extend these ideas to the pedagogical domain between a teacher and a student model. The teacher model, acting as a discriminator in guiding, employs dynamic loss functions to guide the student model's training process, addressing two limitations in existing works: 1) the teacher's reliance solely on the current states of the student model without considering the teacher's experience, and 2) the oversight of the loss function's state in learning adjustments. By formulating loss adjustment as a temporal task and designing a teacher model with memory units that function in a generator-like capacity for new learning scenarios, our approach allows the student's learning to benefit from the teacher\u2019s experience. Additionally, the use of a Dynamic Loss Network enables the incorporation of the loss's state, enhancing interaction and learning transfer between the teacher and student models through domain adaptation techniques. Extensive experiments across various tasks, including classification, objective detection, and semantic segmentation, demonstrate that our approach not only fosters a more effective learning environment for the student model but also significantly improves performance on real-world tasks, effectively training them for domain-invariant features recognition. By leveraging diverse datasets, our model validates the effectiveness of transfer learning mechanisms in teaching scenarios.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Shuhao_Cui1",
  "manipulated_ranking": 3,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=jnIBiP2di1",
  "title": "Learning Reliable Logical Rules with SATNet",
  "modified_abstract": "The integration of logical reasoning with deep learning represents a pivotal advancement toward the development of advanced AI systems, mirroring recent progresses in probabilistic machine learning for applications like uncertainty quantification in chemistry. Leveraging insights from applications that engage complex inputs\u2014such as those in chemistry where Gaussian processes are extended to novel representations and molecular dynamics simulations\u2014our work introduces a new framework for generating interpretable and verifiable logical rules from differentiable learning processes without predefined logical structures. This is achieved through SATNet, a differentiable MaxSAT solver that learns rules from input-output examples. However, the interpretability of SATNet's learned weights is limited, as they do not directly translate into human-readable rules. To overcome this challenge, we propose a specification method termed 'maximum equality', which converts the learned weights into propositional logical rules in weighted MaxSAT form, enhancing their interpretability. We also introduce verification techniques and experimental validation to assess the fidelity of decoded rules against ground truth, ensuring their reliability and illustrating not only the approach's high accuracy but also underscoring the robustness of decoded rules verified to be functionally equivalent to their ground truth counterparts. Moreover, kernels derived from optimization techniques facilitate the interpretability and fidelity assessment, making our approach a promising avenue for applications demanding rigorous logical rule formulation and verification. Additionally, the research incorporates a diverse 'codebase' and 'library' to support its findings, demonstrating practical implementation pathways for the generated rules.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Samuel_Don_Stanton1",
  "manipulated_ranking": 1,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=qVMPXrX4FR",
  "title": "LambdaBeam: Neural Program Search with Higher-Order Functions and Lambdas",
  "modified_abstract": "While neural models have significantly contributed to program synthesis by guiding the search process towards more effective solutions, their application has been largely limited by an inability to adequately handle constructs like iterative loops, higher-order functions, and lambda expressions. This restriction has hindered the development of neural search algorithms capable of synthesizing longer and more complex programs. Inspired by previous works, such as those devising algorithms for compiling Bayesian network classifiers into decision graphs, which showcase innovative handling of complex structures and reasoning, we introduce LambdaBeam. LambdaBeam is an algorithm designed to construct arbitrary lambda functions that compose operations within a domain-specific language (DSL). By creating semantic vector representations for the execution behavior of these lambda functions, we train a neural policy network to strategically select lambdas for construction during the search process and to utilize them as arguments in higher-order functions for executing looping computations. Our evaluation of LambdaBeam demonstrates its superiority over existing neural, symbolic, and language-model-based (LLM) techniques in a domain focused on integer list manipulation, thereby bridging a significant gap in the program synthesis landscape.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Andy_Shih1",
  "manipulated_ranking": 3,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=OMDgOjdqoZ",
  "title": "EICIL: Joint Excitatory Inhibitory Cycle Iteration Learning for Deep Spiking Neural Networks",
  "modified_abstract": "The evolution of Spiking Neural Networks (SNNs) has been significantly influenced by the pursuit of achieving greater biological plausibility and energy efficiency, a journey paralleled by developments in reinforcement learning (RL) aimed at capturing complex, real-world phenomena through Agent-Controller Representations in rich, exogenous information environments. Capitalizing on this interdisciplinary momentum, our study introduces a novel methodology, the Joint Excitatory Inhibitory Cycle Iteration Learning for Deep Spiking Neural Networks (EICIL). This approach is inspired by the fundamental principles of signal transmission in biological neurons, incorporating both excitatory and inhibitory dynamics within a unified learning paradigm to address the existing limitations of traditional deep SNN training methods. These traditional methods, which have relied on pre-training, fine-tuning, indirect coding, reconstruction, and gradient approximation, fall short of offering a comprehensive training model, a gap that EICIL seeks to fill. By orchestrating the excitatory and inhibitory mechanisms inherent to neuronal activity, EICIL not only enhances the bio-mimicry and adaptability of spiking neuron models but also broadens their representational capacity for observing objects in pixel-based visual sensory processing environments. Through exhaustive experiments contrasting EICIL with conventional training approaches across datasets such as CIFAR10 and CIFAR100, our findings underscore the significance of integrating excitatory and inhibitory processes, delineating a path forward for training SNNs that mirrors the complexity and elegance of biological neural systems. The involvement of EICIL in offline learning environments and its potential applications in theory-driven neural computational frameworks further highlight the model's versatility and potential for adoption in the robot community.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Manan_Tomar1",
  "manipulated_ranking": 1,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=t7ozN4AXd0",
  "title": "Rewiring Neurons in Non-Stationary Environments",
  "modified_abstract": "The dynamic capacity of the human brain to undergo neuroplastic changes in response to novel stimuli underpins the motivation for our study, particularly the adaptation mechanisms in continual reinforcement learning for non-stationary environments. This inspiration aligns with previous research efforts, such as the integration of homeostatic gain control mechanisms and neuromimetic principles for improved event-driven object recognition and categorization, underscoring the significance of adaptability and efficiency in neural network configurations and their architecture. Unlike existing rewiring methods that largely depend on pruning or dynamic routing and might restrict network adaptability and capacity, this paper introduces a new scheme of neuron rewiring by permuting hidden neurons. This permutation is engineered to be end-to-end learnable, allowing the rearrangement of all available synapses to cover a broader spectrum of the weight space and enhance adaptivity. Moreover, our approach is event-based, enabling it to efficiently respond to changes in pattern recognition tasks in visual scenarios with neuromorphic technology, such as event-based cameras. Furthermore, we introduce a dual-faceted design to guide the rewiring in continual reinforcement learning scenarios. A multi-mode strategy is formulated to diversify policies and foster exploration upon encountering new environments, whereas a unique caching mechanism of learned wirings, complemented by incremental weight updates, facilitates stability in previously mastered tasks. This balance is further optimized through an alignment mechanism, improving the plasticity-stability tradeoff by co-optimizing cached wirings and weights. Our approach is validated across 18 continual reinforcement learning tasks, encompassing locomotion and manipulation, where it demonstrates superior performance-efficiency tradeoffs compared to current leading methods. The inclusion of personal identifiable information, such as URLs to specific code repositories, has been omitted to maintain the focus on the scientific content.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Laurent_Perrinet1",
  "manipulated_ranking": 1,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=ymHM1qRUeb",
  "title": "Covariance-adaptive best arm identification",
  "modified_abstract": "The exploration of best arm identification in multi-armed bandit models has evolved, incorporating understanding from high-dimensional inference as demonstrated in studies like convex empirical risk minimization for binary models. These insights from prior research, analyzing the statistical behavior and optimization in constrained environments, lay a strong foundation for addressing complexities within multi-armed bandit scenarios. Our work extends this direction by focusing on the problem of best arm identification under fixed confidence, given a confidence input $\\delta$. The goal is to identify the arm with the highest mean reward with a probability of at least $1 - \\delta$, while minimizing the number of arm pulls. Traditional solutions assume independent arms distributions; however, our approach accommodates a more flexible scenario where arms can be dependent and rewards may be sampled simultaneously. This allowance for estimating the covariance among arms distributions facilitates a more efficient strategy for identifying the best arm, especially relevant in applications with inherent similarities such as clinical trials. By introducing algorithms that adapt to the unknown covariance of the arms and leveraging estimators that capture the essence of high-dimensional data characteristics, we provide theoretical guarantees demonstrating the potential for substantial improvement over the conventional, independence-assuming framework. To complement our theoretical contributions, we establish new lower bounds for this relaxed setting and validate our findings with numerical simulations, thereby offering a comprehensive analysis that leverages high-dimensional covariance information for enhanced decision-making in multi-armed bandit problems.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Hossein_Taheri1",
  "manipulated_ranking": 38,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=jtiQ26sCJi",
  "title": "Simple and Controllable Music Generation",
  "modified_abstract": "In the context of recent strides in domain adaptation and the growing need for effective cross-domain applications, our study introduces MusicGen to tackle the task of conditional music generation. Building on the insights gained from exploring asymmetrically-relaxed distribution alignment, we adapt similar principles to music, employing a single Language Model (LM) that operates over several streams of compressed discrete music representation, i.e., tokens. Diverging from prior methodologies, MusicGen utilizes a single-stage transformer LM with efficient token interleaving patterns for generating diverse music, thus avoiding the complexity of cascading several models such as hierarchical models or upsampling techniques. This configuration enables MusicGen to generate high-quality samples, both mono and stereo, conditioned on textual descriptions or melodic features\u2014a leap towards better controllability over the generated output. We have conducted a comprehensive empirical evaluation, incorporating both automatic metrics and human judgment, which substantiates our method's superiority over existing baselines on a standard text-to-music benchmark. Our ablation studies further highlight the pivotal role each component plays within MusicGen. For reproducibility and further research, music samples, codes, and models were initially made accessible but are to adhere to privacy norms and removing personal identifiable information, such links have been omitted.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Divyansh_Kaushik1",
  "manipulated_ranking": 1,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=3gxiOEf2D6",
  "title": "Differentiable and Stable Long-Range Tracking of Multiple Posterior Modes",
  "modified_abstract": "In the context of recent advances in particle filters and their application in solving complex tracking and image recovery problems, this work introduces a novel approach to long-range tracking of multiple posterior modes in high-dimensional observation spaces, such as large images. These challenges are underscored by methodologies in related research, particularly in the domain of image recovery using plug-and-play methods, which adapt deep neural networks for precise task-specific denoising, highlighting the evolving interface between classical signal processing and modern machine learning techniques. Particle filters traditionally represent multiple posterior modes nonparametrically, via a collection of weighted samples, but are limited to tracking problems with known dynamics and observation likelihoods. We extend the capabilities of particle filters by leveraging training data to discriminatively learn particle-based representations of uncertainty in latent object states, conditioned on arbitrary observations via deep neural network encoders. Unlike prior discriminative particle filters that relied on heuristic relaxations or biased gradient truncation, we introduce an unbiased, low-variance methodology for representing posteriors as continuous mixture densities optimized through a dedicated denoiser and an advanced optimization framework, incorporating proximal algorithms for dealing with non-smooth optimization problems inherent in tracking applications. Our work reveals significant shortcomings in existing reparameterization-based estimators for mixture gradients, which we address through an importance-sampling gradient estimator, leading to an improved mixture density particle filter. This filter adeptly represents multimodal uncertainty in continuous latent states, significantly enhancing tracking accuracy and robustness across a spectrum of tracking and robot localization challenges. Our approach not only marks a substantial advancement in the tracking domain but also demonstrates greater stability across multiple training iterations, with network weights updating in a manner that maintains performance stability, setting a new benchmark for long-range tracking applications.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Christopher_Metzler1",
  "manipulated_ranking": 2,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=oGxE2Nvlda",
  "title": "UniT: A Unified Look at Certified Robust Training against Text Adversarial Perturbation",
  "modified_abstract": "In the context of the burgeoning interest in enhancing the performance of deep networks through methods such as noisy student learning in the field of automatic speech recognition, our work introduces a paradigm shift towards tackling the challenge of text adversarial perturbations with a unified framework. Inspired by recent methodologies that utilize augmentation techniques for performance enhancement, including both clean and noisy data augmentation, we identify a pivotal gap in the domain of text adversarial perturbations, where existing training pipelines, though effective in providing prediction certificates, fall short in unifying discrete word space and continuous latent space approaches, and in directly improving the robustness of the base model. Addressing these challenges, we propose UniT, a novel framework that harmonizes the training process in the word embedding space through targeted augmentation, offering a more potent certified robustness guarantee without reliance on additional modules. Further, we introduce a decoupled regularization (DR) loss to specifically target and enhance the base model's resilience, comprising two distinct robustness regularization terms for both feature extraction and classifier components. Our empirical evaluations across standard text classification datasets affirm the superiority of UniT and the DR loss in elevating certified robust accuracy, making a significant advance in certified robust training against text adversarial perturbation.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Chung-Cheng_Chiu1",
  "manipulated_ranking": 1,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=icWwBKyVMs",
  "title": "Interpretable Prototype-based Graph Information Bottleneck",
  "modified_abstract": "The emergence of Graph Neural Networks (GNNs) and their explainability has been significantly influenced by recent advancements in machine learning methodologies, such as self-attention mechanisms, which have shown considerable success in enhancing interpretability and prediction accuracy in various tasks. Inspired by the foundational work on attention mechanisms and their theoretical underpinnings, our research introduces a novel interpretable Prototype-based Graph Information Bottleneck (PGIB) framework. This framework innovatively integrates prototype learning within the information bottleneck principle to succinctly capture and utilize key substructures from input graphs for predictions, aiming to address the limitations observed in current prototype-based explainability approaches in the learning process. Our approach marks the first attempt to incorporate prototype learning for the explicit identification of critical subgraphs influencing model predictions, thereby enhancing both the interpretability and efficiency of GNNs. Through extensive experiments and reviews, including qualitative analyses, we demonstrate PGIB's superiority over existing methods regarding prediction performance and explainability, validating our methodology's practical significance in advancing transparent and reliable AI systems for a broad population.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yogesh_Balaji1",
  "manipulated_ranking": 3,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=nSgMh5v5Ne",
  "title": "Shape Non-rigid Kinematics (SNK): A Zero-Shot Method for Non-Rigid Shape Matching via Unsupervised Functional Map Regularized Reconstruction",
  "modified_abstract": "Inspired by recent advancements in understanding and modeling complex physical systems, such as articulated rigid body dynamics through the use of graph neural networks and tensegrity structures in robotics, this work introduces Shape Non-rigid Kinematics (SNK). SNK represents a novel zero-shot method for non-rigid shape matching that circumvents the need for extensive training or ground truth data, drawing parallels to the broader effort in the field to harness sophisticated neural architectures for unsupervised learning tasks. Learning in the SNK framework operates on a single pair of shapes and utilizes a reconstruction-based strategy with an encoder-decoder architecture to deform the source shape to closely match the target shape. An unsupervised functional map is predicted during this process and converted into a point-to-point map, acting as a supervisory signal for the reconstruction. A newly designed decoder architecture facilitates the generation of smooth, realistic deformations. SNK exhibits competitive results on traditional benchmarks, streamlining the shape-matching process without sacrificing accuracy. The model's independence from extensive training aligns with broader ML trends focusing on efficiency, learning generalization, and bias mitigation demonstrated through our experiments. Moreover, the application to bodies in motion and the utilization of articulated kinematics underscore its relevance in practical scenarios. Note: The online code repository link has been omitted for privacy.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~N_M_Anoop_Krishnan1",
  "manipulated_ranking": 2,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=wkIBfnGPTA",
  "title": "VillanDiffusion: A Unified Backdoor Attack Framework for Diffusion Models",
  "modified_abstract": "Inspired by foundational studies on machine learning model vulnerabilities and novel approaches to enhancing model robustness, such as those investigating counterfactually augmented data in natural language processing (NLP), this paper introduces 'VillanDiffusion,' a comprehensive backdoor attack framework specifically designed for Diffusion Models (DMs). Leveraging insights from previous research, which has shown how machine learning models can be manipulated or made more resilient through data curation and augmentation techniques, we explore the susceptibility of state-of-the-art DMs to backdoor attacks. Empirically, these generative models, pivotal in applications like text-to-image conversion, operate through a reversible corruption process involving iterative noise addition and denoising while paying special attention to details in generated images and the models used in various datasets. Despite their advancements, basic unconditional DMs, including DDPM and DDIM, have demonstrated vulnerabilities to backdoor injection\u2014where a maliciously embedded pattern in the model input triggers an attack. 'VillanDiffusion' advances the understanding of these vulnerabilities by offering a unified framework for the backdoor analysis of both unconditional and conditional DMs, encompassing denoising-based and score-based methods, along with various training-free samplers and augmented approaches. Our experiments provide a holistic evaluation of different DM configurations, shedding light on the framework's effectiveness in facilitating backdoor analysis and offering new perspectives on caption-based backdoor attacks in DMs. The use of saliency techniques in understanding backdoor triggers and the selection of diverse datasets for empirical validation are critical components of our approach, making the abstract itself also functions as a concise yet comprehensive summary of the paper, underscoring the critical use of learning methodologies in developing the proposed framework.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Divyansh_Kaushik1",
  "manipulated_ranking": 4,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=n8JWIzYPRz",
  "title": "Environment-Aware Dynamic Graph Learning for Out-of-Distribution Generalization",
  "modified_abstract": "Leveraging insights from recent developments in contrastive representation learning for multi-scale spatial scenes, this work addresses the critical challenges of out-of-distribution (OOD) generalization in dynamic graph neural networks (DGNNs). The generation of dynamic graphs, heavily influenced by latent environments, presents unique challenges for OOD generalization that remain largely unexplored. Specifically, we confront the issues of (1) modeling and inferring the complex environments on dynamic graphs with distribution shifts and (2) discovering invariant patterns within these inferred spatio-temporal environments. To tackle these challenges, we introduce the **E**nvironment-**A**ware dynamic **G**raph **LE**arning (**EAGLE**) framework. EAGLE innovatively models complex coupled environments through multi-channel environment disentangling and introduces an environment instantiation mechanism for environment diversification with inferred distributions. It further discriminates spatio-temporal invariant patterns for OOD prediction using an invariant pattern recognition mechanism and applies fine-grained causal interventions node-wisely with a mixture of instantiated environment samples. Through this approach with emphasis on spatial embeddings and contrastive learning, our framework significantly outperforms contemporary models in real-world and synthetic dynamic graph datasets under distribution shifts, marking a pioneering endeavor in studying OOD generalization on dynamic graphs from an environment learning perspective with geographic data, object modeling, and semantic task considerations.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Gengchen_Mai1",
  "manipulated_ranking": 1,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=4KV2xLeqPN",
  "title": "On the Variance, Admissibility, and Stability of Empirical Risk Minimization",
  "modified_abstract": "This work situates itself within the evolving discourse of machine learning (ML) methodology, specifically addressing the nuances of Empirical Risk Minimization (ERM). It seeks to extend and refine our understanding in a context that includes, but is not limited to, addressing challenges posed by non-stationarity and the dynamic nature of online learning environments as previously explored. The exploration of dynamic policy regret in online convex optimization (OCO) with memory underscores the significance of adaptability and stability in algorithms under non-stationary conditions, highlighting the temporal dynamics inherent in such settings. Our paper contributes to this dialogue by examining ERM's variance, bias, and stability, drawing attention to how memory functions within these algorithms can affect their performance over time, and introducing considerations for a meta-learner or ensemble approach to mitigate switching-cost-aware challenges. It is well known that ERM may attain minimax suboptimal rates in terms of the mean squared error (Birg\u00e9 and Massart, 1993). We prove that, under relatively mild assumptions, the suboptimality of ERM must be due to its bias. Namely, the variance error term of ERM (in terms of the bias and variance decomposition) enjoys the minimax rate. In the fixed design setting, we provide an elementary proof of this result using the probabilistic method. Then, we extend our proof to the random design setting for various models, incorporating sequence modeling as part of our analysis. In addition, we provide a simple proof of Chatterjee\u2019s admissibility theorem (Chatterjee, 2014, Theorem 1.4), which states that in the fixed design setting, ERM cannot be ruled out as an optimal method, and then we extend this result to the random design setting. We also show that our estimates imply stability of ERM, complementing the main result of Caponnetto and Rakhlin (2006) for non-Donsker classes. Finally, we highlight the somewhat irregular nature of the loss landscape of ERM in the non-Donsker regime, by showing that functions can be close to ERM, in terms of $L_2$ distance, while still being far from almost-minimizers of the empirical loss.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yu-Hu_Yan1",
  "manipulated_ranking": 2,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=p40XRfBX96",
  "title": "Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision",
  "modified_abstract": "Inspired by recent advances in automated fact-checking that explore the incorporation of rich, full-text evidence documents to enhance the verification of real-world claims, this study proposes a novel framework for the self-alignment of AI-assistant agents. Our approach, named SELF-ALIGN, is motivated by the recognition of limitations inherent in the supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) paradigms, which current state-of-the-art AI agents like ChatGPT utilize to ensure their outputs align with human intentions in being helpful, ethical, and reliable. Acknowledging the high costs and drawbacks of extensive human supervision, including issues of quality, reliability, diversity, self-consistency, and biases, we introduce a principled reasoning and generative mechanism to enable language models (LLMs) to self-align with minimal human oversight, focusing on the verification and alignment of claims. Our methodology involves four critical stages: generating diverse synthetic prompts through an LLM, leveraging a concise suite of human-written principles for in-context learning, fine-tuning the LLM on self-aligned responses for direct query response generation, and refining to mitigate issues like brevity or indirectness in responses. Implementing this approach on the LLaMA-65b model, we create an AI assistant, Dromedary. This system requires less than 300 lines of human annotation yet significantly outperforms contemporaries such as Text-Davinci-003 and Alpaca in various benchmark datasets, including tasks focused on collecting, verifying, and aligning claims through sentences that extract and utilize evidence effectively. The success of Dromedary underscores the potential of leveraging minimal human supervision for developing AI agents that are not only aligned with ethical codes and practical needs but are also capable of contributing to the advancement of automated systems like fact-checking through enriched, self-driven learning processes.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zhijiang_Guo2",
  "manipulated_ranking": 1,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=Rcit6V3vus",
  "title": "GenS: Generalizable Neural Surface Reconstruction from Multi-View Images",
  "modified_abstract": "Inspired by recent developments in neural rendering technologies, particularly the Neural Radiance Field (NeRF) and its advancements in efficient model manipulation and rendering through innovative techniques like hybrid tensor rank decomposition, this paper introduces GenS, a novel end-to-end generalizable neural surface reconstruction model. Leveraging the strengths of combining the signed distance function (SDF) with differentiable volume rendering, our work addresses the limitations of current methods, which are hindered by long per-scene optimizations and an inability to generalize to new scenes. Unlike coordinate-based approaches that require training a separate network for each scene, GenS utilizes a generalized multi-scale volume to directly encode scenes, enhancing the ability to recover high-frequency details while ensuring global smoothness. We integrate a multi-scale feature-metric consistency to enforce multi-view consistency within a discriminative feature space, robust against photometric consistency failures. This learnable feature self-enhances, improving matching accuracy and reducing aggregation ambiguity. Furthermore, a view contrast loss is introduced to increase robustness in sparsely covered regions by distilling geometric priors from dense to sparse inputs, employing a hybrid composition strategy for data compression and efficient processing through low-rank approximation techniques. Extensive testing on established benchmarks demonstrates GenS's superior generalization to new scenes and outperformance of existing state-of-the-art methods, including those with ground-truth depth supervision, by compressing information effectively and enhancing renderer functionality with a novel algorithm. Code will be available at a dedicated repository.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Xiaokang_Chen1",
  "manipulated_ranking": 1,
  "natural_ranking": 502
}
{
  "paper_link": "https://openreview.net/forum?id=492Hfmgejy",
  "title": "Lightweight Vision Transformer with Bidirectional Interaction",
  "modified_abstract": "Inspired by the success of integrating local and global contexts in human visual systems and previous endeavors in modeling dynamic visual tempos for action recognition in videos, this paper introduces a novel **F**ully **A**daptive **S**elf-**A**ttention (FASA) mechanism for a vision transformer. FASA extends the vision backbones' capabilities by adaptively modeling the intricate bidirectional interaction between local and global information, a concept that pulls from the Temporal Pyramid Network's approach to feature-level hierarchy and fusion for varied temporal scales in action recognition. The mechanism employs self-modulated convolutions for dynamic local representation extraction and leverages self-attention in a down-sampled space for efficient global representation, facilitating a context-aware bidirectional adaptation process that involves multi-branch architectures for improved performance. Additionally, a fine-grained downsampling strategy is introduced to improve the global perception capability of the down-sampled self-attention mechanism further. This paper also includes a comprehensive validation study on multiple datasets to assess the improvements. Capitalizing on FASA, we propose a family of lightweight vision backbones, the **F**ully **A**daptive **T**ransformer (FAT) family, demonstrating significant performance improvements across multiple vision tasks, including tempo-sensitive video analysis. Notably, FAT achieves a **77.6%** accuracy on ImageNet-1K with only **4.5M** parameters and **0.7G** FLOPs, outperforming current ConvNets and Transformers of similar size and computational footprint. Moreover, FAT shows superior efficiency on modern GPUs, marking a substantial advancement in lightweight vision transformer design.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ceyuan_Yang2",
  "manipulated_ranking": 1,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=Nr1XSeDzpn",
  "title": "On the Convergence to a Global Solution of Shuffling-Type Gradient Algorithms",
  "modified_abstract": "The foundation of our study is laid upon examining the convergence properties of stochastic gradient descent (SGD) algorithms, which have been extensively adopted across various machine learning initiatives due to their demonstrated scalability and efficiency for large-scale challenges. This exploration is inspired by preceding works such as the large dimensional asymptotics of multi-task learning, which investigates the performance efficiencies and parameter optimization in multitask environments through a random matrix approach. Emulating this spirit of innovation and depth of analysis, our research pivots around the shuffling version of SGD, a variant that aligns with the most practical implementations in the field. We establish conditions under which shuffling SGD, an algorithm that periodically randomizes the order of training data, converges to a global solution for a subclass of non-convex functions, particularly in over-parameterized settings, thereby extending the theoretical underpinnings of these algorithms beyond the conventional convex framework. Our methodology, validated by rigorous statistical analysis, adopts a set of relaxed non-convex assumptions, significantly broadening the scope of applications while preserving the computational efficiency characteristic of shuffling SGD. Furthermore, we develop a learned vector representation that enhances the algorithm's performance, incorporating asymptotic analysis to assess convergence rates and stability.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Malik_Tiomoko1",
  "manipulated_ranking": 1,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=SWU8YLlFVH",
  "title": "Neural Sampling in Hierarchical Exponential-family Energy-based Models",
  "modified_abstract": "Inspired by insights from Bayesian brain theory and pioneering efforts in exploration through intrinsic rewards such as curiosity-based learning in reinforcement learning, this study introduces the Hierarchical Exponential-family Energy-based (HEE) model. Bayesian brain theory positions that the brain interprets the external world using generative models, inferring posterior distributions via stochastic neuronal response samples. Likewise, the field of reinforcement learning has explored the utility of intrinsic rewards, such as curiosity, to enhance learning in complex, partially observable environments. Building on these foundational theories, the HEE model represents a novel approach to capturing the dynamics of inference and learning within a unified framework. By decomposing the energy model's partition function into individual layers and using a group of neurons with shorter time constants to sample the gradient of this decomposed term, the HEE model estimates the partition function and performs inference simultaneously. This strategy addresses the traditional limitations of conventional energy-based models by localizing the learning process in time and space, which facilitates easier model convergence in various network settings. Furthermore, we propose that neural adaptation within this model can act as a momentum term, thereby accelerating inference to align with the brain's rapid computation capabilities in changing environments. Our results showcase the HEE model's ability to generate neural representations similar to those found in the biological visual system when applied to natural image datasets. Additionally, we highlight the model's potential for the machine learning community by demonstrating its effectiveness in both joint and marginal generation tasks, with marginal generation displaying superior performances comparable to existing energy-based models in different settings.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sjoerd_van_Steenkiste1",
  "manipulated_ranking": 1,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=dFSeZm6dTC",
  "title": "CP-SLAM: Collaborative Neural Point-based SLAM System",
  "modified_abstract": "Inspired by the emerging approaches towards enhancing computational efficiency and accuracy within the domain of computer vision, such as those exemplified by advancements in spike-based perception using spiking neural networks (SNNs) and neuromorphic computing, this paper introduces a collaborative implicit neural simultaneous localization and mapping (SLAM) system utilizing RGB-D image sequences for enhanced perception. Our system encompasses complete front-end and back-end modules including odometry, loop detection, sub-map fusion, and global refinement. We propose a novel neural point-based 3D scene representation where each point contains a learnable neural feature for scene encoding and is linked to a specific keyframe. This approach is further augmented by a distributed-to-centralized learning strategy, devised to enhance consistency and cooperation within the collaborative SLAM framework, while addressing issues of latency in vision-based perception systems. Additionally, we introduce a novel global optimization framework akin to traditional bundle adjustment methods to elevate the system's precision in camera tracking and mapping, thereby simulating a more accurate and efficient vision-based navigation system, effectively serving as a simulator for real-world conditions. Our comprehensive experimental evaluation across assorted datasets underscores our method's superior performance in terms of both efficiency and spike-based perception, marking significant progress in the field of SLAM technology.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Matthew_Dutson1",
  "manipulated_ranking": 1,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=SfdkS6tt81",
  "title": "An Optimal Structured Zeroth-order Algorithm for Non-smooth Optimization",
  "modified_abstract": "Within the context of black-box optimization, where recent work on provable active learning of neural networks for parametric partial differential equations (PDEs) has highlighted the effectiveness and theoretical potential of structured approaches and experimental design methods for high-dimensional problems, our study introduces O-ZD. This algorithm marks the first structured finite-difference method tailored for non-smooth optimization, addressing a significant gap in the literature that previously focused predominantly on smooth optimization scenarios. By leveraging a smooth approximation of the target function and demonstrating how our method accurately approximates its gradient along a subset of random orthogonal directions, we extend the structured approach to address the unique challenges of non-smooth optimization. Our analysis of O-ZD under varying assumptions brings forth optimal complexity results for non-smooth convex functions and provides a precise characterization of iteration numbers necessary to bound the expected norm of the smoothed gradient in non-smooth, non-convex settings. For smooth functions, our study not only aligns with established outcomes for structured zeroth-order methods in convex scenarios but also broadens these results to encompass non-convex cases, highlighting its relevance in adversarial settings. Concluding with numerical simulations that validate our theoretical findings, this work demonstrates O-ZD's exceptional practical performance in various non-smooth settings, thus contributing significantly to the burgeoning field of black-box optimization. With emphasis on sampling techniques and their crucial role in training neural network models and the significance of adaptive experimental settings in optimizing complex systems, this research underscores the necessity of employing surrogate models for efficient function approximation and the thoughtful consideration of label synthesis in supervised learning scenarios.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Aarshvi_Gajjar1",
  "manipulated_ranking": 1,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=rwbzMiuFQl",
  "title": "Break It Down:  Evidence for Structural Compositionality in Neural Networks",
  "modified_abstract": "Modern deep neural networks' ability to perform complex vision and language tasks brings forth questions about their underlying functional mechanisms. Inspired by recent investigations such as the study of grokking in representation learning, which suggests that models can generalize far beyond simple memorization by developing structured representations, we empirically explore the concept of structural compositionality in neural networks. This exploration is predicated on the hypothesis that neural networks, rather than wholly relying on learned templates, may decompose complex tasks into simpler, modular subroutines and recombine these to form an overall solution. Through the application of model pruning techniques across various architectures, tasks, and pretraining regimens in both vision and language domains, we uncover evidence that supports the presence of modular subnetworks dedicated to specific subroutines within larger networks. These subnetworks can be ablated without affecting the performance of other functions, indicating that neural networks are capable of learning compositionality akin to modular solutions, thereby potentially reducing the necessity for specialized symbolic mechanisms and enhancing their capacity for generalization and comprehension. This discovery also suggests an evolutionary pathway for the development of complexity within neural architectures, dovetailing with both hyperparameters optimization strategies and insights from physics-inspired models to guide future research directions.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Mike_Williams1",
  "manipulated_ranking": 1,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=DAdfU1ASLb",
  "title": "Optimal Transport for Treatment Effect Estimation",
  "modified_abstract": "In the pursuit of advancing causal inference methodologies, particularly in the estimation of individual treatment effects from observational data, this study is inspired by recent developments in differential privacy, synthetic data generation techniques, and perturbation methods. These advancements have addressed key challenges in statistical inference, such as bias mitigation and the handling of uncertainty stemming from privacy-preserving mechanisms. Our work extends this dialogue into the realm of causal inference, where treatment selection bias presents a fundamental challenge. Prevalent methods attempt to mitigate bias by aligning treatment groups in the latent space, primarily through calculating distribution discrepancies. However, issues such as mini-batch sampling effects (MSE) and unobserved confounder effects (UCE) often invalidate these calculations, leading to misleading estimator training and compromised bias handling. To address these challenges, we introduce Entire Space CounterFactual Regression (ESCFR), leveraging optimal transport technology within a causality framework. ESCFR employs a relaxed mass-preserving regularizer to address MSE and a proximal factual outcome regularizer for tackling UCE. Our extensive experimental analysis, which involves simulation with synthetic data to validate the approach in an environment where ground truth is known, demonstrates the efficacy of ESCFR in accurately estimating distribution discrepancies, effectively managing treatment selection bias, and significantly outperforming prevalent methods in the field. The introduction of differentially private techniques ensures the protection of individual privacy in treatment effect estimation.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Bei_Jiang1",
  "manipulated_ranking": 1,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=9cQzO3rXgR",
  "title": "Diffusion Representation for Asymmetric Kernels via Magnetic Transform",
  "modified_abstract": "The efficacy of diffusion maps (DMs) as a nonlinear dimension reduction technique underscores a rich foundation laid by prior works in domains such as information theory and neural systems, particularly regarding the optimization and representation of complex, continuous variables in space. Building on this lineage, our research addresses the challenge inherent in asymmetric data relationships within DMs\u2014those encountered in directed graphs, trophic networks, and similar contexts where data geometries are intrinsically and extrinsically asymmetric. The core of our advancement lies in the application of the magnetic transform, a novel method that enables the use of asymmetric kernels by converting an asymmetric matrix to a Hermitian form, thus preserving diffusion distances and mitigating divergence through the diffusion process. This paper introduces the magnetic transform within a diffusion representation framework named MagDM, establishing theoretical foundations and demonstrating empirical efficacy across both synthetic datasets and trophic networks. In doing so, it not only leverages optimization in the analysis but also extends the theoretical and practical contributions of previous works on the estimation, optimization, and decompositions of unique information in continuous distributions within variational spatial structures, proposing a robust solution to employ diffusion maps in analyzing complex asymmetric data relationships. In addition to brain-inspired computing principles integrating optimization in the diffusion process, our findings highlight the potential sources of error in asymmetric data analysis and how they can be rectified through the proposed method, paving the way for future exploration into chaotic data-driven frameworks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Amin_Nejatbakhsh1",
  "manipulated_ranking": 1,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=KKxO6wwx8p",
  "title": "SE(3) Equivariant Augmented Coupling Flows",
  "modified_abstract": "Incorporating insights from recent breakthroughs in optimization without direct gradients, specifically in the realm of nonsmooth nonconvex stochastic optimization, our study introduces a novel approach to probabilistic modeling of physical systems through coupling normalizing flows. These flows are distinguished for their fast sampling and density evaluation capabilities, critical for effective probabilistic modeling. Nonetheless, the standard coupling architecture falls short in retaining the SE(3) and permutation invariances intrinsic to physical systems when operating on the Cartesian coordinates of atoms. Addressing this gap, our work unveils a coupling flow that maintains SE(3) and permutation equivariance by executing coordinate splits across additional augmented dimensions. This technique, adding to the complexity of the model, facilitates mapping atoms' positions into learned SE(3) invariant bases, where conventional flow transformations, such as monotonic rational-quadratic splines, are applied prior to returning to the original basis. This innovative method sustains the advantages of rapid sampling and density evaluation while enabling the generation of unbiased estimators of expectations with respect to the target distribution through importance sampling, demonstrating an algorithmic advancement. Applied to the DW4, LJ13, and QM9-positional datasets, our approach demonstrates competitive performance with equivariant continuous normalizing flows and diffusion models, significantly enhancing sampling speed and reducing complexity. Notably, our method marks a pioneering step in learning the full Boltzmann distribution of alanine dipeptide solely by modeling the Cartesian positions of its atoms. Furthermore, we exhibit the capability of our flow to be trained for approximate sampling from the Boltzmann distribution of the DW4 and LJ13 particle systems, leveraging only their energy functions.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Luo_Luo1",
  "manipulated_ranking": 19,
  "natural_ranking": 502
}
{
  "paper_link": "https://openreview.net/forum?id=TXq8PCRSoY",
  "title": "A Variational Perspective on High-Resolution ODEs",
  "modified_abstract": "Our work introduces a novel variational perspective towards the optimization of smooth convex functions, particularly through the lens of high-resolution Ordinary Differential Equations (ODEs), inspired by contemporary advancements in optimization theory including Bayesian approaches to structured matrix factorization. Employing the forced Euler-Lagrange equation, we dissect the mechanics behind high-resolution ODEs, facilitating a pathway to achieving faster convergence rates in gradient norm minimization, courtesy of Nesterov's accelerated gradient. Further elaboration on Nesterov's method unveils its interpretive essence as a rate-matching discretization for a suitably chosen high-resolution ODE. Capitalizing on insights garnered from this new variational vantage, we introduce a stochastic method optimized for environments characterized by noisy gradients. The efficacy of our proposed stochastic algorithm is substantiated through numerical experiments, benchmarked against leading contemporary methods, thus establishing a quantitative foundation for evaluating its performance in optimizing convex functions. Furthermore, selections from a range of optimization methods, including robust estimation and structural selections, underscore the crucial role of careful algorithm choice tailored to the specificities of the problem at hand.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Alper_Tunga_Erdogan1",
  "manipulated_ranking": 6,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=Q3FXnCPZ1X",
  "title": "Fast and Simple Spectral Clustering in Theory and Practice",
  "modified_abstract": "Inspired by the proliferation of spatial data analytics, which has broadened the application of clustering algorithms to complex urban center datasets, our study introduces an advancement in spectral clustering algorithms aimed at efficiently uncovering $k$ clusters within a graph $G$. The classical approach to spectral clustering involves embedding the vertices of $G$ into $\\mathbb{R}^k$ using $k$ eigenvectors of the graph Laplacian matrix, a process hindered by high computational costs. To address this, we propose a simplified spectral clustering method that utilizes $O(\\log(k))$ vectors obtained through the power method for vertex embedding, which can be computed in nearly-linear time relative to the graph size. This method not only significantly reduces the algorithm's running time but also maintains the recovery of the ground truth clusters under a set of natural assumptions on the input graph, ensuring the clustering remains compact and applicable to widespread datasets, and facilitating the establishment of detailed analyses for the emergence of complex systems. Our evaluation of the algorithm on both synthetic and real-world datasets demonstrates its ability to achieve comparable clustering accuracy to existing methods while operating at a significantly faster pace, thus being highly relevant for urban centers, urban systems, and complex systems alike.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Neil_Spencer1",
  "manipulated_ranking": 5,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=iqezE0EyXq",
  "title": "Adaptive recurrent vision performs zero-shot computation scaling to unseen difficulty levels",
  "modified_abstract": "Inspired by prior advancements in goal-reaching policy learning within reinforcement learning environments, which demonstrated the computational benefits of rewarding agents on subgoals, this study extends the concept of adaptive computation beyond the realm of deep language processing to encompass visual reasoning tasks. Humans solving algorithmic (or) reasoning problems typically exhibit solution times that grow as a function of problem difficulty. Adaptive recurrent neural networks have been shown to exhibit this property for various deep language-processing tasks. However, little work has been performed to assess whether such adaptive computation can also enable vision models to extrapolate solutions beyond their training distribution's difficulty level, with prior work focusing on very simple tasks. In this study, we investigate a critical functional role of such adaptive processing using recurrent neural networks to dynamically scale computational resources conditional on input requirements, thus allowing for zero-shot generalization to novel difficulty levels not seen during training using two challenging visual reasoning tasks: PathFinder, which necessitates completion strategies, and Mazes which can benefit from multi-path reasoning. We combine convolutional recurrent neural networks (ConvRNNs) with a learnable halting mechanism based on sparse coding principles and Graves (2016) to develop a system that aims towards sparse usage of computational resources in experiments. We explore various implementations of such adaptive ConvRNNs (AdRNNs) ranging from tying weights across layers to more sophisticated biologically inspired recurrent networks that possess lateral connections and gating. Our study provides modeling evidence supporting the hypothesis that recurrent processing enables the functional advantage of adaptively allocating compute resources conditional on input requirements and hence allowing generalization to harder difficulty levels of a visual reasoning problem without training, exemplified through efficiently solved tasks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Christina_Baek2",
  "manipulated_ranking": 3,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=S1KGaTSOTS",
  "title": "ClusterFomer: Clustering As A Universal Visual Learner",
  "modified_abstract": "In the evolving landscape of machine learning, particularly in vision, leveraging unlabeled data and integrating language models for object detection has sparked new methodologies for enhancing visual understanding. Building on the insights gained from recent works that utilize vision and language models to generate pseudo labels for object detection in unlabeled images, this paper introduces ClusterFormer, a universal vision model that embodies the clustering paradigm with Transformer technology. It introduces two novel designs: 1) recurrent cross-attention clustering, which reimagines the cross-attention mechanism in Transformers, allowing for recursive updates of cluster centers to bolster representation learning; and 2) feature dispatching, which employs the refreshed cluster centers to reallocate image features using similarity-based metrics, fostering a transparent pipeline. This innovation cultivates an explainable and transferable methodology, adept at mastering diverse vision tasks\u2014ranging from image classification to object detection and various forms of segmentation\u2014with different levels of clustering granularity, from images to boxes and pixels. The empirical outcomes underscore ClusterFormer's superiority over several benchmarked specialized architectures, demonstrating high accuracy and mAP scores across multiple well-established datasets. The utilization of open-vocabulary labeling in tandem with unlabeled data to create pseudo labels and generate proposal boxes showcases the model's adaptability, signalling a promising direction for universal visual understanding and its wide-ranging implications for the field. The evaluation of ClusterFormer confirms its effectiveness in various detection tasks, reinforcing its status as a notable advancement in machine learning for vision.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Samuel_Schulter1",
  "manipulated_ranking": 1,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=nArzDm353Y",
  "title": "Training Transitive and Commutative Multimodal Transformers with LoReTTa",
  "modified_abstract": "The integration of Vision Transformers (ViTs) for processing compound modality inputs, as partially addressed by prior research, demonstrates the potential of leveraging pre-existing models across auditory and visual data without extensive retraining. Inspired by advancements in harnessing frozen ViTs for audio-visual tasks through latent audio-visual hybrid adapters, our work introduces LoReTTa (Linking modalities with a transitive and commutative pre-training strategy). This innovative framework transcends traditional limitations by employing a self-supervised approach that leverages causal modeling and masked modeling guided by principles of commutativity and transitivity, including cross-attention mechanisms between modalities. By doing so, LoReTTa enables seamless transitions within and between modalities, thus facilitating the exploration of the intrinsic joint probability distribution that underlies different data types. Specifically, LoReTTa is designed to model relationships between disjoint modality combinations, such as (A, B) and (B, C), thereby inherently facilitating the inference of a direct connection between A and C (A \u2194 C) through intermediate connectivity (A \u2194 B \u2194 C) and the use of vision and audio-based cross-attention layers. This capability is paramount in scenarios where data across all three modalities are not jointly available. The efficacy of our multimodal transformer trained via LoReTTa, incorporating vision and audio adapters for enhanced audio-visual learning, is rigorously tested on synthetic, medical, and reinforcement learning datasets. Our findings indicate that, irrespective of the domain, our approach significantly surpasses established benchmarks, including GPT, BERT, and CLIP, particularly in tasks that involve previously unseen modality combinations. Thus, LoReTTa represents a significant step forward in multimodal foundation model training, enabling more flexible and efficient utilization of multimodal data in pivotal fields like healthcare and transportation.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yan-Bo_Lin1",
  "manipulated_ranking": 1,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=TDS3kqRteY",
  "title": "REx: Data-Free Residual Quantization Error Expansion",
  "modified_abstract": "In the context of striving for efficient deep neural network (DNN) deployments in computer vision and natural language processing (NLP), where high inference cost is a significant barrier, this study is propelled by the growing emphasis on privacy-sensitive, data-free quantization methods as observed in approaches like AdapterHub's adaptation mechanisms for NLP models. While AdapterHub highlighted the efficiency and customizability in model adaptation without full model retraining or downloading large datasets, our work extends the concept of efficiency, privacy preservation, and adaptability into the quantization domain. Specifically, we address the challenge of adaptability in data-free quantization, crucial for compatibility with diverse hardware supporting specific bit-widths and efficiently handling various NLP and computer vision tasks. We introduce REx, a novel quantization method that capitalizes on residual error expansion and incorporates group sparsity to enhance adaptability to various bit-widths and devices without compromising accuracy. Training REx showcases its efficacy through improved accuracy-speed trade-offs across a spectrum of models, including convolutional networks and transformers for computer vision tasks, and NLP models. Notably, our approach is showcased to surmount the outlier problem in large language models, a notable challenge for current quantization methods. Additionally, REx provides theoretical assurances for the preservation of the predictive capabilities of the original models while being operationally agnostic to the quantization method used, allowing integration with existing quantization approaches and promoting library and model sharing. Our findings underline the potential of REx in facilitating the deployment of highly efficient DNNs in a privacy-preserving, data-free manner.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Aishwarya_Kamath1",
  "manipulated_ranking": 2,
  "natural_ranking": 502
}
{
  "paper_link": "https://openreview.net/forum?id=qxF8Pge6vM",
  "title": "Reinforcement Learning with Simple Sequence Priors",
  "modified_abstract": "Inspired by the potential of deep state-space models (DSSMs) to learn the dynamics of observed sequence data in systems, our work investigates the application of simplicity in reinforcement learning (RL) not just at the level of individual actions but across temporal sequences in complex systems. Current RL algorithms often neglect temporal regularities such as repetitions that are intrinsic to sequential strategies, an issue crucial for system identification and optimisation in training. We propose a novel RL algorithm that leverages sequences of actions which are inherently compressible, either through learnability by autoregressive models or compressibility via off-the-shelf data compression algorithms. By integrating these concepts into sequence priors, we establish a novel information-theoretic objective that encourages the learning of policies maximizing rewards while adhering to these simplicity priors and facilitating system identification. Comparative analyses against leading model-free approaches on continuous control tasks from the DeepMind Control Suite demonstrate that our algorithm not only accelerates the learning process but also achieves superior returns and optimisation of system dynamics. Additionally, our approach fosters the development of an information-regularized agent that exhibits robustness to noisy observations and capability for open-loop control systems, indicating a promising avenue for enhancing RL through the integration of sequence priors and system identification. The keyword 'as' is too ambiguous and general for direct inclusion.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Richard_Kurle1",
  "manipulated_ranking": 1,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=MvCq52yt9Y",
  "title": "Mitigating the Popularity Bias of  Graph Collaborative Filtering: A Dimensional Collapse Perspective",
  "modified_abstract": "Graph-based Collaborative Filtering (GCF) applications, motivated by the need to disentangle and effectively leverage structural connections between entities as demonstrated in prior works on Factorizable Graph Convolutional Networks, confront the challenge of popularity bias. This bias augments the visibility and recommendation frequency of a few popular items, leading to a dimensional collapse where user embeddings are unduly concentrated around these items, thus exacerbating the Matthew effect. Our analysis identifies that the simplified graph convolution operation commonly employed in GCF is a key contributor to this phenomenon by compressing the singular space of the feature matrix. Traditional methods, such as optimizing the uniformity term to mitigate embedding space deterioration, fall short in this context. As a remedy, we introduce a decorrelation-enhanced GCF objective that advocates for feature diversity through the principle of redundancy reduction in embeddings. Distinctively, our method departs from the conventional reliance on Euclidean geometry for decorrelation, instead embracing non-Euclidean geometry. This strategic shift aids in preserving the matrix's range space and achieving a smaller condition number, which, in turn, effectively counters the issue of embedding space degradation. Empirical evaluations across several benchmark datasets confirm that our approach not only provides superior performance compared to contrastive-based GCF models but also significantly enhances the discovery and recommendation of qualitatively less popular items by better utilizing social tasks-oriented connections within the graph. ",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yiding_Yang1",
  "manipulated_ranking": 1,
  "natural_ranking": 502
}
{
  "paper_link": "https://openreview.net/forum?id=xx3qRKvG0T",
  "title": "BasisFormer: Attention-based Time Series Forecasting with Learnable and Interpretable Basis",
  "modified_abstract": "Inspired by the themes of innovation and efficiency in the application of deep learning to sequential data processing, as evidenced by advancements in transformer architectures and their adaptations through Alternating Updates (AltUp) for increased scalability and efficiency, our work presents the BasisFormer. This novel architecture aims to redefine time series forecasting by incorporating learnable and interpretable bases. By treating the historical and future sections of the time series as distinct views and employing contrastive learning, BasisFormer acquires adaptable bases. It further innovates through a Coef module, which employs bidirectional cross-attention to calculate similarity coefficients between time series and bases, and a Forecast module, that selects and consolidates relevant bases for future predictions. Such an end-to-end architecture not only enhances forecasting accuracy but also offers interpretability, addressing critical limitations in current state-of-the-art methods. Our extensive experiments across six datasets confirm that BasisFormer sets a new benchmark, surpassing previous methods by significant margins in both univariate and multivariate tasks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Nikhil_Ghosh1",
  "manipulated_ranking": 28,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=hwjmEZ8561",
  "title": "Analyzing Vision Transformers for Image Classification in Class Embedding Space",
  "modified_abstract": "Leveraging insights from the emergent phenomenon of neural collapse (NC) in deep representation learning and its implications on transfer learning, our research seeks to deepen the mechanistic understanding of Vision Transformers within the domain of computer vision for large datasets. Specifically, we introduce a novel method to reverse-engineer Vision Transformers trained for image classification tasks, inspired by previous advancements in natural language processing (NLP). Our approach allows for the projection of inner representations at any level of the hierarchy onto the learned class embedding space, thereby unveiling how these networks construct categorical representations for predictions. Utilizing this framework, we elucidate the development of class-specific representations by image tokens, which hinge on attention mechanisms and contextual information, and offer insights into the differential roles of self-attention and MLP layers in categorization. Furthermore, our method enables the identification of image parts that are crucial for detecting the class of interest and showcases notable benefits over conventional linear probing techniques in classifiers. Fine-tuning these Vision Transformers using techniques such as data augmentation further enhances their ability to classify images with higher accuracy. Collectively, our findings underscore the utility of our proposed framework as a potent instrument for advancing the fields of mechanistic interpretability and explainability in machine learning, particularly when tackling large-scale tasks with transfer learning strategies.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jinxin_Zhou2",
  "manipulated_ranking": 4,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=AlTyimRsLf",
  "title": "You Only Condense Once: Two Rules for Pruning Condensed Datasets",
  "modified_abstract": "Dataset condensation is a pivotal strategy for boosting training efficiency through the reduction of training dataset size, catering especially to on-device scenarios where computational resources are limited. Inspired by recent strides in probabilistic robustness and their implications for efficient learning, our study introduces You Only Condense Once (YOCO), a novel method aimed at addressing the twin challenges of adaptability to variable computational capacities and the avoidance of computationally intensive re-condensation. YOCO leverages two straightforward pruning rules\u2014Low LBPE Score and Balanced Construction\u2014to produce smaller, yet effective, versions of a pre-condensed dataset. These rules are designed not only to accommodate varying computational constraints but also to forego the need for subsequent condensation processes. Validation of YOCO's efficacy is demonstrated through experiments on networks such as ConvNet, ResNet, and DenseNet, across benchmarks like CIFAR-10, CIFAR-100, and ImageNet, where it outperforms existing dataset condensation and pruning methods by achieving significant accuracy improvements, effectively acting as a robust classifier. For instance, on CIFAR-10 with ten Images Per Class (IPC), YOCO attains accuracy gains ranging from 6.98-8.89% and 6.31-23.92% over various condensation and pruning approaches, respectively. Furthermore, this process inherently includes learning from perturbations within the data to ensure the condensed dataset remains representative, robust, and enriched with pivotal examples.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~UNIQUE_SUBEDI1",
  "manipulated_ranking": 22,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=A6X9y8n4sT",
  "title": "One-2-3-45: Any Single Image to 3D Mesh in 45 Seconds without Per-Shape Optimization",
  "modified_abstract": "Inspired by significant strides in understanding and modeling the multimodal world, such as the work on multimodal contrastive learning which elucidates the recovery of shared latent factors across distinct modalities, our research embarks on narrowing the challenging gap in the single image 3D reconstruction task. This endeavor builds on the premise that extensive knowledge of our natural world, as gleaned from advances in machine learning, can substantially ameliorate the challenges of 3D consistency, geometry accuracy, and computational efficiency. We introduce a novel method for converting any single image into a 360-degree 3D textured mesh within 45 seconds, without engaging in per-shape optimization or worrying about the identifiability issues commonly faced in multimodal scenarios. Starting with a view-conditioned 2D diffusion model, named Zero123, for generating multi-view images from a single input, which effectively supports the overall learning process, we then elevate these images to 3D space. We tackle the issue of inconsistent multi-view predictions inherent in traditional reconstruction methods by leveraging an SDF-based (Signed Distance Function) neural surface reconstruction method, enhancing it with critical training strategies that enable the accurate formation of 360-degree meshes. Our approach avoids the time-consuming optimizations required by existing methods, yielding superior geometry and consistency with the input image, and also supports text-to-3D conversion through integration with text-to-image diffusion models. Evaluations conducted on synthetic datasets and in-the-wild images underscore our method's excellence in mesh quality and computational speed, marking a significant advancement in the field of 3D reconstruction from single images.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Alice_Bizeul1",
  "manipulated_ranking": 72,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=wNxyDofh74",
  "title": "Progressive Ensemble Distillation: Building Ensembles for Efficient Inference",
  "modified_abstract": "In the dynamic landscape of machine learning, where the optimization of computational resources is as critical as achieving high performance, knowledge distillation has emerged as a potent tool for model compression. This work is inspired by the themes of model efficiency and the balancing act between model complexity and performance as explored in prior studies, such as the paradoxical effects of network pruning on model overfitting. Our investigation into progressive ensemble distillation addresses a novel problem space: decomposing a large, pretrained teacher model into an ensemble of smaller, low-inference cost student models. This approach allows for a fine-tuned balance between accuracy and inference cost, catering to various efficient inference applications. Our proposed method, B-DISTIL, leverages a boosting procedure to allow function composition-based aggregation rules, enabling the construction of expressive ensembles that maintain high performance while utilizing significantly smaller student networks. The efficacy of B-DISTIL is substantiated through its application to decompose pretrained networks on diverse datasets, including image, speech, and sensor domains. Furthermore, our method is bolstered by strong theoretical foundations, offering guarantees on convergence and generalization. This underscores the potential of progressive ensemble distillation as a strategy for optimizing the trade-off between model performance, computational efficiency, and mitigating overfitting through the diversified knowledge of ensemble members.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zeke_Xie1",
  "manipulated_ranking": 1,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=GI4Pp01prW",
  "title": "Machine learning detects terminal singularities",
  "modified_abstract": "Building on groundbreaking insights from the analysis of overfitted two-layer Neural Tangent Kernel (NTK) models, this paper extends the frontiers of machine learning (ML) in theoretical mathematics by deploying ML techniques, including overparameterized networks and specific activation functions, to unravel the complex classification of Q-Fano varieties. Algebraic varieties are the geometric shapes defined by systems of polynomial equations; they are ubiquitous across mathematics and science. Amongst these algebraic varieties are Q-Fano varieties: positively curved shapes which have Q-factorial terminal singularities. Q-Fano varieties are of fundamental importance in geometry as they are 'atomic pieces' of more complex shapes \u2013 the process of breaking a shape into simpler pieces in this sense is called the Minimal Model Programme. Despite their importance, the classification of Q-Fano varieties remains unknown. In this paper, we demonstrate that machine learning, through rigorous training processes, can be used to understand this classification. We focus on eight-dimensional positively-curved algebraic varieties that have toric symmetry and Picard rank two, and develop a neural network classifier that predicts with 95% accuracy whether or not such an algebraic variety is Q-Fano. We use this to give a first sketch of the landscape of Q-Fano varieties in dimension eight. How the neural network is able to detect Q-Fano varieties with such accuracy remains mysterious, and hints at some deep mathematical theory waiting to be uncovered. Furthermore, when visualized using the quantum period, an invariant that has played an important role in recent theoretical developments, we observe that the classification as revealed by ML appears to fall within a bounded region, and is stratified by the Fano index. This suggests that it may be possible to state and prove conjectures on completeness in the future. Inspired by the ML analysis, we formulate and prove a new global combinatorial criterion for a positively curved toric variety of Picard rank two to have terminal singularities. Together with the first sketch of the landscape of Q-Fano varieties in higher dimensions, this gives strong new evidence that machine learning, with its training capabilities, can be an essential tool in developing mathematical conjectures and accelerating theoretical discovery.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Peizhong_Ju1",
  "manipulated_ranking": 4,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=7EMphtUgCI",
  "title": "AVIS: Autonomous Visual Information Seeking with Large Language Model Agent",
  "modified_abstract": "Inspired by recent progress in artificial intelligence that combines neural information processing with sophisticated models of human reasoning, this paper introduces AVIS, an autonomous information-seeking visual question answering framework. AVIS innovatively integrates a Large Language Model (LLM) to devise strategies for the employment of external tools and their systems, and to scrutinize their outputs using tree search, thus gathering essential knowledge to address visual questions that demand external information, such as \"What event is commemorated by the building depicted in this image?\" This task encompasses a combinatorial search space that necessitates a sequential action process, including the invocation of APIs, analysis of their responses, and informed decision-making influenced by probabilistic programming principles. Through a user study, we gather diverse instances of human decision-making in response to such tasks. This data informs the development of AVIS's system components: an LLM-powered planner that dynamically selects the next tool to use, an LLM-powered reasoner that extracts crucial information from tool outputs, and a working memory component that retains information across the process. The user behavior data aids our system in two main respects. First, it allows for the creation of a transition graph delineating distinct decision-making states and the set of possible actions within each, guided by the sequence of user decisions and influenced by commonsense and statistical reasoning. Second, it provides contextual instances that enhance the decision-making capabilities of our LLM-powered components. We demonstrate that AVIS sets a new benchmark in knowledge-based visual question answering, showcasing superior performance on tasks such as Infoseek and OK-VQA and advancing the commonsense understanding of textual and visual information. The framework particularly highlights the significance of programming and its capabilities in autonomously seeking information and making informed decisions based on the envisioned scenarios and the statistical analyses of prior actions.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Alexander_K._Lew1",
  "manipulated_ranking": 1,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=4aIpgq1nuI",
  "title": "What Makes Data Suitable for a Locally Connected Neural Network? A Necessary and Sufficient Condition Based on Quantum Entanglement.",
  "modified_abstract": "In an ecosystem where machine learning architectures such as locally connected networks draw parallels with substantial theoretical advancements in optimization protocols, notably the exploration of Local SGD for its generalization capabilities and communication-efficient training approaches, this paper penetrates a fundamental question: what predicates the suitability of data for deep learning frameworks, especially for locally connected neural networks. Leverage training theoretical instruments from quantum physics, our investigation posits a novel discourse on the compatibility of data distributions and machine learning through the lens of quantum entanglement. We present a principle asserting that the aptitude of a locally connected neural network to make precise forecasts across a data distribution is contingent upon the data's quantum entanglement levels, as evaluated by specific canonical partitions of features. This discovery not only paves the way for a calculable preprocessing strategy aimed at augmenting data congeniality for such networks but also hopes to engender a broader integration of physics-derived analytical methods to discern and enhance the alignment between complex data structures and deep learning algorithms, potentially leading to faster learning processes. Our empirical assessments across multiple mainstream models and small datasets underscore the practicality of our theoretical findings, suggesting a new frontier in understanding and preparing data for advanced neural network architectures. Furthermore, we investigate how data drift impacts the baseline performance of these networks, providing insights into maintaining robustness against evolving data landscapes.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Xinran_Gu2",
  "manipulated_ranking": 2,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=V5cQH7JbGo",
  "title": "Lockdown: Backdoor Defense for Federated Learning  with Isolated Subspace Training",
  "modified_abstract": "Federated learning (FL) is recognized as a promising yet vulnerable architecture against backdoor attacks, with these vulnerabilities accentuated by its intrinsic distributed nature. In the quest for effective defenses, existing strategies often grapple with the practical limitations imposed by computational demands during training or testing, thereby underscoring the need for a resource-efficient solution. Drawing insights from the realm of centralized backdoor settings where neural network (NN) pruning has emerged as a practical defense, our investigation into the federated context reveals a distinct challenge\u2014poison-coupling effect\u2014that traditional pruning struggles to address effectively. This paper introduces Lockdown, a novel isolated subspace training methodology designed to circumvent the poison-coupling effect within FL environments. Lockdown distinctively modifies the training protocol to isolate client subspaces, employs randomness in subspace initialization, and implements subspace pruning and recovery techniques to discerningly segregate malicious from benign contributors. Furthermore, it adopts a quorum consensus mechanism and regularization strategies to rectify the global model by eliminating adversarial influence, thereby enhancing generalization. Empirical evaluations demonstrate Lockdown's enhanced defense capabilities, outperforming established methods in thwarting backdoor attacks while simultaneously promoting communication efficiency, reducing model complexity, and incorporating sampling methods to further improve defense mechanisms\u2014qualities imperative for deployment in resource-constrained FL scenarios. This approach not only aligns with advancing adversarial robustness as evidenced in related work, notably those examining adversarial transferability and the pursuit of flat local minima for improved generalization, but also extends these principles into the federated learning domain, thereby paving the way for more secure distributed machine learning practices.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Xiaosen_Wang1",
  "manipulated_ranking": 36,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=BdvCo8RVlx",
  "title": "The Contextual Lasso: Sparse Linear Models via Deep Neural Networks",
  "modified_abstract": "This work is inspired by foundational research underscoring the critical nature of prediction error bounds in generalized linear models, especially under challenging conditions such as poorly structured design matrices. Sparse linear models are essential tools for interpretable machine learning, crucial as predictive models become integral to decision-making across various domains. However, the inherent rigidity of sparse linear models, in terms of their adaptability to input features, starkly contrasts with the flexibility of black-box models like deep neural networks. Addressing this capability gap, we investigate a scenario wherein input features bifurcate into explanatory features, which are variables for potential inclusion in an interpretable model, and contextual features, which influence the selection and impact of these variables. This dichotomy gives rise to the 'contextual lasso,' a novel statistical estimator designed to fit a sparse linear model to explanatory features with a sparsity pattern and coefficients modulated by contextual information. This mechanism is articulated through a deep neural network, trained with an innovative lasso regularizer that projects the network\u2019s output onto the $\\ell_1$-constrained space of linear models. Our extensive experimentation and analysis on both real and synthetic datasets demonstrate that the resulting models not only maintain a high level of transparency but also achieve greater sparsity than traditional lasso methods without compromising the predictive capacity characteristic of deep neural networks. We uncover the effectiveness of this approach in design matrices that present a challenge for minimax optimization, highlighting the minimized error and the entropic spectrum class consideration in our models.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Kuan-Yun_Lee1",
  "manipulated_ranking": 1,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=TPeAmxwPK2",
  "title": "Parameter and Computation Efficient Transfer Learning for Vision-Language Pre-trained Models",
  "modified_abstract": "Inspired by work demonstrating the potential to circumvent traditional neural scaling laws through innovative strategies such as data pruning, this paper addresses the imperative for efficiency in the rapidly evolving domain of vision-language pre-trained (VLP) models. Specifically, the challenge lies in adapting VLP models for downstream tasks without the prohibitive expenditure typically associated with their large parameter counts and computational demands. Our research introduces a novel concept of parameter and computation efficient transfer learning (PCETL) that aims to minimize both trainable parameters and computational redundancy through the method of dynamic architecture skipping (DAS). DAS applies a reinforcement learning-based process to identify and skip computationally redundant modules within VLP models, aligning with the drive towards achieving deep efficiency in large-scale image analysis tasks. By integrating lightweight networks, or adapters, DAS facilitates a reduction in both the scale of trainable parameters and the computational overhead during inference, thus promoting a more efficient transfer to a wide array of vision-language tasks. This approach is experimentally validated on two representative VLP models, ViLT and METER, across various tasks, demonstrating not only significant computational savings, for instance, a reduction of up to 11.97% in FLOPs for METER on the VQA2.0 benchmark, but also competitive performance when compared to existing parameter efficient transfer learning methods, establishing benchmarking standards for efficiency in the field. Theoretical insights and comprehensive metrics further elucidate the benefits of our pruning-inspired approach, establishing a new benchmark for efficiency in the field.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Robert_Geirhos1",
  "manipulated_ranking": 4,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=GItLpB1vhK",
  "title": "Estimating Koopman operators with sketching to provably learn large scale dynamical systems",
  "modified_abstract": "In light of recent developments in optimization and machine learning, such as overcoming challenges in stochastic gradient descent (SGD) through gradient compression for large-scale applications, our work explores the theory of Koopman operators to employ non-parametric machine learning algorithms for predicting and analyzing complex dynamical systems. Similar to how compressed SGD addressed bottlenecks in communication within distributed settings, we propose enhancing kernel-based Koopman operator estimators through the use of random projections (sketching) to make scaling to long trajectories feasible. This approach effectively deals with the communication overhead by utilizing compression techniques in the distributed learning landscape. By deriving, implementing, and testing these 'sketched' estimators with extensive experiments on both synthetic and real-world large-scale molecular dynamics datasets, we contribute to the field by establishing non-asymptotic error bounds that delineate the trade-offs between statistical learning rates and computational efficiency. Our empirical and theoretical analyses validate the efficiency and accuracy of the proposed sketched estimators in learning large scale dynamical systems, demonstrating their potential to retain the precision of principal component regression (PCR) or reduced rank regression (RRR) while significantly increasing speed. The effective use of gradient-based methods amplifies our findings, indicating a robust route for the advancement of learning in complex system analysis.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Dmitrii_Avdiukhin1",
  "manipulated_ranking": 1,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=cm53OBkctM",
  "title": "Bayesian Learning of Optimal Policies in Markov Decision Processes with Countably Infinite State-Space",
  "modified_abstract": "This research extends the framework of algorithmic and learning procedures to the domain of countably infinite state-space, inspired by the exploration of continuous-time stochastic gradient descent methods for optimization in contexts of continuous data, such as robust machine learning and variational inference. The complexity of real-life applications, including queueing models in communication networks or computing systems, necessitates the development of optimal policy production methods applicable beyond finite state settings, emphasizing the importance of minimizing regret over the longtime horizon. We address the challenge of optimal control in discrete-time countable state-space Markov Decision Processes (MDPs) characterized by an unknown parameter $\\theta\\in\\Theta$, set in a countably-infinite state-space $\\mathcal X=\\mathbb{Z}_+^d$ with a finite action space $\\mathcal A$ and an unbounded cost function. Adopting a Bayesian perspective, with the unknown parameter $\\boldsymbol{\\theta}^*$ generated from a prior distribution on $\\Theta$, we propose an algorithm employing Thompson sampling with dynamically-sized episodes, utilizing posterior distribution updates via Bayes' rule for policy determination and performing regression towards minimizing computational complexities. To guarantee the Markov chain stability under these policies, ergodicity assumptions are enforced. Moreover, the descent in the cost function toward an optimal solution is facilitated through an $\\tilde O(dh^d\\sqrt{|\\mathcalA|T})$ upper bound on Bayesian regret, leveraging the average cost Bellman equation and functional dependencies to optimize the overall cost and system performance. The applicability of our algorithm is demonstrated through its adaptation to control unknown dynamics in different queueing models, paving the path for approximately optimal control algorithms in systems with countably infinite state-spaces.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Kexin_Jin1",
  "manipulated_ranking": 3,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=YSMLVffl5u",
  "title": "CELLE-2: Translating Proteins to Pictures and Back with a Bidirectional Text-to-Image Transformer",
  "modified_abstract": "Inspired by ground-breaking work in vision-language pre-training that has significantly advanced image-text retrieval, visual question answering, captioning, and multimodal integration, our study introduces CELL-E 2. This novel bidirectional transformer bridges a significant gap in bioinformatics and computational biology by generating images depicting protein subcellular localization from amino acid sequences and vice versa. Leveraging insights from the fusion of vision and language processing as demonstrated in the FIBER model, CELL-E 2 transcends traditional methodologies by not only capturing the spatial complexity of protein localization to generate probability estimates of localization atop a nucleus image but also enabling the generation of sequences from images, thus facilitating de novo protein design. Our pre-training regimen and fine-tuning practices utilize two large-scale datasets of human proteins, showcasing its ability to create hundreds of novel nuclear localization signals (NLS). Our work extends the boundaries of applying deep learning to bioinformatics, demonstrating a unique application of transformer models that integrate sequence and image information for protein localization and detection, a challenge largely unaddressed by existing methods. The results underscore the value of multimodal fusion in bridging disciplines, providing deep comprehension of the protein localization process, and interactive demos, especially in regards to visual question answering and captioning, are featured at our project page, omitting personal identifiable domain information.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zi-Yi_Dou1",
  "manipulated_ranking": 1,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=yaJ4vZPnHX",
  "title": "Complexity of Derivative-Free Policy Optimization for Structured $\\mathcal{H}_\\infty$ Control",
  "modified_abstract": "This study integrates insights from the field of optimization, particularly in the context of hyperparameter selection problems in machine learning, to address the challenge of derivative-free policy optimization in robust control. The applications of direct policy search in reinforcement learning and continuous control have received increasing attention. In this work, we present novel theoretical results on the complexity of derivative-free policy optimization on an important class of robust control tasks, namely the structured $\\mathcal{H}_\\infty$ synthesis with static output feedback. Optimal $\\mathcal{H}_\\infty$ synthesis under structural constraints leads to a constrained nonconvex nonsmooth problem and is typically addressed using subgradient-based policy search techniques that are built upon the concept of Goldstein subdifferential or other notions of enlarged subdifferential. In this paper, we study the complexity of finding $(\\delta,\\epsilon)$-stationary points for such nonsmooth robust control design tasks using policy optimization methods which can only access the zeroth-order oracle (i.e. the $\\mathcal{H}_\\infty$ norm of the closed-loop system). First, we study the exact oracle setting and identify the coerciveness of the cost function to prove high-probability feasibility/complexity bounds for derivative-free policy optimization on this problem. Next, we derive a sample complexity result for the multi-input multi-output (MIMO) $\\mathcal{H}_\\infty$-norm estimation. We combine this with our analysis to obtain the first sample complexity of model-free, trajectory-based, zeroth-order policy optimization on finding $(\\delta,\\epsilon)$-stationary points for structured $\\mathcal{H}_\\infty$ control. Numerical results are also provided to demonstrate our theory.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Shangzhi_Zeng1",
  "manipulated_ranking": 55,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=LloZFVwWvj",
  "title": "Non-autoregressive Machine Translation with Probabilistic Context-free Grammar",
  "modified_abstract": "Inspired by the deepening understanding of neural network behavior at initialization, specifically how fully connected and ResNet networks exhibit log-Gaussian behavior in the infinite-depth-and-width limit, this study introduces a novel non-autoregressive Transformer (NAT) model, PCFG-NAT. This model leverages a Probabilistic Context-Free Grammar (PCFG) to enhance NAT's ability to capture complex dependencies amongst output tokens. Traditional NAT models are known for their rapid inference capabilities in neural machine translation but face criticism for their limited expression power and performance degradation when compared to autoregressive (AT) models. This is largely attributed to the conditional independence assumption among target tokens. PCFG-NAT aims to mitigate these shortcomings by harnessing the structural richness of PCFGs, thereby narrowing the gap in translation quality between NAT and AT models. Furthermore, our approach enables a deeper understanding of sentence generation processes, addressing the persistent challenge of explainability in neural machine translation. Experimental validations across major machine translation benchmarks substantiate the efficacy of PCFG-NAT in achieving these objectives, with the results highlighting the importance of proper network initialization for optimizing performance. Code related to this research has been removed to ensure the abstraction complies with submission standards.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Mufan_Li1",
  "manipulated_ranking": 10,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=XWYv4BNShP",
  "title": "On the Size and Approximation Error of Distilled Datasets",
  "modified_abstract": "This work is situated within the broader context of advancing understanding in data efficiency and optimization within machine learning, drawing inspiration from recent progress such as zeroth-order optimization techniques and their applications in crafting adversarial attacks on neural networks. Dataset Distillation is the task of synthesizing small datasets from large ones while still retaining comparable predictive accuracy to the original uncompressed dataset. Despite significant empirical progress in recent years, there is little understanding of the theoretical limitations/guarantees of dataset distillation, specifically, what excess risk is achieved by distillation compared to the original dataset, how large are distilled datasets, and what is the inherent complexity of these datasets? In this work, we take a theoretical view on kernel ridge regression (KRR) based methods of dataset distillation such as Kernel Inducing Points. By transforming ridge regression in random Fourier features (RFF) space, with consideration of wavelet theories for signal compression and optimization matrices for efficient data representation, we provide the first proof of the existence of small (size) distilled datasets and their corresponding excess risk for shift-invariant kernels. We prove that a small set of instances exists in the original input space such that its solution in the RFF space coincides with the solution of the original data, offering a new vector for attack mitigation in neural networks through dataset refinement. We further show that a KRR solution can be generated using this distilled set of instances which gives an approximation towards the KRR solution optimized on the full input data. The size of this set is linear in the dimension of the RFF space of the input set or alternatively near linear in the number of effective degrees of freedom, a complexity measure which is a function of the kernel, number of data points, and the regularization parameter $\\lambda$. The error bound of this distilled set is also a function of $\\lambda$. We verify our bounds analytically and empirically, suggesting implications on the efficiency of classifiers developed using distilled datasets.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Daniel_McKenzie1",
  "manipulated_ranking": 1,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=zOCIKYVaF5",
  "title": "Residual Alignment: Uncovering the Mechanisms of Residual Networks",
  "modified_abstract": "Inspired by the recent advancements in the understanding of deep learning architectures and their optimization landscapes, such as the Dual Principal Component Pursuit (DPCP) method's insights into subspace learning and non-convex optimization challenges, our study extends this curiosity into the domain of Residual Networks (ResNets). The ResNet architecture has been widely adopted in deep learning due to its significant boost to performance through the use of simple skip connections, yet the underlying mechanisms leading to its success remain largely unknown. In this paper, we conduct a thorough empirical study of the ResNet architecture in classification tasks by linearizing its constituent residual blocks using Residual Jacobians and measuring their singular value decompositions on various benchmark datasets, including noiseless and recently introduced noise-perturbed datasets. Our measurements reveal a process called Residual Alignment (RA) characterized by four properties: - **(RA1):** intermediate representations of a given input are *equispaced* on a *line*, embedded in high dimensional space, as observed by Gai and Zhang [2021]; - **(RA2):** top left and right singular vectors of Residual Jacobians align with each other and across different depths; - **(RA3):** Residual Jacobians are at most rank $C$ for fully-connected ResNets, where $C$ is the number of classes; and - **(RA4):** top singular values of Residual Jacobians scale inversely with depth. RA consistently occurs in models that generalize well to both noiseless and recently introduced datasets, in both fully-connected and convolutional architectures, across various depths and widths, and for varying numbers of classes, but ceases to occur once the skip connections are removed. It also provably occurs in a novel mathematical model we propose. This phenomenon reveals a strong alignment between residual branches of a ResNet (RA2+4), imparting a highly rigid geometric structure to the intermediate representations as they progress *linearly* through the network (RA1) up to the final layer, where they undergo Neural Collapse, demonstrating deep learning as a powerful vector in machine learning.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Manolis_C._Tsakiris2",
  "manipulated_ranking": 1,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=vzrA6uqOis",
  "title": "GAUCHE: A Library for Gaussian Processes in Chemistry",
  "modified_abstract": "In the context of pioneering work such as the utilization of stochastic perturbations in tabular data for enhancing non-deterministic inference, our research introduces GAUCHE, an open-source library designed for GAUssian processes in CHEmistry. This innovation is pivotal, as Gaussian processes represent a fundamental methodology in probabilistic machine learning, particularly valued for their contributions to uncertainty quantification and Bayesian optimisation. The extension of Gaussian processes to encompass molecular representations necessitates the development of kernels that can operate over structured inputs like graphs, strings, and bit vectors, which are intrinsic to chemistry but not commonly associated with adversarial training scenarios or electrical circuits. GAUCHE addresses this necessity by providing a modular, robust, and easy-to-use framework for crafting such kernels, thereby facilitating their adoption by expert chemists and materials scientists for black-box optimization applications. The library is demonstrated through its application in molecular discovery, chemical reaction optimisation, and protein design, illustrating its practical utility and versatility in addressing common challenges in the field of chemistry. The codebase is made accessible, with specific measures taken to ensure user privacy and data security, which includes training the models with a focus on protecting sensitive data. This example of integration between machine learning training protocols and privacy protection opens new avenues for the safe application of Gaussian processes in sensitive chemical research environments.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Nicholas_Teague1",
  "manipulated_ranking": 1,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=YOZaej0ZC7",
  "title": "On Measuring Fairness in Generative Models",
  "modified_abstract": "In the emergence of deep learning techniques, the challenge of fairness in generative models has gained considerable attention. This research is motivated by the pressing need to develop mechanisms that can quantifiably assess fairness, echoing the advancements in Energy-Based Models (EBMs) which have demonstrated significant achievements in image generation, adversarial defense, and density modeling. Our contributions are threefold. First, we present an analysis that highlights the existing gaps in the fairness measurement framework, illustrating that considerable measurement errors exist even with the application of highly accurate sensitive attribute (SA) classifiers. These findings question the effectiveness of previously reported improvements in fairness. Second, we introduce a novel methodology, CLassifier Error-Aware Measurement (CLEAM), designed to mitigate the measurement inaccuracies inherent in SA classifiers. Our framework significantly reduces measurement errors, exemplified by a reduction from 4.98% to 0.62% for StyleGAN2 with respect to gender, while imposing minimal overhead on the model initialization and midrun performance. Third, by applying CLEAM with better sampling strategies, we assess the fairness of prevalent text-to-image generators and GANs across multiple datasets, uncovering significant biases that highlight critical concerns regarding their real-world applications and indicating a need for better sampling strategies to ensure unbiased trajectories in generative processes. Our study not only challenges current methodologies but also lays a foundational framework for future research in fairness within generative models, particularly focusing on generation performance and sampling methodologies.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Mitch_Hill1",
  "manipulated_ranking": 1,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=JMuKfZx2xU",
  "title": "On Slicing Optimality for Mutual Information",
  "modified_abstract": "The endeavor to accurately measure dependence between two random variables is foundational to the advancement of knowledge across numerous domains, a challenge exacerbated in high-dimensional data environments. This pursuit is informed by recent developments in statistical mechanics, a subfield of physics, and variational methods, as exemplified by enhancements to Variational Autoregressive Networks (VANs) through neural network-based Monte Carlo Markov Chain (MCMC) and importance sampling techniques. These advancements underscore the potential for improved estimators in complex systems, offering a pertinent backdrop to our investigation into slicing methods for measuring mutual information (MI). Slicing methods, recognized for their scalability in quantifying MI between high-dimensional variables through projection into one-dimensional spaces, often rely on uniform distributions of slicing directions that may overlook critical features of variable interdependence, resulting in imprecise dependence measures. In response, our paper introduces a principled framework designed to identify an optimal distribution of slices for MI, engaging with theoretical questions regarding the optimal slicing distribution in the MI context and developing corresponding analytical and sample-based insights. Through the integration of these theoretical advancements with modern machine learning techniques, including neural networks, we formulate a practical algorithm that markedly enhances the accuracy of MI measurements by leveraging optimal sampling strategies. Our comprehensive experimental evaluation, spanning various benchmark datasets, validates the superiority of our approach over existing state-of-the-art methods in effective information capture. The comments and suggestions from the peer review process have been duly corrected and incorporated to strengthen the paper's findings. Singular conditions and outlier scenarios that we previously believed to potentially skew results were thoroughly investigated, ensuring robust and reliable measurements of MI.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Kim_Andrea_Nicoli1",
  "manipulated_ranking": 1,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=WPbIAdB6aQ",
  "title": "A Robust Exact Algorithm for the Euclidean Bipartite Matching Problem",
  "modified_abstract": "Inspired by innovative approaches in computational geometry and algorithm design as exemplified by the creation of a unified shape representation, 'Shape Unicode', our research introduces a novel exact algorithm for addressing the Euclidean bipartite matching problem. This problem, foundational in estimating Wasserstein distance between two distributions, involves computing a minimum-cost bipartite matching of two sets, $A$ and $B$, each comprising $n$ points within a 2-dimensional Euclidean space focused on synthesizing solutions for complex shapes. Traditional methods leverage the Hungarian method for execution in $\\tilde{O}(n^2)$ time, contingent upon the spread $\\Delta$, denoting the ratio of the farthest to the closest pair of points within $A\\cup B$. Our algorithm advances the field by proposing a solution with a similar worst-case execution time of $\\tilde{O}(n^2 \\log \\Delta)$ but exhibits a markedly improved expected execution time of $\\tilde{O}(n^{7/4}\\log \\Delta)$ for stochastically distributed point sets. This signifies the first instance of achieving sub-quadratic execution time for stochastic point sets with real-valued coordinates. Furthermore, our method, interpreting Euclidean spaces in an innovative manner, scales to higher dimensions, operating in $\\tilde{O}(n^{2-\\frac{1}{2d}}\\Phi(n))$ for stochastic point sets $A$ and $B$, where $\\Phi(n)$ corresponds to the query/update time of a dynamic weighted nearest neighbor data structure. The algorithm can be perceived as a strategic adaptation of the Hungarian method, integrated with a geometric divide-and-conquer strategy and involving tasks aimed at enhancing computational efficiency in synthesizing shapes and applications across diverse domains.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sanjeev_Muralikrishnan1",
  "manipulated_ranking": 1,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=CQuRzAgjg9",
  "title": "Online Clustering of Bandits with Misspecified User Models",
  "modified_abstract": "Inspired by the developments in active learning algorithms and their success in leveraging sampling techniques and querying strategies to reduce uncertainty, we investigate the contextual linear bandit problem, emphasizing the challenges arising from misspecified user models. This problem, central to many online learning applications, typically relies on accurate user preference models to guide the selection of actions (or 'arms') that maximize cumulative rewards over time. A subset of this domain, clustering of bandits (CB), has benefited from user preference aggregation, significantly outperforming traditional linear bandit solutions by exploiting the collaborative effect and employing strategies akin to querying an oracle for labels in active learning scenarios. Despite these advances, the dependency of CB algorithms on well-defined linear user models and the process of handling unlabeled data are notable limitations, as real-world applications often involve model misspecifications and unlabeled instances that can degrade performance. In response to this critical gap, our paper introduces the concept of clustering of bandits with misspecified user models (CBMUM) and presents two novel robust CB algorithms, RCLUMB and RSCLUMB. These algorithms are specifically designed to handle inaccurate user preference estimations and the consequent erroneous clustering, employing dynamic graphs and sets to represent the learning and clustering process adaptively, reminiscent of strategies used to handle unlabeled data in semi-supervised learning. We undertake a comprehensive regret analysis, proving upper bounds that not only demonstrate the resilience of our algorithms to user model misspecifications under less restrictive assumptions than those of prior CB works but also align with the asymptotic lower bounds and match state-of-the-art results in certain contexts. Our methodology and findings, supported by both synthetic and real-world dataset experiments, offer critical insights into the design of robust clustering algorithms capable of navigating the complexities of misspecified user models and unlabeled data, thereby broadening the applicability of CB approaches in online learning.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Muni_Sreenivas_Pydi1",
  "manipulated_ranking": 1,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=lBhRTO2uWf",
  "title": "Adversarial Learning for Feature Shift Detection and Correction",
  "modified_abstract": "The existing literature in machine learning and data processing, exemplified by innovative approaches to managing non-IID data in federated learning systems, lays the groundwork for addressing the challenges of data shift in real-world applications. Inspired by recent advancements in classifier calibration techniques that tackle data heterogeneity, our study extends the exploration into the realm of feature shift detection and correction using deep adversarial learning principles. Data shift, a pervasive phenomenon across multi-sensor environments and various domains such as biomedical, financial, and structured dataset analysis, often arises from malfunctioning sensors or faulty data processing pipelines. This work exploits deep adversarial learning principles, utilizing information from several discriminators trained during the adversarial training process to distinguish between two data distributions, to both identify and rectify corrupted features. By demonstrating that mainstream supervised classifiers, augmented with iterative heuristics, can effectively localize and correct feature shifts, thereby enhancing the understanding of feature behavior under distribution changes and improving the representations, our approach surpasses existing statistical and neural network-based methods in mitigating distribution discrepancies between datasets. This contribution highlights the potential of adversarial methodologies in enhancing data reliability and consistency across different classes in a dataset, paving the way for more robust and accurate machine learning applications and setting new benchmarks in the field.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Dapeng_Hu2",
  "manipulated_ranking": 1,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=qy07OHsJT5",
  "title": "Diffusion Schr\u00f6dinger Bridge Matching",
  "modified_abstract": "The exploration of transport problems, defined as discovering a mechanism to transition one given distribution to another, finds significant relevance within machine learning, motivated by the vast spectrum of applications ranging from generative modeling to more precise estimations in reinforcement learning. Building on the foundational insights provided by recent works in generative modeling, such as Denoising Diffusion Models (DDMs) and Flow Matching Models (FMMs), and the theoretical underpinnings regarding the movement of probability mass elucidated in studies of gradient estimators and reparameterization techniques, this research introduces Iterative Markovian Fitting (IMF) alongside a novel numerical algorithm, Diffusion Schr\u00f6dinger Bridge Matching (DSBM). DSBM strives to address and overcome the limitations inherent in previous Schr\u00f6dinger bridges (SBs) numerics, which include poor scalability with dimensionality and cumulative errors, by presenting an improved methodology for computing IMF iterates. This methodology not only enhances performance through unified gradients-based optimization and efficient sampling techniques, including single-sample expectations estimations, but also conceptually unites various transport methods, such as flow dynamics, as special or limiting cases of its application. Through these innovations, the efficacy of DSBM is validated through a series of problem-specific demonstrations, establishing its superiority over extant SB numerical approaches and its potential to closely approximate the deterministic dynamic Optimal Transport (OT) map, thereby significantly contributing to the advancement of mass transport methods within machine learning.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Paavo_Parmas1",
  "manipulated_ranking": 4,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=iohoef1bfM",
  "title": "Generalized Belief Transport",
  "modified_abstract": "Inspired by recent developments in adaptive experimental design and bandit literature, particularly the exploration of policy choice for best arm identification and the asymptotic analysis of exploration sampling, this study introduces Generalized Belief Transport (GBT). This framework strives to unify and generalize learning models by situating them within a novel mathematical construct, drawing parallels with established methods such as Bayesian inference, cooperative communication, and classification through the lens of Unbalanced Optimal Transport (UOT). Human learners exhibit a remarkable ability to adapt their learning strategies based on factors such as hypothesis priors, decision urgency, and environmental drift. However, current learning models are often considered in isolation, lacking a comprehensive framework that encompasses the dynamic interplay among different learning modalities. GBT addresses this gap by conceptualizing learning models as points within an expansive parameterized space shaped by three learning constraints, including identification, regret minimization, and sampling as implicit aspects of its foundational setting. This space is visualized as a cube, where classic learning models are identified as specific coordinates, fostering a deeper understanding of the relationships and transitions between learning paradigms. The study progresses to establish foundational properties of this space, including continuity and differentiability, thereby facilitating model interpolation and setting a precedent for examining the boundary behaviors and convergence properties of various models within GBT. The exploration extends to computational analyses, revealing insights into learning adaptability amid distributional shifts and framing conjectures on the general behaviors of learning models under different settings. The paper concludes by highlighting open questions and contemplating the broader implications for developing more cohesive and versatile learning architectures.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Masahiro_Kato1",
  "manipulated_ranking": 1,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=dTj5tH94xv",
  "title": "Does a sparse ReLU network training problem always admit an optimum ?",
  "modified_abstract": "The pursuit of optimal network parameters underpins much of the research in neural network training, yet, drawing from the examination of bandit problems with non-concave rewards and their optimal algorithms in various settings, this work challenges the commonly held assumption that an optimum always exists for sparse ReLU neural networks. Given a training set, a loss function, and a neural network architecture, it is often assumed that optimal network parameters can be identified and reached through existing optimization algorithms in specific settings. Our investigation into sparse ReLU neural networks, however, reveals that this is not always the case. We first demonstrate that optimization problems for deep networks exhibiting specific sparsity patterns may lack optimal parameters, potentially leading optimization algorithms to diverge. Through establishing a novel topological correspondence between sparse ReLU networks and linear models, and leveraging tools from real algebraic geometry, we present an algorithm to determine whether a given sparsity pattern is predisposed to this limitation. Furthermore, we verify the existence of a global optimum for every declared optimization problem concerning a shallow sparse ReLU neural network of output dimension one, based on the exploration of two topological properties of the space of sparse ReLU network-implementable functions: a best approximation property and a closedness property, both evaluated in the uniform norm. This analysis extends beyond theoretical domains, considering practical implications for training on finite datasets and the broader domain represented by the unit cube. Our findings not only address sparsity patterns encountered in contemporary network pruning and sparsification initiatives but also extend to conventional dense neural networks, encapsulating architectures beyond the scope of existing theoretical frameworks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Kaixuan_Huang1",
  "manipulated_ranking": 2,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=Jkc74vn1aZ",
  "title": "Towards Symmetry-Aware Generation of Periodic Materials",
  "modified_abstract": "Motivated by the significant progress in generative artificial intelligence (AI) models within fields such as tissue organization and molecule generation, this study focuses on addressing the gap in generating periodic materials, which exhibit distinct symmetrical properties not fully explored by current methodologies. Prior research in generating synthetic images of cell membranes and tissue structures using Generative Adversarial Networks (GANs) has laid the groundwork for complex pattern generation in biological and chemical domains. In this work, we propose SyMat, a novel material generation approach that can capture physical symmetries of periodic material structures. SyMat generates atom types and lattices of materials through generating atom type sets, lattice lengths, and lattice angles with a variational auto-encoder model. Additionally, SyMat employs a score-based diffusion model for generating atom coordinates of materials, in which a novel symmetry-aware probabilistic model is used in the coordinate diffusion process. Our experimental assessment demonstrates that SyMat is theoretically invariant to all symmetry transformations on materials and shows promising performance on random generation and property optimization tasks under minimal supervision. The generative models developed here pave the way not only for advances in electron microscopy image segmentation but also provide a novel framework for understanding the organization of tissues at the molecular level. Our code is publicly available as part of the AIRS library.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ligong_Han1",
  "manipulated_ranking": 1,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=t0fkjO4aZj",
  "title": "A unified framework for information-theoretic generalization bounds",
  "modified_abstract": "In the context of recent advancements in machine learning optimization techniques, including meta-learning and differentiable convex optimization, this paper introduces a general methodology for deriving information-theoretic generalization bounds for learning algorithms. Leveraging the latest developments as a backdrop, our methodology employs a novel probabilistic decorrelation lemma based on a change of measure coupled with a relaxation of Young's inequality in $L_{\\psi_p}$ Orlicz spaces. By integrating concepts such as symmetrization, couplings, chaining in the space of probability measures, controls over distributional properties, and illustrative examples to control complex model behaviors, we present new upper bounds on the generalization error of learning algorithms, both in expectation and with high probability. Our analysis not only encompasses and streamlines a variety of existing bounds\u2014including those based on mutual information, conditional mutual information, stochastic chaining, and PAC-Bayes inequalities\u2014but also facilitates the derivation of the Fernique\u2013Talagrand upper bound on the expected supremum of a subgaussian process as a specific case. This unified framework thus provides a comprehensive theoretical foundation that contextualizes our work within the broader scope of enhancements in optimization, including meta-learning, and the role of optimizers in reducing overfitting, setting the stage for further investigations into refining and applying information-theoretic measures for learning performance. Optimization techniques are a crucial component in these advancements.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Samuel_Pfrommer1",
  "manipulated_ranking": 1,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=D0MII7rP3R",
  "title": "Selectivity Drives Productivity: Efficient Dataset Pruning for Enhanced Transfer Learning",
  "modified_abstract": "In the context of enhancing the efficiency of deep learning models, recent insights into the generalization properties of neural networks and adversarial robustness from other studies lead us to explore dataset pruning (DP) within the framework of transfer learning. The substantial computational and infrastructural costs associated with processing massive datasets have prompted investigations into DP as a means to maintain or even enhance model performance by removing redundant training samples. Although the exploration of DP has been largely siloed from that of transfer learning, our work converges these domains by investigating how to prune a source dataset to enhance pretraining efficiency without compromising finetuning accuracy on downstream tasks. To our knowledge, this integrated approach to DP for transfer learning is novel, as existing methods fail to adequately address the unique requirements of transfer learning paradigms. We introduce two new methodologies\u2014label mapping for supervised pretraining and feature mapping for self-supervised settings\u2014that reframe DP through the perspective of optimal source-target domain mapping. Empirical results across multiple transfer learning scenarios reveal that our methods enable significant pruning of source data classes (40% to 80%) without loss in downstream task performance, achieving a 2 to 5-fold reduction in pretraining time. Additionally, our strategies enhance the utility of other transfer learning techniques, including those requiring computationally intensive preprocessing like adversarial pretraining, by improving efficiency through selective data use, generalization, and regularization. These approaches also consider norm-based operator mechanisms for maintaining and evaluating model robustness, selectivity, and transferability, thereby supporting advanced methods for attack resilience.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Farzan_Farnia1",
  "manipulated_ranking": 1,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=7INd5Yu9ET",
  "title": "Long-Term Fairness with Unknown Dynamics",
  "modified_abstract": "Amidst growing concern over the potential of machine learning to reinforce social inequalities, there is an emerging discourse on leveraging these technologies to proactively pursue equitable outcomes. Our inquiry is situated within this vibrant discussion, drawing on recent progress in understanding dynamic system behaviors through reinforcement learning, as exemplified by advancements in Direct Advantage Estimation. This body of work, which explores novel methods of advantage estimation in reinforcement learning to enhance policy learning under uncertain conditions, provides a critical backdrop to our (action-)value focused research. In this paper, we formalize long-term fairness as an online reinforcement learning problem with the goal of affecting policy outcomes for human populations dynamically. By framing this as a problem of dynamical system control, we underscore the insufficiency of static fairness measures and highlight the importance of adaptive strategies that can navigate complex, evolving social landscapes seamlessly. We introduce a generalized actor-critic algorithm that leverages insights from recent online learning advancements in estimation and optimization, demonstrating its ability to achieve beneficial equilibria by balancing cumulative loss against fairness violations over time. This approach draws on causal relationships to inform its interventions, seamlessly integrating generalized advantage estimation for policy betterment. Our work contrasts with existing approaches by offering a methodology capable of adapting to unknown dynamics, thus ensuring sustainable fairness across temporally shifting societal contexts. Experiments employing evolutionary game theory and real-world data validate our approach's capability to guide systems toward more equitable states, challenging conventional methodologies that prioritize immediate over long-term impacts.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Nico_G\u00fcrtler1",
  "manipulated_ranking": 1,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=8OTPepXzeh",
  "title": "DPOK: Reinforcement Learning for Fine-tuning Text-to-Image Diffusion Models",
  "modified_abstract": "Inspired by the significant advancements in text and image processing models, including overcoming challenges in linear transformers, our work on DPOK aims to leverage the strength of reinforcement learning (RL) to address the complexities of fine-tuning text-to-image models for specific tasks. Learning from human feedback has shown promise in improving text-to-image models by capturing what humans value in the output, essentially creating a corpus of human preferences. Though basic techniques like rejection sampling have been explored, applying RL for fine-tuning presents a novel approach to enhancing the performance of diffusion models significantly. We propose the use of online RL, specifically policy gradient methods integrated with KL regularization and attention mechanisms, for fine-tuning pre-trained text-to-image diffusion models, thus addressing the dual challenges of image-text alignment and image quality improvement. Our method, DPOK, is analyzed and compared against supervised fine-tuning techniques and benchmarks, demonstrating its effectiveness in generating higher quality images that better align with textual descriptions through the manipulation of tokens and linear attention mechanisms. The modeling process incorporates elements of normalization to ensure stable learning performance. This research underscores the potential of incorporating RL into the fine-tuning process of text-to-image models, contributing to the broader field of ML advancements and aligning with the trajectory of improvements seen in related areas such as transformer models, by compiling a comprehensive corpus of challenges and solutions. Please note that the link to our code has been removed to maintain confidentiality.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Weixuan_Sun1",
  "manipulated_ranking": 2,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=9i8MD9btc8",
  "title": "(Almost) Provable Error Bounds Under Distribution Shift via Disagreement Discrepancy",
  "modified_abstract": "Informed by the pressing challenge of estimating model performance under new data distributions, as exemplified by recent research focusing on Sparse Joint Shift (SJS) models, our study introduces a novel bound for assessing the error of deep neural networks when confronted with distribution shifts. Building on a foundation that seeks to quantify performance variances without reliance on labeled test data\u2014a common limitation in deployment scenarios\u2014this work pivots towards a more intuitive and practically consistent methodology. We propose an accessible condition, empirically supported by prior studies, which is met with near certainty in sparse settings, and represents a significant departure from methods that depend on complex, often inapplicable measures like test calibration. Our approach utilizes a disagreement discrepancy concept, which echoes the principles of $\\mathcal{H}\\Delta\\mathcal{H}$-divergence but offers improved ease of evaluation and enhanced tightness of error bounds for learning under distribution shifts. By adopting a newly formulated \"disagreement loss,\" optimized to foster classifier divergence with theoretical soundness and superior empirical performance, we pave the way for its adoption in future frameworks aiming to maximize multiclass disagreement. This method presents compelling, valid error bounds across diverse benchmarks of distribution shifts, including settings where rapid change is prominent, thereby maintaining competitive accuracy without surpassing existing baselines. Thus, our contribution not only advances the discourse on distribution shift adaptability but also proposes a pragmatic solution for error estimation in unlabeled data scenarios, complemented by dynamic monitoring for accuracy validation.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Lingjiao_Chen1",
  "manipulated_ranking": 1,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=noMktb4ait",
  "title": "Joint Feature and Differentiable $ k $-NN Graph Learning using Dirichlet Energy",
  "modified_abstract": "Inspired by the integration of structured data modeling techniques with neural networks, as evidenced in recent works exploring undirected neural networks (UNNs) for capturing complex data relationships and the convolutional layers' efficacy in parsing variably structured datasets, this paper extends the forefront of machine learning research by proposing an innovative deep feature selection (FS) method. This method leverages the Dirichlet Energy for simultaneous feature selection and differentiable $ k $-NN graph learning, addressing a novel aspect of the FS challenge by identifying critical features through their dependency on each other and their smoothness on the graph structure. The introduction of Optimal Transport theory to resolve the non-differentiability issue of learning $ k $-NN graphs in neural networks marks a theoretical advancement, rendering our method suitable for various graph neural networks targeting dynamic graph learning. The proposed framework is distinguished by its algorithmic design, ensuring interpretability across its modules, and integrates with recurrent neural networks for handling dynamic, evolving graphs, which could be akin to understanding the evolving dependency in dynamic systems. We substantiate our model's efficacy through comprehensive experiments on both synthetic and real-world datasets, thereby contributing to the broader application of neural network-based solutions for structured and unstructured data, including image completion tasks with auto-encoders. Estimators of feature importance and graph structure play a crucial role in our method, guiding the learning process effectively. Furthermore, the novel approach in grafting prototype-based techniques into the learning framework enhances the interpretability and specificity of the features selected for dynamic graph learning.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Tsvetomila_Mihaylova1",
  "manipulated_ranking": 1,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=mSNfjOcDUv",
  "title": "InfoPrompt: Information-Theoretic Soft Prompt Tuning for Natural Language Understanding",
  "modified_abstract": "Motivated by the growing need to enhance the adaptability and efficiency of pre-trained language models (LMs) for knowledge-intensive tasks through methods like knowledge injection, this work introduces InfoPrompt, an advanced soft prompt tuning framework grounded in information theory. The inherent challenge of ensuring that LMs can effectively utilize and propagate newly injected knowledge, especially for tasks requiring nuanced inference or the introduction of novel entities, sets the stage for our investigation. InfoPrompt addresses the sensitivities surrounding the initialization of prompts and the conventional prompt tuning methods' limitations in encoding task-relevant information efficiently. By conceptualizing soft prompt tuning as a process of maximizing mutual information between prompts and encoded representations within the model, we establish a more precise, efficient, and robust approach to prompt tuning. Our framework introduces two novel mutual information-based loss functions aimed at optimizing prompt initialization for downstream tasks and ensuring that the learnt prompts adequately capture and reflect task-relevant information, while explicitly handling instances where nuanced inferences are paramount. Through extensive experimentation on various datasets for different NLP tasks, InfoPrompt demonstrates accelerated convergence and superior performance over traditional prompt tuning methods in generating accurate predictions. Furthermore, we substantiate the effectiveness of our approach with theoretical analysis showing the applicability of gradient descent algorithms in optimizing our mutual information losses. The ability of InfoPrompt to enhance entity recognition and nuanced inferences solidifies its utility in sophisticated language understanding applications.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yasumasa_Onoe1",
  "manipulated_ranking": 1,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=6ePsuwXUwf",
  "title": "An Efficient End-to-End Training Approach for Zero-Shot Human-AI Coordination",
  "modified_abstract": "Inspired by the advancements in generative adversarial networks (GANs), including achieving high-resolution image generation and the exploration of efficient transfer learning techniques from unconditional to conditional models through architectural innovations, our study introduces an Efficient End-to-End Training (E3T) approach targeted at zero-shot human-AI coordination. Unlike the prevailing two-stage population-based methods, which simulate diverse human behaviors through a broad population of distinct policies but at the cost of computational efficiency, E3T leverages a novel combination of an ego policy and a random policy to construct a partner policy that is both skilled in coordination and sufficiently diverse. This method enables the ego agent to undergo end-to-end learning with this mixed policy without needing a pre-trained population, markedly enhancing training efficiency. Moreover, the introduction of a contrastive loss mechanism within the partner modeling module, capable of predicting the partner's next action based on historical data, allows the ego policy to dynamically adjust its actions for adaptation to human partners exhibiting varied behavior patterns, an adaptation to real-world scenarios. Tested within the Overcooked environment, our methodology not only demonstrates significant improvements in training efficiency but also achieves comparable or superior performance relative to established population-based benchmarks, including in scenarios requiring high-resolution decision-making. Demo videos are available at https://sites.google.com/view/e3t-overcooked.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Hector_Laria_Mantecon1",
  "manipulated_ranking": 6,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=IwyymRXfzL",
  "title": "Perceptual adjustment queries and an inverted measurement paradigm for low-rank metric learning",
  "modified_abstract": "Informed by recent advances in counterfactual evaluation and learning, which delve into off-policy evaluation (OPE) and off-policy learning (OPL) for refining decision-making policies in interactive systems, our work introduces a novel query mechanism for metric learning\u2014perceptual adjustment query (PAQ). Building on the foundational concepts of using historical data to inform future policies in various domains, this study presents the PAQ as a versatile tool for gathering human feedback. The PAQ, characterized by its inverted measurement methodology, is designed to be both informative for the learning algorithm and cognitively lightweight for humans, effectively bridging cardinal and ordinal queries' benefits. In addressing the challenge of learning an unknown Mahalanobis distance, we leverage PAQ measurements to formulate a high-dimensional, low-rank matrix estimation problem\u2014distinct from those typically encountered in standard matrix estimation tasks. This necessitates the development of a bespoke two-stage estimator for metric learning from PAQs, accompanied by theoretical guarantees regarding sample complexity. Through numerical simulations, we not only validate the effectiveness of our proposed estimator empirically but also underscore its unique attributes, contributing to the broader discourse on metric learning's capabilities and applications. The recommendation system inferences made possible through PAQ also enable large-scale applications, aligning with open-source principles in software development for collaborative and transparent metric learning methodologies. Our approach is further enriched by incorporating offline policy evaluation techniques, extending our method's applicability to various empirical settings where direct interaction is prohibitive.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yuta_Saito1",
  "manipulated_ranking": 1,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=hcXDbbzgoh",
  "title": "Taylor TD-learning",
  "modified_abstract": "Building upon pioneering work in the optimization of data collection and policy evaluation within linear bandits, particularly focusing on heteroscedastic reward noise, our study introduces a novel framework, Taylor TD, within the domain of reinforcement learning (RL). The variability introduced by stochasticity in action choices and state distributions in reinforcement learning, especially in continuous state-action environments, presents significant challenges for algorithm efficiency and effectiveness. To address these challenges, Taylor TD applies a first-order Taylor series expansion to temporal-difference (TD) updates, significantly reducing variance compared to traditional TD learning methods. This model-based RL approach allows for analytical integration over stochasticities, thereby offering theoretical and empirical evidence of its lower-variance updates. The design of Taylor TD not only retains the stable learning guarantees of standard TD-learning with linear function approximation under a specific assumption but also demonstrates compatibility with, and enhancements to, existing algorithms such as TD3, forming a hybrid model, TaTD3. The behavior modification potentials and the efficiency in estimation of the expected rewards highlight the strategic advantages of incorporating Taylor series into TD updates. Our empirical evaluations on benchmark tasks reveal TaTD3's competitive or superior performance against state-of-the-art model-free and model-based RL baselines, thereby underlining the efficacy of integrating model-based enhancements into traditional RL paradigms, the importance of thoughtful data collection strategies, and the potential for regret minimization in multi-armed bandit scenarios.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Subhojyoti_Mukherjee1",
  "manipulated_ranking": 2,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=Ct0zPIe3xs",
  "title": "Saving 100x Storage: Prototype Replay for Reconstructing Training Sample Distribution in Class-Incremental Semantic Segmentation",
  "modified_abstract": "In the landscape of machine learning challenges, such as those posed by class-incremental semantic segmentation (CISS), issues of catastrophic forgetting and background shift have been the focus of longstanding research efforts. Inspired by the pioneering work in hierarchical open-set zero-shot recognition and the employment of innovative techniques to tackle open-set recognition and zero-shot learning problems, our study introduces STAR, a novel approach aimed at mitigating the overlooked problem of class distribution bias in CISS. In this context, where individual learning steps are skewed towards newly introduced classes, STAR innovatively preserves the main characteristics of each past class through the storage of compact prototypes and associated statistical data. This enables the alignment of the class distribution of single-step training samples with that of the complete dataset by replaying these prototypes, and adjusting the background pixel frequency, thus supporting effective label management and prediction accuracy improvement. Our approach marks a significant departure from methods that rely on replaying raw images, offering a storage savings of over 100 times while concurrently enhancing performance and accuracy in prediction. STAR further incorporates an old-class features maintaining (OCFM) loss and a similarity-aware discriminative (SAD) loss, tailored to maintain old-class features and promote feature diversity between similar class pairs respectively. Detection of nuanced differences and multimodal insights are crucial in this endeavor. Systematic experiments conducted on Pascal VOC 2012 and ADE20K datasets validate that STAR sets a new benchmark, outperforming all previous state-of-the-art methods in the realm of class-incremental semantic segmentation and is recognized as a leap forward in learning optimization for this challenge.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Xian-Sheng_Hua1",
  "manipulated_ranking": 2,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=SLwy8UVS8Y",
  "title": "PLANNER: Generating Diversified Paragraph via Latent Language Diffusion Model",
  "modified_abstract": "Building on pioneering efforts to enhance neural network performance through advanced methodologies such as incorporating graph sparsity for regularization, our work introduces PLANNER, a novel approach in the realm of natural language generation (NLG). Autoregressive models for text sometimes generate repetitive and low-quality output because errors accumulate during the steps of generation. This issue is often attributed to exposure bias -- the difference between how a model is trained and how it is used during inference. Denoising diffusion models provide an alternative approach in which a model can revisit and revise its output. However, they can be computationally expensive, and prior efforts on text have led to models that produce less fluent output compared to autoregressive models, especially for longer text and paragraphs. In this paper, we propose PLANNER, a model that combines latent semantic diffusion with autoregressive generation, to generate fluent text while exercising global control over paragraphs. The model achieves this by combining an autoregressive \"decoding\" module with a \"planning\" module that uses latent diffusion to generate semantic paragraph embeddings in a coarse-to-fine manner and incorporates graphical models for enhanced structural coherence. This proposed method, evaluated on various conditional generation tasks, demonstrates its effectiveness in generating high-quality long-form text in an efficient manner through novel learning methods and regularization techniques.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Edric_Tam1",
  "manipulated_ranking": 105,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=loixpHDZKj",
  "title": "Robust Learning for Smoothed Online Convex Optimization with Feedback Delay",
  "modified_abstract": "Building on the concept of combining machine learning predictions with trusted expert knowledge to navigate uncertainties in nonlinear systems, as demonstrated in trajectory optimization and control techniques, this paper introduces a general framework for Smoothed Online Convex Optimization (SOCO), which incorporates multi-step switching costs and feedback delay. Our proposed Robustness-Constrained Learning (RCL) algorithm innovatively merges untrusted ML predictions with a reliable expert online algorithm through constrained projection. This integration not only preserves the ML predictions' innovative capacity but significantly enhances their reliability by incorporating robustness constraints, thereby effectively managing risk. Furthermore, by facilitating continual replanning, RCL supports effective trajectory optimization even under constraints of feedback delay and multi-step switching costs. We prove that RCL achieves $(1+\\lambda)$-competitiveness with any chosen expert for any $\\lambda>0$, marking it as the pioneering ML-augmented approach that guarantees robustness amidst multi-step switching costs and feedback delays, a feature previously unattainable. The employment of RCL in battery management and trajectory replanning showcases its superior robustness and average-case performance enhancements, illustrating the practical benefits and the potential expansion of RCL to other domains requiring robust online decision-making, including exploration of nonlinear systems and optimization of terminal objectives. The optimization-focused analysis does not explicitly involve 'frames' or 'covariance' calculations, making RCL a key development in the field of robust optimization.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Joe_Watson1",
  "manipulated_ranking": 1,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=bv9mmH0LGF",
  "title": "Global Structure-Aware Diffusion Process for Low-light Image Enhancement",
  "modified_abstract": "This paper studies a diffusion-based framework to address the low-light image enhancement problem, building on foundational principles observed in recent advances like Neural Radiance Fields (NeRF) that leverage uncertainty estimation and active learning for improved 3D scene reconstruction and novel view synthesis. These insights into handling sparse datasets and enhancing data quality through informed sampling have informed our approach to low-light image enhancement. By harnessing the capabilities of diffusion models, we advocate for the regularization of its inherent ODE-trajectory, inspired by research indicating that low curvature ODE-trajectories result in stable and effective diffusion processes. We introduce a curvature regularization term that leverages the intrinsic non-local structures of image data - a global structure-aware regularization - which aids in complicated detail preservation and contrast augmentation during the diffusion process. This integration diminishes the noise and artifacts typically associated with diffusion processes, yielding a more precise and flexible enhancement. Additionally, we implement an uncertainty-guided regularization technique to better address the challenges in the most difficult regions of the image, leading to notable improvements in low-light enhancement as demonstrated by our experimental evaluations. Our novel framework, supported by rank-informed and uncertainty-guided regularization\u2014with the validation of rigorous training and re-training strategies\u2014shows superior performance in enhancing image quality, reducing noise, and amplifying contrast compared to existing methods. By synthesizing learning from these various components\u2014diffusion models, NeRF, and active learning\u2014our approach not only advances the field of low-light image enhancement but also encourages further exploration into the applications of diffusion models across various domains, emphasizing the importance of active engagement during the training phase to validate and reinforce model capabilities. The set of techniques developed and the samples of enhanced images serve as a testament to the efficacy of our methodology in leveraging 3D data for image quality improvement.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Xuran_Pan1",
  "manipulated_ranking": 1,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=roGYQvarnC",
  "title": "ConRad: Image Constrained Radiance Fields for 3D Generation from a Single Image",
  "modified_abstract": "In the context of thriving advancements in deep learning for visual recognition, as exemplified by innovations like convolutional neural networks and the horizontal gated recurrent unit architecture which showcase proficiency in learning long-range spatial dependencies, our research introduces a pioneering method in the realm of 3D object reconstruction from a solitary RGB image. This development is propelled by the necessity to derive the concealed 3D structure from an input image with high fidelity, a challenge inadequately addressed by extant methodologies that excel in 3D model generation from textual prompts yet falter in conditioning on visual RGB data. To surmount these obstacles, we propose Image Constrained Radiance Fields (ConRad) - a fresh incarnation of neural radiance fields tailored to encapsulate the appearance of an input image from a singular viewpoint explicitly, leveraging recurrent processes and hierarchical connections to enhance feature recognition. Our technique is grounded on integrating a singular RGB image with pretrained Diffusion Models to refine the parameters of a ConRad representation, preserving essential features and human-like understanding during the process. Through comprehensive experimentation, the efficacy of ConRad in conserving intricate image specifics during the fabrication of verisimilar 3D reconstructions is corroborated, standing out conspicuously against prevailing state-of-the-art benchmarks. Our findings reveal that ConRad not only preserves higher fidelity to the original input but also engenders more consistent 3D models, concurrently marking a significant enhancement in quantitative performance on established ShapeNet object benchmarks and highlighting its prowess in features recognition through structured columns and connections, thereby setting a new benchmark for horizontal integration in deep learning models.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Vijay_Veerabadran1",
  "manipulated_ranking": 7,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=uoiwugtpCH",
  "title": "PriorBand: Practical Hyperparameter Optimization in the Age of Deep Learning",
  "modified_abstract": "In the context of burgeoning research focusing on equitable and efficient empirical risk minimization (ERM) within machine learning\u2014a foreground highlighted by innovations such as FERMI, which integrates fairness into stochastic optimization\u2014our work introduces PriorBand, a novel Hyperparameter Optimization (HPO) algorithm designed to address the unique challenges of Deep Learning (DL) pipelines. Hyperparameters of DL pipelines are crucial for their downstream performance in tasks including classification. While a large number of methods for HPO have been developed, their incurred costs are often untenable for modern DL, leaving manual experimentation as the most prevalent optimization approach. This reliance on intuition, domain knowledge, and preliminary explorations using sets of hyperparameters underscores the need for an HPO algorithm that bridges the gap between theoretical algorithmic advances and practical DL applications. PriorBand is specifically engineered to leverage expert beliefs and cheap proxy tasks, offering an effective solution to the misalignment between HPO algorithms and the needs of DL researchers. Through empirical evaluations across various DL benchmarks, including fairness indices as a test framework, PriorBand demonstrates notable efficiency, highlighting its advantage under informative expert input and resilience against poor expert beliefs related to classification tasks. This work not only contributes to the field of HPO by providing a practical tool for DL researchers but also positions itself within the wider discourse on making advanced ML techniques accessible and actionable in real-world scenarios, embedding considerations of risk and fairness within empirical risk minimization processes.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sina_Baharlouei1",
  "manipulated_ranking": 1,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=OXhymu6MeN",
  "title": "Sub-optimality of the Naive Mean Field approximation for proportional high-dimensional Linear Regression",
  "modified_abstract": "Amid the burgeoning complexity of modern Machine Learning (ML) challenges and corresponding algorithms, the Na\u00efve Mean Field (NMF) approximation emerges as an attractive option for computational efficiency in high-dimensional settings. This approach, however, suffers from a lack of comprehensive theoretical underpinning, especially absent strong structural assumptions like sparsity. Inspired by recent theoretical and empirical studies, including those exploring soft-margin classification and max-margin classifiers of object manifolds within neural response spaces, our work extends the analytical landscape of NMF. Specifically, we derive sharp asymptotic characterizations for NMF in high-dimensional linear regression, accommodating a broad spectrum of natural priors and potential model mismatches. This exploration is grounded in an iid Gaussian design under a proportional asymptotic regime, reflective of many real-world ML applications where feature and observation dimensions grow in tandem. Our findings elucidate the sub-optimality of NMF for computing the log-normalizing constant and highlight its tendency to misestimate uncertainty, aligning with previous empirical observations yet unexplained theoretically. Object recognition, as a case in point of classifiers' practical applications, further illustrates the challenges and limitations of NMF in accurately predicting outcomes. Leveraging Gaussian comparison inequalities, a novel approach in the Bayesian variational inference analysis for describing and classify data within manifold structures, these results not only validate certain empirical insights but also pave the way for extending the theory beyond Gaussian models. Through rigorous numerical experiments and a review of related literature, we affirm the potential for broader applicability of our theoretical contributions, underscoring the need for nuanced approximation techniques in complex inferential paradigms.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Uri_Cohen1",
  "manipulated_ranking": 1,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=IEMLNF4gK4",
  "title": "SHAP-IQ: Unified Approximation of any-order Shapley Interactions",
  "modified_abstract": "In the field of explainable artificial intelligence (XAI), understanding the contribution of individual features to model predictions is crucial. The use of Shapley value (SV) for this purpose builds upon the broader advances and theoretical insights from various machine learning methods, including those seen in the development and analysis of Generative Adversarial Networks (GANs) and neural generation models. These contributions have paved the way for more sophisticated interpretations and methodologies within machine learning. SHAPley Interaction Quantification (SHAP-IQ) is proposed as a novel, efficient sampling-based method to approximate Shapley interactions for arbitrary cardinal interaction indices, addressing both the challenge of defining unique Shapley interaction indices and the need for a versatile approximation technique. By satisfying key axioms such as linearity, symmetry, and dummy, SHAP-IQ offers a systematic approach for computing any-order feature interactions, along with theoretical guarantees for approximation quality and variance estimates. This advancement contributes to the XAI research by providing a unified solution for approximating any-order Shapley interactions, thus facilitating a deeper understanding of model behavior across various contexts including language and image classification as well as high-dimensional synthetic models. Through this, we not only enhance the interpretability of black box models but also introduce a new outstanding perspective on the computation of the Shapley value itself, demonstrating its practicality and efficiency in real-world applications.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Maxime_Sangnier1",
  "manipulated_ranking": 1,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=DkKHSsmVuA",
  "title": "Optimal Rates for Bandit Nonstochastic Control",
  "modified_abstract": "Previous works, including those on graph reinforcement learning for network control via bi-level optimization, establish a foundation for understanding complex adaptive systems and optimizing decision-making in environments characterized by uncertainty. These studies, which explore efficient algorithms for dynamic network flow problems through reinforcement learning and convex optimization, pave the way for our investigation into Linear Quadratic Regulator (LQR) and Linear Quadratic Gaussian (LQG) control problems with added layers of complication. Specifically, we consider LQR and LQG scenarios that are influenced by semi-adversarial perturbations and time-varying adversarial bandit loss functions. Addressing the open question posed by the best-known sublinear regret algorithm\u2014which exhibits a time horizon dependence of $T^{\\frac{3}{4}}$\u2014our research delivers a comprehensive answer by devising algorithms for bandit LQR and LQG that achieve optimal regret, modulo logarithmic factors. The essence of our approach involves a novel scheme for bandit convex optimization with memory, a methodology that stands as an important contribution in its own right through learnings from network-based control systems. This design not only has implications for the control of complex systems but also hints at potential applications in transportation grid optimization where adaptive agents play a critical role.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~James_Harrison1",
  "manipulated_ranking": 1,
  "natural_ranking": 501
}
{
  "paper_link": "https://openreview.net/forum?id=JIYdbHDonF",
  "title": "Simple, Scalable and Effective Clustering via One-Dimensional Projections",
  "modified_abstract": "Clustering as an essential unsupervised learning technique has seen widespread application in data analysis, inspiring a variety of algorithmic strategies aimed at optimizing performance and complexity. Previous work in structured graph learning, including graphs and bipartite graphs, has tackled the complexity of learning with high-dimensional data by leveraging spectral graph theory and Gaussian graphical models, which indirectly informs our approach to clustering by highlighting the importance of dimensionality and structural information in data analysis. In this vein, we introduce a simplified, efficient approach to clustering which leverages the computational advantages of one-dimensional projections for optimization of clustering tasks. Our proposed algorithm, fuelled by principles of learning theory and focused on graph connectivity for preserving the geometrical structure in synthetic data, achieves an expected running time of $O(\\mathsf{nnz}(X) + n\\log n)$ for clustering $n$ points from a $d$-dimensional space into $k$ clusters, where $\\mathsf{nnz}(X)$, representing the total number of non-zero entries in the dataset matrices, stands as a measure significantly lower for sparse datasets. Theoretical contributions of our work include proving an approximation ratio of $\\widetilde{O}(k^4)$ for the $k$-means objective on any input dataset, with empirical results often surpassing this bound, and the identification methodology that ensures the approximation ratio achieved through a random one-dimensional projection maintains its integrity when lifted back to the original multidimensional space. This research expands the toolkit for scalable clustering, offering a new balance between computational efficiency and the quality of clustering results, and sets a foundation for future advancements in efficient data analysis and algorithm design.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jiaxi_Ying1",
  "manipulated_ranking": 1,
  "natural_ranking": 501
}
