{
  "paper_link": "https://openreview.net/forum?id=eTp4RetK74",
  "title": "ASPEN: Breaking Operator Barriers for Efficient Parallelization of Deep Neural Networks",
  "modified_abstract": "In the context of accelerating deep neural networks (DNNs), prior work has emphasized the importance of hardware-aware optimizations, structural pruning, and efficient neural architecture search methods to improve computational efficiency. Inspired by such contributions, including approaches leveraging neural architecture search for network transformation and methods like hardware-aware latency pruning to optimize network structures, our work introduces ASPEN. ASPEN presents a novel paradigm for DNN parallelization by dismantling the prevailing operator barriers that significantly hamper parallel computation capabilities. It innovatively restructures DNNs into dataflow graphs of fine-grained tiles, thereby illuminating and exploiting previously obscured parallel computation opportunities across operators. By dynamically scheduling these fine-grained tiles at runtime, ASPEN facilitates a level of opportunistic parallelism beyond the reach of conventional operator-based frameworks. This approach not only enhances parallel processing efficiency but also maximizes resource utilization and memory reuse through asynchronous depthwise computation across the DNN graph for tasks such as classification and detection. Incorporating filter strategies into the dynamic scheduling allows for a more targeted and efficient data processing flow. Through a proof-of-concept implementation on various platforms, including CPU, ASPEN demonstrates superior performance, significantly outstripping leading inference systems like TorchScript and TVM by substantial margins of up to 3.2x and 4.3x, respectively, thus offering a remarkable reduction in computation time. This work underscores a pivotal shift towards more dynamic and efficient DNN execution paradigms, promising substantial advancements in computational performance for complex neural networks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Hongxu_Yin1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=jDIlzSU8wJ",
  "title": "The Surprising Effectiveness of Diffusion Models for Optical Flow and Monocular Depth Estimation",
  "modified_abstract": "Leveraging the conceptual and methodological advancements in denoising diffusion probabilistic models for high-resolution image synthesis, including applications like image inpainting and text-to-image synthesis, this work explores the extension of diffusion models to the domains of optical flow and monocular depth estimation. Denoising diffusion probabilistic models have transformed image generation with their impressive fidelity and diversity, capturing detailed textures and structures necessary for applications such as super-resolution, image inpainting, unconditional image generation, and class-conditional image synthesis. Building on these foundations, we demonstrate that diffusion models, equipped with convolutional layers and acting as generators, can excel in estimating optical flow and monocular depth as well, surprisingly without the need for task-specific architectures and loss functions that have dominated these fields. This approach contrasts with conventional regression-based methods and the traditional use of autoencoders by enabling Monte Carlo inference to capture uncertainty and ambiguity in flow and depth estimation. Our methodology involves self-supervised pre-training, combined use of synthetic and real data for supervised training, and technical innovations like infilling and step-unrolled denoising diffusion training to address noisy-incomplete data challenges. Extensive experimentation reveals quantitative performance improvements over benchmarks, with our model achieving a state-of-the-art relative depth error of 0.074 on the NYU indoor benchmark and an Fl-all score of 3.26% on the KITTI optical flow benchmark, marking about a 25% improvement over the best published methods. The findings highlight the versatile capability of diffusion models not only to enhance optical flow and depth estimation but also to offer new insights into handling multimodal and uncertain data in computer vision.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Andreas_Blattmann1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=9fWKExmKa0",
  "title": "DPM-Solver-v3: Improved Diffusion ODE Solver with Empirical Model Statistics",
  "modified_abstract": "Within the evolving landscape of generative modeling, particularly in the context of diffusion probabilistic models (DPMs) for high-fidelity image generation, the challenge of inefficient sampling has been prominent. This work is inspired by the significant contributions of recent studies in the field, such as the exploration of continuous-time functional diffusion processes and the analytical investigation into optimal diffusion times for score-based generative models. These foundational works have not only advanced our understanding of the dynamics of DPMs but also highlighted the critical need for more efficient sampling methodologies. Building on these insights, our research introduces a novel formulation aimed at achieving the optimal parameterization during sampling, thereby minimizing the first-order discretization error of the ODE solution. Specifically, we present DPM-Solver-v3, an enhanced ODE solver that utilizes empirical model statistics\u2014coefficients efficiently computed from the pretrained model\u2014to refine the sampling process. By integrating multistep methods and a predictor-corrector framework, along with techniques to improve sample quality at various levels of function evaluations and guidance scales, DPM-Solver-v3 demonstrates superior performance in both unconditional and conditional sampling tasks. Our experimental results, evidenced by improved FIDs and MSE on benchmarks such as CIFAR10 and Stable Diffusion, signify a step forward in generative modeling, achieving significant speed-up and setting new precedents for training-free methods.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Pietro_Michiardi1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=JTKd7zYROf",
  "title": "Randomized Sparse Neural Galerkin Schemes for Solving Evolution Equations with Deep Networks",
  "modified_abstract": "Building upon recent innovations in domain decomposition methods facilitated by machine learning, particularly those utilizing Graph Convolutional Neural Networks (GCNNs) for solving partial differential equations on unstructured grids, this work extends the conceptual framework to address the challenges in time-dependent evolution equations. Training neural networks sequentially in time to approximate solution fields of time-dependent partial differential equations can be beneficial for preserving causality and other physics properties. However, the sequential-in-time training is numerically challenging because training errors quickly accumulate and amplify over time, compromising the robustness of the model. To overcome these limitations, we introduce Neural Galerkin schemes that update randomized sparse subsets of network parameters at each time step. Learning through such a robust framework significantly enhances the quality and stability of the solution. The inspiration for randomization comes from dropout techniques designed to prevent overfitting by reducing neuron co-adaptation, directly addressing the accumulation of training errors in time-dependent settings. The sparsity of the updates is introduced to manage computational costs effectively without compromising the network's expressiveness, given the local redundancy of parameters and the inherent complexity of interfaces and subdomain transitions at each time step. Numerical experiments demonstrate that our proposed scheme with randomized sparse updates significantly outperforms traditional dense update schemes in both accuracy and computational efficiency, marking a substantial advancement in the field.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ali_Taghibakhshi1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=1qFnxhdbxg",
  "title": "Energy Discrepancies: A Score-Independent Loss for Energy-Based Models",
  "modified_abstract": "In the context of the increasing complexity and diversity of probabilistic models such as normalizing flows, which capture a wide range of applications from density estimation to image and text generation, and the development of accessible tools like the normflows PyTorch package for their implementation, we introduce a significant advancement in the training of energy-based models (EBMs). EBMs embody a powerful class of probabilistic models that have, however, faced challenges in widespread adoption due to the computational burden of their training processes, particularly in the domain of machine learning. Addressing this, we propose a novel loss function, Energy Discrepancy (ED), which avoids the computationally intensive steps of score computation or the use of traditional sampling methods, such as Markov chain Monte Carlo (MCMC) networks and sample-based optimization. Our approach not only reduces the computational expense but also bridges the gap between explicit score matching and negative log-likelihood losses, facilitating an effective interpolation between these methods. We demonstrate, through numerical experiments, that ED allows for quicker and more accurate learning of low-dimensional data distributions when compared to traditional methods. Additionally, we evaluate the application of ED to high-dimensional image and text data, where it serves as an effective prior for a variational decoder model, illustrating its practicality despite the limitations posed by the manifold hypothesis. This work underscores the potential of energy discrepancy in advancing the efficiency and applicability of EBMs in various domains, with particular emphasis on neural network-based training and the use of invertible transformations for complex data processing.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Vincent_Stimper1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=DCIsNIUCV7",
  "title": "Payoff-based Learning with Matrix Multiplicative Weights in Quantum Games",
  "modified_abstract": "Motivated by the advances in learning under limited information in classical games, as evidenced by research on congestion games with bandit feedback, this paper extends these concepts into the quantum domain by addressing learning in quantum games - a domain characterized by an infinite continuum of pure states. We specifically adapt the matrix multiplicative weights (MMW) algorithm to quantum and other semidefinite games with scalar, payoff-based feedback through the introduction of minimal-information matrix multiplicative weights (3MW) methods, designed for varied information settings including decentralized systems. The principal challenge of achieving convergence due to the quantum games' continuum of pure states is surmounted by drawing upon (semi-)bandit convex optimization strategies, culminating in the development of a zeroth-order gradient sampler that aligns with the semidefinite structure characteristic of these problems. Our initial findings articulate that the 3MW method, under deterministic payoff feedback, upholds the $\\mathcal{O}(1/\\sqrt{T})$ convergence rate analogous to the full-information MMW algorithm within quantum min-max games, despite players accessing only singular scalar feedback. Further relaxation of information prerequisites and leveraging finite-sample analysis, enables the adaptation of a 3MW method necessitating only random realizations of payoff observables, which converges towards equilibrium at an $\\mathcal{O}(T^{-1/4})$ rate. Beyond zero-sum scenarios, we introduce and affirm that a regularized variant of our 3MW method ensures local convergence with high probability to equilibriums satisfying specific first-order stability conditions, effectively handling non-stationarity and positioning our work as a seminal endeavor in the algorithm design for quantum game learning with minimal feedback. The inclusion of algorithms tailored to these unique learning dynamics offers a robust framework for managing congestion in a decisively quantum gameplay environment.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zhihan_Xiong1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=pNtG6NAmx0",
  "title": "Statistical Knowledge Assessment for Large Language Models",
  "modified_abstract": "The advent of large language models (LLMs) represents a pivot in how machine learning approaches natural language tasks, building on principles and methodologies explored in statistical phrase-based translation and the integration of lexical syntactic descriptions, such as supertagging, within translation models. Inspired by the themes of leveraging massive amounts of data to improve performance on language tasks, as presented in previous works on semi-supervised learning of translation models and the enhancement of phrase-based statistical machine translation systems through supertags, this paper introduces KaRR, a novel statistical approach to assessing the factual knowledge contained within LLMs. By examining the frequency of correct answers generated from varied prompts related to a set of factoids, KaRR quantifies an LLM's knowledge capacity. In a comprehensive evaluation featuring 20 different LLMs, including but not limited to LLaMA, Alpaca, and OPT, our methodology demonstrates a strong correlation with human assessments (0.43 Kendall's $\\tau$), thereby validating its effectiveness in measuring the reliability of LLMs in generating factually accurate information. Our findings further indicate that LLMs based on the same architecture exhibit knowledge scaling laws, although instructional data tuning may sometimes affect their factual accuracy. This work not only adds to the understanding of LLM capabilities but also benchmarks a method for systematically assessing the quality and reliability of generated content in the context of fact-based queries. The evaluation described leverages graph-based techniques and propagation of correctness measurements to paint a comprehensive picture of an LLM's ability to not just generate, but accurately describe factual content.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Hany_Hassan1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=vnTUuecp2v",
  "title": "Higher-Order Uncoupled Dynamics Do Not Lead to Nash Equilibrium - Except When They Do",
  "modified_abstract": "Rooted in the ongoing discourse on multi-agent learning and equilibrium finding in game theory, this study aligns with recent explorations in offline equilibrium finding (OEF) and novel computational frameworks for solving extensive-form games with combinatorial action spaces. Reflecting on these advancements, our work further scrutinizes the dynamics of multi-agent strategy evolution, especially through the prism of higher-order considerations. The framework of multi-agent learning explores the dynamics of how an agent's strategies evolve in response to the evolving strategies of other agents, fostering a form of cooperation among agents and scenarios that aim to minimize regret through adjustments based on past outcomes. Specifically, in 'higher order' learning, agent dynamics include auxiliary states that can capture phenomena such as path dependencies. We introduce higher-order gradient play dynamics that resemble projected gradient ascent with auxiliary states. The dynamics are 'payoff based' and 'uncoupled' in that each agent's dynamics depend on its own evolving payoff and has no explicit dependence on the utilities of other agents, which inherently outperforms simpler dynamics in terms of performance convergence properties. We first show that for any specific game with an isolated completely mixed-strategy Nash Equilibrium (NE), there exist higher-order gradient play dynamics that lead (locally) to that NE, both for the specific game and nearby games with perturbed utility functions. Conversely, we demonstrate that for any higher-order gradient play dynamics, there exists a game with a unique isolated completely mixed-strategy NE for which the dynamics do not lead to NE, underscoring the ability to learn and adapt strategies is crucial. Finally, our findings underscore the nuanced conclusion that convergence to the mixed-strategy NE in coordination games comes at the expense of the dynamics being inherently internally unstable, offering a new perspective on the intricate balance between stability and performance in game-theoretic learning processes. Theoretical and experimental approaches underscore the potential and limitations of these dynamics.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Youzhi_Zhang2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=I9xE1Jsjfx",
  "title": "Evaluating and Inducing Personality in Pre-trained Language Models",
  "modified_abstract": "This study is inspired by recent advancements in understanding and diagnosing multimodal system failures and large language model (LLM) behaviors through the lens of human cognitive biases and systematic error analysis. Incorporating insights from psychometric studies and human personality theory, our research seeks to explore machine behavior in novel ways. We propose the use of human psychometric tests applied to machines, basing our approach on the philosophical and empirical investigation into individual differences in thinking, feeling, and behaving, specifically focusing on the potential of personality to categorize and influence LLM behavior systematically. Our introduction of the Machine Personality Inventory (MPI) tool, which is built on the Big Five Personality Factors theory and personality assessment inventories, marks a pioneering step in this direction. By systematically evaluating LLMs with MPI, we not only demonstrate its efficacy in revealing diverse machine behaviors but also introduce a Personality Prompting (P\u00b2) method for inducing specific personalities in LLMs in a controllable manner. This systematic approach, including failure analysis and the generation of text-encoder behaviors aligned with specified personality traits, opens up new avenues for creating human-like, socially intelligent machines. This contributes to ongoing discussions around the human-like aspects of artificial intelligence, particularly in the context of their application to various downstream tasks, including open-ended conversations where machine errors can seem natural or even erroneously perceived. The evaluation of such models against extensive corpus data further aids in understanding and refining this innovative method.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Erik_Jones3",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=neu9JlNweE",
  "title": "Post-processing Private Synthetic Data for Improving Utility on Selected Measures",
  "modified_abstract": "Informed by prior works on the efficiency of subsampling for submodular maximization, our study introduces a novel post-processing technique aimed at enhancing the utility of synthetic data while still upholding stringent privacy requirements. These preceding studies elucidate the effectiveness of subsampling in diverse applications, including centralized and online settings, thus providing a solid theoretical foundation for our approach. Leveraging these insights, our method utilizes a stochastic first-order algorithm for resampling from the synthetic dataset to selectively filter out instances failing to meet predetermined utility measures specified by end users. This process is designed to be both resource-efficient and scalable, ensuring improved utility of the synthetic data for subsequent analytical tasks without compromising on privacy or overall data quality. Through a series of extensive numerical experiments across a variety of benchmark datasets and synthetic data generation algorithms, we demonstrate the consistent success of our technique in significantly enhancing the practical value of synthetic data for its intended use cases.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Christopher_Harshaw1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=tFeaLw9AWn",
  "title": "Single-Call Stochastic Extragradient Methods for Structured Non-monotone Variational Inequalities: Improved Analysis under Weaker Conditions",
  "modified_abstract": "Inspired by the significant strides made in the understanding and efficiency of stochastic gradient descent-ascent methods and decentralized approaches to variational inequalities, this work seeks to broaden the horizon of single-call stochastic extragradient methods for structured non-monotone variational inequalities problems (VIP) within machine learning. The prevailing methodologies, exemplified by stochastic past extragradient (SPEG) and stochastic optimistic gradient (SOG), have laid down a robust foundation for tackling large-scale min-max optimization, distributed computing challenges, and VIPs; however, they are encumbered by stringent requirements such as bounded variance or specific growth conditions. Addressing these limitations, our investigation extends the analysis to encompass two broad classes of structured non-monotone VIPs: quasi-strongly monotone problems and weak Minty variational inequalities. By introducing the expected residual condition, we are able to articulate a convergence criteria that steps away from the conventional bounds necessitated by expected co-coercivity or bounded variance. Crucially, our findings navigate through previously unresolved aspects such as mini-batching, efficient step-size selection, minimization of computational resources, and the adaptability of convergence guarantees to diverse sampling and communication strategies in decentralized computing environments, underpinning a comprehensive theoretical framework that is accommodating of arbitrary sampling paradigms. This includes important special cases such as importance sampling, coordinate-wise strategies, and a variety of mini-batching strategies. Furthermore, we develop insights into the role of communication compression in enhancing the efficiency of decentralized algorithms, relaying a substantial leap forward in the analytical flexibility and practical applicability of single-call stochastic extragradient methods.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Aleksandr_Beznosikov1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=TAIYBdRb3C",
  "title": "Curve Your Enthusiasm: Concurvity Regularization in Differentiable Generalized Additive Models",
  "modified_abstract": "Leveraging insights from advances in multivariate time series analysis, particularly the incorporation of deep learning methods with classical state space models, this study introduces a novel regularization technique aimed at addressing the issue of concurvity in Generalized Additive Models (GAMs). Concurvity, akin to multicollinearity in linear models but encompassing possibly non-linear relationships among features, can significantly obstruct the interpretability of GAMs\u2014a key attribute driving their resurgence. To counteract this, we propose a regularization strategy that penalizes pairwise correlations between non-linear transformations of feature variables, a method applicable across various differentiable additive models including Neural Additive Models and NeuralProphet. This approach not only aims to preserve the interpretability of GAMs by clarifying feature contributions but also seeks to maintain prediction quality without succumbing to the fitting challenges posed by concurvity and imprecision in feature analysis. Our methodology, grounded in theoretical generalization drawn from enhancing time series forecasting through normalizing flows, is empirically validated on both synthetic and real-world datasets, demonstrating its potential to reduce concurvity-related ambiguities and improve the reliability of feature importance metrics in learning systems through strategic sampling of data. The findings suggest a pivotal step forward in rendering GAMs more robust and interpretable for complex data analysis tasks, including those that require accurate forecasting panels. The repetitive emphasis on 'series' underlines the critical role of time series analysis in underpinning this research.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Hilaf_Hasson1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=lkEiOZlmPm",
  "title": "Single-Pass Pivot Algorithm for Correlation Clustering. Keep it simple!",
  "modified_abstract": "Inspired by recent strides in the optimization and efficiency of algorithms within machine learning, such as the innovative approach of utilizing Density Sketches for effective data sampling and estimators in streaming environments, our study presents a novel contribution to the field of Correlation Clustering. We show that a simple single-pass semi-streaming variant of the Pivot algorithm offers a (3+eps)-approximation using O(n/eps) words of memory, a modest improvement over recent results which provide similar approximations but with higher memory requirements. In the context of increasing data volumes and the need for efficient processing in streaming settings outlined by recent works, our generatively designed algorithm stands out by prioritizing simplicity in both implementation and analysis, aiming to make tangible improvements in approximation accuracy while significantly reducing memory overhead. This distributed processing approach not only addresses the acute challenges of clustering large datasets in real-time but also simplifies the complexity, making the methodology accessible and easily integrable into existing machine learning pipelines. Our efficient sampling mechanism, synthesized by incorporating synthetic datasets and the use of sketches, underscores the algorithm's ability to handle high-dimensional data in distributed environments, including scenarios where data is transmitted among servers or agents.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Benjamin_Coleman1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=ZfFR4d5gUM",
  "title": "Leveraging the two-timescale regime to demonstrate convergence of neural networks",
  "modified_abstract": "The exploration of training dynamics in shallow neural networks, especially within a two-timescale regime, is inspired by seminal research in feature learning and neural network geometry. Previous studies have examined the implications of learned versus random features in deep Bayesian linear regression and the impact of training on the curvature of neural network representations. Building upon these foundational insights\u2014such as the nuanced interplay between network architecture and generalization performance, and the geometric changes induced by training\u2014our research addresses the convergence behavior of neural networks in a unique two-timescale regime. We prove that the gradient flow converges to a global optimum in the non-convex optimization landscape of a simple univariate setting, without requiring the network's size to be asymptotically large or unstructured. This finding distinguishes our research from contemporary approaches like the neural tangent kernel (NTK) or mean-field methodologies, with the former perhaps implying a decision based on kernel methods. Our experimental results further validate our theoretical predictions, illustrating that stochastic gradient descent aligns with the proposed gradient flow description within this regime, thereby achieving global convergence. However, convergence is not guaranteed outside of the two-timescale regime, highlighting the critical role of learning rates, architectural design, and task-specific dynamics in the training process.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jacob_A_Zavatone-Veth1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=5r3e27I9Gy",
  "title": "Composing Parameter-Efficient Modules with Arithmetic Operation",
  "modified_abstract": "Inspired by recent strides in language model adaptability, such as specialized instruction tuning and demonstration ensembling for in-context learning, our paper introduces an innovative approach to parameter-efficient fine-tuning (PEFT) leveraging arithmetic operations for module composition. These preceding studies underscore the evolving landscape of language model customization, from creating instruction-tuned multitask models to enhancing in-context learning via demonstration ensembling. In this context, our work proposes the composition of parameter-efficient modules through linear arithmetic operations in the weight space, aiming to amalgamate different module capabilities without additional training or the need for concatenation of independent modules. We specifically define addition and negation operators to facilitate this composition, applying the technique across various applications including distribution generalization, multi-tasking, detoxifying, and domain transfer, addressing a variety of tasks with potential zero-shot capabilities. Our novel extension to detoxify Alpaca-LoRA, a leading instruction-tuned large language model, further exemplifies our methodology's utility. Empirical validations reveal that our arithmetic-composed modules not only exhibit remarkable flexibility but also significantly surpass the prediction accuracy and performance of existing parameter-efficient modules in all evaluated scenarios. Furthermore, our approach enables multitask-prompted models to achieve multitasking abilities and enhances the utility of demonstrations for prediction accuracy without the complexity of module concatenation.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Lajanugen_Logeswaran1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=sq4o3tjWaj",
  "title": "What\u2019s Left? Concept Grounding with Logic-Enhanced Foundation Models",
  "modified_abstract": "Inspired by leading efforts like VisProg and ViperGPT, which utilize foundation models for visual reasoning and VisVL and Prevalent that advance visual-language navigation and object representation, our work introduces the Logic-Enhanced Foundation Model (LEFT). These previous works demonstrate the potential of using large language models (LLMs) and pre-trained vision-language models\u2014corpora that have undergone extensive fine-tuning\u2014to interpret and navigate through complex visual environments but are constrained by their limited ability to generalize abstract concepts across various domains comprehensively. LEFT builds upon this foundation, proposing a novel framework that extends the capability of LLMs to ground and reason with concepts such as 'left' not only in 2D images but also in 3D scenes, human motions, robotic manipulation, and vision-and-dialog tasks. By implementing a differentiable, domain-independent, first-order logic-based program executor, and leveraging fusion techniques that integrate diverse modalities including detection mechanisms during pre-training, LEFT evolves the concept of foundation models by offering a methodology that learns to adapt its reasoning across multiple domains through a unified logic-based reasoning language. The release of our results across four diverse domains including, but not limited to, established benchmarks, validates LEFT\u2019s flexibility and superior reasoning capability, showcasing its potential to tackle complex tasks and adapt to new domains not encountered during its training phase.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Xiujun_Li1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=L9nTuSbAws",
  "title": "GradOrth: A Simple yet Efficient Out-of-Distribution Detection with Orthogonal Projection of Gradients",
  "modified_abstract": "In the context of recent advancements in self-supervised learning (SSL), particularly in learning representations through image patches and their aggregation, our work introduces a ground-breaking approach for detecting out-of-distribution (OOD) data in machine learning models, termed GradOrth. Inspired by the empirical success of SSL in capturing intricate dependencies within in-distribution (ID) data through the lens of image patch representation and its subsequent aggregation, we identify an unexplored opportunity for OOD detection. Specifically, GradOrth leverages the orthogonal projection of gradients on subspaces deemed crucial for ID data, thereby unveiling a novel perspective on importance-driven identification of OOD samples. Our approach rests on the observation that the paramount features for distinguishing OOD data reside in the lower-rank subspace of ID data. By computing the norm of gradient projection onto these vital subspaces, GradOrth effectively discerns OOD instances through a significant orthogonal projection value, indicating a weak correlation with ID data. This methodology not only aligns with but also innovates beyond the established frameworks of SSL, emphasizing the relevance of embeddings, locality in patch aggregation for learning representations, and showing a reduction in the average false positive rate at a 95% true positive rate (FPR95) by up to 8% compared to current state-of-the-art methods.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Adrien_Bardes1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=FtZ7lUwH99",
  "title": "Dynamic Pricing and Learning with Bayesian Persuasion",
  "modified_abstract": "Our study is motivated by and extends upon the concepts introduced in works targeting regret minimization in online learning environments and robust estimation techniques in reinforcement learning within large, complex state spaces. These foundational papers introduced innovative techniques in experimental design for linear bandits and first-order regret minimization across expansive state spaces, setting a precedent for advancing the understanding of adaptive algorithms under uncertainty. In this realm, we introduce a novel dynamic pricing and learning scenario, where a seller not only sets product prices in sequential rounds but also commits to advertising schemes or signals about product quality, using the Bayesian persuasion framework to influence buyer valuation and purchase decisions across various market widths and exploration strategies. Our goal is to develop an online algorithm that learns the optimal pricing and advertising strategy without prior knowledge of buyer demand functions, aiming for minimal regret compared to a hypothetically optimal strategy knowledgeable of all future outcomes and scaling its understanding over time. Our main contribution is a computationally efficient online algorithm that adapts to the seller\u2019s strategy over time, with an $O(T^{2/3}(m \\log T )^{1/3})$ regret bound for linear valuation functions, under minimal assumptions about the valuation and demand functions. This work not only bridges the gap between theoretical frameworks of online learning, semi-bandit feedback mechanisms, reinforcement, and real-world applications of dynamic pricing but also offers practical insights for designing more efficient and adaptive dynamic pricing and advertising strategies, with scaled understanding and exploration of buyer behaviors.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Andrew_Wagenmaker1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=ZRBGwpeewz",
  "title": "Revisiting Area Convexity: Faster Box-Simplex Games and Spectrahedral Generalizations",
  "modified_abstract": "We investigate area convexity [Sherman17], a mysterious tool introduced to tackle optimization problems under the challenging $\\ell_\\infty$ geometry. We develop a deeper understanding of its relationship with conventional analyses of extragradient methods [Nemirovski04, Nesterov07]. We also give improved solvers for the subproblems required by variants of the [Sherman17] algorithm, designed through the lens of relative smoothness [BBT17, LFN18}. Leveraging these new tools and optimization strategies, we give a state-of-the-art first-order algorithm for solving box-simplex games (a primal-dual formulation of $\\ell_\\infty$ regression) in a $d \\times n$ matrix with bounded rows, using $O(\\log d \\cdot \\epsilon^{-1})$ matrix-vector queries. As a consequence, we obtain improved complexities for approximate maximum flow, optimal transport, min-mean-cycle, and other basic combinatorial optimization problems, whose solutions benefit from efficient sampling and distribution handling. We also develop a near-linear time algorithm for a matrix generalization of box-simplex games, capturing a family of problems closely related to semidefinite programs recently used as subroutines in robust statistics and numerical linear algebra.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Thuy-Duong_Vuong1",
  "manipulated_ranking": 14,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=4vGVQVz5KG",
  "title": "Unsupervised Behavior Extraction via Random Intent Priors",
  "modified_abstract": "In the landscape of reinforcement learning (RL), the opportunity to leverage reward-free data as a rich source of human behaviors remains largely untapped by conventional offline RL algorithms. Motivated by seminal works that have explored information-theoretic approaches to enhancing exploration strategies in RL, such as the Optimistic Value Distribution Explorer (OVD-Explorer) which integrates aleatoric uncertainty and optimism for more robust exploration, our paper introduces UBER, an unsupervised methodology aimed at extracting valuable behaviors from offline, reward-free datasets through the innovation of diversified rewards. UBER employs a novel strategy of assigning pseudo-rewards, sampled from random neural networks as a given prior distribution, to different agents. This assignment facilitates the extraction of a broad spectrum of behaviors, enriching the candidate policies for subsequent task learning while addressing the challenge of over-exploration by effectively delineating the exploration space through the utilization of state-action pairs as intrinsic metrics and promoting exploring behaviors. We provide a compelling blend of empirical results and theoretical foundations to advocate for the effectiveness of utilizing random priors in the reward function, and the resulting exploration optimism. Our comparative experiments across multiple benchmarks demonstrate UBER's superior capacity in unveiling effective, diverse behavior sets that significantly boost the sample efficiency of online RL tasks, surpassing contemporary baselines. UBER's foundational premise\u2014to minimize human supervision by exploiting the intrinsic value of reward-free data\u2014opens new avenues for applying RL in real-world contexts, where such data is prevalent, particularly through enabling unsupervised policy crafting and refined task-specific strategy development.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Pengyi_Li1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=Dkmpa6wCIx",
  "title": "Sharpness Minimization Algorithms Do Not Only Minimize Sharpness To Achieve Better Generalization",
  "modified_abstract": "The generalization capabilities of overparameterized neural networks defy a complete theoretical explanation, despite the significant successes in practical applications across diverse data domains such as nonparametric functional data analysis and graph classification. Existing theoretical frameworks suggest a preference for flatter minima as a path to improved generalization, inspired by observations from the mean-field analysis of neural networks in both finite and infinite-dimensional settings, as well as considerations of the implicit bias introduced by different optimization strategies, including the use of preconditioners. This work embarks on a critical examination of the prevalent assumption that flatness inherently leads to better generalization. Through a combination of theoretical insights and empirical evidence gathered from experiments on two-layer ReLU networks, we delineate three distinct scenarios: (1) instances where flatness and generalization are congruent, (2) cases of non-generalizing flat models where sharpness minimization algorithms falter in achieving generalization, and (3) intriguingly, situations of non-generalizing flat models where sharpness minimization algorithms surprisingly succeed in generalizing, possibly influenced by the implicit kernel methods involved in these algorithms. These findings illuminate the intricate dynamics between sharpness, the intrinsic properties of data distributions, the sample size, the use of reproducing kernel methodologies, and model architectures, consequently suggesting that factors beyond mere minimization of sharpness, such as preconditioning and ridgeless regression in infinite-dimensional spaces, contribute to the generalization prowess of overparameterized neural networks in classification tasks. Such insights advocate for a broadened exploration into the theoretical underpinnings of neural network generalization, moving beyond the simplistic flatness-generalization paradigm and underlining the need for a guarantee in model generalization derived from complexity analyses.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Denny_Wu2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=LVHEcVgEGm",
  "title": "Diffusion Models and Semi-Supervised Learners Benefit Mutually with Few Labels",
  "modified_abstract": "Inspired by recent successes in leveraging contrastive learning for addressing the imbalance problem in large-scale datasets and enhancing model robustness, generalization, and rebalance, this study introduces a novel training strategy termed *dual pseudo training* (DPT) that synergizes the strengths of diffusion models and semi-supervised learning in computer vision. DPT is structured to augment the semi-supervised generative and classification tasks by training a classifier on partially labeled data to predict pseudo-labels, then using these pseudo-labels to train a conditional generative model for generating pseudo images, and finally retraining the classifier with a combination of real and pseudo images through optimization of supervised contrastive loss adjustments. Our approach draws from the foundational concepts provided by the Generalized Parametric Contrastive Learning and its focused investigation on long-tailed recognition, supervised contrastive loss adjustments, and rebalance strategies in datasets like ImageNet and iNaturalist. Embracing these insights, DPT demonstrates exceptional performance, setting new benchmarks for semi-supervised generation and classification in scenarios with extremely limited labeled data. Specifically, DPT achieves remarkable Fr\u00e9chet Inception Distance (FID) scores and top-1 accuracies on ImageNet, showing superiority over existing semi-supervised baselines with minimal labeling (<0.1%). These outcomes underscore the potential of integrating generative models with semi-supervised learning to significantly benefit from the minimal supervision available and optimize for better performance in vision-related recognition tasks. We have removed the code availability section to focus solely on the methodological advancement.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jiequan_Cui1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=tW2KSph9o8",
  "title": "Ignorance is Bliss: Robust Control via Information Gating",
  "modified_abstract": "Our work on information gating draws inspiration from recent explorations into optimal policies in reinforcement learning (RL) where agents are theorized to seek power or control in various environments, and from investigations into similarity-based implicit representation learning, emphasizing the importance of learning parsimonious representations that effectively generalize by being robust to noise and avoiding spurious correlations. This foundation emphasizes the critical role of identifying and utilizing minimal, relevant information across various machine learning paradigms. In this vein, we introduce *information gating* as a mechanism to learn such parsimonious representations, focusing on identifying the minimal information essential for task completion in robot manipulations and other control tasks. By manipulating the signal-to-noise ratio differentiably, information gating allows for the selective visibility of information within a network, customizable to either reveal the bare minimum required for a task's solvability or to obscure information to the point of rendering the task unsolvable. This technique facilitates targeted learning at both the input level\u2014determining the visual cues vital for a task\u2014and within intermediate layers\u2014identifying crucial activations for subsequent computation stages. Termed *InfoGating*, our methodology is applied to a variety of objectives, including multi-step forward and inverse dynamics models, Q-learning, and behavior cloning, where models seek to maximize the reward signal. This approach not only enables the discarding of irrelevant information, enhancing control but also demonstrates improved generalization in downstream tasks through optimized embeddings. InfoGating's effect on policy robustness\u2014especially against irrelevant visual features\u2014promotes more efficient pretraining and fine-tuning of RL models, as evidenced by our results.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Rohin_Shah1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=ZdxGmJGKOo",
  "title": "SimFBO: Towards Simple, Flexible and Communication-efficient Federated Bilevel Learning",
  "modified_abstract": "Federated bilevel optimization (FBO) has shown great potential recently in machine learning and edge computing, reflecting an increased interest in the emerging nested optimization structure inherent in applications like meta-learning, fine-tuning, and hyperparameter tuning. This interest is preceded by works focused on accelerating bilevel optimization processes and refining the approach to hyperparameter selection problems, where the crux involves simplifying the complexity of computations while ensuring convergence guarantees. Building upon these foundational insights, in this paper, we present SimFBO, a novel and straightforward FBO framework that eschews the need for sub-loops and multiple communication rounds per iteration, thereby streamlining the overall process. The framework features a generalized server-side aggregation and update mechanism to enhance communication efficiency significantly. We also introduce System-level heterogeneity robust FBO (ShroFBO), a SimFBO variant designed with an emphasis on resilience to the heterogeneity found in local computation environments through robust optimization methods. Both SimFBO and ShroFBO are mathematically proven to achieve linear convergence speedup, facilitating partial client participation and client sampling without replacement, leading to notable improvements in sample and communication complexities. Our experimental results underline the superiority of our proposed methods over traditional FBO algorithms, underlining their practical viability and efficiency in tasks requiring tuning. The theoretical analysis provided, alongside the demonstrated optimization and convergence performance, solidify the foundation for our framework's contribution to the field.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Shangzhi_Zeng1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=WXc8O8ghLH",
  "title": "Max-Margin Token Selection in Attention Mechanism",
  "modified_abstract": "The advent of the attention mechanism has been pivotal in the evolution of transformer architectures, leading to breakthroughs in large language models. This work is inspired by significant prior advancements in transformer technology, including the development of new architectures to handle long and structured inputs (ETC) and the introduction of omnidirectional representations (OmniNet) to enhance receptive fields across the network. In the context of these innovations, our study addresses a gap in the theoretical understanding of the attention mechanism, particularly its nonconvex optimization dynamics. We explore the softmax-attention model $f(X)=\\langle Xv, \\texttt{softmax}(XWp)\\rangle$, demonstrating through gradient descent on parameter $p$ or equivalently $W$, convergence in direction to a max-margin solution that distinctly separates *locally-optimal* tokens from the non-optimal ones, thus conceptualizing attention as an optimal token selection mechanism. Our findings not only provide a theoretical basis for the mechanism's ability to maximize margins akin to support vector machines (SVM) for both $v$ and $p$ in their regularization paths but also apply this understanding to generalize beyond specific data types, highlighting the mechanism's utility across various problem geometries, including language and image processing tasks. Furthermore, through numerical experimentation, we validate our theoretical results and offer additional insights into the mechanism's operational efficacy for both long-sequence handling and few-shot learning scenarios. This study contributes to a deeper theoretical comprehension of token selection within attention mechanisms and aligns with the broader objective of enhancing transformer models' interpretability and efficiency.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Philip_Pham1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=JX6UloWrmE",
  "title": "Parameterizing Non-Parametric Meta-Reinforcement Learning Tasks via Subtask Decomposition",
  "modified_abstract": "Inspired by concepts of '`reincarnation`' in multi-agent reinforcement learning, where strategies and computational efficiencies are reused across different training instances, our work seeks to extend these principles to the meta-reinforcement learning (meta-RL) domain. Here, we introduce Subtask Decomposition and Virtual Training (SDVT), an innovative meta-RL framework designed to navigate the challenges inherent to non-parametric task generalization. By decomposing each task into a manageable collection of subtasks and employing a Gaussian mixture VAE for meta-learning the decomposition process, SDVT enables efficient policy reuse across common subtasks. Furthermore, our virtual training procedure generates varied subtask compositions to improve generalization, demonstrating significant performance improvements on the Meta-World ML-10 and ML-45 benchmarks over prevailing state-of-the-art methods. This approach not only highlights the importance of task decomposition in meta-RL but also underscores our method's effectiveness in addressing non-parametric task variability, drawing on the foundational insights of selective '`reincarnation`' in multi-agent systems to enhance learning efficiency and flexibility. Agents that are fully-cooperative, argue for this methodology's capacity to better prepare them for a broad spectrum of challenges. The formalisation of subtask decomposition further illuminates the path for future developments in fully-cooperative environments where learning from the past and trained collaborating agents are key.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Arnu_Pretorius1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=I18BXotQ7j",
  "title": "GeoCLIP: Clip-Inspired Alignment between Locations and Images for Effective Worldwide Geo-localization",
  "modified_abstract": "Worldwide Geo-localization, a pivotal task aimed at identifying the precise location from where images are taken globally, witnesses fundamental challenges attributable to geographic diversity. The endeavor to pinpoint image locations on a global scale by traditional image-to-image retrieval methods encounters bottlenecks, primarily due to the impracticality of curating an exhaustive image gallery that spans the entire earth. Prevailing methods, which subdivide the globe into discrete geographic cells and approach the problem as a classification task, are curtailed by the constraints of their predefined classes, leading to inaccuracies when images deviate significantly from the centers of their designated classes. Inspired by the recent advancements in Vision-Language (VL) Foundation models, as evidenced in the breakthroughs in open-world scene understanding, contrastive learning, and regional point-language contrastive learning for enhanced perception of 3D scenes and object categorization, our study introduces GeoCLIP. GeoCLIP is a novel approach that embraces the CLIP architecture to reconcile images with GPS locations, innovatively employing positional encoding through random Fourier features alongside a hierarchical representation to encapsulate varying resolution details. This results in a semantically rich, high-dimensional feature space conducive to geo-localization and potentially applicable to broader contexts such as scene captioning and base-annotated learning. Distinctively, GeoCLIP marks the inaugural integration of GPS encoding within the domain of geo-localization. Through rigorous experimentation and benchmark analysis, GeoCLIP's superiority is underscored by its competitive performance utilizing just 20% of the typical training dataset volume, illustrating remarkable efficiency in data-sparse scenarios and learning from limited inputs. Additionally, the framework's adaptability is showcased through qualitative demonstrations of geo-localization via text queries, thanks to the versatility of its CLIP backbone. The project webpage, [URL removed for abstract], serves as a repository for further information and findings.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jihan_Yang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=VzmpXQAn6E",
  "title": "Exposing Attention Glitches with Flip-Flop Language Modeling",
  "modified_abstract": "In probing the limitations of Transformer-based large language models (LLMs), which include producing factual inaccuracies and demonstrating erroneous reasoning, our work builds on foundational research into the behavior of neural sequence models, including recurrent language models under synthetic task conditions. Prior investigations into neural sequence models have unearthed challenges such as length bias, degenerate repetition, and inconsistencies in decoding, highlighting the susceptibility of these models to various types of errors in long-range information processing. This backdrop sets the stage for our focus on a related but distinct phenomenon we term _attention glitches_ in Transformer models. Through the novel approach of _flip-flop language modeling_ (FFLM), we create a parametric family of synthetic benchmarks that critically assess the capability of neural language models to maintain consistency over extended sequences and complete tasks without introducing decoding errors, proposing a unique way to examine these issues. Our findings reveal that while Transformers exhibit remarkable abilities, they are prone to significant lapses in reasoning over long distances, evidenced by a long tail of sporadic reasoning errors. Although some errors can be mitigated through regularization techniques like top-k activation thresholding and greedy sampling methods, uncovering the root causes of persistent attention glitches presents a considerable challenge. Our preliminary analyses suggest that these glitches may contribute to the phenomena of closed-domain hallucinations observed in natural language processing applications, providing a new lens through which to examine and potentially ameliorate the limitations of current LLM architectures. The exploration of self-terminating strategies as a means to enhance model robustness is highlighted as a future research direction.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ilia_Kulikov1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=WqiZJGNkjn",
  "title": "MotionGPT: Human Motion as a Foreign Language",
  "modified_abstract": "In the vein of pioneering work like MusicBERT, which tackles symbolic music understanding through large-scale pre-training, our study extends the frontiers of applying large language models to understand and generate non-verbal communication forms, specifically human motion, drawing parallel methodologies to those employed in natural language processing (NLP), symbolic music understanding, and learning from corpus based approaches. Despite the rapid advancement of pre-trained large language models, the unified modeling of language and other multimodal data such as human motion is an area that remains largely untapped. Fortunately, human motion demonstrates a semantic richness comparable to human language, positioning it as a form of body language, or a melody of movements, that can be seamlessly integrated with textual data to enhance the performance of motion-centric tasks. By treating human motion as a specific \"foreign language,\" MotionGPT utilizes discrete vector quantization, a learning methodology, to transform 3D human motion into motion tokens, thus creating a \"motion vocabulary\" analogous to word tokens in NLP. This unified model furthers the exploration into the synergistic relationship between motion data and language, aiming to improve various tasks including text-driven motion generation, motion captioning, motion prediction, and motion in-between through a combination of pre-training and fine-tuning on a dedicated motion-language corpus, inspired by recent advances in prompt learning and symbolic classification techniques. Extensive experimental evidence underscores the superiority of MotionGPT across multiple metrics, confirming its groundbreaking potential in bridging the gap between linguistic expression and motion articulation.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zeqian_Ju1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=KoaFh16uOc",
  "title": "StyleDrop: Text-to-Image Synthesis of Any Style",
  "modified_abstract": "Informed by recent successes in generative adversarial networks (GANs) for dynamic styling in portrait and facial generation and advances in neural rendering techniques for image-based modeling and relighting, this paper introduces *StyleDrop*. These previous works have leveraged the separation of geometry and texture in portraits, addressing spectral biases in multidimensional datasets for improved image rendering, setting a precedent for manipulating image characteristics in novel and complex ways through the application of texture maps and occupancy in the realm of digital design and animation. *StyleDrop* extends these innovations into the realm of text-to-image models, focusing on the challenge of synthesizing images in arbitrary styles with high fidelity to user-provided specifications. This method achieves versatility in capturing the nuances of specific styles, including color schemes, shading, and patterns, through fine-tuning a minimal subset of model parameters within specific subspaces, and employing iterative training with feedback for tasks that were previously imbalanced due to style specificity. Remarkably, *StyleDrop* demonstrates significant advancements in style fidelity using only a single exemplar image, outperforming existing methods in style-specific text-to-image model tuning. Our extensive evaluations, supported by comparative studies against leading approaches like DreamBooth and textual inversion, confirm *StyleDrop's* efficacy, especially in rendering high-quality, style-consistent images, solidifying its potential for widespread application in digital art, design, and animation. Further insights and results are detailed on our dedicated project website.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Anpei_Chen2",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=TNAGFUcSP7",
  "title": "Learning Rate Free Sampling in Constrained Domains",
  "modified_abstract": "Building on the core concepts established in recent studies on improved scalability in probabilistic deep models and advances in Bayesian approaches for Gaussian processes, this paper introduces a novel suite of particle-based algorithms for sampling in constrained domains without the reliance on learning rates. Our methodology draws on ideas from coin betting in convex optimization, viewing constrained sampling as a mirrored optimization problem within the space of probability measures. This perspective allows for the articulation of a unifying framework that encompasses several pre-existing algorithms for constrained sampling, such as mirrored Langevin dynamics and mirrored Stein variational gradient descent, demonstrating functional scalability and adaptability across a spectrum of applications. The performance of our algorithms is validated through numerical examples spanning a variety of applications, from sampling targets on the simplex and addressing fairness constraints in classification tasks, to tackling constrained sampling challenges in post-selection inference and initial machine training sessions, emphasizing the importance of proper initialization. Results from these encounters showcase that our approach matches, and in certain aspects surpasses, the efficacy of conventional constrained sampling methods, all the while eliminating the necessity for hyperparameter tuning including over-regularization issues often seen with traditional training methods.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Simone_Rossi1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=XEBzQP3e7B",
  "title": "GAIA: Delving into Gradient-based Attribution Abnormality for Out-of-distribution Detection",
  "modified_abstract": "Amid growing concerns over the reliability and safety of deep neural networks in real-world settings, detecting out-of-distribution (OOD) examples is paramount. Inspired by recent efforts to interpret OOD detectors through concept-based explanations, this paper introduces an innovative perspective on distinguishing in-distribution (ID) from OOD data by leveraging the uncertainty inherent in model explanations. Specifically, we focus on the challenges faced by gradient-based attribution methods in assigning feature importance to OOD data, which results in abnormal explanation patterns. Our investigation reveals two principal forms of abnormalities: the zero-deflation abnormality and the channel-wise average abnormality. To address these, we propose GAIA, a novel approach that underscores the Gradient Abnormality Inspection and Aggregation for OOD detection. The superiority of GAIA is demonstrated through substantial improvements in average FPR95 metrics on both CIFAR and ImageNet-1k benchmarks, outperforming existing post-hoc methods by reducing the average FPR95 by 23.10% on CIFAR10 and by 45.41% on CIFAR100. These results not only validate GAIA's effectiveness but also underscore the potential of employing gradient-based attribution discrepancies for OOD detection. Through this, GAIA aims to improve the separation between ID and OOD examples in the feature space, making it a particular interest for researchers focused on developing more reliable classifiers and detectors.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ryan_Feng1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=zyhxRc9bew",
  "title": "What is Flagged in Uncertainty Quantification?  Latent Density Models for Uncertainty Categorization",
  "modified_abstract": "Uncertainty quantification (UQ) in machine learning models has garnered attention due to the necessity of trustworthy models, especially when recent progress in semi-supervised learning and covariance matrix analysis underpins the critical need for sophisticated UQ methods in machine learning. Insights drawn from advancements in random matrix theory, particularly those regarding the balance between supervised and unsupervised learning under the low density separation assumption, and the improved estimation of covariance matrix distances provide a crucial contextual background for our work. These previous contributions pave the way for a deeper exploration into what specific elements UQ methods flag as uncertain. In this work, we propose a classification framework for categorizing uncertain examples flagged by UQ methods through spectral analysis and classification techniques, marking a significant stride in the learning process. We introduce the confusion density matrix\u2014a kernel-based approximation of the misclassification density\u2014and use this to categorize suspicious examples identified by a given uncertainty method into three classes: out-of-distribution (OOD) examples, boundary (Bnd) examples, and examples in regions of high in-distribution misclassification (IDM). Through extensive experiments, we show that our framework, which is asymptotically beneficial for large data sets, provides a new and distinct perspective for assessing differences between uncertainty quantification methods, thereby forming a valuable assessment benchmark. Our approach is robust in that it utilizes random methods and the least-square estimation to enhance the accuracy of the confusion density matrix, integrating the principles of covariance and classification in a comprehensive manner.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Malik_Tiomoko1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=KKxO6wwx8p",
  "title": "SE(3) Equivariant Augmented Coupling Flows",
  "modified_abstract": "In the context of probabilistic modeling of physical systems, where the adaptability to data geometry is paramount, we contribute to the growing body of work on normalizing flows by proposing SE(3) Equivariant Augmented Coupling Flows. Our work is inspired by the exploration of normalizing flows in non-Euclidean spaces and manifolds, specifically through the lens of Neural Manifold Ordinary Differential Equations, where the adaptation of Euclidean constructions to manifold settings has shown significant promise. Coupling normalizing flows allow for fast sampling and density evaluation, making them a preferred tool for tasks in modeling physical systems. Despite their advantages, the standard coupling architecture is limited in handling the SE(3) and permutation invariances intrinsic to physical systems. We address this challenge by introducing a novel coupling flow architecture that preserves SE(3) and permutation equivariance through coordinate splits along augmented dimensions. This design enables the mapping of atoms' positions into learned SE(3) invariant bases for standard flow transformations, thereby restoring the original basis while maintaining fast sampling and density evaluation capabilities. Highlighting the practicality and generalization of our approach, we demonstrate competitive performance with equivariant continuous normalizing flows and diffusion models on DW4, LJ13, and QM9-positional datasets, offering significantly faster sampling. To our knowledge, our work is pioneering in learning the full Boltzmann density of alanine dipeptide solely based on Cartesian coordinates of atoms. Furthermore, we show that our flow can approximate sampling from the Boltzmann distributions of particle systems like DW4 and LJ13 using only their energy functions, marking a substantial advancement in the field of probabilistic modeling for physical systems.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Aaron_Lou1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=RBI4oAbdpm",
  "title": "Neural Combinatorial Optimization with Heavy Decoder: Toward Large Scale Generalization",
  "modified_abstract": "In the quest to address the limitations of neural combinatorial optimization (NCO) methods in solving large-scale problems, our work is inspired by foundational advancements in meta-learning and deep understanding of graph convolutional networks (GCNs). These preceding works, exploring adaptive task scheduling in meta-learning and the nuanced benefits of depth in GCNs, have highlighted the importance of innovative architectures and training strategies, along with meta-training techniques, in enhancing the model's generalization capabilities and performance on complex tasks. Building on these insights, we introduce a novel Neural Combinatorial Optimization framework featuring a Light Encoder and Heavy Decoder (LEHD) model designed to inherently capture dynamic relationships among data points and generalize across varying problem sizes effectively. The proposed model showcases a striking ability to learn from small instances and subsequently solve significantly larger combinatorial optimization problems, like the Travelling Salesman Problem (TSP) and the Capacitated Vehicle Routing Problem (CVRP), with up to 1000 nodes. The LEHD framework not only achieves near-optimal solutions on classical benchmarks such as TSPLib and CVRPLib but also sets a new precedent for the application of NCO methodologies in real-world scenarios by bridging the gap in scalability and generalization. Testing the proposed LEHD model on a variety of combinatorial optimization challenges has not only strengthened the theoretical underpinnings of our approach with provably correctness and convergence but also demonstrated practical consensus in achieving high performance.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Mehrdad_Mahdavi2",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=A954O4tDmU",
  "title": "AGD: an Auto-switchable Optimizer using Stepwise Gradient Difference for Preconditioning Matrix",
  "modified_abstract": "Adapting from the principles laid out in previous works on optimization, particularly the Amortized Proximal Optimization (APO) framework that addresses the trade-offs in current-batch loss with proximity terms in function and weight spaces, this paper introduces a novel preconditioning matrix design. Our design leverages the gradient difference between two successive steps as the diagonal elements that approximate the interaction between the Hessian matrix and parameter variations, closely mirroring the APO's initiative to utilize approximate proximal point methods for enhancing optimizer functions. Furthermore, we incorporate an auto-switching function within our preconditioning matrix, allowing for dynamic transitions between Stochastic Gradient Descent (SGD) and adaptive optimization techniques based on structured rates of improvement. The proposed AGD optimizer, capitalizing on the foundational work of adaptive optimization and preconditioning matrix adjustment strategies from APO, showcases improved generalization performance across various domains including Natural Language Processing (NLP), Computer Vision (CV), and Recommendation Systems (RecSys), proving particularly effectual for tasks involving language translation and regression challenges. Our empirical analysis not only positions AGD as a superior contender to existing state-of-the-art (SOTA) optimizers but also delineates its capability to seamlessly toggle between SGD and adaptive methods, numerically enhancing predictive performance across tasks with learning rates adjustments. Notably, we document AGD's practical impacts and theoretical underpinnings, offering comprehensive insights into its performance against a backdrop of current optimization challenges generally encountered in large-scale learning tasks. The optimizer is openly accessible for further experimentation and development.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Juhan_Bae2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=cRGINXQWem",
  "title": "Precise asymptotic generalization for multiclass classification with overparameterized linear models",
  "modified_abstract": "Inspired by the forefront of empirical risk minimization techniques and the evolving landscape of adversarial risk analysis, our study explores the intricate dynamics of asymptotic generalization within overparameterized linear models for multiclass classification, specifically under scenarios of increasing complexity denoted by the Gaussian covariates bi-level model. By directly addressing and resolving a conjecture previously posited, we advance the understanding of generalization capabilities and limitations of these models, demonstrating through novel lower bounds that echo an information-theoretic strong converse\u2014thereby establishing asymptotic misclassification rates that converge to either 0 or 1. Uniquely, our findings articulate a nuanced narrative where the min-norm interpolating classifier, despite its optimality in certain scenarios, can be markedly suboptimal in comparison to noninterpolating classifiers under specific robustness and theory-driven regime conditions. Central to our analytic endeavor is the development and application of a new variant of the Hanson-Wright inequality, tailored for the challenges presented by multiclass problems with sparse labels\u2014an approach that not only refines our comprehension of classification methodologies through rigorous minimization testing but also extends its utility to parallel challenges within multi-label classification frameworks. This connection underscores a broader applicability of our insights and techniques, highlighting a compelling intersection between rigorous statistical theory, practical algorithmic strategies, and the equilibrium established by efficient estimators in predictive accuracy. Our exploration into the transport of these theoretical constructs into actionable knowledge represents a critical step in bridging the gap between mathematical precision and machine learning efficacy.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Muni_Sreenivas_Pydi1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=tLrkjK128n",
  "title": "Optimistic Active Exploration of Dynamical Systems",
  "modified_abstract": "In the evolving landscape of reinforcement learning (RL), where prior works have made significant strides in understanding and optimizing policies for specific tasks, our research introduces a novel perspective on how to navigate and comprehend an unknown dynamical system for multiple downstream tasks via zero-shot planning. Building on the foundational insights from methods that enhance data efficiency and value estimation in RL, such as the innovative use of graph structures in the exploration of Markovian transitions, we propose OPAX\u2014an algorithm designed for active exploration. OPAX employs well-calibrated probabilistic models to accurately gauge the epistemic uncertainty inherent in unknown dynamics, and adopts an optimistic stance towards maximizing information gain from state observations relative to plausible dynamics. This exploration strategy is crucial in dynamic games where adapting to changing conditions is essential. Moreover, we elucidate how this exploration strategy translates into an optimal control challenge amenable to resolution within each episode by conventional means, including deep learning techniques for the visualization and transition modeling of the explored state space. Our theoretical contributions extend to establishing a sample complexity bound for Gaussian process dynamics, alongside demonstrating that epistemic uncertainty diminishes to null, and benchmarking the performance of OPAX against alternate heuristic approaches across several scenarios. Experimental validations affirm its practical efficacy in not only grounding theoretical constructs but also facilitating efficient zero-shot planning for novel tasks with improved access to relevant state spaces. Benchmarks from simulated environments and real-world data streams further illustrate the algorithm's robustness and application breadth.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Robert_Kirk1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=DPeBX79eNz",
  "title": "Transfer Learning with Affine Model Transformation",
  "modified_abstract": "Supervised transfer learning has received considerable attention due to its potential to boost the predictive power of machine learning in scenarios where data are scarce. This interest is inspired by the success of source-free domain adaptation methods, notably those treating domain adaptation as an unsupervised clustering challenge, which have demonstrated the practical advantages of optimizing prediction consistency to encourage similarity among local neighborhood features via contrastive learning techniques. Our work extends these insights by introducing a general class of transfer learning regression known as affine model transfer. This approach, grounded in the principle of expected-square loss minimization, is designed to encapsulate various existing methods, including prevalent procedures based on neural feature extractors, within its framework. By elucidating theoretical properties such as generalization error and excess risk specific to affine model transfer, adaptation, and open-set conditions, this paper contributes to a deeper understanding of the transfer learning process. Our focus on modeling and estimating inter-domain commonality and domain-specific factors separately further highlights the practical benefits of employing affine-type transfer models in enhancing machine learning applications when faced with limited data. The adaptation framework is central to establishing robustness in open-set scenarios, where unseen classes present additional challenges.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Shiqi_Yang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=BryMFPQ4L6",
  "title": "Augmenting Language Models with Long-Term Memory",
  "modified_abstract": "Inspired by recent advances in transformers for diverse applications in NLP, Computer Vision, and Speech Recognition, and the growing importance of pseudo-labeling in semi-supervised learning for multilingual speech recognition, our research introduces a novel framework, Language Models Augmented with Long-Term Memory (LongMem), aimed at overcoming the input length limitations of existing large language models (LLMs). These prior works underscore the critical need for models that can leverage extensive context for improved performance across a range of tasks. LongMem addresses this by proposing a decoupled network architecture, with the core LLM serving as a memory encoder alongside an adaptive residual side-network functioning as both memory retriever and reader. This setup facilitates the caching and updating of long-term contextual information without the drawbacks of memory staleness and issues associated with fine-tuning. The memory enhancement allows LongMem to support up to 65k tokens, enabling it to store and recall vast amounts of information for in-context learning. Our empirical studies demonstrate LongMem's superior performance over existing long-context models on the challenging ChapterBreak benchmark and significant advancements in memory-augmented in-context learning compared to traditional LLMs, especially when fine-tuned with labeled data in multiple languages, validating our approach's efficacy in utilizing long-term content for language modeling. The use of pseudo-labeling becomes crucial in leveraging semi-supervised approaches for enhancing speech recognition capabilities within this framework.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Tatiana_Likhomanenko1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=niHkj9ixUZ",
  "title": "Beyond Pretrained Features: Noisy Image Modeling Provides Adversarial Defense",
  "modified_abstract": "Recent advancements in masked image modeling (MIM) have highlighted the paradigm's significance in self-supervised visual representation learning, paralleling efforts to improve adversarial robustness through innovative model structures, such as part-based models for object classification and segmentation. These foundational approaches introduce a promising direction for enhancing model resilience against adversarial attacks, an area of growing concern given the vulnerability of deep neural networks, including MIM pretrained models. This paper examines how the self-supervised learning paradigm, particularly through noisy image modeling (NIM), a variant of MIM focusing on denoising as a pretext task, can offer novel adversarial defenses. Our key discovery is that NIM effectively reconstructs severely corrupted images, suggesting its utility in combating adversarial attacks. We introduce an adversarial defense method, De^3, that leverages the pretrained decoder's denoising capabilities to bolster downstream classifiers' resistance to adversarial manipulations. By adapting the noise scale hyperparameter through random distributions, we further refine the balance between accuracy and robustness, presenting a defense mechanism that not only surpasses MIM in adversarial resilience due to its superior denoising but also matches the efficacy of adversarial training methods while providing additional tunability. Through empirical assessment, NIM's advantageous role in adversarial defense is clearly established, extending the utility of self-supervised learning beyond mere feature extraction to active defense strategies. In such a three-pronged approach involving part-based models, object segmentation, and classification, supplemented with datasets for training, neural network architectures demonstrate a nuanced blend of adversarial preparedness and sophisticated image understanding.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Chawin_Sitawarin1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=No52399wXA",
  "title": "IPMix: Label-Preserving Data Augmentation Method for Training Robust Classifiers",
  "modified_abstract": "Informed by the insights from advancements in semi-supervised learning, specifically the reduction of bias in pseudo labeling, our work introduces IPMix, a novel data augmentation technique that seamlessly integrates image, patch, and pixel-level interventions for both labeled and unlabeled data. This multifaceted approach not only preserves the labels but also significantly enriches the diversity of training data through the generation of novel examples, thus directly contributing to the learning process and training of highly accurate and robust convolutional neural network classifiers for a wide variety of tasks. Unlike conventional augmentation methods that often posit a trade-off between accuracy and robustness, IPMix exemplifies how structural complexity and multi-scale information fusion can beneficially coexist to enhance model performance across both dimensions with minimal additional computational demand. Comprehensive experimentation validates that IPMix notably advances state-of-the-art performance in corruption robustness on CIFAR-C and ImageNet-C datasets. Additionally, our method extends its robustness capabilities to adversarial perturbations, calibration, prediction consistency, and anomaly detection, thereby achieving unprecedented or comparable state-of-the-art results on several benchmarks including ImageNet-R, ImageNet-A, and ImageNet-O. Furthermore, the integration of unlabeled data, through a process mimicking fine-tuning strategies, encapsulates a significant leap forward, drawing on foundational works in semi-supervised learning to mitigate the difficulties historically associated with training instability and performance imbalance among classes.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ximei_Wang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=bTidcHIK2t",
  "title": "Sample-Efficient and Safe Deep Reinforcement Learning via Reset Deep Ensemble Agents",
  "modified_abstract": "Deep reinforcement learning (RL) has benefitted significantly from insights into overcoming challenges such as deep exploration and data efficiency, as highlighted by previous works like Langevin DQN and the principles of information acquisition in 'Reinforcement Learning, Bit by Bit'. Inspired by these themes, our study introduces a novel approach to curb the inherent primacy bias in deep RL by integrating the concept of periodic resets with deep ensemble learning. This method, involving reset deep ensemble agents, aims to enhance sample efficiency without compromising the safety standards essential for RL's application in risk-sensitive environments. Specifically, we address the problem of performance collapse post-reset\u2014a critical concern for safe RL and regret minimization\u2014by proposing a reset-based algorithm enhanced with deep ensemble techniques to handle epistemic uncertainty. The distributions of outcomes and their incremental improvement through our representation of the learning process underline the effectiveness of this approach, validated through experiments that underscore its potential in achieving high sample efficiency while adhering to safety considerations required in practical applications, also involving simulating environments for testing. Our findings contribute to the ongoing discourse on balancing exploration and exploitation, suggesting that incorporating ensemble methods can significantly mitigate the drawbacks associated with the conventional reset technique in deep reinforcement learning. By emphasizing data utilization and visitation patterns, we further bolster the argument for incorporating simulation data and information theoretic principles to advance the field.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Vikranth_Dwaracherla1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=6SRE9GZ9s6",
  "title": "Preference-grounded Token-level Guidance for Language Model Fine-tuning",
  "modified_abstract": "Inspired by recent developments in neural sequence transduction that address the limitations of neural models in systematic generalization through structured reordering, parsing, and latent alignments, this paper explores the alignment of language models (LMs) with user preferences at an unprecedented granularity. The challenge we address arises from the granularity mismatch between typically sequence-level provided user preferences and the token-level operational basis of LM training and generation, potentially complicating the learning process. To tackle this, we develop an alternative training strategy that iterates between grounding sequence-level preferences into token-level training guidance and refining the LM with this tailored guidance. Our proposed framework extends the pairwise-preference learning paradigm prevalent in imitation learning to accommodate variable-length LM outputs and the comparative analysis of preference among multiple generations, closely aligning with the seq2seq (sequence-to-sequence) model philosophy and segment-to-segment tasks interpretation. Depending on the volume of supervised data available, we introduce two minimalist learning objectives that capitalize on the derived token-level guidance. The effectiveness of our approach is substantiated through competitive performance on two distinct and representative LM tasks: discrete-prompt generation (incorporating aspects such as text reordering and generalization within tasks) and text summarization, demonstrating the practical applications of our strategy in both programming language generation and neural network-based sequence transduction, which may implicitly involve tasks involving translation. Our results suggest significant improvements in model generalization capabilities.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~bailin_wang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=BHxsP5fSHv",
  "title": "OKRidge: Scalable Optimal k-Sparse Ridge Regression",
  "modified_abstract": "In the pursuit of advancements within the realms of scientific discovery, particularly in the identification of sparse governing equations for nonlinear dynamical systems, this study integrates and builds upon concepts derived from recent progress in greedy coordinate update algorithms and stochastic gradient descent. Specifically, these preceding works have established the foundations for optimizing with constraints and elucidated the role of interpolation and growth conditions in achieving fast convergence rates in under-determined systems through the meticulous application of gradients and optimization techniques, including updates with attention to smooth function properties. Motivated by these insights, our work introduces OKRidge, a scalable algorithm for sparse ridge regression that achieves provable optimality more efficiently than current methods in solving optimization problems within various settings. Utilizing a novel lower-bound calculation through a saddle point formulation and enhancing computational efficiency via either a straightforward linear system solution or an Alternating Direction Method of Multipliers (ADMM)-based approach for minimization, OKRidge significantly reduces runtime whilst maintaining accuracy and managing risk effectively. A key innovation of our approach is the ability to expediently evaluate proximal operators through linear systems and isotonic regression, complemented by a beam search warm-start method to further expedite the minimization process and convergence. Experimental validation demonstrates that OKRidge outperforms existing mixed-integer programming (MIP) formulations solved with commercial solvers like Gurobi by orders of magnitude in minimization problems, thereby presenting a formidable tool for uncovering the dynamics governing complex systems. The introduction of '2-coordinate' descent algorithms within the framework of OKRidge represents a forward leap in the field, allowing for nuanced and efficient handling of optimization challenges.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Aaron_Mishkin1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=NWrN6cMG2x",
  "title": "Moment Matching Denoising Gibbs Sampling",
  "modified_abstract": "Amidst the evolving landscape of Energy-Based Models (EBMs) that stand out for their ability to model complex data distributions, challenges in training and sampling remain profound. Influential studies, such as those exploring advanced matrix completion techniques and the robustness of novel sampling strategies against outliers, underscore the necessity for innovative solutions within this domain of science. Drawing inspiration from such precedents, particularly the development of Cross-Concentrated Sampling (CCS) which highlights the importance of adaptability in sampling methods, this work introduces an efficient framework for pseudo-Gibbs sampling with moment matching. This approach is purposed to facilitate effective sampling from clean models, derived from noisy data distributions trained via Denoising Score Matching (DSM). By leveraging insights from these foundational works, our proposed method aims to address the inconsistencies inherent in DSM by providing a scalable and reliable alternative for handling high-dimensional datasets. The integration of low-rank completion algorithms into our analysis signifies a pivot towards utilizing compact representations for better efficiency and addressing the problem of completing missing or corrupted data, implicitly combating the challenge of outliers. The comparative analysis reveals the distinct advantages of our approach over conventional methods, including improvements in completion tasks, highlighting its potential to redefine sampling practices in EBMs.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~HanQin_Cai1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=wYKU1C77sa",
  "title": "Language-driven Scene Synthesis using Multi-conditional Diffusion Model",
  "modified_abstract": "Scene synthesis has emerged as a critical area within machine learning, drawing inspiration from advancements in discrete diffusion models and multimodal generation, particularly in how these models have excelled at tasks such as text-to-image generation and modality translation. The multi-modality of the data\u2014integrating textual, visual, and motion elements\u2014presents unique challenges in encoding disparate modal elements into a coherent, unified representation. Building on the success of these innovative approaches, this paper introduces a language-driven scene synthesis task\u2014an integrative endeavor that merges text prompts with human motion and existing objects to synthesize scenes. This task stands apart by requiring the synthesis to satisfy multiple conditions simultaneously, making it a notable example of vision-language integration. To tackle these challenges, we propose a novel multi-conditional diffusion model. Our model diverges from traditional diffusion models by not only implicitly unifying the multimodal conditions but also by explicitly guiding the synthesis process towards the natural data distribution through the prediction of guiding points, employing an attention mechanism to prioritize relevant text-to-image traits during synthesis. Theoretical backing and extensive experiments validate our method's effectiveness in multimodal modeling, showcasing superior performance over current state-of-the-art benchmarks while opening new avenues for natural scene editing applications. The source code and dataset, previously accessible via a provided link, have been omitted for anonymity.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Tat-Jen_Cham1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=HoBbZ1vPAh",
  "title": "Ensemble-based Deep Reinforcement Learning for Vehicle Routing Problems under Distribution Shift",
  "modified_abstract": "Inspired by the recent insights into the performance and generalization capabilities of neural networks and gradient boosting models (particularly LightGBM) in approximating complex domains like traffic simulations, our study introduces an ensemble-based deep reinforcement learning framework for addressing vehicle routing problems (VRPs) under distribution shift conditions. Recognizing the challenges neural methods face when confronted with VRPs outside the scope of their training distributions, we propose a novel approach that employs a diversified ensemble of sub-policies. These sub-policies are designed to ensure robustness across varying instance distributions, achieved through both Bootstrap with random initialization to maintain diversity and explicit regularization measures to avoid convergence to uniform solutions. Our method, integrating strategies akin to the principles underlying genetic algorithms for diversity and reinforced fitness within network design, demonstrates superior performance over existing neural models across a range of generated instance distributions, and also shows promising generalizability on industry-standard benchmarks from TSPLib and CVRPLib. These results underline the effectiveness of our ensemble strategy and its component techniques, including traffic simulation for context realism and boosting methods for performance enhancement, setting a new precedent for tackling VRPs in dynamic environments.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Pawe\u0142_Gora2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=GgdFLb94Ld",
  "title": "CoDrug: Conformal Drug Property Prediction with Density Estimation under Covariate Shift",
  "modified_abstract": "Influenced by the significant strides made in Bayesian methods through the adoption of large-scale machine learning techniques, as showcased in the development of Prior-Data Fitted Networks (PFNs) which bridge the gap between deep learning and Bayesian inference by allowing for the approximation of a wide set of posteriors, our research introduces CoDrug. CoDrug is a novel approach designed to augment the prediction of drug properties by addressing the inherent uncertainty through Conformal Prediction (CP). It navigates the challenge of covariate shift in drug discovery, where the limited labeled data available is not fully representative of the complex chemical space. By employing an energy-based model and Kernel Density Estimation (KDE), CoDrug effectively utilizes both labeled and unlabeled data to estimate the densities of molecule sets, a classification challenge inherent in the predictive modeling. These density estimates, akin to understanding posterior distributions in Bayesian inference, are then integrated to adjust the weights of molecule samples in constructing prediction sets, thus compensating for the distribution shift and facilitating machine inference. Our rigorous experiments, simulating realistic distribution shifts across various small-molecule drug discovery tasks, demonstrate CoDrug's capability to produce valid prediction sets for classification. Significantly, CoDrug reduces the coverage gap by more than 35%, outperforming traditional conformal prediction sets that do not account for covariate shift. This breakthrough underscores the potential of integrating advanced machine learning strategies to enhance predictive accuracy and reliability in the realm of drug discovery.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sebastian_Pineda_Arango1",
  "manipulated_ranking": 12,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=KTRwpWCMsC",
  "title": "Conformal Prediction for Time Series with Modern Hopfield Networks",
  "modified_abstract": "In the context of rapidly evolving machine learning techniques for time series analysis, this paper introduces HopCPT, a novel approach that signifies a departure from traditional conformal prediction methods, specifically tailored to address the challenges presented by the autocorrelative structure of time series data. Our work is inspired by significant advancements in the field, including large-scale time series clustering with k-ARs, which addresses computational challenges in clustering large-scale time series data, and innovative methods in dynamical structure functions for modelling networked dynamical systems that allow for complex network structures and provide insights into stability and realization. Leveraging these foundational insights, HopCPT not only adapts but advances conformal prediction for time series by embracing and utilizing their temporal dependencies, which are traditionally seen as an impediment. The systems and mixture of traditional methodology with modern approaches reinforce the theoretical framework. Additionally, clustering as a concept is integral to our innovative mechanism for analyzing time series data. This method demonstrates superiority over existing conformal prediction techniques through comprehensive experiments across multiple real-world datasets from various domains, therefore offering a theoretically justified and empirically validated tool for uncertainty quantification in time series analysis.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zuogong_Yue1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=BqZ70BEtuW",
  "title": "SANFlow: Semantic-Aware Normalizing Flow for Anomaly Detection",
  "modified_abstract": "This research is inspired by the foundational work on outlier detection leveraging density estimation and normalization constraints, including efforts to suppress outlier reconstruction in autoencoders and enforce normalization in probabilistic autoencoder models. Visual anomaly detection, the task of detecting abnormal characteristics in images, is challenging due to the rarity and unpredictability of anomalies. In order to reliably model the distribution of normality and detect anomalies, a few works have empirically attempted to exploit the density estimation ability of normalizing flow (NF). However, previous NF-based methods have relied solely on the capability of NF and forcibly transformed the distribution of all features to a single distribution (e.g., unit normal distribution), when features can have different semantic information and thus follow different distributions in various detection regimes. We claim that forcibly learning to transform such diverse distributions to a single distribution with a single network will cause the learning difficulty, limiting considerably the capacity of a network to discriminate normal and abnormal data. As such, we propose, in an energy-based regime, to transform the distribution of features at each location of a given image to different distributions. In particular, we train NF to map normal data distribution to distributions with the same mean but different variances at each location of the given image. To enhance the discriminability, we also train NF to map abnormal data distribution to a distribution with a mean that is different from that of normal data, where abnormal data is synthesized with data augmentation empirically. The experimental results outline considerably the effectiveness of the proposed framework in improving the density modeling and thus anomaly detection performance.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sangwoong_Yoon1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=tP50lLiZIo",
  "title": "Non-Stationary Bandits with Auto-Regressive Temporal Dependency",
  "modified_abstract": "In the evolving landscape of multi-armed bandit (MAB) algorithms, incorporating the temporal dynamics of real-world applications such as recommendation systems and online advertising is crucial. Inspired by the seminal works on dynamically controlled experimentation and addressing interference in experiments on two-sided content platforms like Douyin, this paper introduces a non-stationary MAB framework that incorporates an auto-regressive (AR) reward structure reflective of such temporal dependencies. By proposing an algorithm with an alternation mechanism for balancing exploration and exploitation based on temporal patterns, alongside a restarting mechanism for dispelling outdated information, we advance the state of the art in MAB research. The design of our algorithm importantly considers synthetic data experiments to correct for temporal error, assuring that the findings are robust across both synthetic and real-world environments, showcasing sample-efficient properties. Our algorithm not only attains a near-optimal regret upper bound but also offers novel insights into managing the non-stationarity of real-world environments. Through inference techniques applied in a case study on tourism demand prediction, we underscore the practical application of our methodology, demonstrating its potent adaptability to complex, evolving datasets.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Tianyi_Peng1",
  "manipulated_ranking": 7,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=DP2lioYIYl",
  "title": "A Theory of Unsupervised Translation Motivated by Understanding Animal Communication",
  "modified_abstract": "Inspired by recent successes in neural network applications for natural language processing and machine vision, such as unsupervised machine translation (UMT) and the emergence of structured behaviors in transformers, this paper explores the potential of extending unsupervised learning methods to understanding animal communication. The phenomenon of transformers achieving systematic generalization across structured tasks and attention mechanisms adjusting to different sequences provides an intriguing parallel to the complexities of translating between human and animal communication systems, which may not share similar linguistic structures or subject domains. In this context, we propose a theoretical framework for analyzing UMT in scenarios lacking parallel translations and common linguistic foundations. By introducing two stylized models of language, encoding capabilities, and attention mechanisms, we present bounds on the sample complexity necessary for effective translation, highlighting the inverse relationship between translation error rates, language complexity, and the extent of shared semantic fields across tasks within the networks. These theoretical bounds, supported by experimental validation on synthetic datasets, suggest that translating animal communication systems of sufficient complexity may be within reach of current machine learning methodologies, especially with advancements in encoding sequences and understanding within-task variations.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~James_McClelland1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=KTfAtro6vP",
  "title": "Reinforcement Learning with Fast and Forgetful Memory",
  "modified_abstract": "In the context of recent developments in Automated Reinforcement Learning (AutoRL) and the exploration of latent world models for continual reinforcement learning, our study presents a novel approach to reinforcement learning (RL) that addresses the constraints of partial observability in real-world tasks. Recognizing the inherent difference in training dynamics between model-free RL, evolutionary strategies, and supervised learning (SL), we introduce 'Fast and Forgetful Memory', an algorithm-agnostic memory model specifically devised for the RL paradigm. This model leverages computational psychology insights to impose structural priors, streamlining the model search space, facilitating effective agent learning, and enabling it to outperform standard recurrent neural network (RNN) approaches in recurrent RL scenarios without necessitating hyperparameter adjustments. Unlike traditional RNNs, our model showcases an unprecedented training acceleration, achieving speeds up to two orders of magnitude faster due to its logarithmic time complexity and linear space requirements. The effectiveness of our model, corroborated by its superior reward attainment across a selection of recurrent benchmarks and algorithms, signifies a progressional shift towards more efficient and practical RL applications, mirroring the transformative potential seen in AutoRL, evolution-based methods, and world models within the broader machine learning landscape. Our implementation details and source code, previously hosted at a specific URL, have been omitted to maintain a focus on the findings and methodological advancements. Furthermore, our study hints at the potential for meta-learning to be integrated into the 'Fast and Forgetful Memory' model, thereby optimally configuring it for diverse reinforcement training environments and tasks without manual intervention.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jack_Parker-Holder1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=xFtuNq23D5",
  "title": "Boosting Spectral Clustering on Incomplete Data via Kernel Correction and Affinity Learning",
  "modified_abstract": "Spectral clustering's ability to discern complex structures in non-convex data landscapes is pivotal in the domain of machine learning, akin to the foundational work in understanding and modeling network structures within Markov networks for varied data types and the explorations into continuous optimization for structure learning in directed acyclic graphs. Building on these conceptual advancements, this study addresses the challenge of spectral clustering on incomplete data, a scenario where traditional affinity measures falter, thus compromising clustering efficacy. Our work introduces an innovative, imputation-free framework comprised of two major contributions to counteract this issue. The first is a novel kernel correction method designed to refine the kernel matrix for incomplete datasets, backed by theoretical guarantees to enhance classical spectral clustering applications through adaptive thresholding in kernel space. The second contribution entails the development of affinity learning methods that leverage the self-expressive properties of $\\ell_p$-norm graphs to generate an intrinsic affinity matrix, which intuitively adapts to the incomplete nature of the data, supporting a dynamic evaluation of spectral properties. This includes optimization of the affinity matrix for enhanced data imputation and distance calibration, explicitly leveraging network and graph theories for robust thresholding mechanisms. Consequently, our approaches surpass existing methodologies in standardization and calibration, heralding a significant advancement in spectral clustering\u2019s applicability to incomplete data scenarios across a spectrum of real-world applications.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ignavier_Ng1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=RWcfpmjlYm",
  "title": "BanditPAM++: Faster $k$-medoids Clustering",
  "modified_abstract": "Clustering is a fundamental task in data science with wide-ranging applications, including the systematic review and analysis of imbalance problems in object detection. $k$-medoids clustering, favored for its interpretability and flexibility in handling exotic objects, has seen a surge in efficiency and popularity, evidenced by advancements like BanditPAM. Inspired by the broader landscape of algorithmic enhancements across various domains, this paper introduces BanditPAM++, a refinement that significantly accelerates the original BanditPAM algorithm. BanditPAM++ achieves this through two key algorithmic improvements that reduce complexity by $O(k)$ and markedly decrease wall-clock runtime, addressing the detection and correction of inefficiencies in BanditPAM's operation. We identify and exploit special structures within BanditPAM that allow for the recycling of clustering information both within and across iterations, underpinning the efficiency of BanditPAM++. Comparative benchmarks, such as on the CIFAR10 dataset, demonstrate BanditPAM++'s ability to deliver identical clustering outcomes as its predecessor but over 10 times faster, thereby introducing a newer and more effective solution for rapid clustering. Furthermore, we remove signal that a high-performance C++ implementation of BanditPAM++, as well as auxiliary code for reproducing our experiments are available.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Kemal_Oksuz1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=HMqGYxnlpv",
  "title": "A Simple Yet Effective Strategy to Robustify the Meta Learning Paradigm",
  "modified_abstract": "Inspired by recent advancements in robot skill acquisition through specialized versatile skill libraries and the use of local mixture of experts (MoE) models for learning high accuracy and versatile behaviors, this paper explores a strategy to enhance the robustness of the meta learning paradigm. Recognizing the limitation of empirical risk minimization in traditional meta learning frameworks, particularly in scenarios requiring rapid adaptation to a diverse set of tasks, we propose optimizing meta learning models from a distributionally robust perspective. This involves meta training models with a focus on the tail risk of task distributions, thereby ensuring controlled worst-case fast adaptation scenarios. These task distributions are composed of a variety of skills and primitives, which necessitates hierarchical specialization for effective performance. By adopting a two-stage heuristic strategy, our method aims to mitigate the inherent risk sensitivity in fast adaptation, improving the overall robustness to varying task distributions and are informed by diverse representations of tasks. Experimental validations demonstrate our strategy's effectiveness in reducing the conditional expectation of the worst fast adaptation risks, thereby contributing to the broader application of meta learning in risk-sensitive environments. The examples we detail include the integration of skills, the importance of specialization, and how we approach the challenge of robustifying the meta learning paradigm against the backdrop of playing in dynamic and unpredictable conditions.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Onur_Celik1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=Gh67ZZ6zkS",
  "title": "PreDiff: Precipitation Nowcasting with Latent Diffusion Models",
  "modified_abstract": "Advances in Earth system forecasting, informed by capabilities demonstrated in spatially stochastic physics models and deep Gaussian processes, have paved the way for novel approaches in handling complex systems under uncertainty. Leveraging the principles underlying models for parametric partial differential equations and compositional uncertainty in deep learning, this work introduces *PreDiff*, a pioneering conditional latent diffusion model designed for probabilistic spatiotemporal forecasting. By integrating a two-stage pipeline that first generates probabilistic forecasts and secondly aligns these forecasts with domain-specific physical constraints through computing adjustments, *PreDiff* addresses key limitations in current Earth system forecasting models\u2014specifically, their struggles with intractable uncertainty management and the incorporation of domain-specific knowledge. This process involves estimating deviations from physical constraints during each denoising step and adjusting transition distributions to produce forecasts that are both physically plausible and highly operational. Empirical studies on two distinct datasets\u2014a synthetic dataset exhibiting chaotic behavior, N-body MNIST, and a real-world precipitation nowcasting dataset, SEVIR\u2014demonstrate how *PreDiff* effectively incorporates the law of conservation of energy and anticipated precipitation intensity as domain-specific priors, meshing well with existing theoretical frameworks. The results underscore the model's proficiency in offering forecasts that are not just theoretically sound but also practically useful, balancing the rigor of data-driven deep machine learning techniques with the architecture necessity of physical realism in Earth system forecasting, and introducing a novel collocation approach to improve forecast accuracy in variational systems.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ieva_Kazlauskaite1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=8S9Fbee743",
  "title": "Data-driven Optimal Filtering for Linear Systems with Unknown Noise Covariances",
  "modified_abstract": "Inspired by the growing body of work on data-driven control methods and adaptive filtering techniques, this paper extends the discourse by examining learning the optimal filtering policy for linear systems when faced with the challenge of unknown noise covariance matrices. This investigation is sparked by recent innovations in learning-based control for systems with latent variables and the development of adaptive filters through Gaussian process regression, highlighting the paramount importance of dealing with uncertainties and noise in practical applications. Specifically, we focus on learning the Kalman gain for a linear system using noisy output data from sensors. The learning problem is articulated as a stochastic policy optimization challenge, poised at the nexus between data-driven optimal control and optimal filtering, aiming to refine output prediction accuracy. Our contributions include a detailed convergence analysis of the stochastic gradient descent algorithm under the constraints of biased gradients and system stability, capturing the trajectory of states in the presence of uncertain noise, and the formulation of bias-variance error bounds that benefit from a high-dimensional statistical framework. These contributions present a significant step towards bridging theoretical and practical aspects of optimal filtering, especially in systems where noise characteristics cannot be precisely determined and internal dynamics may require the integration of a low-pass filter to minimize the effect of high-frequency noise. Adaptation to these conditions is further facilitated by using online models that incorporate latent representations of the dynamic systems, emphasizing the influence of physics-based insights on enhancing filtration accuracy.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Armin_Lederer1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=VMAgvbBBts",
  "title": "UP-DP: Unsupervised Prompt Learning for Data Pre-Selection with Vision-Language Models",
  "modified_abstract": "Inspired by studies on universal representation learning for few-shot image classification and multi-task learning from partially annotated (labeled and unlabelled) data, this research extends the exploration of leveraging untapped potentials within unsupervised contexts, particularly in the domain of data pre-selection. In this study, we address the challenge of data pre-selection, which is critical for optimizing performance in downstream tasks with limited annotation resources. Unlike previous methods that predominantly focus on visual features from models like CLIP and BLIP-2, we posit that text-visual pairs features, when leveraged alongside visual cues, can significantly enhance the pre-selection process. Our proposed UP-DP method employs an unsupervised prompt learning strategy with the vision-language model, BLIP-2, to extract superior joint features through networks that ensure a comprehensive representation of the dataset. By freezing the BLIP-2 parameters and optimizing text prompts through unsupervised training, we demonstrate not only improved representation through a diverse cluster structure but also remarkable generalizability across different benchmarks and exceptional adaptation to various settings, achieving up to a 20% performance improvement over state-of-the-art techniques. UP-DP's pioneering approach to incorporating unsupervised prompt learning for data pre-selection exemplifies the potential of integrating vision and text features through prompt training to refine the utility and efficiency of pre-selecting data for annotation in machine learning applications.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Wei-Hong_Li1",
  "manipulated_ranking": 5,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=YmEDnMynuO",
  "title": "GraphAdapter: Tuning Vision-Language Models With Dual Knowledge Graph",
  "modified_abstract": "In the evolving landscape of transfer learning and unsupervised domain adaptation, particularly through advanced representations such as the bidirectional cross-attention mechanisms, our work introduces an innovative approach to enhance the performance of vision-language models (VLMs) under conditions of sparse data. Acknowledging the foundational contributions of prior works in domain adaptation that leveraged vision transformers and the bidirectional cross-attention to minimize domain discrepancies, our proposal, GraphAdapter, extends these paradigms by addressing two key limitations predominantly present in adapter-style learning: a singular modality focus and the neglect of inter-class relationships in downstream tasks. GraphAdapter employs an adapter-style tuning strategy that incorporates dual-modality structure knowledge through a meticulously designed dual knowledge graph, consisting of both textual and visual knowledge sub-graphs to model the correlations of different semantics/classes across modalities. This dual knowledge graph facilitates the leveraging of domain-invariant task-specific structural knowledge from both textual and visual modalities for each prompt, leading to a more effective classifier for downstream tasks. Notably, the neural-based insight driving the design of GraphAdapter allows for the integration of convolution-based techniques in processing visual information, enhancing the model's ability to learn from varied data sources effectively. Our empirical evaluation across 11 benchmark datasets confirms that GraphAdapter substantially surpasses existing adapter-based methodologies, illustrating the practicality and effectiveness of integrating dual knowledge graphs and convolutions in the fine-tuning of VLMs for enhanced task-specific performance.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Pengxin_Guo1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=1vzF4zWQ1E",
  "title": "Rethinking Bias Mitigation: Fairer Architectures Make for Fairer Face Recognition",
  "modified_abstract": "Face recognition systems, integral to safety-critical applications such as law enforcement, display pronounced biases along socio-demographic lines like gender and race, undermining their efficacy and fairness. Traditional bias mitigation strategies, focusing on pre-processing training data, incorporating training penalties, or post-processing predictions, have achieved limited success in complex tasks like face recognition. Our research builds on insights from studies addressing the dichotomy of robust and non-robust features in adversarial examples, suggesting that biases can also be intrinsic to the neural network architectures themselves and can be exacerbated by adversarial attacks. By reframing the discussion towards the structural propensities for bias within these architectures and proposing a novel approach, we conduct groundbreaking neural architecture and hyperparameter searches emphasizing the attention mechanism. This innovative approach, accounting for the perturbation effects and focusing on representation learning, yields a range of models that Pareto-dominate existing high-performance architectures and bias mitigation techniques in both accuracy and robustness on prominent face identification datasets--CelebA and VGGFace2. Significantly, these models also demonstrate robust generalization across varied datasets and sensitive attributes, showcasing their resistance to adversarial perturbations and highlighting the importance of robust representation for fairness. To facilitate further research, we have made our code, models, and raw data files available, excluding the external link as per requirements.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Byung-Kwan_Lee1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=tLTtqySDFb",
  "title": "Not All Neuro-Symbolic Concepts Are Created Equal: Analysis and Mitigation of Reasoning Shortcuts",
  "modified_abstract": "Inspired by the burgeoning interest in understanding and enhancing human cognition through advanced computational models, as evidenced by works on meta-learned models of cognition and the cognitive analysis of large language models like GPT-3, this paper probes into the intrinsic challenges of Neuro-Symbolic (NeSy) predictive models. These models aim to reconcile the realms of symbolic reasoning with sub-symbolic data processing to achieve systematic generalization, interpretability, and compliance with constraints informed by prior knowledge in psychology. However, the advent of reasoning shortcuts, where these models preferentially leverage particular concepts with unintended semantics for high accuracy, highlights a gap in fulfilling these objectives fully. Reasoning shortcuts not only undermine the trustworthiness and interpretability of NeSy models but also pose significant questions about their practical utility and the extent to which they truly integrate and understand high-level concepts, impacting decision-making processes. By systematically characterizing reasoning shortcuts as unintended optima and identifying the conditions precipitating their emergence, including the design of the models and the behavioral patterns observed in empirical experiments, this work provides a critical foundation for developing strategies aimed at mitigating their impact. Through both theoretical analyses and empirical validation involving agents in various decision-making scenarios, we illustrate the formidable nature of these shortcuts in corrupting model interpretability and reliability, increasingly involving the modeling of subjects related to human reasoning. Our findings call into question the essence of what it means for predictive models to 'understand' in a manner that aligns with human reasoning and accentuate the need for future research to address these pitfalls explicitly.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Marcel_Binz1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=5Fgdk3hZpb",
  "title": "Squeeze, Recover and Relabel: Dataset Condensation at ImageNet Scale From A New Perspective",
  "modified_abstract": "Inspired by recent advancements in self-supervised learning and the exploration of novel distillation strategies to enhance model performance on image datasets, our study introduces a groundbreaking dataset condensation framework, Squeeze, Recover and Relabel (SRe$^2$L). This framework uniquely decouples the bilevel optimization of model and synthetic data during training, thereby facilitating the handling of varying scales of datasets, model architectures, and image resolutions for efficient dataset condensation. Distillation, as an integral part of our approach, further empowers the synthetic data generation process through self-supervised techniques, aiming for high-fidelity instances that closely represent complex datasets. Proof of our method's flexibility and superiority is demonstrated through extensive experiments on Tiny-ImageNet and full ImageNet-1K datasets. Remarkably, at 50 Images Per Class (IPC), SRe$^2$L achieves unparalleled validation accuracy rates of 42.5% and 60.8% on Tiny-ImageNet and ImageNet-1K, respectively, outstripping all prior state-of-the-art methods in model teaching efficiency. Moreover, our approach notably supersedes existing methods like MTT in terms of speed, being approximately 52\\times and 16\\times faster for ConvNet-4 and ResNet-18 architectures, while concurrently reducing memory consumption by 11.6\\times and 6.4\\times during data synthesis. In reflection of our commitment to open science and the advancement of machine learning research, we have made our code and condensed datasets of 50, 200 IPC with a 4K recovery budget publicly available. Furthermore, the aggregation of synthetic data demonstrates our innovative use of bagging techniques to enhance model robustness, accuracy, and potentially boosting overall student model performance.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Haohang_Xu1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=0x2Ou3xHbH",
  "title": "On the Convergence of No-Regret Learning Dynamics in Time-Varying Games",
  "modified_abstract": "The study of learning dynamics in games traditionally assumes static environments; however, real-world scenarios often involve changes over time, highlighting the need for adaptable algorithms. Inspired by foundational work on differentiable games, bilinear forms, and the acceleration of smooth games through spectral manipulation and matrix theory, this paper expands the investigation into how no-regret learning dynamics, specifically optimistic gradient descent (OGD), converge in time-varying games. We provide a detailed characterization of OGD's convergence, offering tight convergence bounds for the equilibrium gap in zero-sum games. These bounds are parameterized on natural variation measures of the dynamic game sequence, enhancing our understanding beyond static settings. Additionally, we present novel second-order variation bounds under strong convexity-concavity, assuming games are played repeatedly, and extend our analysis to general-sum multi-player scenarios through correlated equilibria and consensus on strategies. This exploration not only answers open questions regarding variation-dependent regret bounds but also introduces new insights into dynamic regret and the shape of convergence in static games, thereby broadening the scope of learning in time-varying settings. Utilizing operators that align with the gradient dynamics and machine learning further contributes to the machinery of machine learning, enhancing algorithmic performance and theoretical insights.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Wa\u00efss_Azizian1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=Fdfyga5i0A",
  "title": "Mnemosyne: Learning to Train Transformers with Transformers",
  "modified_abstract": "Inspired by recent innovations in machine learning and optimization, including advances in understanding the effects of mixup regularization on learned representations and the robustness of Transformers versus traditional CNNs for image classification, we introduce Mnemosyne. This new class of learnable optimizers is grounded in the novel concept of spatio-temporal low-rank implicit attention within Transformers, enabling the training of neural network architectures, including other Transformers, with unprecedented efficiency. Mnemosyne demonstrates significant advancements by (a) surpassing the capabilities of LSTM-based optimizers through innovative feature engineering aimed at overcoming LSTM's limitations, thereby establishing new baselines, (b) enabling the successful training of Transformers using straightforward meta-training tactics that minimize computational demands, and (c) achieving parity with state-of-the-art (SOTA) hand-designed optimizers on accuracy, often generating leading models without the need for extensive hyper-parameter tuning. The robustness of Mnemosyne is evident as it introduces additional robustness against adversarial impacts through the strategic integration of specific attention layers, significantly enhancing classification performance. Notably, Mnemosyne offers space complexity on par with manual first-order optimization methods and introduces a loss reduction strategy that facilitates its application to larger parameter sets. Our empirical evaluation spans a spectrum of application scenarios, including fine-tuning varied Vision Transformer (ViT) models, pre-training BERT frameworks with loss patches removal for optimization, and soft prompt-tuning on extensive 11B+ T5XXL models. These findings are supported by a detailed theoretical analysis of the compact associative memory mechanism and patches networks utilization employed by Mnemosyne, marking a pioneering investigation into its underlying principles.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Andreas_Veit1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=llP6lmMiXE",
  "title": "A General Framework for Robust G-Invariance in G-Equivariant Networks",
  "modified_abstract": "Inspired by the exploration of intrinsic dimensionality and geometric manifolds in data representations within deep neural networks and its impact on the generalization capabilities of these models, our work introduces a novel methodology for enhancing the robustness and generalization of group-equivariant convolutional neural networks ($G$-CNNs) through the implementation of a $G$-triple-correlation ($G$-TC) layer. This approach is grounded in the theory of triple-correlation on groups, which represents the unique, lowest-degree polynomial invariant map that is also \\textit{complete}, distinguishing itself from many commonly used, yet incomplete, invariant maps like \\texttt{max} by preserving all information about the structure of the signal while removing only the variation attributable to the actions of the group. The completeness of the triple correlation attribute of the $G$-TC layer and its linearized estimates of the geometric properties are pivotal for its demonstrated strong resistance to invariance-based adversarial attacks and its capacity to achieve significant improvements in classification accuracy over the conventional Max $G$-Pooling in $G$-CNN architecture. Our methodology provides a general and efficient framework for implementation across any discretized group, facilitated by a simple table defining the group's product structure, and layers whose efficacy is substantiated through empirical testing on both $G$-MNIST and $G$-ModelNet10 datasets, involving $G$-CNNs defined on various groups ($SO(2)$, $O(2)$, $SO(3)$, and $O(3)$) acting on $\\mathbb{R}^2$ and $\\mathbb{R}^3$. This approach not only exemplifies how geometric properties of data representations, as highlighted in previous studies, can be leveraged for the development of more robust and generalizable models but also extends the applicability of these concepts to the realm of group-equivariant neural networks, progressively generalizing across different data processing units.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Alessio_ansuini1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=j7U4pFkCYB",
  "title": "DynPoint: Dynamic Neural Point For View Synthesis",
  "modified_abstract": "Inspired by recent breakthroughs in neural radiance fields, video understanding through stochastic image-to-video synthesis, and advances in high-resolution image synthesis with latent diffusion models, our investigation into view synthesis extends these foundational achievements. These prior works have outlined the potential of neural networks in capturing and manipulating visual information across frames for tasks ranging from generating video sequences from still images to producing high-fidelity images through innovative diffusion processes. Building on this, DynPoint is introduced as an algorithm designed to facilitate the rapid synthesis of novel views for unconstrained monocular videos, addressing the limitations of existing algorithms in uncontrolled or lengthy scenarios which demand extensive training time for each new case. Unlike approaches that rely on encoding scenario information into a latent representation, DynPoint focuses on predicting explicit 3D correspondences between neighboring frames to enable information aggregation, including techniques relevant to inpainting where missing information must be synthesized and super-resolution for enhancing the quality of aggregated views. This is achieved through consistent depth and scene flow estimation across frames, leveraging hierarchical neural point clouds and cross-attention mechanisms to aggregate information from multiple reference frames to a target frame effectively. The resulting framework not only significantly reduces training time, often by an order of magnitude, but also demonstrates robustness in handling long-duration videos without necessitating a canonical representation of video content. Conditional generators are also explored as a means to further optimize view synthesis, offering improved scalability. Experimental results support the effectiveness of DynPoint in achieving comparable, if not superior, view synthesis outcomes with enhanced efficiency and scalability, underscoring the significance of video understanding and autoencoder-based architectures in optimizing the process.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Andreas_Blattmann1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=bLB4vTwSbC",
  "title": "Greatness in Simplicity: Unified Self-Cycle Consistency for Parser-Free Virtual Try-On",
  "modified_abstract": "Inspired by recent developments in category-level 6D object pose estimation through self-supervised deep learning, prior deformation, and dual network approaches for refined pose consistency learning, our work addresses the image-based virtual try-on challenge. These innovative methods set a precedent for addressing complex spatial transformations and domain adaptation, pivotal to our approach in modeling non-rigid garment deformation without the reliance on auxiliary disentanglement tasks. Image-based virtual try-on remains challenging, primarily due to inherent complexities associated with non-rigid garment deformation modeling and strong feature entanglement of clothing within the human body. Recent formulations, such as in-painting, cycle consistency, and knowledge distillation, have facilitated self-supervised generation of try-on images through a series of complex learning tasks including regression techniques for better alignment. However, these paradigms necessitate the disentanglement of garment features within human body poses through auxiliary tasks, such as leveraging 'teacher knowledge' and dual generators. The potential presence of irresponsible prior knowledge in the auxiliary task can serve as a significant bottleneck for the main generator (e.g., 'student model') in the downstream task, and training strategies have been developed to mitigate these effects. Moreover, existing garment deformation methods lack the ability to perceive the correlation between the garment and the human body poses in the real world, leading to unrealistic alignment effects. To tackle these limitations, we present a new parser-free virtual try-on network based on unified self-cycle consistency (USC-PFN), which enables robust translation between different garments using just a single generator, faithfully replicating non-rigid geometric deformation of garments in real-life scenarios. Specifically, we first propose a self-cycle consistency architecture with a circular mode. It utilizes real unpaired garment-person images exclusively as input for training, effectively eliminating the impact of irresponsible prior knowledge at the model input end. Additionally, we formulate a Markov Random Field to simulate a more natural and realistic garment deformation. Furthermore, USC-PFN can leverage a general generator for self-supervised cycle training tasks. Experiments demonstrate that our method achieves state-of-the-art performance on a popular virtual try-on benchmark.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jiehong_Lin1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=ouLe91yibj",
  "title": "On the Properties of Kullback-Leibler Divergence Between Multivariate Gaussian Distributions",
  "modified_abstract": "Kullback-Leibler (KL) divergence is a cornerstone in the measurement of discrepancies between probability distributions, crucial for advancements in unsupervised learning and the estimation of performance metrics in machine learning models. Building on the foundational principles highlighted in recent studies on robust learning algorithms and the estimation of weighted areas under receiver operating characteristic (ROC) curves, our work extends the investigation into the realm of information theory by focusing on the KL divergence between multivariate Gaussian distributions. We theoretically explore several properties of KL divergence. Firstly, we establish bounds on the KL divergence for any two $n$-dimensional Gaussian distributions, quantifying the approximate symmetry for small divergences and providing conditions for the attainment of supremum and infimum values. Secondly, we examine the relationship between three Gaussian distributions and demonstrate that KL divergence adheres to a relaxed form of the triangle inequality, under certain conditions related to learning processes. These findings, independent of the distribution dimension, underline the nuanced characteristics of KL divergence, functions, and its potential implications for deep learning, reinforcement learning, and sample complexity studies. Our results contribute to a deeper understanding of the mathematical averages and underpinnings of KL divergence, offering new insights that can inform the design of learning algorithms and the theoretical analysis of their performance.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Andreas_Maurer1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=L7Whl9pXd0",
  "title": "Efficient Batched Algorithm for Contextual Linear Bandits with Large Action Space via Soft Elimination",
  "modified_abstract": "This work is inspired by key findings in the identification of best policies in linear Markov Decision Processes (MDPs), elucidating the intricate nature of optimal sampling and the challenges associated with large action spaces. In this context, we present the first efficient batched algorithm for contextual linear bandits with large action spaces, a significant advancement over prior methodologies that are not feasible for large sets due to their reliance on action elimination. Our approach is a unique amalgamation that utilizes both learning theory and a linear optimization oracle over the action set to inform policy design, setting a new benchmark in the field of bandit problems. The algorithm we propose achieves an asymptotically optimal regret upper bound $\\tilde{O}(\\sqrt{T})$ with high probability and necessitates only $O(\\log\\log T)$ batches, aligning with the lower bound on batch numbers for efficient learning. When applied to linear bandits, it attains a high probability gap-dependent regret of $\\tilde{O}(1/\\Delta_{\\min})$, with $\\Delta_{\\min}$ representing the minimal reward gap, and maintains the optimal number of batches, $\\log T$. The efficacy of our algorithm is rooted in a novel 'soft elimination' technique, allowing for the strategic manipulation of action sets at each batch to efficiently pinpoint (nearly) optimal actions for sampling and identification. This generative approach not only paves the way for significant advancements in handling large action spaces in the domain of bandit problems but also sets a precedent for the development of more nuanced and scalable bandit algorithms, enhancing the process of identification of optimal actions. Furthermore, our method introduces a moderate-confidence framework that refines sampling efficiency and decision-making process.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yassir_Jedra1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=uiiVSVADDc",
  "title": "Annotator: A Generic Active Learning Baseline for LiDAR Semantic Segmentation",
  "modified_abstract": "Inspired by the significant strides in weakly supervised learning for amodal instance segmentation and affordance segmentation in RGB images, our research introduces Annotator, a novel active learning framework tailored for LiDAR semantic segmentation. These foundational works, which leverage novel data manipulation techniques and multiscale convolutional neural networks to address challenges in segmenting objects, image segmentation, object affordance identification, and occlusions in complex indoor and outdoor environments, underscore the importance of efficiently handling large and complex datasets. Annotator capitalizes on this premise by offering a voxel-centric online selection strategy that aims to efficiently probe and annotate salient and exemplar voxel grids within each LiDAR scan, even in the face of distribution shifts and occlusions, which are particularly challenging for accurately completing the 3D scene understanding. Through a comprehensive evaluation of common selection strategies and the introduction of voxel confusion degree (VCD) to utilize local topology relations and point cloud structures for improved completion and segmentation accuracy, Annotator achieves substantial reductions in the necessity for manual annotation. Remarkably, it requires labeling merely five voxels per scan in the SynLiDAR \rightarrow SemanticKITTI task, delivering up to 94.4% performance of fully-supervised models in various adaptive scenarios (AL, ASFDA, and ADA), thereby demonstrating its efficacy in indoor and outdoor settings alike. With these contributions, we aim to lay down a versatile and efficient groundwork for future advancements in label-efficient 3D applications, marrying active learning's potential with the inherent complexities of LiDAR data processing.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sinisa_Todorovic1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=ETk6cfS3vk",
  "title": "SlotDiffusion: Object-Centric Generative Modeling with Diffusion Models",
  "modified_abstract": "The evolution of object-centric learning, especially with the integration of advanced architectures such as Transformers in vision tasks, has greatly influenced our approach to generative modeling. Previous efforts like the k-means Mask Transformer have demonstrated the potency of using Transformer-based models for enhancing image recognition, segmentation, and the handling of visual data via self-attention mechanisms. These advances set the stage for innovative methods in unsupervised object discovery and the structured representation of visual information. Building upon these foundational works, SlotDiffusion represents a novel step forward, targeting the challenges of generative modeling within an object-centric framework. This paper introduces SlotDiffusion, an object-centric Latent Diffusion Model (LDM) that significantly advances the state-of-the-art in unsupervised object segmentation and visual generation across a spectrum of datasets. Unlike earlier slot-based models that often resulted in blurry images and distorted objects due to inadequate segmentation techniques, SlotDiffusion leverages the robust modeling capacity of LDMs and the design principles of augmentation strategies to produce high-quality visual outputs. Demonstrated across six datasets, SlotDiffusion outperforms its predecessors not only in generating more precise images but also in object detection and segmentation, offering improvements in video prediction quality and temporal reasoning tasks. Additionally, when integrated with self-supervised pre-trained image encoders, SlotDiffusion shows remarkable scalability to complex, real-world datasets like PASCAL VOC and COCO, marking a significant leap in the realm of object-centric generative modeling. The integration of k-means clustering within this framework is particularly noteworthy for its role in refining segmentation outcomes and handling pixels in a set-based learning approach.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Siyuan_Qiao1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=AnFUgNC3Yc",
  "title": "Resetting the Optimizer in Deep RL: An Empirical Study",
  "modified_abstract": "In the evolving landscape of deep reinforcement learning (RL), optimizing the approximation of the optimal value function is a pivotal task. This involves addressing a sequence of optimization challenges where the loss function dynamically alters across iterations. Traditional practices leverage sophisticated variants of the stochastic gradient descent algorithm, such as Adam, which retain internal parameters to inform subsequent optimization efforts. Notably, these parameters - encapsulating first-order and second-order gradient moment estimates - evolve, potentially distorting the optimization process due to the mutable optimization landscape inherent in deep RL. Inspired by recent advancements in value estimation methods, such as the generalized-activated deep double deterministic policy gradients and the Double Actors Regularized Critics (DARC) algorithm, which underscore the importance of accurate value estimation, bias mitigation, and critic role enhancement in varying contexts of RL, our study probes the merit of resetting an optimizer's internal parameters at the onset of new iterations within deep RL frameworks. We hypothesize that such a reset can control for and mitigate the adverse effects of residual optimization information. Through an empirical lens, this investigation harnesses several optimizers within the Rainbow algorithm's architecture, showcasing that this nuanced yet straightforward intervention markedly improves deep RL performance on the Atari benchmark suite, outperforming previous benchmarks. This revelation not only signals an efficacious strategy for improving the control mechanics and mitigation of the adverse effects of residual optimization information but also aligns with the broader enterprise of refining value function approximation through methodological innovation, including varying activation functions.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jiafei_Lyu1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=aW9BqtRQkh",
  "title": "Language Models Can Improve Event Prediction by Few-Shot Abductive Reasoning",
  "modified_abstract": "Inspired by recent innovations in conditional language generation and closed-book question answering, this study investigates the potential of large language models (LLMs) to reason about real-world events and enhance the prediction performance of event sequence models. These prior works have significantly advanced our capabilities in generating coherent and context-relevant texts, thereby highlighting the quality of generation, as well as efficiently answering multifaceted questions with minimal prompts, setting a solid foundation for exploring abductive reasoning in event prediction. We introduce LAMP, a novel framework that marries a large language model's reasoning prowess with event sequence prediction. Specifically, LAMP utilizes abductive reasoning, where the language model, guided by a few expert-annotated demonstrations, identifies possible causes behind each prediction made by the event sequence model. A dedicated search module is then used to locate past events that correlate with these causes, and a scoring function, bolstered by specific metrics, evaluates the likelihood of these events causing future occurrences. Furthermore, the training phase for LAMP emphasizes on conditioning the language model to adhere to task-specific prompts, ensuring task-aligned generation of interpretive reasoning. Our extensive experiments across several real-world datasets convincingly show that LAMP significantly outperforms current state-of-the-art event sequence models, underscoring the transformative role that LLMs' reasoning capabilities can play in predictive analytics and the generation of abstractive summaries pertinent to future event prediction.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Shashi_Narayan1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=NvcVXzJvhX",
  "title": "Sheaf Hypergraph Networks",
  "modified_abstract": "Motivated by recent revelations on the efficacy of untrained subnetworks in graph neural networks (GNNs) for tasks requiring nuanced understanding of structure and robustness against random perturbations, our work expands the conceptual and practical boundaries of modeling complex interactions in higher-order structures. Higher-order relations, prevalent across various natural and human-made systems, pose unique challenges and opportunities for computational representation and analysis. The conventional tools, such as GNNs and their extensions, have started to explore these dimensions with varying degrees of success. By focusing on sparse structures and the matching problem, where the goal is to find optimum pairings within the data, we further tailor our approach to address these complex scenarios. Building upon these foundational insights, we introduce the concept of cellular sheaves for hypergraphs, a sophisticated extension that encapsulates additional structural information over standard hypergraphs, thereby enhancing their representational capacity while preserving the intrinsic higher-order connectivity. By developing novel formulations of sheaf hypergraph Laplacians\u2014both linear and non-linear\u2014we empirically validate our theoretical advancements through the lens of sheaf hypergraph neural networks and sheaf hypergraph convolutional networks. These proposed models not only innovate beyond classical Hypergraph Networks but, through extensive experiments, also demonstrate superior performance across multiple benchmark datasets in the realm of hypergraph node classification, suggesting a promising avenue for research into more expressive and efficient methods of modeling complex data structures.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Tianjin_Huang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=7uPnuoYqac",
  "title": "Federated Learning with Manifold Regularization and Normalized Update Reaggregation",
  "modified_abstract": "Building upon prior explorations in optimizing machine learning (ML) models through Learning to Optimize (L2O) and enhancing robustness via novel sampling models like Cross-Concentrated Sampling (CCS) in matrix completion, our study introduces a sophisticated approach in the federated learning (FL) domain. Federated Learning is an innovative collaborative ML framework allowing multiple clients to train a global model without sharing their datasets. The primary challenge in FL is the model inconsistency caused by local data heterogeneity across clients, leading to the slowing down of convergence. Unlike most previous works that attempt to minimize the parameter or gradient differences between local and global models\u2014a method insufficient for addressing model inconsistency due to the complex ML model structures and the limitations of Euclidean geometric representations\u2014our approach, FedMRUR, incorporates manifold regularization with a hyperbolic graph manifold regularizer. This regularizer enforces the proximity of data representations in local and global models within a low-dimensional subspace, exploiting the manifold structures for significant inconsistency reduction. Additionally, FedMRUR introduces a reaggregated global update norm, countering the norm reduction from near-orthogonality in client updates, thereby enhancing convergence efficiency. Theoretical proofs confirm FedMRUR's capability for linear speedup under partial client participation in non-convex settings, offering a $\\mathcal{O}(\\frac{1}{\\sqrt{SKT}})$ speedup where $S$ is the number of participating clients, $K$ the local interval, and $T$ the total communication rounds. Empirical evidence showcases FedMRUR's achievement of new state-of-the-art accuracy with reduced communication needs, thus addressing key FL challenges highlighted by previous works and advancing the field significantly.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~HanQin_Cai1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=cQdc9Dyk4i",
  "title": "GraphMP: Graph Neural Network-based Motion Planning with Efficient Graph Search",
  "modified_abstract": "This paper builds upon the recent advancements in learning-based motion planners and the application of graph neural networks (GNNs) for complex tasks such as trajectory prediction, visual representation learning in embodied navigation, and self-supervised learning mechanisms for enhancing robotic perception. Motion planning, a crucial task in robotic systems, seeks to identify high-quality, collision-free paths within the configuration space. While GNN-powered planners have demonstrated promising planning capabilities in processing images and spatial information for navigation tasks, their efficiency in graph extraction and learning does not directly translate to improved graph search processes, which is essential for motion planning. Addressing this gap, we introduce GraphMP, a neural motion planner that excels in both low and high-dimensional planning tasks through a bespoke model architecture and training mechanism, utilizing strategies such as pretraining and augmented observations. GraphMP aims to enhance the efficiency of graph pattern extraction and search, achieving notable improvements in path quality and planning speed across diverse environments, from simple 2D mazes to complex 14D dual KUKA robotic arm setups. Comparative experiments demonstrate GraphMP's superior performance against both state-of-the-art learning-based and traditional planners, underlining its efficacy and potential in advancing robotic motion planning. Moreover, the adoption of augmented state-action representations further bolsters GraphMP's planning capabilities, offering a more refined approach to motion forecasting and decision-making in embodied agents.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Arjun_Majumdar2",
  "manipulated_ranking": 7,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=nCwStXFDQu",
  "title": "FouriDown: Factoring Down-Sampling into Shuffling and Superposing",
  "modified_abstract": "In light of recent strides in convolutional neural network (CNN) architectures and their reliance on down-sampling techniques, including max-pooling, average-pooling, and other forms of pooling, for efficient image processing, this research introduces FouriDown. This novel down-sampling paradigm factors traditional spatial down-sampling into shuffling and superposing within the Fourier domain, addressing the static weighting strategy's biases in prior works such as Frequency Pooling and Dilated Convolutions for Single Image Super-Resolution. Inspired by the signal sampling theorem, FouriDown reimagines down-sampling through a Fourier lens, offering a learnable, context-adaptive operator that supersedes the non-parameter static weighting of earlier models. By organizing frequency positions in a physically-closed manner and deploying multiscale, point-wise channel shuffling against aliasing, it ensures consistent learning of weighting parameters. FouriDown encapsulates four key processes: 2D discrete Fourier transform, context-aware shuffling, adaptively superposing via Fourier weighting, and 2D inverse Fourier transform, seamlessly integrating into existing image restoration and training frameworks based on its innovative approach. Through rigorous testing on image de-blurring and low-light enhancement projects, FouriDown's superiority in performance is evident over single-image-based traditional methods. The impending code release aims to catalyze further advancements and applications of our approach.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zhendong_Zhang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=GlWzQhf2lV",
  "title": "Exploiting Contextual Objects and Relations for 3D Visual Grounding",
  "modified_abstract": "Influenced by prior advancements in 3D object detection, both from lidar and camera sensors in autonomous driving, and novel approaches in monocular 3D object detection leveraging reciprocal appearance-localization features, our research introduces a groundbreaking framework, CORE-3DVG, for 3D visual grounding\u2014a task essential for machines to interpret and act within complex three-dimensional environments. This task necessitates the understanding of 3D contextual information to accurately distinguish target objects within these environments, a challenge compounded by the lack of explicit annotations for contextual objects and relations. The CORE-3DVG framework addresses these challenges by employing three sequentially connected modular networks: a text-guided object detection network to enhance depth perception through monocular cues, a relation matching network, and a target identification network, which collectively learn and leverage contextual objects and their relations through a fusion of sensor data, including deep learning techniques for robust feature extraction. Through innovative training techniques, including a pseudo-label self-generation strategy for deep self-learning and a weakly-supervised method for contextual understanding, our model significantly enhances the ability to interpret natural language inputs in relation to 3D scenes and improve autonomous driving applications. The efficacy of our approach is demonstrated through superior performance on the Nr3D, Sr3D, and ScanRefer datasets, setting new benchmarks in 3D visual grounding and detection capabilities. The public release of our code aims to facilitate further research in this rapidly evolving domain.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Errui_Ding2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=ZwQJRXLjVm",
  "title": "Rehearsal Learning for Avoiding Undesired Future",
  "modified_abstract": "Incorporating insights from novel techniques in uncertainty quantification, such as Evidential Turing Processes, our work introduces a framework for rehearsal learning aimed at preemptively addressing undesired outcomes predicted by machine learning (ML) models. This effort builds on the essential triad of fitting to target domain data, calibrating class probabilities in challenging scenarios, and recognizing out-of-domain queries, as detailed in prior advancements in probabilistic classifiers. Within this context, we propose a rehearsal learning approach that identifies actionable decisions to avoid potential negative events through the manipulation of variables in a structured manner. By leveraging a probabilistic graphical model, referred to as rehearsal graphs, alongside structural equations, our framework performs reasoning within a Bayesian paradigm to find persuasive decision options. Additionally, we anchor the decision-making process in a theoretical foundation by deriving a probably approximately correct (PAC) bound to evaluate the risk associated with a given action and ensure a unified approach in calibration. Through experimentation, we validate both the efficacy of our rehearsal learning method in identifying preventive decisions and the utility of the PAC bound in conveying the reliability of these decisions, demonstrating its application in both neural network-based deep learning contexts and traditional classification tasks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Manuel_Haussmann1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=WRGldGm5Hz",
  "title": "DYffusion: A Dynamics-informed Diffusion Model for Spatiotemporal Forecasting",
  "modified_abstract": "Inspired by a growing body of work that aims to incorporate geometric and dynamic priors into machine learning models for enhanced inference in scientific domains\u2014such as the group equivariant neural posterior estimation, which integrates equivariances to solve inverse problems with high accuracy\u2014our paper introduces a novel approach to dynamics forecasting. We extend the capabilities of diffusion models beyond their traditional domain of static images to address the complex requirements of spatiotemporal forecasting. By leveraging the inherent temporal dynamics within data, our model directly couples these dynamics with the diffusion steps, a strategy informed by the need to accurately capture multi-step and long-range dependencies in dynamic systems. This integrated approach enables highly flexible, continuous-time sampling trajectories for dynamics forecasting and introduces the possibility to trade-off between forecasting performance and computational efficiency at inference. Our dynamics-informed diffusion model not only demonstrates competitive performance on probabilistic skill score metrics across varied applications such as sea surface temperature forecasting, Navier-Stokes flows, and spring mesh systems but also establishes a methodological advancement by incorporating a strong inductive bias that bypasses the inefficiencies of traditional Gaussian noise-based diffusion processes. Specifically, it achieves a new benchmark in solving astrophysical and complex dynamic forecasting challenges, addressing the struggle in accurately capturing gravitational-wave signals among other phenomena. The model achieves this by employing networks that are sensitively adjusted to the dynamic group properties of the underlying data, ensuring a robust posterior in solution strategies.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Michael_Deistler1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=f56xMRb7Vt",
  "title": "Norm-guided latent space exploration for text-to-image generation",
  "modified_abstract": "Building upon the critical observations in active retrieval augmented generation and hierarchical prompting for large language models, which showcase advanced techniques in handling language understanding and decision-making tasks, this paper ventures into the novel realm of text-to-image diffusion models. These foundational works underline the significance of tailored approaches to interacting with latent spaces or external data to enhance the performance and accuracy of generative models. Our approach draws parallels with the retrieve-and-generate methodology, further emphasizing navigation through the latent space of text-to-image models guided by norm-based metrics. This exploration is critical for tasks that are knowledge-intensive, requiring a nuanced understanding to synthesize images that accurately reflect complex sentence structures and predictions about novel scenarios. Following these precedents, we explore the latent space of text-to-image diffusion models, which has shown great potential in synthesizing a wide variety of concepts in novel compositions and scenarios. We observe that simple operations in this space, such as interpolation and centroid finding, are hindered by the conventional use of Euclidean or spherical metrics, largely due to the narrow range of norm values observed in current training procedures. To address this, we propose a novel method for seed interpolation that introduces a non-Euclidean metric accounting for a norm-based prior, significantly enhancing the generation of rare concept images. Our methods yield notable improvements in few-shot and long-tail learning tasks, surpassing previous approaches in generation speed, image quality, and semantic richness, thereby contributing to a deeper understanding and more effective manipulation of latent spaces in generative models.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Frank_F._Xu1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=3ZICE99e6n",
  "title": "ReTR: Modeling Rendering Via Transformer for Generalizable Neural Surface Reconstruction",
  "modified_abstract": "Inspired by significant breakthroughs in techniques for dense 3D reconstruction, such as those demonstrated by Bi-level Neural Volume Fusion (BNV-Fusion), which integrates depth maps into global neural implicit volume representations, our study introduces Reconstruction TRansformer (ReTR), a paradigm shift in the domain of neural surface reconstruction. ReTR leverages the transformer architecture to address and overcome the critical limitations faced by generalizable neural surface reconstruction techniques, including the challenges of low confidence in depth distribution measurements and inaccuracies in surface reasoning. Our framework introduces a novel $\\textit{meta-ray token}$ and employs the cross-attention mechanism to simulate complex interactions during the rendering process, thus significantly enhancing surface reconstruction accuracy and measurements of confidence. Moreover, by operating within a high-dimensional feature space, ReTR effectively reduces sensitivity to projected colors from source views, a common shortfall in existing methods and applications. The effectiveness of the ReTR framework is empirically validated across various datasets, where it demonstrates superior performance in comparison to state-of-the-art approaches, with particular improvements noted in reconstruction quality, generalization capabilities, and its potential for applications in fields such as 3D modeling and robotic vision. This abstract is modified to exclude the GitHub link previously mentioned, adhering to the request for removal of personal identifiable information.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Victor_Adrian_Prisacariu1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=1recIOnzOF",
  "title": "Decorate3D: Text-Driven High-Quality Texture Generation for Mesh Decoration in the Wild",
  "modified_abstract": "Inspired by pioneering efforts in text-guided texture generation for 3D shapes, this paper introduces Decorate3D, a novel, versatile, and user-friendly approach for the creation and editing of 3D objects using images. Our method innovatively models a real-world object of interest by using a neural radiance field (NeRF) and decomposes this NeRF representation into an explicit mesh representation with geometries, a view-dependent texture, and a diffuse UV texture. This enables users to either manually edit the UV texture or provide a text prompt to automatically generate a new, 3D-consistent texture, showcasing the uses of AI in creative processes. To produce textures of high quality, we propose a structure-aware score distillation sampling method that optimizes a neural UV texture based on user-defined text, integrating it with an image diffusion model designed for 3D-consistent generation. Additionally, by introducing a few-view resampling training method, which marks a progression in the field, and leveraging a super-resolution model, we are able to produce refined high-resolution UV textures (2048x2048) for 3D texturing, transitioning through different states of texture quality from low to high resolution. The significant advancements of Decorate3D in retexturing real-world 3D objects are substantiated through extensive experiments, validating its superior performance and potentially transforming depth-to-image transfer and texture editing tasks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yuval_Alaluf1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=XqcXf7ix5q",
  "title": "Locality-Aware Generalizable Implicit Neural Representation",
  "modified_abstract": "Inspired by recent state-of-the-art advancements in generative models and scene generation techniques, such as DreamTeacher's innovative approach to feature representation learning and NeuralField-LDM's utilization of hierarchical latent diffusion models for 3D environment synthesis, we propose a novel framework for generalizable implicit neural representation (INR). Our method enhances INR's ability to localize and capture fine-grained details across scenes and instances by integrating a transformer encoder with a locality-aware INR decoder. The transformer encoder is crafted to predict a set of latent tokens from data instances, embedding local information within each token. This is crucial for addressing the limitations of current modulation techniques in INR, which often struggle with the precise representation of detailed data entities. The locality-aware INR decoder then extracts modulation vectors through selective aggregation of latent tokens via cross-attention mechanisms for coordinate inputs, followed by output prediction through progressive decoding with coarse-to-fine modulation across multiple frequency bandwidths. This multi-tiered approach facilitates learning of locality-aware representations in both spatial and spectral domains, significantly outperforming existing models in generalizable INRs on tasks like large-scale image generation, with a strong emphasis on the importance of self-supervised pretraining and the utility of image annotations for model improvement. Our framework's efficacy underscores the value of locality-aware latents in enhancing the performance of downstream applications, marking a pivotal step forward in the development of more versatile and detailed implicit neural representations. In particular, the integration of a latent-autoencoder helps in compressing and understanding the high-dimensional voxel-based data, enabling efficient learning and generation of complex scenes.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Daiqing_Li1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=r6xGZ0XL2g",
  "title": "Meta-Learning Adversarial Bandit Algorithms",
  "modified_abstract": "Building on the insights from seminal works in online prediction with strategic experts, known as forecasters, and interactive combinatorial bandits, this paper ventures into the uncharted territory of the adversarial online-within-online partial-information setting. Our research is inspired by the foundational contributions to no-regret learning in dynamic environments and the exploration of strategic behavior in expert selection, as well as the novel challenges posed by combinatorial and competitive interactions in bandit problems, which inherently involve a problem of submodularity in decision-making. By synthesizing these themes, we aim to enhance the performance of meta-learning in adversarial contexts, particularly focusing on multi-armed bandits (MAB) and bandit linear optimization (BLO). For MAB, we propose meta-algorithms that utilize the Tsallis-entropy generalization of Exp3, demonstrating that task-averaged regret can be improved by tuning to the entropy of the optimal actions in hindsight. In the case of BLO, we extend the methodology to online mirror descent (OMD) with self-concordant barrier regularizers, illustrating how task-averaged regret correlates with an action space-dependent measure, notably incorporating the curvature parameter $1/\\kappa_{f}$. Our contributions include establishing that a two-tier hyperparameter tuning strategy, applied to unregularized follow-the-leader algorithms, is sufficient for learning a sequence of affine functions that provides tight bounds on the regret of OMD in non-Lipschitz and sometimes non-convex settings. These findings yield valuable recommendations for both the design and analysis of meta-learning algorithms in adversarial bandit scenarios, demonstrating a nuanced understanding of prediction challenges and the indispensable role of submodular decision-making strategies.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Omid_Sadeghi1",
  "manipulated_ranking": 5,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=TDS3kqRteY",
  "title": "REx: Data-Free Residual Quantization Error Expansion",
  "modified_abstract": "Addressing the critical need for resource-efficient deep neural network (DNN) deployment, particularly in contexts requiring stringent privacy measures, this work introduces REx, a novel quantization method that without relying on data, significantly enhances the adaptability of DNNs across diverse hardware platforms. Recognizing that DNNs are central to advancements in fields such as computer vision and natural language processing (NLP), yet are hampered by high inference costs, our approach to quantization\u2014converting floating point operations to a lower bit-width format\u2014aligns with efforts to surmount these challenges while adhering to privacy constraints. Building upon foundational research in Quantization Neural Networks (QNN) and quantization-aware Neural Architecture Search (NAS), which have explored the intersection of architecture optimization, efficiency through methods like Once Quantization-Aware Training (OQAT), and the progressive bit-width adaptation strategy of Once Quantized for All (OQA), training REx innovatively expands the residual error. This expansion not only solves the outlier problem pervasive in state-of-the-art quantization methods but also provides a versatile solution applicable across differing bit-widths and devices, including those of large and huge configurations. Experimentally validated across convolutional networks and transformers for both computer vision and NLP models, REx facilitates superior accuracy-speed trade-offs without requiring access to the original dataset. Furthermore, its training seamlessly integrates with existing quantization frameworks, enhances their ranking, and asserts itself as a significant advancement in the realm of data-free model optimization, backed by robust theoretical assurances of model function preservation. The training of such quantized models, optimized for search efficiency and making them more compact, underscores the importance of attention to detail in addressing the scalability challenges in deploying sophisticated DNNs.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Mingzhu_Shen1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=9pLaDXX8m3",
  "title": "NeRF-IBVS: Visual Servo Based on NeRF for Visual Localization and Navigation",
  "modified_abstract": "Inspired by recent advancements in scalable multi-task visual imitation learning and benchmarks for real robot learning, this paper introduces a novel visual localization and navigation system, NeRF-IBVS, that significantly reduces the need for large datasets and dense 3D labels traditionally required for training such systems. Visual localization, a core task in robotics, often relies on extensive posed image datasets and accurate 3D labels for training, a process that is both challenging and resource-intensive in real-world settings. By leveraging a few posed images with coarse pseudo-3D labels generated via Neural Radiance Fields (NeRF) to train a coordinate regression network, and then optimizing pose estimation through image-based visual servoing (IBVS) using NeRF-derived scene priors, our method achieves precise localization with significantly reduced data requirements. Notably, our approach allows for efficient visual navigation without needing custom markers or depth sensors, marking a critical step forward in practical application scenarios. Extensive testing on the 7-Scenes and 12-Scenes datasets confirms that our method surpasses current state-of-the-art approaches with only 5% to 25% of the training data typically required. Additionally, we demonstrate the utility of our method in visual navigation tasks within augmented reality and simulation environments, potentially applicable in various real-world settings such as kitchen automation, showcasing its potential for real-world applicability and further research into efficient and adaptive robotics systems. This work contributes to the learning-based segment of robotics, emphasizing the reduction of resource needs for system training and highlighting the practical benefits of imitation and augmented learning methods.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Aravind_Rajeswaran1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=alLs7EtRJP",
  "title": "Factorized Contrastive Learning: Going Beyond Multi-view Redundancy",
  "modified_abstract": "Building on the recent advances in multimodal tasks offered by unsupervised domain adaptation (UDA) and domain adaptation (DA), which leverage transformers and attention mechanisms, including cross-attention, to bridge domain gaps, our work introduces an innovative approach for learning self-supervised multimodal representations. While UDA and DA have significantly advanced the learning of domain-invariant features across visual tasks, leveraging convolution-based approaches for vision analysis, they predominantly focus on shared information across domains. Our research, FactorCL, extends these concepts by addressing a critical gap in contrastive learning: the challenge of effectively capturing both shared and modality-unique information relevant to downstream tasks. FactorCL distinguishes itself through three main contributions: (1) factorizing representations into shared and unique components, (2) optimizing information capture through mutual information (MI) bounds, and (3) employing multimodal data augmentations to infer task relevance in the absence of labels. Tested against large-scale, real-world datasets, FactorCL not only encapsulates shared and unique information but also sets new benchmarks across six diverse tasks. This approach not only leverages insights from the transformative capabilities of transformers and model architecture in UDA and DA but also pioneers the exploration of multimodal representation beyond multi-view redundancy. It engages deeply with the challenge and opportunity to learn and infer from complex data without direct supervision.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Pengxin_Guo1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=rUf0GV5CuU",
  "title": "Locality Sensitive Hashing in Fourier Frequency Domain For Soft Set Containment Search",
  "modified_abstract": "This investigation is inspired by groundbreaking approaches in searching and indexing high-dimensional data for applications such as multilingual information access and cross-lingual corpus analysis, where learning hash functions for cross-view similarity search and improving bilingual projections through sparse covariance matrices have set the stage. Our work extends these concepts into the domain of fast retrieval systems, particularly focusing on the challenges posed by passage retrieval, text entailment, and subgraph search, where both the query and documents are sets of elements. In contrast to previous methods which deal with atomic IDs and exact set containment, we address the more complex scenario of soft set containment using embedded representations and interlingual compatibility. We propose transforming the asymmetric hinge distance used for ranking candidate documents into a novel dominance similarity measure, which we further process through a Fourier transform to enable locality-sensitive hashing (LSH) in the Fourier frequency domain. Learning from previous insights, this allows us to approximate the dominance similarity between sets as an expectation of the inner product of functions in the frequency domain, efficiently handled by our FourierHashNet through importance sampling during the training phase. Our methodology benefits from the aligned insights gained in both bilingual and interlingual indexing applications and refines them within the context of soft set containment. Experiments demonstrate that our approach enables efficient retrieval with a favorable query time versus retrieval quality tradeoff, distinguished by the application of Fourier transform and trainable hash codes in the frequency domain for enhanced performance. The integration of diverse views in data representation and the focus on bilingual contexts underscore the significance of continuous learning in developing advanced retrieval systems.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Raghavendra_Udupa1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=WK8LQzzHwW",
  "title": "Unsupervised Anomaly Detection with Rejection",
  "modified_abstract": "Anomaly detection, a key challenge in unsupervised machine learning, endeavors to identify unusual patterns or behaviors within data. This domain has seen iterative advancements through both theoretical exploration, such as the establishment of multi-class $H$-consistency bounds and practical algorithm development, notably in frameworks dealing with abstention in multi-class classification. Our work is motivated by these pivotal contributions, especially in addressing the innate uncertainties that traditional anomaly detection strategies encounter due to their reliance on heuristic, intuition-based decision boundaries. By integrating principles from the study of predictor-rejector relationships, surrogate loss functions, convex optimization, and $H$-consistency, our paper introduces a novel approach to enhancing anomaly detection by enabling the model, acting as a learner, to reject predictions with high uncertainty in an open set recognition scenario. This is achieved through applying a constant rejection threshold to a stability metric that we analyze theoretically, effectively using abstention as a surrogate for direct anomaly rejection. The proposed method improves upon existing metric-based anomaly detection by offering not only a practical solution to the selection of confidence metrics and threshold settings without labels but also by providing strong guarantees on the test rejection rate and the expected prediction cost through optimized loss functions. Moreover, by incorporating auxiliary information, we open a new direction for research that seeks to refine the performance of unsupervised models in distinguishing between normal behavior and anomalies. Through theoretical analysis and empirical testing, our study advances the field by demonstrating the effectiveness of our method against standard benchmarks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yutao_Zhong1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=3xSwxlB0fd",
  "title": "Uncoupled and Convergent Learning in Two-Player Zero-Sum Markov Games with Bandit Feedback",
  "modified_abstract": "Our research is inspired by the exploration of strategic interactions in large action spaces and the development of risk-averse strategies in multi-agent systems, highlighting the importance of adaptability, exploitation of strategic adversaries, and safety-awareness in the learning process. Specifically, we focus on the design of algorithms where each learner is capable of independently adapting to the evolving dynamics of interactions without explicit communication, an approach mirroring developments in population-based intelligence. This provable framework emphasizes the learner's ability to independently navigate through an environment filled with an adversary, demonstrating an intelligent ability to analyze and exploit adversarial vulnerabilities. We revisit the problem of learning in two-player zero-sum Markov games, focusing on developing an algorithm that is *uncoupled*, *convergent*, and *rational*, with non-asymptotic convergence rates to Nash equilibrium, providing a solid theoretical foundation for these concepts. This endeavor is crucial for training agents to effectively respond to adversaries in environments characterized by uncertainty and limited information. We start from the case of stateless matrix game with bandit feedback as a warm-up, showing an $\\tilde{\\mathcal{O}}(t^{-\\frac{1}{8}})$ last-iterate convergence rate. To the best of our knowledge, this is the first result that obtains finite last-iterate convergence rate given access to only bandit feedback. We extend our result to the case of irreducible Markov games, providing a last-iterate convergence rate of $\\tilde{\\mathcal{O}}(t^{-\\frac{1}{9+\\varepsilon}})$ for any $\\varepsilon>0$. Finally, we study Markov games without any assumptions on the dynamics, and show a *path convergence* rate, a new notion of convergence we defined, of $\\tilde{\\mathcal{O}}(t^{-\\frac{1}{10}})$. Our algorithm removes the synchronization and prior knowledge requirement of Wei et al. (2021), which pursued the same goals as us for irreducible Markov games. Our algorithm is related to Chen et al. (2021) and Cen et al. (2021)'s and also builds on the entropy regularization technique. However, we remove their requirement of communications on the entropy values, making our algorithm entirely uncoupled.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Oliver_Slumbers1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=BdvCo8RVlx",
  "title": "The Contextual Lasso: Sparse Linear Models via Deep Neural Networks",
  "modified_abstract": "The evolution of interpretable machine learning, as evidenced by the relationship between model complexity and interpretability in recent literature, underpins the development of our work. This includes significant insights from the study of optimization and the dynamics of double descent for deep neural networks, which highlights the nuanced trade-offs between model size, overfitting, generalization, and the growing sample size in the learning setting. Our research contributes to this dialogue by introducing the contextual lasso, a novel statistical estimator designed to bridge the gap between the interpretability of sparse linear models and the functional flexibility of deep neural networks. By distinguishing between explanatory and contextual features, the contextual lasso dynamically selects variables and adjusts their influence through a deep learning framework, incorporating a unique lasso regularizer to enforce sparsity in the model coefficients. This approach not only maintains the transparency critical to interpretable machine learning but also leverages the depth of neural networks to adaptively tune the sparsity and coefficients of the linear model in response to the contextual information. Evaluative studies on both real and synthetic datasets validate our method's ability to achieve sparser models than traditional lasso, without compromising the predictive capabilities of deep neural networks. The descent curve analysis further underscores the optimization efficiency and effectiveness of our proposed method in various settings, while the model's design allows for the interpretation of complex functions and the calculation of component influences via the pseudoinverse of sparse matrices.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Amal_Rannen-Triki1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=fxNQJVMwK2",
  "title": "Text-to-Image Diffusion Models are Zero Shot Classifiers",
  "modified_abstract": "Building on recent developments in generative models, particularly the exploration of diffusion models for zero-shot capabilities and their application in open-vocabulary segmentation and layout control using cross-attention guidance, our study ventures into new territory. The excellent generative capabilities of text-to-image diffusion models suggest they learn informative representations of image-text data, seamlessly grouping related concepts and utilizing an unlimited mechanism of generation. However, what knowledge their representations capture is not fully understood, and they have not been thoroughly explored on downstream tasks such as editing and controllable image manipulation. We investigate diffusion models by proposing a method for evaluating them as zero-shot classifiers, benchmarking their performance quantitatively on various tasks. The key idea is using a diffusion model's generator to denoise a noised image given a text description of a label as a proxy for that label's likelihood. We apply our method to Stable Diffusion and Imagen, using it to probe fine-grained aspects of the models' knowledge and comparing them with CLIP's zero-shot abilities. They perform competitively with CLIP on a wide range of zero-shot image classification benchmarks, including tests on segmentation and control tasks. Additionally, they achieve state-of-the-art results on shape/texture bias tests and can successfully perform attribute binding while CLIP cannot. Although generative pre-training is prevalent in NLP, visual foundation models often use other methods such as contrastive learning. Based on our findings, we argue that generative pre-training should be explored as a compelling alternative for vision and vision-language problems, with a strong emphasis on quantitative analysis.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Andrea_Vedaldi1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=oDtyJt5JLk",
  "title": "Directional diffusion models for graph representation learning",
  "modified_abstract": "The utilization of diffusion models in unsupervised graph representation learning presents an innovative approach to addressing the limitations observed in traditional graph learning methods, inspired by the breakthroughs in fields such as image synthesis and 3D molecule generation. Our work is motivated by the success of these models in various domains and seeks to extend their application to graph-structured data, demanding a novel methodology for handling anisotropic structures within graphs. Recognizing a critical limitation in the conventional forward diffusion process\u2014its tendency to excessively dilute anisotropic signals with isotropic Gaussian noise\u2014we propose a new class of models known as directional diffusion models. These models introduce data-dependent, anisotropic, and directional noises to preserve the integrity of anisotropic signals during the diffusion process. Through comprehensive experiments on 12 publicly available datasets, focusing on two distinct graph representation learning tasks including classification, our findings significantly demonstrate the superior capability of our models over existing baselines in capturing meaningful graph representations. This research casts new light on the forward process intricacies in diffusion models and opens new avenues for exploring their potential in graph-related tasks. Our innovative approach, inspired by previous achievements in reservoir computing and topology-preserving maps, leverages the strengths of diffusion models and neural network-based aggregators to overcome the challenges faced by current graph neural network methodologies in training, thereby providing enhanced graph representation learning capabilities with notable performance improvements.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Luca_Pasa1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=HvhagNdf5z",
  "title": "Synthetic-to-Real Pose Estimation with Geometric Reconstruction",
  "modified_abstract": "This work is inspired by the success of unsupervised domain adaptation techniques in 3D object detection and semantic segmentation, as evidenced by recent advancements. Our focus shifts to the challenge of pose estimation, a field where leveraging synthetic data for real-world applications remains a complex task due to the domain gap between synthetic and real imagery. We address the issue of adapting models trained on synthetic data to real-world target domains with minimal labeled data during training. A pervasive challenge in this transition is the generation of high-quality pseudo-labels, an area where existing methodologies, though successful in contexts such as 3D object detection and segmentation, reveal limitations for pose estimation. To bridge this gap, we propose a novel strategy that combines pseudo-labeling with a reconstruction-based approach and a deep learning algorithm to refine the accuracy of pseudo-labels in the context of domain adaptation. By geometrically transforming a base image according to predicted keypoints and enforcing a reconstruction loss, our approach innovatively corrects keypoint locations that are confidently predicted yet inaccurately placed. This method not only provides an effective solution for synthetic-to-real adaptation but also advances the state-of-the-art in pose estimation on several real-world datasets, showcasing significant improvements especially in critical landmarks such as fingertips and heads. Our research contributes to the learning methodologies enabling efficient adaptation of synthetic data principles to real-world scenarios in both pedestrian and indoor environments.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jihan_Yang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=BmIW6U0rz8",
  "title": "Koopman Kernel Regression",
  "modified_abstract": "Leveraging insights from the field of dynamical systems, specifically the novel belief space planning technique for continuous dynamics and the application of stochastic sequential action control as proposed in related works, we extend these foundational concepts into the realm of machine learning for decision-making via Koopman operator theory. Our focus is on the application of Koopman operator theory, a powerful tool for converting nonlinear dynamical systems forecasts into linear time-invariant (LTI) ordinary differential equations (ODEs), thereby simplifying multi-step forecasting processes into efficient sparse matrix multiplication. Despite the advancement of various learning approaches, most lack the critical learning-theoretic guarantees necessary for understanding model behavior with increasing data and dimensionality. Addressing this gap, we introduce a universal Koopman-invariant reproducing kernel Hilbert space (RKHS) that synthesizes and is exclusively dedicated to transformations leading to LTI dynamical systems. The Koopman Kernel Regression (KKR) framework that emerges from our work facilitates the application of statistical learning techniques to achieve novel convergence results and establish generalization error bounds under more lenient assumptions than previously available. Our experiments showcase the superior forecasting performance of the KKR framework when compared to existing methods for Koopman operator and sequential data prediction within an RKHS context.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Haruki_Nishimura1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=nDIrJmKPd5",
  "title": "Private Distribution Learning with Public Data: The View from Sample Compression",
  "modified_abstract": "Our investigation is inspired by the challenges and strategies discussed in prior works on the efficiency and privacy of information retrieval and the optimal communication strategies in coded environments within distributed storage systems, where potential adversarial collusion can render some channels unreliable. Specifically, the concepts of Private Information Retrieval (PIR) in the presence of arbitrary collusion patterns and PIR from maximum distance separable (MDS) coded data in distributed storage systems introduce foundational principles relevant to addressing privacy in complex data scenarios, employing various schemes to ensure data privacy. We study the problem of private distribution learning with access to public data, which we term *public-private learning*. In this setup, the learner is provided with both public and private samples from an unknown distribution $p$ within a class $\\mathcal Q$, with the aim of estimating $p$ while ensuring pure differential privacy for the private samples only, without revealing sensitive information. Our findings highlight the interconnection between public-private learnability of a class $\\mathcal Q$ and the existence of a sample compression scheme for $\\mathcal Q$, as well as an intermediate notion we define as *list learning*. This approach (1) verifies previous findings on Gaussians over $\\mathbb R^d$, and (2) introduces novel contributions, including sample complexity upper bounds for arbitrary $k$-mixtures of Gaussians over $\\mathbb R^d$, results for agnostic and distribution-shift resistant learning, and closure properties for public-private learnability, particularly when considering mixtures and product distributions. Moreover, through the prism of list learning, we demonstrate that for Gaussians in $\\mathbb R^d$, at least $d$ public samples are indispensable for private learnability, closely aligning with the established upper boundary of $d+1$ public samples for effective communication within such systems, and especially in servers where data must be protected from revealing to unauthorized entities.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Razane_Tajeddine1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=2nTpPxJ5Bs",
  "title": "Double Auctions with Two-sided Bandit Feedback",
  "modified_abstract": "Motivated by the significant contributions of recent works on fair allocation mechanisms and the computational complexity of equilibrium computation in various auction formats, this paper explores double auctions in the unique context of two-sided bandit feedback. Double Auction enables decentralized transfer of goods between multiple buyers and sellers, thus underpinning the functioning of many online marketplaces, a natural extension of traditional market mechanisms. Buyers and sellers compete in these markets through bidding but do not often know their own valuation a-priori. As the allocation and pricing happen through bids, the profitability of participants, hence the sustainability of such markets, depends crucially on learning respective valuations through repeated interactions. We initiate the study of Double Auction markets under bandit feedback on both buyers' and sellers' sides, employing computing efficient algorithms for solving this problem. We show with confidence bound based bidding, and 'Average Pricing,' there is an efficient price discovery among the participants. In particular, the regret on combined valuation of buyers and sellers -- a.k.a. the social regret -- is $O(\\log(T)/\\Delta)$ in $T$ rounds, where $\\Delta$ is the minimum price gap. Moreover, buyers and sellers exchanging goods attain $O(\\sqrt{T})$ regret, individually. The buyers and sellers who do not benefit from exchange in turn only experience $O(\\log{T}/ \\Delta)$ regret individually in $T$ rounds. We augment our upper bound by showing that $\\omega(\\sqrt{T})$ individual regret, and $\\omega(\\log{T})$ social regret, is unattainable in certain Double Auction markets. Our paper is the first to provide decentralized learning algorithms in a two-sided market where both sides have uncertain preference that need to be learned, through computing efficient algorithms and approximating the -equilibrium for these markets.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Philip_Lazos2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
