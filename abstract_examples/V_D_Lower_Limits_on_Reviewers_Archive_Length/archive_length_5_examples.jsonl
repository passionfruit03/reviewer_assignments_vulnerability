{
  "paper_link": "https://openreview.net/forum?id=eTp4RetK74",
  "title": "ASPEN: Breaking Operator Barriers for Efficient Parallelization of Deep Neural Networks",
  "modified_abstract": "The scalable execution of Deep Neural Networks (DNNs) on hardware presents substantial challenges, necessitating innovative approaches to breaking the computational barriers inherent in traditional operator-based DNN frameworks. Informed by preceding efforts in hardware-aware network transformation, latency-sensitive structural pruning, and Vision Transformer compression, our investigation introduces ASPEN, a paradigm shift in DNN parallelization. These prior studies, addressing network acceleration, structural pruning for improved inference speed, and transformer model optimization for reduced computational cost, collectively highlight the critical need for effective utilization of hardware resources and latency reduction in AI models. ASPEN leverages these insights to dismantle the synchronization barriers imposed by tensor operators, thereby unveiling and exploiting the vast potential for parallel computation across DNN operators. By reimagining DNNs as dataflow graphs of fine-grained tiles, ASPEN transcends traditional parallelism limits, achieving fine-grained dynamic execution that accommodates opportunistic parallelism through automated detection of parallelizable operations. This approach significantly enhances resource utilization and memory reuse, enabling asynchronous, depth-wise graph traversal. Our proof-of-concept implementation demonstrates ASPEN\u2019s superior performance, markedly outpacing leading inference systems on CPU, thereby affirming the substantial benefits of integrating hardware-aware optimizations and compression techniques into DNN execution strategies. The integration of saliency-based filter techniques further aids in optimizing network training and classification tasks, ensuring only the most impactful computations are prioritized for acceleration.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Hongxu_Yin1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=jDIlzSU8wJ",
  "title": "The Surprising Effectiveness of Diffusion Models for Optical Flow and Monocular Depth Estimation",
  "modified_abstract": "Denoising diffusion probabilistic models have recently demonstrated their transformative potential in high-resolution image synthesis, significantly impacting tasks ranging from photorealistic generation to class-conditional synthesis and super-resolution. These developments, leveraging the power of latent diffusion models and novel retrieval-augmented techniques, highlight the models' flexibility and efficiency across varied image generation challenges, including inpainting and instance-specific synthesis. Inspired by this leap in image synthesis capabilities, our work extends the application of diffusion models beyond their conventional domain. We show that denoising diffusion probabilistic models also excel in estimating optical flow and monocular depth, surprisingly without the need for task-specific architectures and loss functions that traditionally dominate these areas. Our approach contrasts with conventional regression-based methods by employing attention mechanisms to enable Monte Carlo inference to capture uncertainty and ambiguity in flow and depth estimations, a significant departure from deterministic point estimates. Employing a combination of self-supervised pre-training, the strategic use of synthetic and real data for supervised training, memory-efficient techniques, and technical innovations in infilling and step-unrolled denoising diffusion training to accommodate noisy or incomplete training data, we demonstrate how state-of-the-art diffusion models can be adeptly trained for depth and optical flow estimation, including generation of complex scenes. These models further facilitate zero-shot coarse-to-fine refinement for high-resolution estimates. Through extensive experimentation, including rigorous quantitative benchmarks, ablation studies, and evaluations of the model's ability to express uncertainty and impute missing values, our diffusion model establishes new standards in compressing relative depth error and Fl-all score performance on the indoor NYU and KITTI optical flow benchmarks, thereby marking a significant advancement in the field.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Andreas_Blattmann1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=9fWKExmKa0",
  "title": "DPM-Solver-v3: Improved Diffusion ODE Solver with Empirical Model Statistics",
  "modified_abstract": "This work is inspired by the foundational principles and empirical findings from a diverse range of machine learning advancements, including but not limited to, score-based generative models, Gaussian process approximations, and stochastic differential equation solvers for Bayesian posterior sampling. These prior studies underline the significance of optimal parameterization, efficiency in sampling, particularly in the context of mini-batch processing, and the integration of empirical data with theoretical models, all of which inform the development of our proposed solution. In the domain of diffusion probabilistic models (DPMs), known for their prowess in high-fidelity image generation, inefficiency in sampling remains a critical drawback, significantly when scaled to large datasets for scalability. Building on recent attempts to accelerate this process through fast ODE solvers tailored to the DPM framework, we identify a gap in the optimization of parameterization during inference, encompassing noise and data prediction elements. Addressing this, our research introduces a novel formulation designed to minimize the first-order discretization error of the ODE solution, leading to the development of \\textit{DPM-Solver-v3}. This enhanced solver, underpinned by \\textit{empirical model statistics} computed from the pretrained model during training, integrates multistep methods and a predictor-corrector framework to refine sample quality, especially under conditions of limited function evaluations or extended guidance scales. Through rigorous experimentation, backward identification techniques combined with DPM-Solver-v3 demonstrate superior or on-par performance in both unconditional and conditional sampling across various DPM tasks, significantly in scenarios of 5\\textasciitilde10 NFEs, achieving notable speed-ups and efficiency metrics over previous models. The reduction in first-order discretization error alongside improved log-likelihood scores for generated images further validates our approach.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Pietro_Michiardi1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=JTKd7zYROf",
  "title": "Randomized Sparse Neural Galerkin Schemes for Solving Evolution Equations with Deep Networks",
  "modified_abstract": "This work is inspired by recent advances in solving partial differential equations using machine learning techniques, such as the domain decomposition methods enhanced by Graph Convolutional Neural Networks and the development of multigrid graph neural networks (MG-GNN) for learning multilevel domain decomposition (subdomains) methods. Furthermore, efforts to optimize sparse matrix operations for reverse mode automatic differentiation and the use of reinforcement learning in optimizing algebraic multigrid coarsening have informed our approach to robust training networks. Training neural networks sequentially in time to approximate solution fields of time-dependent partial differential equations can be beneficial for preserving causality and other physics properties; however, the sequential-in-time training is numerically challenging because training errors quickly accumulate and amplify over time. Our work introduces Neural Galerkin schemes that update randomized sparse subsets of network parameters at each time step. The randomization avoids overfitting locally in time and so helps prevent the error (loss) from accumulating quickly over the sequential-in-time training, which is motivated by dropout that addresses a similar issue of overfitting due to neuron co-adaptation. The sparsity of the update reduces the computational costs of training without losing expressiveness because many of the network parameters are redundant locally at each time step. In numerical experiments with a wide range of evolution equations, the proposed scheme with randomized sparse updates is up to two orders of magnitude more accurate at a fixed computational budget and up to two orders of magnitude faster at a fixed accuracy than schemes with dense updates, optimizing both accuracy and computational efficiency.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ali_Taghibakhshi1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=1qFnxhdbxg",
  "title": "Energy Discrepancies: A Score-Independent Loss for Energy-Based Models",
  "modified_abstract": "Amidst ongoing research on probabilistic models for complex data understanding\u2014including normalizing flows for density estimation and independent mechanism analysis for representation learning\u2014energy-based models (EBMs) stand out for their generative capabilities and theoretical appeal. However, EBMs have not been fully exploited in practical applications due to the computational intensiveness of their training processes and the reliance on extensive sampling methodologies. Our work introduces a novel loss function, Energy Discrepancy (ED), designed to address this gap by not requiring the computation of scores or the use of Markov chain Monte Carlo (MCMC) methods, while still subtly incorporating mixtures and histograms analysis within our evaluations. This loss function is foundational, as it encapsulates the strengths of explicit score matching and negative log-likelihood approaches under various conditions, offering a new perspective in the optimization landscape for EBMs. By demonstrating that minimum energy discrepancy can circumvent the short-sightedness of score-based methods while preserving theoretical integrity and network efficiency, we lay the groundwork for more efficient training of energy-based models without the conventional necessity for extensive sampling. Empirical evidence, bolstered by numerical experiments on low-dimensional distributions and high-dimensional image data, suggests that ED is capable of learning data distributions with greater speed and accuracy than its predecessors, changing the dynamics of how these networks are trained. Furthermore, by exploring the limitations imposed by the manifold hypothesis on our method, and illustrating its utility in conjunction with a variational decoder model for specific tasks, we provide a comprehensive overview of the potential and versatility of energy discrepancy in advancing the field of generative modeling, even outside the traditional domains such as text generation.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Vincent_Stimper1",
  "manipulated_ranking": 20,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=DCIsNIUCV7",
  "title": "Payoff-based Learning with Matrix Multiplicative Weights in Quantum Games",
  "modified_abstract": "This work extends the understanding of learning dynamics in complex environments by studying the problem of learning in quantum games - and other classes of semidefinite games - with scalar, payoff-based feedback. Building on the methodologies and insights from previous works on congestion games, offline congestion games, and exploration strategies in Markov Decision Processes, which address learning under various forms of limited information feedback, we introduce a novel framework for learning in the previously underexplored domain of quantum games. Specifically, we focus on the matrix multiplicative weights (MMW) algorithm and introduce a suite of minimal-information matrix multiplicative weights (3MW) methods tailored to different information scenarios in quantum settings. Addressing the unique challenge presented by quantum games\u2019 infinite continuum of pure states, we incorporate zeroth-order gradient sampling, inspired by bandit convex optimization, adapted to the semidefinite geometry characteristic of quantum games. We demonstrate that the 3MW method with deterministic payoff feedback maintains the $\\mathcal{O}(1/\\sqrt{T})$ convergence rate of the full-information MMW algorithm in quantum min-max games, using only scalar feedback. Further, we present a 3MW method requiring only observation of random payoff realizations, achieving equilibrium convergence at an $\\mathcal{O}(T^{-1/4})$ rate. Beyond zero-sum frameworks, we also establish local convergence with high probability to equilibria meeting a first-order stability condition through a regularized version of 3MW. This exploration uniquely contributes to the intersection of machine learning and quantum game theory, offering insights into algorithmic design for semidefinite and quantum strategic interactions.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zhihan_Xiong1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=pNtG6NAmx0",
  "title": "Statistical Knowledge Assessment for Large Language Models",
  "modified_abstract": "The evolution of Large Language Models (LLMs) has been significantly influenced by prior advancements in machine translation, statistical modeling, and model efficiency. Studies ranging from the development of new translation rules through semi-supervised, graph-based learning, to the exploration of supertagged phrase-based statistical machine translation, and the improvement of machine translation through monolingual continuous representations, all contribute to the current understanding of how LLMs can be optimized for performance and efficiency. These works highlight the critical aspects of adaptability, efficiency, and knowledge encoding in models, setting the groundwork for assessing the knowledge capacity of LLMs in translating not just languages but also translating knowledge into performance. Furthermore, innovations in model compression and efficiency, such as the quantization of Mixture of Experts (MoE) models and the introduction of sparsely activated transformer networks, reflect on the scalability and operational efficiency of LLMs. Drawing on these insights, this paper introduces KaRR, a novel statistical approach designed to quantify the factual knowledge contained in an LLM. By estimating the ratio of an LLM generating text corresponding to the right answer entity for a given set of diverse prompts versus random generation, KaRR offers a structured method to evaluate the knowledge representation of twenty LLMs of varying sizes, incorporating task-specific and phrase-based methodologies within an auto-regressive architecture framework. This method demonstrates a strong correlation (0.43 Kendall's $\\tau$) with human assessments of LLMs, affirming the utility of our approach in discerning the knowledge integrity of LLMs. Our findings further reveal that knowledge in LLMs, providing a new baseline for factual accuracy, with identical architectural backbones respects the scaling law, although instructive data training can sometimes detract from a model's factual accuracy. This analysis provides a comprehensive framework for future exploration into the knowledge assessment of LLMs, suggesting pathways for enhancing model reliability and factual correctness.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Hany_Hassan1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=vnTUuecp2v",
  "title": "Higher-Order Uncoupled Dynamics Do Not Lead to Nash Equilibrium - Except When They Do",
  "modified_abstract": "Our investigation within the multi-agent learning framework is inspired by preceding studies that have advanced our understanding of equilibrium finding in various settings, such as offline reinforcement learning (RL), solving extensive-form games with combinatorial action spaces, and computing ex ante coordinated equilibria in multiplayer games using prominent computing technologies. These foundational works highlight the complex nature of agent interactions, the challenges involved in achieving equilibrium states, whether they be Nash Equilibria (NEs), Correlated Equilibria (CEs), or Team-Maxmin Equilibria (TMEs), and the integral role of regret minimization in these processes. Building upon these insights, we introduce the concept of higher-order gradient play dynamics, which incorporate auxiliary states to account for phenomena such as path dependencies, marking a novel approach in the exploration of strategy evolution among agents. We show that these higher-order dynamics, while payoff based and uncoupled, can indeed locally converge to a NE in certain specific games and under precise conditions, entirely contributing to the theory of multi-agent systems. Notwithstanding, we also demonstrate that for any given set of higher-order dynamics, one can construct a game where these dynamics fail to achieve NE, highlighting a nuanced understanding of the conditions under which convergence is possible. Moreover, our work explores the inherent trade-offs in ensuring that the dynamics leading to mixed-strategy equilibria in coordination games do not foster internal instability. This research contributes to a deeper understanding of the potential and limitations of higher-order uncoupled dynamics in multi-agent learning and their relationship with established equilibrium concepts.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Youzhi_Zhang2",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=I9xE1Jsjfx",
  "title": "Evaluating and Inducing Personality in Pre-trained Language Models",
  "modified_abstract": "This study is inspired by existing research efforts that have explored systematic failure patterns in multimodal systems, the role of human cognitive biases---systematic biases in evaluating large language models (LLMs), and novel auditing methodologies via discrete optimization to preempt catastrophic deployments. Such prior works provide a compelling backdrop for assessing machine behaviors beyond mere functionality and correctness, including the generation of open-ended responses that exhibit human-like personality traits. Standardized and quantified evaluation of machine behaviors is a crux of understanding LLMs. Leveraging human personality theory, originating from philosophical inquiries into human behavior, facilitates a principled and quantitative examination of machine behaviors. Our study introduces the Machine Personality Inventory (MPI) tool, grounded in the Big Five Personality Factors (Big Five) theory, to systematically evaluate LLMs, presenting evidence of MPI's efficacy in studying machine behaviors and in generating examples of these behaviors. Additionally, we propose a Personality Prompting (P$^2$) method, offering a controllable approach to imbue LLMs with specific personality traits. This not only produces diverse and verifiable behaviors but also aligns with the ongoing discourse on making LLMs exhibit human-like traits for varied downstream applications, ensuring a broader agreement on social compatibility and roles. Our work is poised to enhance the understanding of LLMs, offering insights into their potential social compatibility and roles.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Erik_Jones3",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=neu9JlNweE",
  "title": "Post-processing Private Synthetic Data for Improving Utility on Selected Measures",
  "modified_abstract": "Building on recent advancements in submodular maximization and experimental design for covariate balance, our research introduces a novel post-processing technique that directly addresses the challenge of optimizing synthetic data utility for specific downstream tasks, such as recommendation systems, while maintaining strong privacy guarantees. The essence of this technique is to selectively resample the synthetic data, thereby filtering out samples that fall short of predefined utility metrics, which is particularly relevant for applications involving large datasets in both centralized and online contexts. This method draws inspiration from the principles of subsampling for submodular maximization and the Gram-Schmidt walk for balancing covariates, adapting these concepts to the context of synthetic data generation. By employing an efficient stochastic first-order algorithm, our approach efficiently identifies optimal resampling weights, achieving asymptotically an (k+3)-approximation to the optimal solution. This paradigm not only preserves the quality and privacy of the data but also significantly enhances its utility for end-user applications, as empirically demonstrated through an extensive evaluation across various benchmark datasets and synthetic data generation algorithms. Our post-processing technique demonstrates the consistently improved performance in meeting the selected utility measures without compromising data privacy or integrity.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Christopher_Harshaw1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=tFeaLw9AWn",
  "title": "Single-Call Stochastic Extragradient Methods for Structured Non-monotone Variational Inequalities: Improved Analysis under Weaker Conditions",
  "modified_abstract": "Inspired by the evolving landscape of stochastic optimization algorithms and their foundational role in addressing min-max optimization and variational inequality problems across a broad spectrum of machine learning tasks, this work aims to extend the analytical horizon of such methods. Notable prior advancements have focused on the development and convergence analysis of stochastic gradient descent-ascent algorithms, decentralized stochastic methods for variational inequalities, and specialized techniques for non-convex minimization and distributed computing with an emphasis on communication efficiency. Leveraging insights from these related areas, which inherently involve decentralized architectures and distributed systems, our study introduces an improved analysis of single-call stochastic extragradient methods, specifically stochastic past extragradient (SPEG) and stochastic optimistic gradient (SOG), under significantly relaxed assumptions. Despite the wide acceptance of these methods for large-scale optimization problems embedded within machine learning applications, including adversarial training scenarios, current convergence theories necessitate stringent conditions such as bounded variance and specific growth constraints. By addressing unresolved issues including mini-batching, step-size adaptability, sampling strategy impacts through decentralized approaches, and coordinate updating techniques, we extend the utility of SPEG and SOG to a broader class of structured non-mononote variational inequalities, encompassing quasi-strongly monotone and weak Minty variational inequalities. The introduction of the expected residual condition as a central analytical tool and the application of compressions in communication protocols enable us to surpass traditional limitations in growth conditions, expected co-coercivity, and variance bounds underpinning our findings with convergence guarantees that are adaptable to the arbitrary sampling paradigm, which includes techniques from importance sampling to diverse mini-batching strategies. Emphasizing the utility of varied network topologies, this study lays the foundation for more efficient train-phase optimizations in machine learning applications, drawing significant improvements from compressed information communication protocols.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Aleksandr_Beznosikov1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=TAIYBdRb3C",
  "title": "Curve Your Enthusiasm: Concurvity Regularization in Differentiable Generalized Additive Models",
  "modified_abstract": "Inspired by recent discussions and findings in the literature of time-series analysis and forecasting, particularly those emphasizing the importance of addressing imprecision in model assumptions and advocating for robustness against adversarial attacks, our research introduces an innovative approach to enhancing the interpretability of Generalized Additive Models (GAMs) in large learning contexts. Generously acknowledged for their interpretability, GAMs uniquely express the target value as a sum of non-linear transformations of the features in multivariate data scenarios. However, the issue of concurvity - namely, dependencies, possibly non-linear, between the models' features - has been largely unaddressed, posing a significant hurdle to fully leveraging the interpretative power of GAMs. Our study proposes a conceptually straightforward, yet potent, regularizer aimed at minimizing pairwise correlations of the non-linearly transformed feature variables. This methodology is adaptable to any differentiable additive models, inclusive of Neural Additive Models or NeuralProphet, serving to enhance interpretability by dispelling ambiguities resultant from self-cancelling feature contributions in large datasets through judicious learning and sampling techniques. Through experimental validation on both synthetic and large-scale real-world datasets pertinent to forecasting and tabular data, our regularizer proves its efficacy in tempering concurvity within GAMs without markedly detracting from prediction quality. This not only enriches interpretability but also decreases variance in feature importance assessments, burgeoning our understanding and manipulation of GAMs in the field.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Hilaf_Hasson1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=lkEiOZlmPm",
  "title": "Single-Pass Pivot Algorithm for Correlation Clustering. Keep it simple!",
  "modified_abstract": "In the context of recent advancements in machine learning spanning efficient data summarization through Density Sketches, sketching-based novel approaches to near neighbor search combining group testing with locality-sensitive hashing, and the development of algorithms for vector set search with strong theoretical underpinnings, our work introduces a simple single-pass semi-streaming variant of the Pivot algorithm for Correlation Clustering. We achieve a (3+eps)-approximation using O(n/eps) words of memory, marking a slight improvement over the recent results by Cambus, Kuhn, Lindy, Pai, and Uitto, who achieved a similar approximation using O(n log n) words of memory, and Behnezhad, Charikar, Ma, and Tan, who offered a 5-approximation with O(n) words of memory. Our main contributions are the simplicity of the algorithm and its analysis, aiming for accessibility and ease of understanding while maintaining rigorous performance validation through streaming data and sampling techniques, enhanced by re-sampled synthetic instances. The relevance of this work spans not only theoretical machine learning communities but also practical applications where data summarization and efficient processing are crucial,",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Benjamin_Coleman1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=ZfFR4d5gUM",
  "title": "Leveraging the two-timescale regime to demonstrate convergence of neural networks",
  "modified_abstract": "Building on seminal works that explore the dynamics of neural network training, feature learning's impact on generalization, and the geometric implications of training on neural network representations, our research presents a novel examination of shallow neural network training dynamics within a distinct two-timescale regime. These investigations into deep Bayesian linear regression, the role of training on shaping the curvature of neural network representations, the storage capacity, and kernel limits of wide neural networks, and their implications for various machine learning tasks provide a critical backdrop for our study. Specifically, we extend the understanding of how training dynamics\u2014when stepsizes for the inner layer are significantly smaller than those for the outer layer\u2014facilitate convergence to a global optimum in a non-convex optimization problem, a goal often sought but seldom achieved in practical training regimes. Our findings, centered around the utilization of kernels and the kernel-limit concept in correspondence with neural density layers, are substantiated through experiments that illustrate stochastic gradient descent\u2019s adherence to our gradient flow characterization, affirming global optimum convergence within the two-timescale regime, and detailing failure scenarios outside this regime. This work contributes significantly to our goals of both theoretical insight and practical application in neural network optimization and underscores the importance of tasks-specific customization.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jacob_A_Zavatone-Veth1",
  "manipulated_ranking": 10,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=5r3e27I9Gy",
  "title": "Composing Parameter-Efficient Modules with Arithmetic Operation",
  "modified_abstract": "Inspired by recent breakthroughs in applying fine-tuning techniques across various domains including language instruction, demonstration ensembling, subgoal planning, and compositional task learning, this paper introduces a novel method for adapting pretrained language models via parameter-efficient fine-tuning (PEFT). Our method stands on the shoulders of these diversified approaches, particularly leveraging insights from the domains of expert language models, in-context learning, actionable planning with minimal supervision, and compositional task execution informed by language instructions. As an efficient alternative to conventional full fine-tuning, PEFT is becoming the prevailing method to adapt pretrained language models for improved behavioral predictions and robust text understanding. In PEFT, a lightweight module is learned on each dataset while the underlying pretrained language model remains unchanged, resulting in multiple compact modules representing diverse skills when applied to various domains and tasks. In this paper, we propose to compose these parameter-efficient modules through linear arithmetic operations in the weight space, thereby integrating different module capabilities for accurate predictions and addressing navigation challenges. Specifically, we first define an addition and negation operator for the module, and then further compose these two basic operators to perform flexible arithmetic. Our approach requires no additional training and enables highly flexible module composition to address challenges in distribution generalization, multi-tasking, detoxifying, and domain transfer without the need for a subsequent release of entirely new models. Additionally, we extend our approach to detoxify Alpaca-LoRA, the latest instruction-tuned large language model based on LLaMA, by using data systematically held together in novel configurations. Empirical results demonstrate that our approach produces new and effective parameter-efficient modules that significantly outperform existing ones across all settings.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Lajanugen_Logeswaran1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=sq4o3tjWaj",
  "title": "What\u2019s Left? Concept Grounding with Logic-Enhanced Foundation Models",
  "modified_abstract": "Inspired by prior advancements in visual and language models, such as the development and application of transformer-based architectures for Natural Language Processing (NLP) and object-centric models for Vision-Language (VL) tasks, our research introduces the Logic-Enhanced Foundation Model (LEFT). These precedents demonstrate the effectiveness of leveraging large datasets and sophisticated model architectures to improve task-specific performance, from navigation based on visual and language inputs to enhancing visual representations in VL tasks and detection scenarios. Our study extends these foundational insights by addressing the challenge of concept grounding beyond constrained domains. While existing models like VisProg and ViperGPT have successfully utilized foundation models for tasks in limited contexts, such as 2D image reasoning through the execution of programs produced by Large Language Models (LLMs), and fine-tuning them on specialized corpora, their application to abstract concepts and broader data domains has been limited. LEFT offers a novel fusion solution by learning to ground and reason across diverse domains, including 2D images, 3D scenes, human motions, and robotic manipulation, through a domain-independent, differentiable logic-based program executor. This approach, evaluated against multimodal benchmarks, is built upon a unified framework that allows LEFT to flexibly learn and apply concepts across these varied domains, demonstrating strong reasoning capability and improved performance in both familiar and novel tasks. Our contributions highlight the potential of integrating logic enhancements with foundation models to achieve a more generalized, domain-agnostic capability for concept understanding and reasoning.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Xiujun_Li1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=L9nTuSbAws",
  "title": "GradOrth: A Simple yet Efficient Out-of-Distribution Detection with Orthogonal Projection of Gradients",
  "modified_abstract": "Inspired by recent findings in self-supervised learning (SSL), which emphasize the importance of understanding representation learning, particularly through mechanisms like variance-invariance-covariance regularization, normalization techniques, and the effective aggregation of image patch embeddings, we introduce a novel out-of-distribution (OOD) detection method, GradOrth. Our approach is grounded in the observation that the gradients of the most important parameters of a pre-trained network, with respect to in-distribution (ID) data, reside in a lower-rank subspace that can be leveraged for efficient OOD detection. By projecting the gradients onto the subspaces deemed important for ID data and utilizing principles from encoders within these networks, including tuning strategies to enhance localization and representation specificity of tasks, GradOrth calculates the norm of this orthogonal projection, where a significant orthogonal projection value (indicative of a small projection value) flags the sample as OOD due to its weak correlation with ID data, accounting for locality in the data representation through patches. This method, while simple, demonstrates remarkable performance improvements, including a reduction in the average false positive rate at a 95% true positive rate (FPR95) of up to 8%, compared to current state-of-the-art methods. By drawing on principles from both contrastive and non-contrastive self-supervised (SSL) approaches, enhancing memory efficiency, and integrating insights on image patch embedding and normalization, GradOrth capitalizes on the nuanced yet powerful aspects of gradient information to enhance OOD detection significantly.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Adrien_Bardes1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=FtZ7lUwH99",
  "title": "Dynamic Pricing and Learning with Bayesian Persuasion",
  "modified_abstract": "This work situates itself within a growing body of machine learning research that explores the interplay between decision-making and data-driven inference, including frameworks like experimental design for regret minimization in linear bandits, robust estimation in reinforcement learning with linear function approximation, and the integration of offline data with online interactions in reinforcement learning. Informed by these advances, we propose a novel dynamic pricing and learning setting that incorporates Bayesian persuasion to model the influence of advertising schemes on buyer behavior. Specifically, our approach examines how a seller can strategically choose both the price and the information disclosed about a product to maximize expected revenue over sequential rounds of sales without prior knowledge of the buyer's demand function. Utilizing the Bayesian persuasion framework, we introduce a computationally efficient online algorithm that adaptively learns optimal pricing and advertising strategies based on buyer responses, with a design-based and provably effective analysis revealing a $O(T^{2/3}(m \\log T )^{1/3})$ regret bound for linear valuation functions, with $m$ representing the cardinality of the product quality domain and $T$ the time horizon. This provable finding not only aligns with established regret lower bounds for dynamic pricing but also extends the understanding of how advertising schemes can be optimized in real-time to balance revenue maximization with customer engagement, even within the state of limited sample size of customer interactions and striving towards instance-optimal solutions. Furthermore, our research contributes to the broader discourse on applying machine learning methods to economic problems, offering insights into the potential for algorithmic decision-making to enhance commercial strategies in settings where consumer preferences and behaviors are initially unknown.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Andrew_Wagenmaker1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=ZRBGwpeewz",
  "title": "Revisiting Area Convexity: Faster Box-Simplex Games and Spectrahedral Generalizations",
  "modified_abstract": "Our exploration into area convexity, a robust mechanism introduced by Sherman for optimization under the challenging $\\ell_\\infty$ geometry, is partially inspired by seminal contributions in dimension reduction, optimization on discrete domains, determinant maximization techniques, and advancements in sampling of spanning trees and determinantal point processes. These preceding works have not only paved the way for more efficient algorithms but have also expanded the realm of possible applications of mathematical optimization and sampling methods, including those that employ distributed computing and sophisticated samplers. Building upon these foundations and the quest for understanding complex geometric intricacies in optimization, we delve deeper into the relationship between area convexity and conventional analyses of extragradient methods. Further, we develop improved solvers for the intricate subproblems required by variants of the Sherman algorithm via the notion of relative smoothness. Using these refined tools, we present a state-of-the-art first-order algorithm for solving box-simplex games, characterized by a primal-dual formulation of $\\ell_\\infty$ regression, in a $d \\times n$ matrix context with bounded rows. This achievement culminates in a compelling $O(\\log d \\cdot \\epsilon^{-1})$ matrix-vector query complexity, significantly sample-efficient. Consequently, this advancement yields improved complexities for a suite of fundamental combinatorial optimization problems, such as approximate maximum flow, optimal transport, and min-mean-cycle. Through a design of numerical methods and the utilization of graph theory, in a significant leap forward, we also introduce a near-linear time algorithm for a matrix generalization of box-simplex games, thereby encapsulating a family of problems that find synergies with semidefinite programs, determinants, and have widespread applications in robust statistics and numerical linear algebra.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Thuy-Duong_Vuong1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=4vGVQVz5KG",
  "title": "Unsupervised Behavior Extraction via Random Intent Priors",
  "modified_abstract": "This work is inspired by recent strides in reinforcement learning (RL) that focus on optimizing exploration strategies, addressing hybrid action spaces, and enhancing multi-agent collaboration, as seen in our review of related works such as OVD-Explorer's information-theoretic approach for RL, PMIC's framework for high-dimensional multi-agent reinforcement learning, and HyAR's method for discrete-continuous action RL. These innovations provide a backdrop against which we introduce UBER, an unsupervised approach to extract useful behaviors from offline reward-free datasets via diversified rewards. UBER leverages the concept of utilizing diversified rewards, where different pseudo-rewards sampled from a given prior distribution are assigned to different agents to extract a diverse set of behaviors, and reuse them as candidate policies for learning new tasks. Surprisingly, we demonstrate that rewards generated from random neural networks can be sufficient to extract diverse and useful behaviors, some even close to expert-level performances in high-dimensional control tasks. Both empirical and theoretical evidences are provided to justify the use of random priors for the reward function, highlighting their role in state-action space exploration and approximation of complex behaviors. Our experiments across multiple benchmarks highlight UBER's capability to learn effective and diverse behavior sets that enhance sample efficiency for online RL, outperforming contemporary baselines. By reducing the dependence on human-derived rewards, UBER paves the way for broader application of RL in real-world scenarios where reward-free data is plentiful.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Pengyi_Li1",
  "manipulated_ranking": 7,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=Dkmpa6wCIx",
  "title": "Sharpness Minimization Algorithms Do Not Only Minimize Sharpness To Achieve Better Generalization",
  "modified_abstract": "In the quest to understand the generalization capabilities of overparameterized neural networks, our work is inspired by and builds upon pivotal research in the optimization and generalization properties of neural networks, including mean-field analysis of two-layer networks, the impact of optimization techniques such as preconditioning on generalization, and exploration of feature learning through high-dimensional asymptotics and global convergence analysis in neural network optimization. This context frames our critical examination of the widely accepted notion that minimizing sharpness\u2014or flattening the loss landscape\u2014leads directly to better generalization in neural networks. Through theoretical and empirical investigation into two-layer ReLU networks, we uncover scenarios that challenge this belief: (1) where flatness and generalization align, (2) where flattest models do not generalize well, and algorithms aimed at minimizing sharpness or utilizing gradient adjustments, kernel methods, and ridgeless training strategies fail to improve generalization, and (3) counterintuitively, where non-generalizing flattest models still see generalization benefits from sharpness minimization algorithms. These findings reveal that the connection between sharpness and generalization varies with data distributions and model architectures, introducing particle dynamics and dual theories as potential factors, and indicating that mechanisms other than sharpness minimization are at play in securing generalization of overparameterized networks. This revelation prompts a broader exploration beyond sharpness, aiming to uncover new principles that govern the generalization of neural networks, including the role of regression in these dynamics.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Denny_Wu2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=LVHEcVgEGm",
  "title": "Diffusion Models and Semi-Supervised Learners Benefit Mutually with Few Labels",
  "modified_abstract": "Our work builds upon the seminal principles established in recent advancements in machine learning, particularly in enhancing long-tailed recognition through techniques such as Generalized Parametric Contrastive Learning, Parametric Contrastive Learning, Residual Learning, and Rebalanced Siamese Contrastive Mining. These foundational studies have paved the way for innovative approaches in addressing class imbalance and improving model generalization through optimization of training processes. In an effort to further advance semi-supervised generative and classification tasks, especially in computer vision, we propose a simple yet effective training strategy called *dual pseudo training* (DPT), which adaptively rebalances training focus between labeled and pseudo-labeled data. Built upon strong semi-supervised learners and diffusion models, DPT operates in three stages: training a neural network classifier on partially labeled data to predict pseudo-labels; training a conditional generative model using these pseudo-labels to generate high-frequency pseudo images; and retraining the classifier with a mix of real and pseudo images, thereby potentially mitigating the loss of fidelity in generated images while striving to rebalance the input. Empirically, DPT consistently achieves state-of-the-art (SOTA) performance of semi-supervised generation and classification across various settings. In particular, with one or two labels per class, DPT achieves a Fr\u00e9chet Inception Distance (FID) score of 3.08 or 2.52 on ImageNet $256\\times256$. Moreover, DPT outperforms competitive semi-supervised baselines substantially on ImageNet classification tasks, achieving top-1 accuracies of 59.0 (+2.8), 69.5 (+3.0), and 74.4 (+2.0) with one, two, or five labels per class, respectively. Notably, our results demonstrate that diffusion models can generate realistic high-frequency images with only a few labels (e.g., $<0.1$%) and that generative augmentation remains viable for semi-supervised classification, making it a dual approach between generative models and semi-supervised learning strategies.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jiequan_Cui1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=tW2KSph9o8",
  "title": "Ignorance is Bliss: Robust Control via Information Gating",
  "modified_abstract": "Our work builds upon critical insights from several key areas of machine learning research, including reinforcement learning's resource and power-seeking dynamics, the representation learning from raw states in robotics, and the foundational principles of imitation and representation learning. Inspired by how optimal policies seek power in Markov decision processes, as well as the methods for identifying and isolating causal task features in robotic systems and the challenges of imitation learning, we introduce *information gating* as a method for discerning and utilizing only the minimal necessary information for a given task. This approach, which we term *InfoGating*, leverages a differentiable parameterization of the signal-to-noise ratio to selectively gate information across the network. It can be applied dynamically to various levels within a model, such as erasing irrelevant pixels in input images or gating activations in intermediate layers, to focus on the most crucial information for task completion. This technique enables our models to learn parsimonious representations that improve generalization by being robust to noise and irrelevant information. Applying *InfoGating* across various domains, including multi-step forward and inverse dynamics models, Q-learning, behavior cloning, and self-supervised learning in classification and similarity tasks, demonstrates its effectiveness in discarding non-essential information, thereby enhancing the robustness and pretraining and fine-tuning capabilities of reinforcement learning models against irrelevant visual features in diverse environments. The use of embeddings enriches the informational content that is selectively conveyed for improved decision-making. Our algorithms, fundamentally designed to govern the reward mechanisms and handle queries about environment states efficiently, underscore the significance of self-supervised learning principles within *InfoGating* processes.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Rohin_Shah1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=ZdxGmJGKOo",
  "title": "SimFBO: Towards Simple, Flexible and Communication-efficient Federated Bilevel Learning",
  "modified_abstract": "Federated bilevel optimization (FBO) has shown great potential recently in machine learning and edge computing, leveraging the emerging nested optimization structure in diverse applications such as meta-learning, fine-tuning, and hyperparameter tuning. These applications underscore the growing complexity and communication demands of bilevel optimization strategies, as evidenced by previous works which highlighted challenges in achieving fast convergence, addressing the bilevel nature of hyperparameter optimization, and formulating efficient aggregation in the absence of lower-level problem convexity. In response to these challenges, this paper introduces the SimFBO framework for federated bilevel optimization, designed for simplicity, adaptability, and reduced communication overhead. SimFBO eschews the complex computational requirements typical of existing FBO algorithms by eliminating the need for sub-loops and implementing a generalized server-side aggregation and update mechanism. We also introduce System-level heterogeneity robust FBO (ShroFBO), a variant that enhances the framework's tolerance to discrepancies in local computation capabilities. Our theoretical analysis validates that SimFBO and ShroFBO achieve linear convergence, allowing for efficient partial client participation and sampling without replacement, thereby significantly reducing sample and communication complexities. Experimental validation confirms the superiority of our methods over conventional FBO algorithms, demonstrating their practical utility and effectiveness.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Shangzhi_Zeng1",
  "manipulated_ranking": 5,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=WXc8O8ghLH",
  "title": "Max-Margin Token Selection in Attention Mechanism",
  "modified_abstract": "Our study extends the dialogue on the versatility and efficacy of attention mechanisms, inspired by foundational advancements presented in works on Transformer architectures, such as ETC and OmniNet, and methodologies like Neural Structured Learning. These innovations have underscored the significance of attention in achieving state-of-the-art results in language processing, translation, and representation learning, even in the face of long and structured inputs or sequences. Reflecting on this progression, we address a critical gap in the understanding of the theoretical principles that underlie attention mechanisms, particularly focusing on its optimization dynamics. We explore the softmax-attention model $f(X)=\\langle Xv, \\texttt{softmax}(XWp)\\rangle$, proving that gradient descent on parameters $p$ or $W$ converges to a max-margin solution, thus formalizing attention as an optimal token selection mechanism for not only language but potentially visual-spatial tasks and recognition systems. Our findings offer a precise characterization of token *optimality* through value embeddings $Xv$ and problem geometry, and extend to general data applications. The encoding of sequences within this framework is particularly noteworthy as it adds another layer of significance to the attention mechanism. Moreover, the introduction of a broader regularization path analysis elucidates the margin maximizing nature of attention, even with nonlinear prediction heads. When optimizing parameters $v$ and $p$ under the logistic loss, we pin down conditions for their pathways to converge directionally to hard-margin SVM solutions in a few-shot learning context, showcasing a unique interplay between attention mechanisms and support vector geometry amidst adversarial conditions. This work not only validates the theoretical underpinnings of attention via numerical experiments but also sets the stage for future explorations into its optimizational landscape.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Philip_Pham1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=JX6UloWrmE",
  "title": "Parameterizing Non-Parametric Meta-Reinforcement Learning Tasks via Subtask Decomposition",
  "modified_abstract": "Inspired by recent advances in multi-agent reinforcement learning (MARL) and the recognition of challenges in standardizing performance evaluations as well as dataset generation for cooperative offline MARL, our work introduces a novel method to parameterize non-parametric meta-reinforcement learning (meta-RL) tasks. The selective reincarnation across heterogeneous agents in MARL and the emphasis on developing robust evaluation protocols underline the importance of adaptability, observability, benchmarking, and formalisation in reinforcement learning (RL). Against this background, our proposed Subtask Decomposition and Virtual Training (SDVT) approach aims to address generalization issues prevalent in meta-RL, particularly the challenge of adapting to non-parametric task variations. SDVT decomposes each non-parametric task into a collection of elementary subtasks and parameterizes the task based on its decomposition. Utilizing a Gaussian mixture VAE to meta-learn the decomposition process, the approach facilitates policy reuse across common subtasks, enhancing the model's generalization capabilities to previously unseen subtask arrangements with credible performance metrics. Additionally, a virtual training procedure augments the learning process by generating hypothetical subtask compositions within fully-cooperative systems. This methodology not only significantly bolsters performance on the Meta-World ML-10 and ML-45 benchmarks but also positions our approach as a pioneering stride towards overcoming the constraints of current state-of-the-art techniques in meta-RL regarding non-parametric task variability and distributed learning environments. The open-source release of our implementation aims to encourage reproducibility and credibility within the research community.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Arnu_Pretorius1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=I18BXotQ7j",
  "title": "GeoCLIP: Clip-Inspired Alignment between Locations and Images for Effective Worldwide Geo-localization",
  "modified_abstract": "Amid the burgeoning research landscape that seeks to enhance machine learning's ability to understand and interpret the world through images, text, and 3D data, our work, GeoCLIP, introduces a novel approach aimed at pushing the boundaries of worldwide geo-localization. Our method is inspired by the principles underlying recent achievements in language-driven 3D scene understanding, region-specific language-contrastive learning, knowledge distillation for efficient object detection, unsupervised domain adaptation in 3D object detection, segmentation, and sim-to-real domain adaptation for 3D indoor semantic segmentation. This lineage of contributions underscores the pivotal role of fusing multimodal data and leveraging vast, pre-trained models to surmount the challenges inherent in tasks that require nuanced understanding of complex, real-world phenomena. GeoCLIP proposes a CLIP-inspired Image-to-GPS retrieval approach that enforces alignment between the visual content of images and their corresponding GPS locations through a method that parallels supervision techniques, albeit in a less conventional fashion. Our method capitalizes on a location encoder that models the Earth as a continuous function, utilizing positional encoding through random Fourier features to construct a hierarchical, voxel-based representation. This captures information at varying resolutions, yielding a semantically rich high-dimensional feature vector\u2014benefitting from a form of latent self-training. To the best of our knowledge, this is the first work that employs GPS encoding for geo-localization. Through extensive experiments and ablations on benchmark datasets, GeoCLIP demonstrates its effectiveness, achieving competitive performance with just 20% of the required training data\u2014a testament to its efficiency in data-limited contexts. Additionally, we qualitatively showcase the capability of geo-localization via text queries, leveraging the foundational CLIP architecture of our image encoder. This project marks a significant advancement in geo-localization techniques, offering a robust and flexible method that transcends the limitations of existing approaches, with potential applications in multi-view categorization.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jihan_Yang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=VzmpXQAn6E",
  "title": "Exposing Attention Glitches with Flip-Flop Language Modeling",
  "modified_abstract": "This work draws inspiration from a constellation of issues identified in current neural language technologies, ranging from problems of consistency in recurrent language models to the logical deficiencies and replication of illogical patterns in neural dialogue models, as explored in prior research. These issues collectively highlight the fragility and limits of neural models' understanding and reasoning capabilities, thus framing our investigation into the phenomenon of _attention glitches_. Such glitches indicate instances where the Transformer architecture's inductive biases intermittently fail to capture robust reasoning, a fundamental problem not yet fully understood in the context of large language models (LLMs). To better understand and isolate this problem, we introduce _flip-flop language modeling_ (FFLM), a novel approach designed to probe the extrapolative behavior of trained neural language models directly through decoding and text generation mechanisms. Through this parametric family of synthetic benchmarks, which necessitates a model's ability to copy binary symbols over multiple generations over long-range dependencies while ignoring intermediate tokens, we uncover that Transformers exhibit a long tail of sporadic reasoning errors in generation. While some errors can be mitigated through existing regularization techniques, our analyses suggest that addressing the entirety of the problem, potentially responsible for closed-domain hallucinations observed in natural LLMs during text generation, poses a significant challenge. This investigation into attention glitches, therefore, contributes to our broader understanding of the inherent limitations of current LLMs and opens new avenues for developing more reliable models in the future.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ilia_Kulikov1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=WqiZJGNkjn",
  "title": "MotionGPT: Human Motion as a Foreign Language",
  "modified_abstract": "As the exploration of pre-trained large language models progresses, the integration of language with other modalities, such as motion, remains a frontier. This work is inspired by developments in areas such as symbolic music understanding, template-based lyric-to-melody generation, latent diffusion models for zero-shot speech and singing synthesis, and large-scale medical dialogue dataset construction. The similarity in methodological advancements, such as the encoding of music and speech into structured formats for deep learning, mirrors the underpinnings of our exploration into human motion. We propose MotionGPT, a unified model that leverages language data and large-scale motion models for enhancing motion-related tasks. By treating human motion as a specific language, using discrete vector quantization to translate 3D motion into motion tokens, we create a 'motion vocabulary'. MotionGPT is pre-trained with a mixture of motion-language data and fine-tuned on prompt-based tasks, elevating its utility across applications like text-driven motion generation, motion captioning, and prediction. Our extensive experiments validate MotionGPT's robustness and superior performance in multimodal learning, offering a novel perspective on the synthesis between audio elements and human motion representation.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zeqian_Ju1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=KoaFh16uOc",
  "title": "StyleDrop: Text-to-Image Synthesis of Any Style",
  "modified_abstract": "The evolution of text-to-image synthesis has been significantly influenced by advancements in generative models, particularly with applications spanning dynamic styling in portrait generation, improving neural image-based rendering and relighting, and compact yet detailed neural representations for various signal processing tasks in multidimensional subspaces. Inspired by the themes of dynamic styling through Generative Adversarial Networks (GANs), anisotropic feature mapping for improved neural rendering, and compact neural signal processing presented in related works, we introduce *StyleDrop*. This method enables the synthesis of images in any desired style with high fidelity to user-specified details such as color schemes, shading, and design patterns, utilizing text-to-image models. By learning new styles through fine-tuning less than 1% of total model parameters and incorporating iterative training with feedback from perceptrons in targeted subspaces, StyleDrop achieves a notable level of efficiency and versatility in image completion and style synthesis that captures both local and global stylistic effects based on a single provided image, possibly adopting regression techniques for more predictable results. In our extensive comparison, StyleDrop demonstrated superior performance in style tuning over other methods like DreamBooth and textual inversion applied to models like Imagen or Stable Diffusion. Results showcasing this method's capabilities, relevant processing techniques, and applications are available on our project website, ensuring that the website link remains omitted for neutrality in presentation.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Anpei_Chen2",
  "manipulated_ranking": 15,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=TNAGFUcSP7",
  "title": "Learning Rate Free Sampling in Constrained Domains",
  "modified_abstract": "Our work is inspired by the themes of scalability, Bayesian inference, and efficient learning in constrained domains as explored in the realm of Gaussian Processes, Bayesian deep learning, and score-based generative models. We introduce a suite of new particle-based algorithms for sampling in constrained domains that are entirely learning rate free. Our approach employs coin betting ideas from convex optimization and views constrained sampling as a mirrored optimization problem on the space of probability measures. Additionally, we provide a unifying framework for several existing constrained sampling algorithms, such as mirrored Langevin dynamics and mirrored Stein variational gradient descent. Our algorithms are tested across a variety of numerical examples including sampling from targets on the simplex, sampling with fairness constraints, and constrained sampling problems in post-selection inference. The results of our tests indicate that our algorithms achieve competitive performance with current constrained sampling methods, without the necessity to fine-tune any hyperparameters, thereby addressing critical challenges in model selection, optimization, and inference in the specific context of constrained domains.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Simone_Rossi1",
  "manipulated_ranking": 5,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=XEBzQP3e7B",
  "title": "GAIA: Delving into Gradient-based Attribution Abnormality for Out-of-distribution Detection",
  "modified_abstract": "Amidst evolving machine learning paradigms, this study is inspired by the seminal concepts emerging from recent investigations into concept-based explanations for out-of-distribution (OOD) detectors, advanced adversarial robustness techniques against deepfakes, and stateful defenses against black-box adversarial examples. These foundational works collectively underscore the necessity for innovative mechanisms to enhance the reliability and safety of deep neural networks in real-world settings. In this vein, our paper introduces GAIA, an approach that quantifies the disparities between in-distribution (ID) and OOD data by analyzing the uncertainty in model explanations, specifically through gradient-based attribution methods. We identify unique challenges in attributing feature importance to OOD examples, which manifest as divergent explanation patterns. By exploring these phenomena, we propose the zero-deflation and channel-wise average abnormalities as novel indicators for OOD detection. GAIA's methodology, which employs Gradient Abnormality Inspection and Aggregation, marks a significant departure from traditional post-hoc methods and reconfigured classifiers, showcasing a remarkable improvement in detecting OOD instances on both standard and large-scale benchmarks. Our findings, benchmarked on CIFAR and ImageNet-1k datasets, illustrate GAIA's efficacy in reducing the average false positive rate significantly, thus contributing a novel perspective and robust toolset for addressing the critical problem of OOD detection in machine learning applications. The integration of ensembles, which can be viewed as a practical form of partitioning the problem space, further enhances GAIA's capability to discern between ID and OOD instances, offering a fortified layer of defense against various attack vectors.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ryan_Feng1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=zyhxRc9bew",
  "title": "What is Flagged in Uncertainty Quantification?  Latent Density Models for Uncertainty Categorization",
  "modified_abstract": "This research situates itself at the intersection of recent breakthroughs in uncertainty quantification (UQ) within machine learning and pioneering work in random matrix theory and its applications across various disciplines, including supervised and unsupervised learning, covariance matrix estimation, and multi-task learning. Leveraging insights from the sophisticated analytical frameworks and methodologies developed in these areas, such as random matrix-improved low-rank estimation techniques, eigenvalue analysis, and the balance between supervised and unsupervised learning under the low density separation assumption, we propose an innovative framework for categorizing uncertain examples flagged by UQ methods. Through the introduction of the confusion density matrix\u2014a kernel-based approximation of the misclassification density, which benefits from smooth functions to approximate the density landscapes asymptotically\u2014we categorize suspicious examples identified by uncertainty methods into three classes: out-of-distribution (OOD) examples, boundary (Bnd) examples, and examples in regions of high in-distribution misclassification (IDM). Our framework uniquely integrates least-square adjustment techniques within the categorization process to better define the boundaries and distributions of each category. Extensive experiments validate our framework and underscore its potential as a valuable assessment benchmark for evaluating different uncertainty quantification methods, offering a new and distinct perspective for assessing their relative merits and implications for classification tasks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Malik_Tiomoko1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=KKxO6wwx8p",
  "title": "SE(3) Equivariant Augmented Coupling Flows",
  "modified_abstract": "Inspired by recent groundbreaking work in normalizing flows on manifolds, equivariant manifold flows, and refined generative modeling techniques such as reflected diffusion models and differentiating through the Fr\u00e9chet mean, this paper extends the existing framework of probabilistic modeling to physical systems with specific focus on the SE(3) equivariance and permutation invariance required for accurate modeling of molecular and atomic systems. Coupling normalizing flows allow for fast sampling and density evaluation, making them the tool of choice for probabilistic modeling of physical systems. However, the standard coupling architecture precludes endowing flows that operate on the Cartesian coordinates of atoms with the SE(3) and permutation invariances of physical systems. This work proposes a coupling flow that preserves SE(3) and permutation equivariance by performing coordinate splits along additional augmented dimensions. At each layer, the flow maps atoms' positions into learned SE(3) invariant bases, where we apply standard flow transformations, such as monotonic rational-quadratic splines, before returning to the original basis. Crucially, our flow preserves fast sampling and density evaluation, and may be used to produce unbiased estimates of expectations with respect to the target distribution via importance sampling. When trained on the DW4, LJ13, and QM9-positional datasets, our flow is competitive with equivariant continuous normalizing flows and diffusion models, while allowing sampling more than an order of magnitude faster. Moreover, to the best of our knowledge, we are the first to learn the full Boltzmann distribution of alanine dipeptide by only modeling the Cartesian positions of its atoms. Lastly, we demonstrate that our flow can be trained to approximately sample from the Boltzmann distribution of the DW4 and LJ13 particle systems using only their energy functions.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Aaron_Lou1",
  "manipulated_ranking": 7,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=RBI4oAbdpm",
  "title": "Neural Combinatorial Optimization with Heavy Decoder: Toward Large Scale Generalization",
  "modified_abstract": "This work is informed by a wide array of groundbreaking research in machine learning, encompassing meta-learning, graph convolutional networks, nonconvex minimax optimization, adversarially robust domain adaptation, and distributionally robust optimization. These studies collectively highlight the evolving complexity and scalability challenges within diverse ML (machine learning) domains, from enhancing meta-model generalization to addressing adversarial robustness and domain adaptation. Inspired by these themes, we tackle a fundamental bottleneck in neural combinatorial optimization (NCO): the difficulty of solving large-scale instance problems without resorting to expert-designed algorithms. We introduce a novel Light Encoder and Heavy Decoder (LEHD) model that exhibits a strong ability to generalize across problem scales by learning dynamic relationships between nodes of varying sizes, partly due to provably efficient meta-training strategies. Our data-efficient training scheme and the flexible solution construction mechanism enable the LEHD model to perform near-optimally on classical problems like the Travelling Salesman Problem (TSP) and the Capacitated Vehicle Routing Problem (CVRP) with instance sizes of up to 1000 nodes. Moreover, the model demonstrates an impressive capacity to tackle real-world problems evidenced by its performance on TSPLib and CVRPLib datasets, thereby setting a new benchmark for constructive machine learning approaches in neural combinatorial optimization.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Mehrdad_Mahdavi2",
  "manipulated_ranking": 5,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=A954O4tDmU",
  "title": "AGD: an Auto-switchable Optimizer using Stepwise Gradient Difference for Preconditioning Matrix",
  "modified_abstract": "In the evolving landscape of deep learning optimization, this paper introduces the AGD optimizer, a novel approach that addresses the challenges of preconditioning matrix design by leveraging the inspirations from recent advancements in adaptive optimization techniques and hyperparameter optimization frameworks. Adaptive optimizers, such as Adam, have achieved remarkable success in network training for deep learning. A key component of these optimizers is the so-called preconditioning matrix, providing enhanced gradient information and regulating the step size of each gradient direction. The proposed approach utilizes the gradient difference between two successive steps as the diagonal elements of the preconditioning matrix, which are closely related to the Hessian and can be perceived as an approximation of the inner product between the Hessian row vectors and the difference of the adjacent parameter vectors. Moreover, we introduce an auto-switching function that enables the preconditioning matrix to dynamically switch between Stochastic Gradient Descent (SGD) and the adaptive optimizer, elegantly addressing error propagation issues through diligent approximation. This capability echoes the adaptability seen in recent developments such as Amortized Proximal Optimization and Delta-STN, which demonstrate the significance of optimizing the optimization process itself, whether through meta-learning or efficient bilevel optimization techniques. AGD enhances the generalization performance across multiple domains, including Natural Language Processing (NLP), Computer Vision (CV), and Recommendation Systems (RecSys), potentially improving upon traditionally difficult tasks such as regression in these areas. Our experimental results showcase AGD\u2019s superiority over state-of-the-art (SOTA) optimizers, achieving highly competitive or significantly better predictive performance. Additionally, we delve into AGD\u2019s ability to automatically switch between SGD and adaptive optimization, unfolding its impacts across different scenarios.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Juhan_Bae2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=cRGINXQWem",
  "title": "Precise asymptotic generalization for multiclass classification with overparameterized linear models",
  "modified_abstract": "Our investigation draws from recent investigations in robust optimization, adversarial risk analysis, and active learning strategies, reflecting a broader trend towards understanding the intricate dynamics of overparameterized models in machine learning. By engaging with the conjectures presented in the realm of overparameterized linear models for multiclass classification, particularly under conditions introduced by Subramanian et al. at NeurIPS'22, our work extends the dialogue into asymptotic generalization with a specific focus on Gaussian covariates in a bi-level model setup where data points, features, and classes scale concurrently. We confirm the regimes Subramanian et al. outlined, substantiating the conditions under which these models generalize or fail to do so. Our findings include establishing new lower bounds that act similarly to an information-theoretic strong converse, asserting that the misclassification rate asymptotically approaches either 0 or 1. Notably, our analysis reveals that the min-norm interpolating classifier, while optimal in certain regressor scenarios, can be asymptotically suboptimal in multiclass classification, a result that contrasts with intuition derived from simpler model frameworks. The deployment of a novel variant of the Hanson-Wright inequality, aimed specifically at tackling multiclass problems marked by sparse labels, and the enhancement of classifiers through robust estimators, represents a key methodological contribution. This analytical tool not only fortifies our primary investigation through estimators and classifiers but also proves versatile enough to be applied in examining multi-label classification issues within similar statistical ensembles, pointing towards a broader applicability of our theoretical advancements in robust theory and understanding model behavior across a range of complex machine learning tasks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Muni_Sreenivas_Pydi1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=tLrkjK128n",
  "title": "Optimistic Active Exploration of Dynamical Systems",
  "modified_abstract": "Inspired by recent advances in reinforcement learning (RL) that focus on data efficiency, model generalization, and adapting to novel scenarios, this paper introduces an innovative algorithm, OPAX, for active exploration in unknown dynamical systems with the aim of enabling zero-shot generalization across multiple downstream tasks. Building on foundational concepts from studies that have utilized graph structures for improved value estimation and rewards prediction in data-efficient RL, and extensive surveys as well as practical approaches to zero-shot generalization and domain generalization in model-based offline and off-policy RL, our work leverages well-calibrated probabilistic models to navigate the epistemic uncertainty inherent in exploring dynamical systems. By adopting an optimistic stance towards plausible dynamics, OPAX significantly enhances information gain between unknown system dynamics and observed states through a novel method that frames the exploration challenge as an optimizable control problem solvable in each episode with conventional techniques. This approach also facilitates adaptation to novel scenarios by automatically adjusting to the dynamics potentially inferred from demonstrators' data in a multi-demonstrator environment, further enriching the training data without direct interaction. We substantiate our theoretical contributions with a comprehensive analysis within general model frameworks, providing a sample complexity bound for Gaussian process dynamics that highlights convergence of epistemic uncertainty to zero. Our empirical validations, demonstrated across various benchmarks, confirm OPAX's superiority not only in theoretical soundness but also in practical efficacy for zero-shot planning on novel tasks, in comparison to existing heuristic active exploration methods.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Robert_Kirk1",
  "manipulated_ranking": 4,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=DPeBX79eNz",
  "title": "Transfer Learning with Affine Model Transformation",
  "modified_abstract": "Supervised transfer learning has received considerable attention due to its potential to boost the predictive power of machine learning in scenarios where data are scarce. Generally, a given set of source models and a dataset from a target domain, possibly including unlabeled or partially labeled (partial-set) instances, are used to adapt the pre-trained models to a target domain by statistically learning domain shift and domain-specific factors. Inspired by recent developments in source-free domain adaptation and the concept of universal domain adaptation to address domain and category shifts with or without access to source data, this paper introduces a comprehensive approach known as affine model transfer. This method follows the principle of expected-square loss minimization and leverages clustering techniques to further understand neighborhood (neighbors) relationships within the data, a process that is shown to broadly encompass various existing methods, including those based on neural feature extractors. Furthermore, this paper clarifies theoretical properties of the affine model transfer, such as generalization error and excess risk, and emphasizes its validation through rigorous benchmarks. Through several case studies, we demonstrate the practical benefits of modeling and estimating inter-domain commonality and domain-specific factors separately with the affine-type transfer models. Our approach leverages insights from the latest work in source-free domain adaptation, open-set recognition under domain shifts, generalized source-free domain adaptation, and extends these foundations to include partial-set scenarios to address both the practical and theoretical challenges in transferring learning across domains efficiently and effectively.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Shiqi_Yang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=BryMFPQ4L6",
  "title": "Augmenting Language Models with Long-Term Memory",
  "modified_abstract": "This research builds on foundational techniques in transforming language models and speech recognition systems through pretraining strategies, pseudo-labeling, and novel architectural innovations, addressing a critical gap: the limitation of existing large language models (LLMs) in harnessing rich long-context information due to their input length constraints. To surmount this, we introduce Language Models Augmented with Long-Term Memory (LongMem), a framework designed to enable LLMs to memorize extensive history. Our approach, inspired by advancements in transformer architectures and semi-supervised learning procedures, employs a novel decoupled network architecture. This architecture keeps the backbone LLM intact as a memory encoder, while an adaptive residual side-network functions as a memory retriever and reader, facilitating the caching and updating of long-term contexts without memory staleness. Enhanced through memory-augmented adaptation training, recently developed regularization techniques, and under varying levels of supervision, LongMem can preserve vast past contexts, leveraging long-term memory for language modeling across potentially unlimited-length contexts. This method significantly extends the long-form memory capacity up to 65k tokens, thereby enriching in-context learning with many-shot demonstration examples. Evaluation on the ChapterBreak, a long-context modeling benchmark, and in memory-augmented in-context learning scenarios, which includes components of fine-tuning strategies and pseudo-label techniques, shows that our model surpasses existing long-context models in several applications, demonstrating the efficacy of augmenting language models with long-term memory capabilities.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Tatiana_Likhomanenko1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=niHkj9ixUZ",
  "title": "Beyond Pretrained Features: Noisy Image Modeling Provides Adversarial Defense",
  "modified_abstract": "Inspired by recent initiatives in enhancing adversarial robustness through part-based models, curriculum-based adversarial training, and realistic adversarial patch benchmarking, our research introduces a novel adversarial defense mechanism leveraging the strengths of noisy image modeling (NIM). Recognizing the vulnerabilities of masked image modeling (MIM) pretrained models to adversarial attacks, this study explores how a variant of MIM, with denoising as a pretext task, can fortify downstream classifiers against such attacks. Our proposed method, De^3, harnesses the denoising capability of the pretrained decoder within NIM, surpassing the conventional usage of MIM for feature extraction. Networks undergoing our procedure benefit significantly, especially in applications requiring high degrees of segmentation accuracy amidst adversarial conditions. We further refine this via a strategic adjustment of the noise scale hyperparameter, drawing from random distributions to balance accuracy and robustness adeptly, akin to the function of a softmax layer providing a probabilistic understanding of model predictions. Empirical validation demonstrates NIM's superior denoising efficacy, hence its enhanced adversarial robustness, compared to its MIM counterparts. Remarkably, our approach not only matches the adversarial training benchmarks in terms of defense performance but also introduces a novel aspect of tunability between model accuracy and robustness, making a compelling case for NIM's applicability in adversarial scenarios, including those in autonomous systems where object detection is critical.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Chawin_Sitawarin1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=No52399wXA",
  "title": "IPMix: Label-Preserving Data Augmentation Method for Training Robust Classifiers",
  "modified_abstract": "Inspired by recent breakthroughs in semi-supervised learning, particularly the introduction of debiased self-training methods that enhance classifier performance in noisy and sparse label environments, and the innovative strides in enhancing data efficiency for both classification and regression in deep learning through novel modeling techniques, this research introduces IPMix. IPMix represents a convergence of these advancements into the domain of data augmentation to address a critical issue in training high-accuracy convolutional neural network classifiers: the balance between accuracy on clean data and robustness against data distribution shifts. Data augmentation has been an effective strategy to prevent overfitting; however, maintaining robustness under real-world scenarios where data distribution frequently shifts remains a formidable challenge. IPMix proposes a novel approach by integrating three levels of data augmentation\u2014image-level, patch-level, and pixel-level\u2014into a coherent, label-preserving technique. This strategy not only increases the diversity of training data but also introduces structural complexity at each level to generate more diverse images through the generation of attention-driven transformations, leveraging a random mixing method for multi-scale information fusion, akin to a game where the minimax principle can be observed in maintaining a balance between divergent objectives. Such an approach is poised to enhance robustness without compromising clean accuracy, a trade-off often highlighted in existing research through the lens of semi-supervised learning, self-training methodologies, and attention mechanisms in deep learning architectures. Our comprehensive experiments on benchmarks such as CIFAR-C and ImageNet-C demonstrate that IPMix excels in improving robustness to data corruption, outperforming state-of-the-art methods in this space. Furthermore, IPMix significantly enhances other safety measures, including robustness to adversarial perturbations, calibration, prediction consistency, and anomaly detection, with state-of-the-art or comparable results on several benchmarks including ImageNet-R, ImageNet-A, and ImageNet-O.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ximei_Wang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=bTidcHIK2t",
  "title": "Sample-Efficient and Safe Deep Reinforcement Learning via Reset Deep Ensemble Agents",
  "modified_abstract": "Deep reinforcement learning (RL) methodologies, historically bolstered by advances in deep exploration, data efficiency, uncertainty quantification, and posterior predictive distribution evaluations, face inherent challenges related to primacy bias, which results in an over-reliance on early experiences. Inspired by preceding work on advanced RL strategies such as Langevin DQN, techniques for enhancing data efficiency, Bayesian approaches to evaluating posterior distributions, and the application of epistemic neural networks for approximating Thompson sampling, this paper introduces a novel reset-based method in the RL domain that integrates deep ensemble learning. Our approach is formulated to counteract primacy bias and bolster sample efficiency without compromising safety\u2014a vital consideration in real-world applications. By strategically resetting portions of deep RL agents while preserving their replay buffers and employing ensemble learning to mitigate performance dips, we aim to ensure enhanced sample efficiency and safety simultaneously. Moreover, the complexity of this task prompts a detailed discussion on the information theoretic and network complexity implications of our method. Validation through extensive experiments within safe RL domains illustrates the proposed method's potential, affirming its competence in addressing critical challenges associated with conventional deep RL practices. The introduction of distribution approximations in both ensemble methods and the reset mechanism underscores the significance of our contributions in the broader context of advancing deep RL, enhancing our understanding of posterior predictive distributions.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Vikranth_Dwaracherla1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=6SRE9GZ9s6",
  "title": "Preference-grounded Token-level Guidance for Language Model Fine-tuning",
  "modified_abstract": "The endeavor to align language models (LMs) with preferences at the sequence level, while addressing the granularity mismatch between these preferences and token-level LM training, is inspired by prior contributions in sequence transduction, semantic parsing, and data synthesis for semantic parsing. These works have laid the groundwork in understanding how neural models can generalize systematically beyond trained distributions, incorporate structured reordering for latent alignments, and leverage pre-training for compositional generalization. Our investigation introduces an alternate training process to bridge this granularity mismatch by grounding sequence-level preferences into token-level training guidance. We introduce a framework that extends pairwise-preference learning to accommodate variable-length LM generation and exploit preference among multiple generations, with insights gained from analyzing relevant parsing datasets. Furthermore, our method integrates ideas from seq2seq (sequence-to-sequence) models in programming the models for nuanced learning objectives towards better generalization. Based on the volume of supervised data, we propose two minimalist learning objectives that incorporate the learned guidance. Experimentation on discrete-prompt generation and text summarization tasks demonstrates competitive performance, showcasing our method's potential in enhancing LM fine-tuning with preference-grounded token-level guidance.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~bailin_wang1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=BHxsP5fSHv",
  "title": "OKRidge: Scalable Optimal k-Sparse Ridge Regression",
  "modified_abstract": "This study extends the current trajectory of optimization in machine learning, influenced by recent breakthroughs in algorithmic efficiency and theoretical foundations, such as fast converging greedy algorithms, intricate line-search strategies for stochastic gradient descent, including the effective management of gradients, and improved analyses of coordinate updates in constrained settings under specific conditions. In particular, we address an essential problem in scientific discovery, identifying sparse governing equations for nonlinear dynamical systems through sparse ridge regression to provable optimality. To achieve this, we introduce OKRidge, a scalable algorithm that incorporates a novel lower bound calculation rooted in a saddle point formulation and leverages the efficiency of gradient methods everywhere throughout its architecture. Depending on the computational context, this involves either solving a linear system or applying an ADMM-based approach, where proximal operators, akin to those used in Lasso regression for promoting sparsity, are efficiently computed by resolving a linear system alongside an isotonic regression problem, effectively addressing challenges related to the 1-norm. Additionally, we employ a beam search technique to warm-start our solver effectively, optimizing the computational pathway for both regression and, potentially, classification tasks in large datasets. Our experimental findings underscore OKRidge's superiority in achieving provable optimality at significantly lower computational costs compared to existing Mixed Integer Programming (MIP) solutions, including those solved with commercial solvers like Gurobi. By harnessing insights from works focusing on optimization under constraints, interpolation theories in learning, and recently developed techniques for enhancing the efficiency of gradient methods, OKRidge sets a new benchmark in solving sparse ridge regression problems at scale, thereby contributing to the advancement of methods for identifying key variables that govern complex systems dynamics.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Aaron_Mishkin1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=NWrN6cMG2x",
  "title": "Moment Matching Denoising Gibbs Sampling",
  "modified_abstract": "Inspired by recent algorithmic innovations in matrix completion, tensor completion, and robust principal component analysis, our work introduces a novel framework for addressing the challenges in training and sampling from Energy-Based Models (EBMs). Just as these preceding works have demonstrated efficiency and robustness in handling high-dimensional data and outliers in their respective fields, our contribution focuses on the improvement and consistency in EBM training methods amidst nonconvex optimization challenges. Furthermore, completion tasks in both matrix and tensor domains directly inform the development of our approach. Energy-Based Models offer a versatile framework for modeling complex data distributions. However, training and sampling from EBMs continue to pose significant challenges. The widely-used Denoising Score Matching (DSM) method for scalable EBM training suffers from inconsistencies, causing the energy model to learn a noisy data distribution. In this work, we propose an efficient sampling framework: (pseudo)-Gibbs sampling with moment matching, which enables effective sampling from the underlying clean model when given a noisy model that has been well-trained via DSM. We explore the benefits of our approach compared to related methods through rigorous experiments and demonstrate how to scale the method to high-dimensional datasets employing algorithms designed for low-rank approximation and matrix/tensor completion.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~HanQin_Cai1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=wYKU1C77sa",
  "title": "Language-driven Scene Synthesis using Multi-conditional Diffusion Model",
  "modified_abstract": "The quest for advanced scene synthesis has been notably influenced by pioneering research employing multi-modal inputs such as text, images, and other sensory data to enhance generation tasks. These foundational works, ranging from unified discrete diffusion models for vision-language generation to novel approaches in text-conditional image generation and depth-aided panorama outpainting, underpin the necessity for integrating various modalities to achieve higher quality and contextually accurate synthetic outputs. Inspired by the themes espoused in these prior works, this paper introduces a language-driven scene synthesis task, marking a novel intersection of text prompts, human motion, and object presence for comprehensive scene construction. Our proposed multi-conditional diffusion model, leveraging pre-trained networks for pivotal language and vision integration, stands distinct from traditional implicit unification approaches by overtly predicting guiding points for the original data distribution, which is theoretically grounded and experimentally validated across diverse datasets. This approach not only contributes to the domain of scene synthesis by allowing natural scene editing applications but also represents a critical step forward in multi-modal learning, leveraging the complex interplay of language, attention mechanisms, external data fusion, and object data to generate coherent and contextually rich scenes. The text-to-image conversion ability of our model particularly demonstrates the powerful fusion of linguistic and visual elements, providing a tangible stride in artificial intelligence capabilities for natural scene composition. Through the emphasis on rotations in object and motion representation, we further refine our synthesis process, ensuring dynamic and perceptually realistic scenes.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Tat-Jen_Cham1",
  "manipulated_ranking": 4,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=HoBbZ1vPAh",
  "title": "Ensemble-based Deep Reinforcement Learning for Vehicle Routing Problems under Distribution Shift",
  "modified_abstract": "The dynamic and ever-evolving nature of vehicle routing problems (VRPs) has been a prominent challenge addressed by numerous neural network methodologies, particularly in navigating the intricacies of traffic optimization and the accurate approximation of traffic simulation outcomes through both simulation techniques and neural predictions. Building on prior investigations into the performance of neural networks and gradient boosting models in traffic optimization, and the innovative use of BERT for predicting traffic signal timings, this work introduces an ensemble-based deep reinforcement learning method tailored for VRPs under distribution shift conditions. We propose a novel strategy that employs a group of diverse sub-policies, each designed to effectively adapt to varying instance distributions. This diversity is achieved through Bootstrap methods with random initialization and is further reinforced by incorporating regularization terms during training to prevent the convergence of parameters and ensure sub-policy inequality. Moreover, the boost in performance and flexibility is attributed to the sequential boosting technique, which iteratively refines the learning process. Our experimental evaluation, conducted on both randomly generated instances and benchmark instances from TSPLib and CVRPLib, confirms our method's superiority over state-of-the-art neural baselines in terms of performance and generalizability, thereby validating the efficacy of our ensemble-based approach and its constituent designs, including the application of genetic algorithms for route optimization.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Pawe\u0142_Gora2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=GgdFLb94Ld",
  "title": "CoDrug: Conformal Drug Property Prediction with Density Estimation under Covariate Shift",
  "modified_abstract": "In the landscape of computational models for drug discovery, where previous efforts have significantly leveraged machine learning techniques, including advancements in Bayesian inference and the exploration of deep learning for domain-specific predictive analytics, our work introduces a novel perspective. Inspired by recent progress in transformer-based neural Bayesian inference, meta-learning for efficient Bayesian optimization, and the exploration of transfer learning in hyperparameter optimization, we identify a critical gap in the application of these advanced methodologies to the field of drug discovery. Specifically, we address the challenge of predicting pharmaceutical properties under the covariate shift, a prevalent issue due to the vast and scarcely labeled chemical space in drug discovery tasks. Our method, CoDrug, combines conformal prediction (CP) with an energy-based model and Kernel Density Estimation (KDE) to not only generate prediction sets with a coverage guarantee but also adjust for distribution shifts by leveraging both labeled and unlabeled data. This allows for more reliable uncertainty estimates through posteriors and improving our machine learning model's neural-based ability to jointly learn from density and distribution information. Through experiments on small-molecule discovery tasks that involve realistic distribution drifts, CoDrug significantly outperforms traditional CP approaches by reducing the coverage gap by over 35%, showcasing its potential to enhance the predictive frameworks within the de novo drug design models. Enhanced by joint learning techniques, CoDrug demonstrates its robustness in learning under the complex distribution shifts encountered in drug discovery.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sebastian_Pineda_Arango1",
  "manipulated_ranking": 16,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=KTRwpWCMsC",
  "title": "Conformal Prediction for Time Series with Modern Hopfield Networks",
  "modified_abstract": "In the context of recent developments that address the complexity of time series data through technologies like k-ARs for large-scale clustering, dynamic network reconstruction from heterogeneous datasets, and comparative methods in network analysis via frequency domain persistent homology, our research introduces HopCPT. This novel conformal prediction method for time series, HopCPT, is tailored to address the highly autocorrelative structure of time series data, which traditionally challenges conformal prediction paradigms. By integrating advancements in understanding time series clustering, network stability, and comparison of complex networks through sparse and systematic reconstructions, we establish a theoretical foundation and methodological advancements for conformal prediction in temporally structured data. Our approach takes into consideration temporal dependencies and leverages them to enhance prediction accuracy, as opposed to treating them as a violation of foundational assumptions. Our experimental validation across diverse real-world datasets underscores the superiority of HopCPT over existing state-of-the-art methods, demonstrating its effectiveness in quantifying uncertainty within the domain-specific challenges of time series data. Moreover, our formulation explicitly accounts for perturbations in time series systems, further enriching the prediction model's robustness and reliability. Additionally, we implemented comprehensive benchmarking procedures to validate the performance of HopCPT against traditional and contemporary approaches, showing how our topology-aware generation strategies effectively manage signals and temporal patterns.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zuogong_Yue1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=BqZ70BEtuW",
  "title": "SANFlow: Semantic-Aware Normalizing Flow for Anomaly Detection",
  "modified_abstract": "Inspired by prior explorations into outlier detection and normalization strategies within machine learning frameworks such as Energy-Based Autoencoders, Normalized Autoencoders, as well as techniques addressing Out-of-Distribution (OOD) detection through adversarial approaches and evaluation via generative methods, this work presents SANFlow. SANFlow embraces a novel perspective on visual anomaly detection by recognizing the limitations intrinsic to the application of normalizing flow (NF) models that homogenize diverse feature distributions into a singular format, overlooking the semantic variance inherent among features. To counteract this oversight and enhance anomaly detection capabilities, SANFlow proposes a paradigm wherein NF models are tailored to differentiate and map the distributions of features within an image to multiple, semantically congruent distributions, rather than enforcing a uniform distribution model across all features. This methodology, employing a meticulous sampling strategy and leveraging tasks for manifold learning, enables the distinction between normal and abnormal data distributions through differential variance for normal features and deviant means for synthesized abnormal data, facilitated by data augmentation. The efficacy of SANFlow is demonstrated through systematic experimentation and benchmarking, yielding advancements in the precision of density estimation and anomaly detection. Furthermore, by introducing a retrieval process that isolates outlier data points based on their deviation from the semantically appropriate distributions, the research contributes to the broader discourse on anomaly detection by leveraging semantic distinctions within data, thereby augmenting the discriminative power of NF models in identifying anomalies.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sangwoong_Yoon1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=tP50lLiZIo",
  "title": "Non-Stationary Bandits with Auto-Regressive Temporal Dependency",
  "modified_abstract": "The evolution of experimental design and analysis methodologies in online platforms, content marketplaces, and dynamical systems has exposed the crucial impact of temporal dynamics and interference on the robustness of experimental results. Insights gained from exploring synthetically controlled bandits, correcting for interference in two-sided content marketplaces like Douyin, and examining Markovian interference phenomena, have underscored the significance of accounting for temporal dependencies and interferences in shaping the landscape of experimental analysis within various domains. Building on these foundational insights, this paper introduces a novel non-stationary multi-armed bandit (MAB) framework that adeptly captures the temporal structure of real-world dynamics through an auto-regressive (AR) reward structure. Our contribution lies in proposing an algorithm that synergizes an alternation mechanism for leveraging temporal dependencies with a restarting mechanism for discarding outdated information, thus dynamically balancing exploration and exploitation in online contexts. Empirical validation, including a real-world case study on tourism demand prediction conducted with a simulator, showcases the potential of our framework to address complex, evolving time series challenges. Theoretical inference is utilized to develop estimators and heuristics that attain a regret upper bound closely aligned with a robust dynamic benchmark, illuminating a path for future research in non-stationary environments characterized by marketplaces and interference.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Tianyi_Peng1",
  "manipulated_ranking": 4,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=DP2lioYIYl",
  "title": "A Theory of Unsupervised Translation Motivated by Understanding Animal Communication",
  "modified_abstract": "This work is inspired by recent innovations in neural networks, particularly in unsupervised learning and the understanding of complex systems, as evidenced by the explored themes in systematic generalization in transformers, the role of language in cognitive systems, and emergent properties of in-context learning in large models. Neural networks are capable of translating between languages\u2014in some cases even between two languages where there is little or no access to parallel translations, in what is known as Unsupervised Machine Translation (UMT). Given this progress, it is intriguing to ask whether machine learning tools can ultimately enable understanding animal communication, particularly that of highly intelligent animals. We propose a theoretical framework for analyzing UMT when no parallel translations are available and when it cannot be assumed that the source and target corpora address related subject domains or possess similar linguistic structure. Specifically, our discussion includes the role of attention mechanisms in transformers and their significance in mimicking the causal links found in natural languages, possibly shedding light on query-based learning in animal communication systems. We exemplify this theory with two stylized models of language, for which our framework provides bounds on necessary sample complexity; the bounds are formally proven through experiments and experimentally verified on synthetic data. These bounds show that the error rates are inversely related to the language complexity and amount of common ground, a factor critical for large-scale neural network models and indicative of satisfaction-based learning models. This suggests that unsupervised translation of animal communication may be feasible if the communication system is sufficiently complex and recurrent attention mechanisms are employed to mimic natural language processes.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~James_McClelland1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=KTfAtro6vP",
  "title": "Reinforcement Learning with Fast and Forgetful Memory",
  "modified_abstract": "Nearly all real world tasks are inherently partially observable, necessitating the use of memory in Reinforcement Learning (RL). Most model-free approaches summarize the trajectory into a latent Markov state using memory models borrowed from Supervised Learning (SL), even though RL tends to exhibit different training and efficiency characteristics. Inspired by a rich body of works that tackle various challenges in RL\u2014from the sensitivity of RL agents to training conditions, the use of world models for continual learning, to exploring human-like adaptation in open-ended task spaces and innovative methods like synthetic experience replay for enhancing training data\u2014our study introduces Fast and Forgetful Memory, an algorithm-agnostic memory model designed specifically for RL. Addressing the noted discrepancy, our approach constrains the model search space via strong structural priors inspired by computational psychology, thereby enhancing the adaptation capabilities and facilitating exploration in complex environments. It is a drop-in replacement for recurrent neural networks (RNNs) in recurrent RL algorithms, achieving greater reward than RNNs across various recurrent benchmarks and algorithms without changing any hyperparameters. Moreover, Fast and Forgetful Memory exhibits training speeds two orders of magnitude faster than RNNs, attributed to its logarithmic time and linear space complexity, effectively solving the common bottlenecks in training speed and scalability. Our implementation is deep into details and available at a publicly accessible repository.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jack_Parker-Holder1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=xFtuNq23D5",
  "title": "Boosting Spectral Clustering on Incomplete Data via Kernel Correction and Affinity Learning",
  "modified_abstract": "Leveraging insights from the development of generalized models for estimating structure in Markov networks, causal discovery in the presence of missing data, and advances in nonlinear independent component analysis (ICA), our research addresses the challenge of spectral clustering with incomplete data. Recognizing the importance of high-quality affinity measures for clustering, particularly when data incompleteness can significantly degrade performance, we propose an imputation-free framework to enhance spectral clustering. This framework introduces a novel kernel correction method with theoretical guarantees to improve the quality of kernel matrices estimated on incomplete data, rooted in the understanding gleaned from structured learning and causal analysis with incomplete observations. Additionally, by reformulating affinity learning methods that draw on the principles of $\\ell_p$-norm minimization, we construct adaptive, intrinsic affinity matrices inspired by advances in understanding structural sparsity and mixing processes for the identifiability of nonlinear mixtures. This imputation-free approach, enriched with graphs as a formalism to encapsulate relationships in data, offers a robust alternative to traditional data imputation and distance calibration techniques, providing a new avenue for spectral clustering under weak supervision and demonstrating superior performance on benchmark datasets. Our methods present a promising avenue for applying spectral clustering in various real-world scenarios where incomplete data is a common issue.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ignavier_Ng1",
  "manipulated_ranking": 4,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=RWcfpmjlYm",
  "title": "BanditPAM++: Faster $k$-medoids Clustering",
  "modified_abstract": "Clustering is a cornerstone of data analysis, providing critical insights across diverse fields. This work is contextualized within an environment of heightened computational demands and enhanced methodologies, as evident in the literature related to imbalance problems in object detection and innovations in evaluation metrics for visual detection tasks. These discussions underscore the importance of efficiency and accuracy in algorithmic development. In the realm of $k$-medoids clustering, a technique renowned for its interpretability and flexibility with distance metrics, we introduce BanditPAM++, an extension of the recent BanditPAM algorithm. BanditPAM++ leverages a unique reuse of clustering information within and across iterations, resulting in $O(k)$ complexity reduction and significantly faster runtime performance without sacrificing clustering quality. Through experiments, including on the CIFAR10 dataset, BanditPAM++ not only maintains the accuracy standards set by its predecessor but also achieves over 10x improvement in speed. Furthermore, we provide a C++ implementation, which is accessible from Python and R, supporting wide adoption and further experimentation. The extension directly addresses the imbalance issue by its efficient handling of rare categories, acting as a sophisticated detector that ensures accurate clustering even when data points are sparse or unevenly distributed. Additionally, by integrating with open source programming environments, it encourages the development of mask techniques that further enhance the interpretability of the $k$-medoids methodology.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Kemal_Oksuz1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=HMqGYxnlpv",
  "title": "A Simple Yet Effective Strategy to Robustify the Meta Learning Paradigm",
  "modified_abstract": "Inspired by recent advancements in robotics and reinforcement learning that emphasize the development of versatile skill libraries, primitives, and the optimization of learning methods to handle complex, multi-modal tasks, this paper introduces a novel approach to enhance the meta learning paradigm. Previous efforts have demonstrated the efficacy of models that can adapt to new tasks rapidly, a core objective of meta learning. However, these methods often fall short in risk-sensitive scenarios where fast adaptation is critical. Drawing parallels from strategies like the use of Mixture of Experts for skill specialization, region-based optimization for tailoring local decision regions, and chance-constrained optimization for handling uncertainty in non-linear systems, our work proposes a distributionally robust optimization method for meta learning. By optimizing for the tail risk of task distributions within a specific region, we aim to control the worst-case fast adaptation outcomes within a probabilistic framework. This approach not only addresses the weaknesses in current meta learning models but also sets a new precedent for developing robust learning systems that are capable of managing the multifaceted challenges presented by real-world applications. Programatically, our method integrates intuitive decisions and teaching methods within the optimization process, essentially mimicking aspects of imitation learning and enhancing the search methodology for better task-specific strategies. Experimental validations of our method demonstrate significant improvements in the robustness of meta learning models to task distribution variability and a reduction in the conditional expectation of the worst-case adaptation risks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Onur_Celik1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=Gh67ZZ6zkS",
  "title": "PreDiff: Precipitation Nowcasting with Latent Diffusion Models",
  "modified_abstract": "Inspired by seminal works in physics-informed and data-driven machine learning models, such as those addressing the dynamics of partial differential equations, compositional uncertainty in deep Gaussian processes, probabilistic ice core dating, climate model optimization for extreme weather forecasting, and unsupervised learning alignments through Gaussian processes, our study introduces PreDiff. This paper addresses the challenges in Earth system forecasting, a field traditionally anchored in physically intricate and computationally intensive models, by innovating with data-rich, deep learning methodologies. These earlier efforts have expanded our understanding of how to integrate domain-specific knowledge with machine learning, navigate the variational uncertainties inherent in complex systems, and refine our approach to global phenomena forecasting. We build on this foundation with a two-stage pipeline for probabilistic spatiotemporal forecasting: PreDiff, a conditional latent diffusion model designed for probabilistic forecasts that we introduce, and an explicit knowledge alignment mechanism, which ensures our forecasts adhere to physical laws such as energy conservation and precipitation dynamics, embodying an ensemble approach for model robustness. Through rapid prototyping and empirical studies on the N-body MNIST and SEVIR datasets, PreDiff demonstrates superior capability in handling uncertainty, assimilating domain-specific prior knowledge, maintaining the integrity of the forecasting process, and generating operationally useful forecasts. This not only addresses the current limitations of existing forecasting models but also sets a new benchmark for the integration of scientific knowledge into deep learning frameworks, by elevating the class of predictive analytics. Our methodology effectively reinterprets random data fluctuations as a source of information, enhancing model reliability and maintainability.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ieva_Kazlauskaite1",
  "manipulated_ranking": 14,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=8S9Fbee743",
  "title": "Data-driven Optimal Filtering for Linear Systems with Unknown Noise Covariances",
  "modified_abstract": "In the pursuit of advancing control and filtering techniques within linear systems characterized by uncertain parameters and dynamics, this study draws inspiration from a spectrum of related works, including methodologies for uncovering optimal control policies in complex systems with latent states, adaptive filtering through Gaussian processes, and diffeomorphic approaches to learning stable Koopman operators. Furthermore, networked online learning predicated on Gaussian processes for control of safety-critical, resource-constrained systems informs our approach to addressing the challenges inherent in optimizing filtering policies for systems equipped with sensors, accounting for the adaptation to unknown conditions. Specifically, this paper examines learning the optimal filtering policy, known as the Kalman gain, for a linear system with unknown noise covariance matrices using noisy output data. The learning problem is formulated as a stochastic policy optimization problem, aiming to minimize the output prediction error through adaptation and tracking performance. This formulation provides a direct bridge between data-driven optimal control and its dual, optimal filtering, positioning our work at the state-of-the-art intersection of these fields. Our contributions are twofold. Firstly, we conduct a thorough convergence analysis of the stochastic gradient descent algorithm, adopted for the filtering problem, accounting for biased gradients and stability constraints. Secondly, we carefully leverage a combination of tools from linear system theory, high-dimensional statistics, and aspects of unsupervised learning to derive bias-variance error bounds that scale logarithmically with problem dimension, and, in contrast to subspace methods, the length of output trajectories only affects the bias term, benefiting from efficient memory use.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Armin_Lederer1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=VMAgvbBBts",
  "title": "UP-DP: Unsupervised Prompt Learning for Data Pre-Selection with Vision-Language Models",
  "modified_abstract": "Inspired by recent breakthroughs in few-shot learning, dense prediction tasks from partially annotated data, and cross-domain feature adaptation, this study ventures into the arena of data pre-selection, a critical yet underexplored facet of machine learning. Leveraging innovations such as universal representation learning and task-specific adapters, we identify a significant opportunity to enhance data pre-selection processes through the adaptation of networks. By acknowledging the precedent set by these works, which unveil the untapped potential of integrating disparate feature sources and adapting models to novel domains with minimal supervision, our research introduces UP-DP. This novel methodology employs an unsupervised prompt learning approach to harness the combined strength of vision and language features within vision-language models, such as BLIP-2, for data pre-selection. UP-DP stands out by training text prompts to extract enriched joint features from images, fostering a diverse clustering of the dataset that facilitates selection for labeling across various domains. Our comprehensive evaluation across seven benchmark datasets relevant for classification and dense prediction tasks, reveals that UP-DP not only surpasses existing methods by up to 20% but also demonstrates exceptional generalizability, with prompts trained on one dataset significantly improving BLIP-2's feature extraction capabilities on others. This pioneering application of unsupervised prompt learning in vision-language models for data pre-selection paves a new path for optimizing performance in downstream tasks with limited annotation resources.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Wei-Hong_Li1",
  "manipulated_ranking": 4,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=YmEDnMynuO",
  "title": "GraphAdapter: Tuning Vision-Language Models With Dual Knowledge Graph",
  "modified_abstract": "In the context of recent advancements in efficient transfer learning (ETL) and domain adaptation, our work introduces GraphAdapter, an adapter-style tuning strategy aimed at enhancing the performance of vision-language models (VLMs) especially in low-data regimes. This novel approach leverages insights from previous studies on domain adaptation via bidirectional cross-attention transformers and multi-objective meta-learning, where the focus has been on learning domain-invariant feature representations and optimizing across multiple objectives without manually tuning weights. These innovations underline the significance of incorporating diverse knowledge sources and optimizing multifaceted objectives in machine learning. GraphAdapter extends these ideas by explicitly modeling dual-modality structure knowledge through a dual knowledge graph consisting of textual and visual sub-graphs to overcome limitations faced by most adapter-style methods, which include the modeling of task-specific knowledge in a single modality and the oversight of exploiting inter-class relationships in downstream tasks. Our experiments across 11 benchmark datasets demonstrate that GraphAdapter not only significantly outperforms previous adapter-based methods but also capitalizes on the interplay between textual and visual modalities to yield more effective classifiers for downstream tasks within a multi-task learning framework. The adaptation process facilitated by GraphAdapter, through a gradient-based approach, embodies evolutionary principles by dynamically optimizing the incorporation of new knowledge, thereby participating in an ongoing learning process and addressing the needs of multi-task learners.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Pengxin_Guo1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=1vzF4zWQ1E",
  "title": "Rethinking Bias Mitigation: Fairer Architectures Make for Fairer Face Recognition",
  "modified_abstract": "Amidst growing concerns over bias in face recognition systems used in safety-critical applications such as law enforcement, and drawing inspiration from recent advancements in understanding adversarial examples, attention mechanisms, and causal mechanisms in machine learning, this work presents a novel perspective. Instead of attributing model biases solely to biased training data or adversarial attack strategies, this study posits that biases are inherent to neural network architectures themselves. Leveraging insights from the analysis of robust and non-robust features in adversarial examples, and causal parameter estimation techniques, we conduct the first neural architecture search designed explicitly for fairness, alongside hyperparameter optimization, and consider the impact of defense mechanisms and attention mechanisms on model fairness and robustness. This groundbreaking approach allows us to identify a set of networks that significantly outperform current high-performance architectures and bias mitigation methods in terms of both accuracy and fairness on key datasets like CelebA and VGGFace2, even under adversarial perturbations. These models also show promising generalizability across various datasets, sensitive attributes, and optimization scenarios. Importantly, all the resources related to our research, including code and models, have been made available, in compliance with open science principles. This body of work can be seen as playing a foundational role in what could be perceived as a game-changing era in bias mitigation for facial recognition technology. (Note: Personal identifiable information such as specific URLs have been omitted.)",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Byung-Kwan_Lee1",
  "manipulated_ranking": 4,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=tLTtqySDFb",
  "title": "Not All Neuro-Symbolic Concepts Are Created Equal: Analysis and Mitigation of Reasoning Shortcuts",
  "modified_abstract": "This research is inspired by groundbreaking work in meta-learning, understanding large language models through the lens of cognitive psychology, elucidating the origins of human heuristics, investigating the effects of inducing anxiety in large language models, and modeling human exploration with resource-rational reinforcement learning. These previous investigations into the cognitive and computational underpinnings of learning processes and behavioral outcomes in both humans and artificial agents, including subjects from a wide range of demographics and neurotypes, set the stage for our critical examination of Neuro-Symbolic (NeSy) predictive models. NeSy models promise enhanced compliance with constraints, systematic generalization, and interpretability by reasoning over high-level concepts derived from sub-symbolic inputs. However, these models are susceptible to *reasoning shortcuts*, where high accuracy is achieved through concepts with unintended semantics, undermining the anticipated benefits. Our work systematically characterizes reasoning shortcuts as unintended optima of the learning algorithms and identifies four key conditions promoting their emergence. Deliberation on these conditions allows for the formulation of various mitigation strategies, evaluated for their efficacy through both theoretical analysis and empirical validation among individual cases. Our analysis incorporates elements of the multi-armed bandit problem to further understand the decision-making processes underlying the emergence of reasoning shortcuts. Our findings highlight the complexity of addressing reasoning shortcuts in NeSy models, questioning the trustworthiness and interpretability of current implementations when scaled to large contexts.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Marcel_Binz1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=5Fgdk3hZpb",
  "title": "Squeeze, Recover and Relabel: Dataset Condensation at ImageNet Scale From A New Perspective",
  "modified_abstract": "Influenced by recent developments in self-supervised learning, unified models for semantic segmentation that leverage fragmented annotations, and semi-supervised contrastive learning frameworks that efficiently use unlabeled data, this work introduces a novel dataset condensation framework named Squeeze, Recover and Relabel (SRe$^2$L). This framework decouples the bilevel optimization of model and synthetic data during training to address the challenge of dataset scalability across varying scales, model architectures, and image resolutions for efficient dataset condensation. Self-training components are integrated into SRe$^2$L to refine synthetic data prediction accuracy, utilizing both labeled and unlabeled data for training, an approach that synergizes with our use of fragmented annotations. SRe$^2$L exhibits flexibility and several advantages including the production of synthetic images of arbitrary resolutions, lower training costs, and memory consumption with high-resolution synthesis, and scalability to various evaluation network architectures. Conducted experiments on Tiny-ImageNet and full ImageNet-1K datasets under 50 Images Per Class (IPC) showcase our approach's superiority in leveraging a form of teacher-student methodology for dataset condensation, achieving the highest validation accuracy on Tiny-ImageNet and ImageNet-1K, outpacing all prior state-of-the-art methods significantly. Moreover, compared to the Modified Triplet Loss (MTT) method, our approach is substantially faster with significantly less memory required during data synthesis, and conducts a form of meta-learning that encapsulates prediction error correction as part of the training process. Our approach embodies the principles derived from the state-of-the-art methodologies in related works while addressing the gaps in dataset condensation at scale, ensuring intra-class manifold consistency in the condensed dataset. The synthesis of this investigation, with emphasis on manifold learning and segmentation, will be shared for further academic exploration and application, furthering the advancement in efficient machine learning practices.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Haohang_Xu1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=0x2Ou3xHbH",
  "title": "On the Convergence of No-Regret Learning Dynamics in Time-Varying Games",
  "modified_abstract": "This paper extends the inquiry into learning dynamics within the context of time-varying games, a domain not comprehensively explored in existing literature that largely focuses on static game settings. Inspired by prior works that characterized the convergence properties of gradient-based methods, including descent and acceleration techniques, and optimistic mirror descent in differentiable games and variational inequalities, we specifically address the convergence of optimistic gradient descent (OGD) in time-varying games. By integrating insights from analyses on static, stochastic settings, employing a norm-based approach to measure game variance, and examining the geometrical properties and transformations of the game sequence, our investigation provides sharp convergence bounds for the equilibrium gap of OGD in zero-sum games, parameterized by natural variation measures and geometrical properties of the game sequence. The study further delineates improved second-order variation bounds under strong convexity-concavity, conditional on each game's repetition within the sequence, and provides classification into different classes based on these bounds. Our findings extend to general-sum multi-player games through a bilinear correlation of equilibria, positing new implications for meta-learning and variation-dependent regret bounds\u2014thus engaging with unresolved queries from earlier research and providing practical examples to illustrate theoretical concepts. Additionally, we draw on our framework to offer fresh perspectives on dynamic regret in static games and consensus algorithms in distributed machine learning, thereby contributing novel insights to the discourse on learning dynamics in time-varying games.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Wa\u00efss_Azizian1",
  "manipulated_ranking": 9,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=Fdfyga5i0A",
  "title": "Mnemosyne: Learning to Train Transformers with Transformers",
  "modified_abstract": "Inspired by recent innovations in learning paradigms, this work introduces Mnemosyne, a new class of learnable optimizers based on spatio-temporal low-rank implicit attention Transformers. Mnemosyne embodies the convergence of implicit attention mechanisms and low-rank approximations for efficient training, a concept echoed in transformative research across natural language processing (NLP), image classification, and efficient model training methodologies. By leveraging the novel spatio-temporal low-rank implicit attention mechanism, Mnemosyne learns to train entire neural network architectures, including Transformers, without any task-specific optimizer tuning. Demonstrating its prowess, Mnemosyne outperforms popular LSTM optimizers, even those enhanced with feature engineering to mitigate catastrophic forgetting, and successfully trains Transformers with minimal computational resources. It also matches accuracy-wise with state-of-the-art (SOTA) hand-designed optimizers, often outperforming models tuned with carefully adjusted hyper-parameters. Additionally, Mnemosyne offers space complexity on par with hand-designed first-order optimizers, facilitating scalability to larger parameter sets. Extensive empirical evaluations across tasks\u2014including fine-tuning Vision Transformers (ViTs), pre-training BERT models, and soft prompt-tuning large 11B+ T5XXL models\u2014combined with a comprehensive theoretical analysis of the compact associative memory, underscore Mnemosyne's novel approach and its contributions to the training of neural networks for generalization. This innovative optimizer has been benchmarked against traditional methods, showing exceptional potency in a variety of scenarios including those involving complex data patches and necessitating the guidance of a teacher for knowledge distillation. Through the learning of image patches processing and leveraging the language of optimization, Mnemosyne ushers a new era in neural network training.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Andreas_Veit1",
  "manipulated_ranking": 4,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=llP6lmMiXE",
  "title": "A General Framework for Robust G-Invariance in G-Equivariant Networks",
  "modified_abstract": "In the landscape of deep neural networks, understanding and manipulating the geometric properties of data representations has gained pivotal importance, as highlighted by studies on the intrinsic dimensionality of data representations and the examination of hidden layers in both conventional and pruned convolutional neural networks, as well as large transformer models. Motivated by these investigations into the geometrical and statistical properties underlying successful neural architectures, this paper introduces a novel method for achieving robust group-invariance in group-equivariant convolutional neural networks ($G$-CNNs), referred to as the $G$-triple-correlation ($G$-TC) layer. Our method is grounded on the theory of triple-correlation on groups, distinguishing itself as the unique, lowest-degree polynomial invariant map that is also complete. Tackling the ubiquitous challenge of incomplete invariant maps, the $G$-TC layer's completeness preserves all information about the signal's structure while removing variance solely due to group actions, thereby enhancing resistance to invariance-based adversarial attacks and improving classification accuracy over standard Max $G$-Pooling in $G$-CNN architectures. Offering a generalized and efficient implementation applicable to any discretized group, we demonstrate the method's efficacy across various group actions\u2014$SO(2)$, $O(2)$, $SO(3)$, and $O(3)$\u2014and its tangible benefits in applications such as $G$-MNIST and $G$-ModelNet10 datasets. The introduced robust $G$-invariance method also addresses the evolution of representational dimensionality, providing a framework for future tests of shallow and deep $G$-equivariant networks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Alessio_ansuini1",
  "manipulated_ranking": 6,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=j7U4pFkCYB",
  "title": "DynPoint: Dynamic Neural Point For View Synthesis",
  "modified_abstract": "Drawing insights from recent advancements in neural radiance fields, video synthesis through conditional invertible neural networks, and the significant strides made in image and video generation using latent diffusion models, this work presents DynPoint. This algorithm emerges as a solution to the challenges posed by uncontrolled or extensive scenarios in view synthesis from monocular videos, which often demand prohibitive training time for each new setting. Unlike approaches that rely on encoding scenario information into compact latents, DynPoint predicts explicit 3D correspondence between adjacent frames to aggregate information efficiently. This is accomplished by estimating consistent depth and scene flow across frames, paving the way for the generation of hierarchical neural point clouds that facilitate the rapid synthesis of novel views for any given target frame through convolutional layers and cross-attention mechanisms. Experimental outcomes underscore the efficacy of DynPoint in significantly speeding up the training process\u2014often by an order of magnitude\u2014while maintaining robust performance in processing long-duration videos without necessitating the learning of a canonical video representation. Dramatically reducing computational overhead, DynPoint integrates the notion of dynamic correspondence and information aggregation in a novel framework, enabling swift and accurate view synthesis. Furthermore, the method suggests possibilities for future applications in video inpainting and text-to-video synthesis, benefiting from its autoregressive characteristics and instance-specific adaptability for enhancing visual quality and coherence in synthesized views. Notably, the framework's ability to adjust resolution on the fly alludes to its potential in super-resolution tasks for future exploration.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Andreas_Blattmann1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=bLB4vTwSbC",
  "title": "Greatness in Simplicity: Unified Self-Cycle Consistency for Parser-Free Virtual Try-On",
  "modified_abstract": "Inspired by the recent advancements in deep learning methods such as self-supervised learning, cycle consistency, and SE(3)-equivariant feature learning for tasks including 3D object pose estimation and tracking, this work introduces a novel approach to image-based virtual try-on challenges. It leverages the essence of simplicity in modeling complex problems, particularly non-rigid garment deformation and feature entanglement. By incorporating principles from foundational works in deep prior deformation networks and sparse steerable convolutions, we extend these concepts into the realm of fashion technology with a unified self-cycle consistency (USC-PFN) framework. This parser-free virtual try-on network effectively obviates the need for auxiliary tasks and dual generator mechanisms, thus mitigating the impact of potentially irresponsible prior knowledge. USC-PFN employs a single-generator architecture for robust garment-person image translation, innovatively applying a self-cycle consistency model to substantially improve upon the realism of garment deformations without manual parser intervention. Our approach, through exhaustive training instances, significantly advances the current state-of-the-art by directly addressing the limitations present in previous methods, achieving unprecedented performance levels on well-established virtual try-on benchmarks. Spanning across convolutional techniques and leveraging foundational learning methodologies, USC-PFN establishes a new paradigm in the virtual try-on industry by keenly focusing on shape processing and pose adjustments in garment images, substantiated through empirical validations on multiple datasets.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jiehong_Lin1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=ouLe91yibj",
  "title": "On the Properties of Kullback-Leibler Divergence Between Multivariate Gaussian Distributions",
  "modified_abstract": "The calculation of the difference between probability distributions using Kullback-Leibler (KL) divergence is foundational in understanding various phenomena in machine learning, including unsupervised learning, the estimation of model performance, the learning of dynamical systems, and subspace identification. This paper extends upon the theoretical underpinnings provided by prior works on L-statistic minimization, regression methods including Koopman operator regression, weighted areas under the ROC curve, perturbation resilience in unsupervised learning environments, and concentration inequalities under sub-Gaussian and sub-exponential conditions. Specifically, we theoretically study several properties of KL divergence between multivariate Gaussian distributions. Firstly, we prove that for any two $n$-dimensional Gaussian distributions $\\mathcal{N}_1$ and $\\mathcal{N}_2$, a bound on the supremum of $KL(\\mathcal{N}_1||\\mathcal{N}_2)$ can be derived based on the principal branch of the Lambert $W$ function, offering insights into the approximate symmetry for small KL divergences between Gaussian distributions. Secondly, for any three $n$-dimensional Gaussian distributions, their interaction showcases that an upper bound of $KL(\\mathcal{N}_1||\\mathcal{N}_3)$ follows a relaxed triangle inequality, indicative of a broader applicability in analyzing complex systems, clustering techniques, and empirical research methodologies. Lastly, we discuss the implications of our findings in deep learning, reinforcement learning, and sample complexity research, connecting the theoretical properties of KL divergence with practical applications in machine learning.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Andreas_Maurer1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=L7Whl9pXd0",
  "title": "Efficient Batched Algorithm for Contextual Linear Bandits with Large Action Space via Soft Elimination",
  "modified_abstract": "Taking inspiration from the exploration of policy identification in MDPs, decoding latent states in Block MDPs, and regret minimization strategies across various bandit and control systems, we introduce the first efficient batched algorithm for contextual linear bandits with large action spaces. These foundational contributions underscore the importance of domain-specific optimizations, sample efficiency, and regret minimization in machine learning. Unlike existing batched algorithms that rely on action elimination, which are not implementable for large action sets, our algorithm utilizes a linear optimization oracle and sampling techniques over the action set to design the policy, incorporating estimation methods to refine the action-selection process with identification of optimal actions. The proposed algorithm achieves an asymptotically optimal regret upper bound $\\tilde{O}(\\sqrt{T})$ with high probability, and uses $O(\\log\\log T)$ batches, matching the lower bound on the number of batches. When specialized to linear bandits, our algorithm can achieve a high probability gap-dependent regret bound of $\\tilde{O}(1/\\Delta_{\\min})$ with the optimal $\\log T$ number of batches, where $\\Delta_{\\min}$ is the minimum reward gap between a suboptimal arm and the optimal, appealing to both theoretical players and practitioners looking for efficient designs. Our result is achieved via a novel soft elimination approach, that entails $\\text{``}$shaping$\\text{\"}$ the action sets at each batch so that we can efficiently identify (near) optimal actions. This work thus not only advances the state of the art in batched algorithm designs for bandit problems but also presents a novel perspective on managing large action spaces through soft elimination.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yassir_Jedra1",
  "manipulated_ranking": 12,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=uiiVSVADDc",
  "title": "Annotator: A Generic Active Learning Baseline for LiDAR Semantic Segmentation",
  "modified_abstract": "In light of recent significant strides made in machine learning for various applications such as an amodal segmenter using boundary uncertainty estimation, multi-scale CNNs for affordance segmentation in RGB images, incremental few-shot instance segmentation, and boosting few-shot segmentation techniques, our research introduces Annotator, a versatile and efficient active learning approach tailored for LiDAR semantic segmentation. These preceding works have showcased sophisticated strategies for handling complex data characteristics and learning tasks with limited supervision or incremental class information, often distinguishing between an object's foreground and its surrounding environment. Inspired by the methodologies and challenges outlined in these studies, Annotator is designed to tackle the inherent challenges of working with high-volume point clouds in LiDAR data, focusing on the labor-intensive and costly process of annotation. By offering a voxel-centric online selection strategy, Annotator efficiently identifies and annotates crucial voxel grids within each LiDAR scan for segmenting and adapting effectively even under distribution shifts. Through deep analysis and development of a novel selection metric, the voxel confusion degree (VCD), which leverages the local topological relationships and structures within point clouds for part-based completion and training evolution, this work achieves remarkable efficiency and demonstrates significant reductions in annotation requirements. Specifically, Annotator requires labeling merely five voxels per scan for the SynLiDAR \rightarrow SemanticKITTI task, culminating in impressive performance metrics across various benchmarks and scenarios, including active learning, active source-free domain adaptation, and active domain adaptation. This foundation sets a new standard for label-efficient 3D application endeavors, offering a streamlined and theoretically-informed approach for future research and application developments in LiDAR semantic segmentation. The evaluation of Annotator's performance is critical, providing quantitative evidence of its effectiveness in enhancing segmenter accuracy while minimizing the human efforts required for instance-level annotations.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sinisa_Todorovic1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=ETk6cfS3vk",
  "title": "SlotDiffusion: Object-Centric Generative Modeling with Diffusion Models",
  "modified_abstract": "This research is inspired by recent innovations in utilizing transformers for vision tasks, k-means based segmentation, and alternating network architectures combining convolution and attention, each contributing to the advancement of machine learning applications from image and video segmentation to object detection and recognition. Object-centric learning, a method aimed at representing visual data with structured entities known as slots, aims to enhance systematic generalization by providing structured representations. Leveraging advancements in architectures like Transformers and multi-layer processing, significant progress has been made in unsupervised object discovery, with slot-based representations promising enhanced capabilities for generative modeling, including controllable image generation and object manipulation in image editing. Despite current advancements, slot-based methods often result in generation quality issues, such as blurry images or distorted objects. Focusing on improving slot-to-image decoding for high-quality visual generation, we introduce SlotDiffusion, an object-centric Latent Diffusion Model (LDM) tailored for both image and video data. Benefiting from the substantial modeling prowess of LDMs, SlotDiffusion not only outperforms existing slot models in unsupervised object segmentation and visual generation across various datasets but also enhances video prediction quality and downstream temporal reasoning tasks when its learned object features are applied to existing object-centric dynamics models. Moreover, by applying k-means clustering for mask detection techniques and handling queries in a structured slot-based approach, we validate SlotDiffusion's scalability to real-world datasets like PASCAL VOC and COCO through integration with self-supervised pre-trained image encoders.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Siyuan_Qiao1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=AnFUgNC3Yc",
  "title": "Resetting the Optimizer in Deep RL: An Empirical Study",
  "modified_abstract": "In the burgeoning field of deep reinforcement learning (RL), optimizing the approximation of the optimal value function remains a cornerstone challenge, influenced by recent advancements in value estimation and policy improvement strategies, particularly the actor-critic frameworks. Our study is underpinned by insights from the literature on enhancing value estimation accuracy, leveraging double actors for bias alleviation in continuous control, and exploring conservatism in offline and online RL for robust value function approximation. These prior works, while focused on different aspects of reinforcement learning, converge on the critical importance of precise and effective optimization strategies for generalization across tasks. In this context, we examine the conventional approach of employing modern variants of the stochastic gradient descent algorithm, such as Adam, for solving a sequence of optimization problems in deep RL. Noting that these optimizers maintain internal parameters that are updated across iterations and may carry forward outdated information due to the dynamic optimization landscape of RL, we propose and empirically investigate the effects of resetting these internal optimizer parameters at the commencement of each new optimization iteration. Utilizing a range of modern optimizers in conjunction with the Rainbow algorithm\u2014an integration of distributional value estimation methods and multiple actor-critic improvements\u2014our experiments demonstrate that this simple yet novel modification\u2014resetting the optimizer's internal parameters\u2014significantly enhances deep RL performance on the Atari benchmark, potentially setting a new direction for optimizer application in deep RL. The distributional aspect of the Rainbow algorithm, alongside its optimized action selection policy, underscores our findings on the empirical benefits of optimizer resetting.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jiafei_Lyu1",
  "manipulated_ranking": 9,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=aW9BqtRQkh",
  "title": "Language Models Can Improve Event Prediction by Few-Shot Abductive Reasoning",
  "modified_abstract": "Inspired by prior works that have collectively advanced the understanding and application of large language models (LLMs) in tasks such as conditional language generation, long-form question answering, and neural summarization, this paper explores a novel application of LLMs in event prediction through abductive reasoning. These foundational studies have demonstrated LLMs' remarkable ability to process and generate texts based on nuanced understanding and reasoning, presenting opportunities for innovative uses in predictive models. We present LAMP, a framework that synergizes LLMs with event sequence models to enhance prediction performance empirically. LAMP leverages the inherent reasoning capabilities of LLMs to analyze past events and propose possible causes for future occurrences, which are then evaluated by an event sequence model. This process is informed by a limited number of expert-annotated demonstrations, enhancing the model's capacity for abductive reasoning with minimal inputs and ensuring normalization of data for quality control. Our experiments across several challenging datasets demonstrate significant performance improvements over existing state-of-the-art models, underlining the potential of integrating reasoning-based approaches with predictive modeling for generating high-quality predictions. This success is particularly notable in the context of mitigating miscalibration and enhancing generating realistic outcomes, thus reaffirming the versatility of LLMs and opening new pathways for improving event prediction methodologies.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Shashi_Narayan1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=NvcVXzJvhX",
  "title": "Sheaf Hypergraph Networks",
  "modified_abstract": "Higher-order relations are widespread in nature, with numerous phenomena involving complex interactions that extend beyond simple pairwise connections. Inspired by recent developments in Graph Neural Networks (GNN), which explored the phenomena of untrained sparse subnetworks and the challenges posed by sparse neural network (SNN) algorithm evaluation, our work introduces cellular sheaves for hypergraphs, a mathematical construction that enhances conventional hypergraph representation while preserving their local, higher-order connectivity. By incorporating concepts from these critical areas, including the importance of sparse representations and the rigorous evaluation of algorithms, we have developed two novel formulations of sheaf hypergraph Laplacians: linear and non-linear, which theory indicates provide a more expressive inductive bias than standard hypergraph diffusion for modeling complex data structures across various domains. Leveraging this enhanced representation and through rigorous training methodologies, we propose Sheaf Hypergraph Neural Networks and Sheaf Hypergraph Convolutional Networks, which generalize classical Hypergraph Networks, showing significant performance improvements on benchmark datasets for hypergraph node classification. Our approach not only captures the local connectivity of data but also addresses the need for models with architectures that efficiently handle higher-order relations, thus pushing the boundaries of what is achievably known in the realms of fairness, robustness, and privacy. The open-source release of our frameworks encourages further research and evaluation in the community, fostering innovation and ensuring our methods are widely accessible and verifiable.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Tianjin_Huang1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=7uPnuoYqac",
  "title": "Federated Learning with Manifold Regularization and Normalized Update Reaggregation",
  "modified_abstract": "The advent of Federated Learning (FL) has ushered in a collaborative era in machine learning, wherein multiple clients collectively train a global model without sharing their datasets, echoing recent methodological innovations across learning to optimize (L2O), matrix completion, and tensor decompositions. Inspired by these advancements, which cumulatively emphasize the significance of optimizing mathematical structures, robust sampling, and decomposition techniques to enhance computational efficiency and generalization capacity, our study introduces FedMRUR. This novel framework integrates manifold regularization with normalized update reaggregation to address the persistent challenge of model inconsistency due to data heterogeneity across clients\u2014a factor that induces near-orthogonality in client updates and subsequently slows convergence in FL environments. By adopting a hyperbolic graph manifold regularizer, FedMRUR effectively narrows the representation gap between local and global models in a low-dimensional subspace, leveraging the intrinsic graph structure of machine learning models to more accurately reflect model bias through hyperbolic rather than Euclidean distances. The application of FedMRUR in real-world scenarios demonstrates its robustness against outliers, enhancing its appeal. Concurrently, the mechanism for reaggregating client update norms under FedMRUR amplifies each client's contribution to the global update, counteracting the norm reduction issue. The theoretical underpinnings validate the algorithm's efficiency, guaranteeing a linear speedup property $\\mathcal{O}(\\frac{1}{\\sqrt{SKT}})$ in non-convex settings with partial client participation and embracing low-rank approximation techniques to further refine learning outcomes. Empirical evaluations across standard benchmarks further attest to FedMRUR's capacity to set new state-of-the-art accuracy metrics while reducing communication overhead, thus forging a path toward more robust, efficient, and harmonized federated learning systems.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~HanQin_Cai1",
  "manipulated_ranking": 4,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=cQdc9Dyk4i",
  "title": "GraphMP: Graph Neural Network-based Motion Planning with Efficient Graph Search",
  "modified_abstract": "Our study draws inspiration from recent advancements in applying graph neural networks (GNNs) and self-supervised learning approaches to a variety of domains, including sequential decision making, embodied navigation, and visual representation learning. These advancements highlight the potential of leveraging GNNs for complex spatial and sequence modeling tasks, which is particularly relevant to our investigation into motion planning in robotic systems. Motion planning, a fundamental task in robotic systems, aims to find a high-quality collision-free path in the configuration space. While learning-based motion planners, particularly those powered by GNNs, have shown promising planning performance, the integration of GNNs with the graph search process remains an open challenge. Our work introduces GraphMP, a novel neural motion planner that synergizes the graph pattern extraction capabilities of GNNs with an efficient graph search algorithm, tailored for both low and high-dimensional planning tasks. By innovating on model architecture, including the use of convolutions for spatial feature extraction, and training mechanisms, GraphMP achieves significant improvements in path quality and planning speed over existing state-of-the-art learning-based and classical planners, across a range of environments from 2D Mazes to complex 14D dual KUKA robotic arm setups, while maintaining competitive success rates. This approach not only advances the field of robotic motion planning but also demonstrates the transformative potential of GNNs in synthesizing efficient solutions to traditionally computational and algorithmically intensive problems. Moreover, by projecting this knowledge into multimodal and possibly augmented learning scenarios, GraphMP inherently learns to navigate the challenges posed by zero-shot conditions, where prior knowledge is minimal, further validating its utility in dynamic, indoor and real-world applications.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Arjun_Majumdar2",
  "manipulated_ranking": 10,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=nCwStXFDQu",
  "title": "FouriDown: Factoring Down-Sampling into Shuffling and Superposing",
  "modified_abstract": "The efficacy of spatial down-sampling methods, such as strided convolution, Gaussian, and Nearest down-sampling, within deep neural networks is undisputed, serving as a cornerstone for numerous breakthroughs in image processing, model efficiency, and network architecture design. Inspired by the latest advancements in frequency domain analysis and manipulation, as evidenced by research into frequency pooling, including max-pooling, dilated convolutions for enhancing resolution, and techniques for suppressing high-frequency components for adversarial defense, our study introduces FouriDown\u2014a novel paradigm in down-sampling. FouriDown innovatively combines shuffling and superposing within the Fourier domain, transcending traditional static weighting strategies toward a learnable and context-adaptive framework. Drawing from the signal sampling theorem, we reinterpret the down-sampling operator through a Fourier function, organizing frequency positions in a physically closed configuration. This allows for point-wise channel shuffling based on aliasing susceptibility, thereby facilitating consistent weighting parameter learning during the convolutional network training phase. Encompassing 2D discrete Fourier transform, context shuffling, adaptive superposing, and a reverse transform as its core components, FouriDown is not only versatile, integrating seamlessly into existing restoration and convolutional networks but also demonstrably enhances performance in applications such as image de-blurring and low-light enhancement across various datasets. The forthcoming public release of our code aims to encourage further exploration and a new perspective in the application of FouriDown in broader image processing arenas, including state-of-the-art classification tasks. By potentially replacing or complementing existing down-sampling techniques in architectures, even those as deep as 20-layer networks, FouriDown promises improvements in both model efficiency and classification accuracy through enhanced training methodologies.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zhendong_Zhang1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=GlWzQhf2lV",
  "title": "Exploiting Contextual Objects and Relations for 3D Visual Grounding",
  "modified_abstract": "Inspired by recent innovations in 3D object detection, real-time semantic segmentation, and document image pre-training within the machine learning domain, this paper introduces a novel approach to the challenge of 3D visual grounding. The task involves identifying visual objects within 3D scenes based on natural language inputs, a critical capability for interactive machines in real-world environments, including driving scenarios where camera imagery, aggregating information from both low-resolution and high-resolution textures and overhead perspectives, is paramount. This work builds upon the understanding that efficiently capturing 3D contextual information is paramount for distinguishing target objects within complex scenes. Despite the lack of annotations for contextual objects and relations, which exacerbates this challenge, we propose CORE-3DVG, a model designed to explicitly learn about contextual objects and relations through devices that capture varied data types. CORE-3DVG achieves 3D visual grounding through three sequential modular networks: a text-guided object detection network that incorporates attention mechanisms, a dual-stream relation matching network, and a target identification network. To facilitate learning in the absence of direct annotations, a pseudo-label self-generation strategy and a weakly-supervised method are introduced, enhancing the model's focus on referred objects by better grasping their 3D context through recognizing their spatial relations and attributes. Evaluations on Nr3D, Sr3D, and ScanRefer datasets exhibit state-of-the-art performance by our model, affirming the effectiveness of our novel approach.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Errui_Ding2",
  "manipulated_ranking": 4,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=ZwQJRXLjVm",
  "title": "Rehearsal Learning for Avoiding Undesired Future",
  "modified_abstract": "Inspired by the significant strides made in quantifying predictive uncertainties through Evidential Deep Learning and the principled modeling of uncertainty in Bayesian neural networks, this paper introduces a novel approach towards utilizing machine learning (ML) models for decision-making processes. Specifically, we focus on the scenario where, rather than merely making predictive statements about future outcomes, the goal is to identify courses of action that can prevent undesired futures as predicted by ML models, thereby necessitating the calibration of predictive models for higher accuracy. To achieve this, the existing calibration methods are examined and enhanced to ensure reliability in predictions. We present a rehearsal learning framework characterizing the generative process of variables with structural rehearsal models, which comprises a probabilistic graphical model known as rehearsal graphs and structural equations. This framework appropriately harnesses deep neural nets for the dynamical modeling of complex scenarios, enabling the execution of queries and the discovery and recommendation of actionable decisions that can alter the predicted outcome, using reasoning under a Bayesian framework. Furthermore, we introduce a probably approximately correct (PAC) bound to quantify the associated risks of a decision and facilitate classification of actions based on their predicted outcomes. Experiments validate the proposed rehearsal learning framework's effectiveness in the calibration process and the informativeness of the introduced bound in guiding decision-making to avoid adverse outcomes. Notably, the large-scale data handling capability of this approach and its potential applicability to image-based predictions confirm its broad utility across various domains.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Manuel_Haussmann1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=WRGldGm5Hz",
  "title": "DYffusion: A Dynamics-informed Diffusion Model for Spatiotemporal Forecasting",
  "modified_abstract": "Inspired by the foundational contributions of simulation-based inference, variational methods, and group equivariant neural posterior estimation, this work presents DYffusion, a novel application of diffusion models tailored for dynamics forecasting. These preceding studies have set the stage by demonstrating the potential of leveraging structured knowledge and inductive biases within simulation-based frameworks for high-fidelity inference across various domains, including astrophysics and neuroscience. Building on this momentum, DYffusion extends the utility of diffusion models beyond their traditional confines of generating static images. By integrating temporal dynamics directly within the diffusion process, we introduce a new paradigm for training diffusion models that are inherently capable of capturing multi-step and long-range dependencies in spatiotemporal datasets. Our approach utilizes a stochastic, time-conditioned interpolator alongside a backbone forecaster network, embodying the forward and reverse processes characteristic of diffusion models but with a significant novelty\u2014dynamics-informed diffusion steps. This methodology not only enhances the model's forecasting capabilities through flexible, continuous-time sampling trajectories but also achieves a trade-off between performance and group-informed sampling efficiency at inference time. Furthermore, the incorporation of dynamics as an inductive bias results in improved computational efficiency relative to traditional noise-based or intractable problem-solving diffusion models. Estimators specialized for this setup, requiring less computational effort, have shown to be effective. Applied to the complex dynamical forecasting of sea surface temperatures, Navier-Stokes flows, and spring mesh systems, DYffusion demonstrates competitive performance on probabilistic skill score metrics, heralding a new direction for dynamics forecasting in machine learning.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Michael_Deistler1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=f56xMRb7Vt",
  "title": "Norm-guided latent space exploration for text-to-image generation",
  "modified_abstract": "In the context of ongoing innovations in machine learning models for natural language understanding, web navigation, autonomous agent tasks, and the augmentation of language models with retrieval mechanisms, this paper addresses a key challenge in the text-to-image synthesis domain. Text-to-image diffusion models have demonstrated significant potential in generating images from textual sentences, merging concepts in novel compositions and scenarios. Yet, the manipulation and understanding of latent spaces, particularly the initialization seeds, remain a critical bottleneck. These latent spaces' structure profoundly impacts the capacity to generate diverse concepts accurately. Notably, traditional operations such as interpolation and centroid determination underperform when applying standard Euclidean or spherical metrics. This study stems from the observation that diffusion models, routinely engaging inputs across a constrained norm value range during training, present a facet with significant implications for seed manipulation techniques crucial for image generation in both few-shot and long-tail learning tasks. By proposing a novel interpolation method that defines a new, non-Euclidean metric attentive to a norm-based seed prior within a certain architecture, we offer an innovative approach to navigating latent spaces. This method enables more effective centroid and interpolation operations, thereby improving the generation of images for rare concepts through enhanced semantic analysis and reasoning. Our approach underscores the importance of considering the normative characteristics of latent seeds and sets new benchmarks in few-shot and long-tail learning applications through enhanced generation speed, image quality, and semantic fidelity in diverse environments. The k-nearest neighbors algorithm, adapted to this non-Euclidean metric, further optimizes the selection process for initialization seeds, making it a cornerstone of our proposed solution for more accurate and contextually relevant image synthesis.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Frank_F._Xu1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=3ZICE99e6n",
  "title": "ReTR: Modeling Rendering Via Transformer for Generalizable Neural Surface Reconstruction",
  "modified_abstract": "Inspired by recent advances in neural implicit representations, dense 3D reconstruction, and absolute pose regression with feature matching, we introduce Reconstruction TRansformer (ReTR), a framework that innovatively applies the transformer architecture to the domain of neural surface reconstruction. Generalizable neural surface reconstruction techniques, pivotal for a myriad of applications from mixed reality to automated navigation, are often hindered by the limitations of oversimplified volume rendering processes. These processes traditionally result in low confidence in depth distribution and inaccuracies in surface reasoning. Leveraging foundational progress in the field, including but not limited to advancements in neural volume fusion and direct feature matching for enhanced pose regression, as well as multi-view geometry and single-image understanding for accurate modeling, ReTR reimagines rendering via transformers. It introduces a learnable $\\textit{meta-ray token}$ and employs a cross-attention mechanism to intricately model the interaction between the rendering process and sampled points, rendering observed color in a high-dimensional feature space. This advancement reduces the sensitivity to projected colors from source views, significantly improving surface reconstruction accuracy and confidence. Tested across various datasets, including indoor environments, ReTR demonstrates superior performance in reconstruction quality and generalizability compared to state-of-the-art methods, even when dealing with data from a single camera. This merging of transformer architecture with neural surface reconstruction contributes to the ongoing conversation about neural surface reconstruction by providing a novel approach that is informed by and builds upon preceding works in related domains, incorporating elements of fusion and absolute understanding in its methodology.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Victor_Adrian_Prisacariu1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=1recIOnzOF",
  "title": "Decorate3D: Text-Driven High-Quality Texture Generation for Mesh Decoration in the Wild",
  "modified_abstract": "In the evolving landscape of 3D object editing and texturing, inspired by the foundational works on text-guided texturing of 3D shapes, the intricacies of StyleGAN's learned latent space, and advancements in text-to-image generation using textual inversion and diffusion priors, this paper introduces Decorate3D. Our method presents a novel, intriguing approach to creating and editing 3D objects using images, integrating the versatility of neural radiance fields (NeRF) with a structure-aware score distillation sampling method and a few-view resampling training approach across a variety of scenes. Decorate3D excels in modeling real-world objects by decomposing the NeRF representation into an explicit mesh representation, a view-dependent texture, and a diffuse UV texture. Users can edit the UV manually or employ text prompts for the automatic generation of a new 3D-consistent texture tailored to personalized requirements. To achieve high-quality 3D texture generation, we harness the power of an image diffusion model, enhanced for 3D-consistent generation capability, and employ a super-resolution model for refined high-resolution UV textures. The architecture of Decorate3D is pivotal, incorporating generative algorithms that play a crucial role in our system, enabling the superior performance of Decorate3D in retexturing real-world 3D objects and making significant strides in the task of 3D texturing and mesh decoration.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yuval_Alaluf1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=XqcXf7ix5q",
  "title": "Locality-Aware Generalizable Implicit Neural Representation",
  "modified_abstract": "Inspired by recent advances in generative model architectures and techniques, such as DreamTeacher's self-supervised feature representation learning, NeuralField-LDM's scene generation using hierarchical latent diffusion models, Polymorphic-GAN's multi-domain sample generation with morph maps, and GET3D's explicit textured 3D mesh generation, our research proposes a novel framework for generalizable implicit neural representation (INR) that emphasizes the importance of locality for capturing fine-grained data details in areas such as faces and other complex structures. Our approach innovatively combines a transformer encoder with a locality-aware INR decoder. The encoder processes data instances to predict a set of latent tokens that encapsulate local information, while our decoder leverages these tokens through cross-attention and multi-band feature modulation to achieve spatial and spectral locality-awareness in representation. This strategy aims to overcome the limitations of current generalizable INRs in localizing and detailing specific data points such as pixels and rays, thus significantly outperforming them in tasks like image generation, image-to-image translation, and photo editing. Our results not only exhibit the framework's superiority over existing approaches but also validate the utility of locality-aware latents in various downstream applications, providing a substantial new direction for research in INR methodologies and opening possibilities in fields such as adversarial training techniques and robotics for enhanced sample generation and application. The learned representations are generative in nature, enabling broad applications including those requiring precise image translation and editing.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Daiqing_Li1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=r6xGZ0XL2g",
  "title": "Meta-Learning Adversarial Bandit Algorithms",
  "modified_abstract": "This work is inspired by recent advances in online learning and optimization, spanning diverse applications from strategic expert learning to interactive combinatorial bandits and online submodular maximization under adversarial or stochastic constraints. These foundational efforts highlight the growing importance of developing algorithms that are capable of adapting to various settings, including adversarial environments and those requiring long-term budget management or consumption. Building on these insights, we study online meta-learning with bandit feedback, aiming to improve performance across multiple settings given natural similarity measures related to the consumption patterns of users. As the pioneering effort in the adversarial online-within-online partial-information setting, we design meta-algorithms that adjust initialization and hyperparameters of an inner learner for both multi-armed bandits (MAB) and bandit linear optimization (BLO). For MAB, our meta-learners employ the Tsallis-entropy generalization of Exp3, optimizing task-averaged regret based on the entropy of optima-in-hindsight, a property closely related to submodularity. In BLO, we initiate and fine-tune online mirror descent (OMD) with self-concordant barrier regularizers for supermodular objectives, demonstrating that task-averaged regret correlates with an action space-dependent measure, aiming for sublinear regret growth. Our contributions rely on innovative proofs showing that unregulated follow-the-leader, combined with low-dimensional hyperparameter tuning and forecasters' strategic predictions, suffices for learning sequences of affine functions of challenging Bregman divergences, thus bounding the regret of OMD effectively, even under scenarios where constant environmental watching is required.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Omid_Sadeghi1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=TDS3kqRteY",
  "title": "REx: Data-Free Residual Quantization Error Expansion",
  "modified_abstract": "While the fields of computer vision and natural language processing continually evolve towards complexity, constraints such as privacy rights and device adaptability necessitate innovative solutions in neural network deployment. Building upon the notion of efficiency in neural network operations as explored in recent quantization research\u2014where the focus ranged from architecture-aware quantization strategies to benchmarking model quantization deployability and privacy-centered data-free model compression\u2014our study introduces REx, a novel quantization method. REx leverages residual error expansion and incorporates group sparsity to address the high inference cost of Deep Neural Networks (DNNs) without relying on original training data. This method uniquely combines the deep learning flexibility required for adapting to specific hardware bit widths with the robustness needed for maintaining high accuracy across different model architectures. Experimental validation shows that REx not only achieves superior performance in balancing the accuracy-speed trade-off on a variety of platforms but also effectively mitigates the outlier problem that significantly challenges existing state-of-the-art quantization methods. Moreover, REx is noted for its theoretical foundations in maintaining predictive function fidelity after quantization, offering insights into its agnostic nature towards different quantization operators and seamless integration with prior works. Through this framework, REx sets a new precedent for data-free quantization, exemplifying how deep learning models can be efficiently compressed and deployed in a privacy-preserving manner while adapting to a broad spectrum of device constraints, even in applications dealing with huge datasets or high-resolution images.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Mingzhu_Shen1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=9pLaDXX8m3",
  "title": "NeRF-IBVS: Visual Servo Based on NeRF for Visual Localization and Navigation",
  "modified_abstract": "Inspired by recent advancements in robot learning, including scalable multi-task learning, real-world benchmarking through remote access to shared robots, and the acceleration of visual model-based reinforcement learning with demonstrations, this paper introduces a novel approach for visual localization and navigation. Visual localization is a fundamental task in computer vision and robotics, necessitating a large number of posed images for generalization to novel views and dense ground truth 3D labels for supervision, both of which are challenging and costly to acquire in real-world settings. Our method, NeRF-IBVS, overcomes these obstacles by leveraging a few posed images with coarse pseudo-3D labels generated by Neural Radiance Fields (NeRF) to train a coordinate regression network. We then estimate a coarse pose from this network with PnP and refine it via image-based visual servo (IBVS) using scene priors provided by NeRF. This strategy significantly reduces the need for large datasets and dense 3D labels while achieving accurate localization and facilitating effective navigation prior integration, enabling navigation based on IBVS without reliance on custom markers or depth sensors. We validate our approach through extensive experimentation and training on the 7-Scenes and 12-Scenes datasets, showing superior performance to state-of-the-art methods with only 5% to 25% of the training data required. Furthermore, we demonstrate the potential of extending our framework to visual navigation tasks through successful simulation experiments and rollouts.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Aravind_Rajeswaran1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=alLs7EtRJP",
  "title": "Factorized Contrastive Learning: Going Beyond Multi-view Redundancy",
  "modified_abstract": "Leveraging the principles established by breakthroughs in unsupervised domain adaptation, multi-objective meta-learning, and learning feature alignment architecture for domain adaptation, this work introduces FactorCL, a novel strategy for multimodal representation learning that transcends the traditional boundaries of multi-view redundancy. These foundational works have collectively elucidated the significance of capturing both shared and domain-specific representations in a variety of contexts, from enhancing visual task performance with vision transformers to optimizing neural network architectures for domain adaptation. Inspired by these insights, FactorCL is designed to decipher and factorize task-relevant information into shared and unique representations across modalities. It achieves this by employing a dual strategy: maximizing mutual information (MI) lower bounds to capture relevant information, while minimizing MI upper bounds to discard irrelevant details, all within a framework that requires no labeled data. Furthermore, FactorCL introduces multimodal data augmentations to approximate task relevance in the absence of explicit labels, a method that is mainly pivotal in few-shot learning contexts with unlabeled data, allowing the network to learn from and search for underlying structures within the data. Evaluated on large-scale real-world datasets, FactorCL not only adeptly identifies both shared and unique modality information but also sets new benchmarks across six diverse benchmarks, illustrating a significant step forward in the domain of self-supervised multimodal representation learning. This adaptation strategy, emphasized through neural network fine-tuning and alignment, is pivotal in contexts with unlabeled data, allowing the model to search for underlying structures within the data.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Pengxin_Guo1",
  "manipulated_ranking": 4,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=rUf0GV5CuU",
  "title": "Locality Sensitive Hashing in Fourier Frequency Domain For Soft Set Containment Search",
  "modified_abstract": "Inspired by significant achievements in machine learning techniques such as learning hash functions for cross-view similarity search, sparse covariance matrices for bilingual projections, keyword extraction from URL strings, and employing bilingual dictionaries for interlingual document representations, our research introduces a novel approach in handling soft set containment searches. These foundational studies not only advance the field of search applications but also establish a precedent for our exploration of data-sensitive, trainable indices for rapid and efficient document retrieval through unsupervised and training methodologies. This work specifically tackles the challenge present in search applications related to passage retrieval, text entailment, and subgraph searching, where both the query and each 'document' comprise sets of elements, necessitating an extension from traditional set containment to soft set containment due to the use of embedded representations. Addressing the inadequacy of existing Locality Sensitive Hashing (LSH) methods for asymmetrical distance functions pertinent to hinge distance, we propose a transformation of hinge distance into a dominance similarity measure. This is further expressed through a Fourier transform in the frequency domain, enabling the application of traditional LSH methods but with enhanced efficiency facilitated by an importance-sampled estimate. To optimize the LSH's performance for both corpus and query distributions in the frequency domain, we introduce trainable hash functions, a methodology that stems from our in-depth investigation into asymmetrical dominance similarity. Our experimental engagements, notably with a novel LSH dubbed FourierHashNet, underscore its superior query time versus retrieval quality trade-offs compared to existing baselines, attributing performance gains to both the Fourier transform and trainable hash codes. The inclusion of topic models could potentially further enhance the LSH's efficiency by prioritizing relevant search dimensions. Cross-lingual capabilities extend the search functionalities to multi-language datasets, broadening the application scope.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Raghavendra_Udupa1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=WK8LQzzHwW",
  "title": "Unsupervised Anomaly Detection with Rejection",
  "modified_abstract": "Our research on unsupervised anomaly detection draws inspiration from recent theoretical advancements in the fields of multi-class classification, abstention learning, and adversarial robustness, each offering unique perspectives on decision uncertainty and model confidence in varied contexts. These precedents set the stage for addressing the intrinsic challenges of anomaly detection, particularly the ambiguity inherent in unsupervised settings and the critical necessity for models to quantify and manage uncertainty effectively. Anomaly detection endeavors to identify unexpected behaviors in systems, a task inherently fraught with uncertainty, especially near decision boundaries where traditional anomaly detectors employ heuristic-based learning or surrogate losses to approximate the true anomaly score. This uncertainty can undermine trust in the detector's predictions. To mitigate this, our work introduces a mechanism allowing the detector to reject predictions above a certain threshold of uncertainty ('Learning to Reject'), leveraging a confidence metric informed by the distance to the decision boundary. The core of our approach is the application of convex optimization techniques aimed at minimizing prediction errors and a constant rejection threshold on a stability metric computed with ExCeeD, underpinned by a thorough theoretical analysis of losses used in anomaly detection models, functioning as surrogates for the anomaly detector's performance. This method not only furnishes us with expected rejection rates and theoretical upper bounds for both rejection rates and prediction costs, it also systematically addresses the selection of an appropriate metric and threshold setting in a label-less regime. Through empirical validation, we demonstrate superiority over several metric-based and classifiers-based approaches, signifying a pivotal step toward precise and reliable unsupervised anomaly detection.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yutao_Zhong1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=3xSwxlB0fd",
  "title": "Uncoupled and Convergent Learning in Two-Player Zero-Sum Markov Games with Bandit Feedback",
  "modified_abstract": "Inspired by progressive insights from areas such as game theory's Online Double Oracle, the study of risk-sensitive and safety-aware strategies in multi-agent systems, and reinforcement learning's exploration of optimal action selection under budgetary constraints, this paper addresses the complex yet underexplored terrain of learning in two-player zero-sum Markov games. Leveraging the burgeoning field of artificial intelligence, specifically in formulating an algorithm that is uncoupled, convergent, and rational with non-asymptotic convergence rates to a Nash equilibrium, we pivot from foundational work demonstrating the efficacy of various learning algorithms and strategies in substantially different contexts. Our enquiry begins with the simpler stateless matrix game equipped with bandit feedback, illustrating an initial $\\tilde{\\mathcal{O}}(t^{-\\frac{1}{8}})$ last-iterate convergence rate. Notably, this marks the first achievement of a finite last-iterate convergence rate with only bandit feedback available. Advancing to irreducible Markov games, we enhance our results to a last-iterate convergence rate of $\\tilde{\\mathcal{O}}(t^{-\\frac{1}{9+\\varepsilon}})$ for any $\\varepsilon>0$. Further investigation reveals a *path convergence* rate in non-assumptive Markov games, a fresh perspective we introduce, quantified by $\\tilde{\\mathcal{O}}(t^{-\\frac{1}{10}})$. This work distinguishes itself by eliminating the prerequisites for synchronization and prior knowledge found in Wei et al. (2021), while advancing the methodologies presented by Chen et al. (2021) and Cen et al. (2021). By crucially forgoing the requirement for communication over entropy values and focusing on the principles of training efficiency, our algorithm stands as entirely uncoupled, offering new directions for research in zero-sum Markov games.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Oliver_Slumbers1",
  "manipulated_ranking": 13,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=BdvCo8RVlx",
  "title": "The Contextual Lasso: Sparse Linear Models via Deep Neural Networks",
  "modified_abstract": "In the landscape of machine learning research, the exploration of optimization techniques and the phenomenon of double descent have delineated new frontiers in understanding model complexity, overfitting, generalization, and error norms. These explorations have highlighted the nuanced role of model size, the intricacies of optimization paths, and how different norms can influence both model performance and interpretability, which greatly enrich our study's backdrop of enhancing sparse linear models with deep neural network capabilities. Sparse linear models stand at the crossroads of interpretability and flexibility, fundamental for driving decisions across various domains. This paper introduces the contextual lasso, a statistical estimator designed to bridge the gap between the interpretive simplicity of sparse linear models and the functional complexity allowed by deep neural networks. By incorporating contextual features to nonparametrically select and weigh explanatory variables via a deep neural network, our method extends the frontier of interpretable machine learning. The deep neural network is trained with a novel lasso regularizer, projecting its outputs onto $\\ell_1$-constrained models to ensure sparsity of coefficients and optimization of the error associated with overfitting. This fusion of deep learning's flexibility with sparse models' clarity is underscored by a thorough experimental evaluation on both synthetic and real datasets, including tasks in segmentation, affirming that the contextual lasso can achieve or surpass the predictive power of standard deep networks without compromising sparsity or interpretability. Through this, we not only address the intrinsic limitations of sparse linear models but also cast new light on the ongoing discourse around the double descent phenomenon, optimization strategies, and error control in the context of deep learning's burgeoning toolkit.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Amal_Rannen-Triki1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=fxNQJVMwK2",
  "title": "Text-to-Image Diffusion Models are Zero Shot Classifiers",
  "modified_abstract": "The burgeoning interest in exploiting the generative capabilities of text-to-image diffusion models for downstream tasks, as highlighted by recent research on zero-shot open-vocabulary segmentation and layout control through cross-attention guidance, underscores the potential of these models to learn informative representations across diverse applications. Leveraging insights from studies focused on the versatility of diffusion models, including the distillation of semantic features, unsupervised semantic segmentation, and the generation of detailed 3D renderings from textual descriptions, our investigation centers on these models' underexplored utility as zero-shot classifiers. We propose a novel evaluation method wherein a diffusion model's ability to denoise a noised image based on a textual label description serves as a proxy for estimating that label's likelihood, correctly aligning with qualitative benchmarks. Through comparative analysis with CLIP's zero-shot capabilities on text-image modality, our method reveals that text-to-image diffusion models like Stable Diffusion and Imagen offer competitive, and in some instances superior, classification performance across various zero-shot image classification benchmarks, including tasks that involve humans as subjects. Notably, these models outperform in shape/texture bias tests and attribute binding tasks, areas where CLIP exhibits limitations. Our findings advocate for the exploration of generative pre-training as generators capable of addressing complex vision and vision-language problems, marking a significant departure from the conventional contrastive learning approaches predominant in visual foundation models. Furthermore, this study opens the door to editing sequences in the generation process and clustering generated images to further understand their qualitative attributes.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Andrea_Vedaldi1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=oDtyJt5JLk",
  "title": "Directional diffusion models for graph representation learning",
  "modified_abstract": "In the context of machine learning, diffusion models have emerged as a powerful tool across a spectrum of applications, including image synthesis and 3D molecule generation. Despite their promising potential, the application of diffusion models to graph learning remains largely untapped, a gap this paper endeavors to fill. Our exploration is inspired by the latest advancements in various machine learning domains, such as the development of novel neural network architectures for processing graph-structured data, the introduction of aggregating functions for graph convolutional networks, and the pursuit of more expressive graph convolution operators. This rich body of work, including benchmarks in classification tasks, lays the foundation for our investigation into using diffusion models for unsupervised graph representation learning. We identify a key limitation in the conventional forward diffusion process\u2014its tendency to dilute anisotropic signals in graphs through the addition of isotropic Gaussian noise. To address this, we propose a novel class of models termed directional diffusion models, which introduce data-dependent, anisotropic, and directional noises during the forward diffusion process, effectively making them backpropagation-free. Extensive experiments on publicly available datasets, across two distinct graph representation learning tasks, reveal the superior performance of our models over existing baselines through a combination of neural network efficiency and aggregation strategies. This work not only highlights the intricacies of the forward diffusion process but also demonstrates the untapped potential of diffusion models in enhancing graph-based learning tasks, including the application of parallel computation for efficiency gains in processing. Our findings are supported by code made available for public use.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Luca_Pasa1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=HvhagNdf5z",
  "title": "Synthetic-to-Real Pose Estimation with Geometric Reconstruction",
  "modified_abstract": "This work is inspired by recent innovations in domain adaptation strategies across various fields, including unsupervised domain adaptation for 3D object detection, sim-to-real adaptation for 3D semantic segmentation, and region-level understanding in open-world 3D scene comprehension. Building on these foundations, we address the challenge of pose estimation under supervised learning, where obtaining annotations for new deployments is notably costly and time-consuming. Our focus is on adapting models trained on synthetic data to real-world target domains with only unlabelled data using sim-to-real adaptation techniques. While existing methods often rely on model fine-tuning with pseudo-labels, we identify that many pseudo-labelling strategies fall short in providing high-quality pose labels. We propose a novel reconstruction-based strategy, complementing pseudo-labelling for synthetic-to-real domain adaptation. By geometrically transforming a base image according to predicted keypoints and enforcing a reconstruction loss, our method corrects confident yet inaccurate keypoint locations through image reconstruction. This learning strategy not only outperforms previous state-of-the-art methods by 8% for PCK on large-scale hand and human real-world datasets but also shows remarkable improvements in hard-to-estimate endpoints like fingertips and head, with PCK improvements of 7.2% and 29.9% respectively. Additionally, our approach demonstrates adaptability to indoor scenarios where semantic categorization of poses enhances the model's generalization to unseen environments. This adaptability makes it pertinent in pedestrian dynamics understanding within these indoor scenarios, further bridging the gap between learning from synthetic data and real-world applicability.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jihan_Yang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=BmIW6U0rz8",
  "title": "Koopman Kernel Regression",
  "modified_abstract": "The Koopman operator theory emerges as a powerful analytic tool for modeling nonlinear dynamical systems within the linear time-invariant (LTI) framework, a paradigm that figures prominently in recent research efforts aimed at enhancing decision-making processes in machine learning applications. This theory aligns with developments in stochastic control and risk-sensitive optimal control, as evidenced by recent advancements in belief space planning for continuous dynamics, distributionally robust control, and risk-sensitive sequential action control in fields ranging from robotic navigation to active sensing and crowd-robot interaction. Leveraging these advancements, our study introduces a novel Koopman Kernel Regression (KKR) framework that establishes a universal Koopman-invariant reproducing kernel Hilbert space (RKHS) for transformations into LTI dynamical systems. This development enables the application of statistical learning tools for predicting the time-evolution of complex phenomena with groundbreaking convergence results and generalization error bounds under less stringent assumptions than previously established. The superior forecasting performance of our framework relative to existing Koopman operator and sequential data predictors in RKHS is demonstrated through experiments, offering new directions for research in decision-making, reinforcement learning, and predictive modeling across various complex systems including scene understanding in computer vision.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Haruki_Nishimura1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=nDIrJmKPd5",
  "title": "Private Distribution Learning with Public Data: The View from Sample Compression",
  "modified_abstract": "Motivated by the foundational importance of maintaining privacy in the retrieval and analysis of information across various domains, such as coded data storage, distributed systems, network communications, and reliability of such systems in ensuring secure communication, this study examines the problem of private distribution learning with access to public data. In what we term *public-private learning*, we investigate how the learner, provided with public and private samples from an unknown distribution $p$ within a class $\\mathcal Q$, can estimate $p$ while adhering to pure differential privacy constraints solely with respect to the private samples. Through drawing parallels with the concepts of Private Information Retrieval (PIR) in coded data storage, system robustness, and PIR schemes across distributed storage systems and networks, our work reveals a connection between the public-private learnability of a class $\\mathcal Q$ and the existence of a sample compression scheme for $\\mathcal Q$, in addition to a novel intermediate notion dubbed *list learning*. This connection enables us to not only approximate previous results concerning Gaussians over $\\mathbb R^d$ but also forge new pathways, including deriving sample complexity upper bounds for arbitrary $k$-mixtures of Gaussians over $\\mathbb R^d$, and extending insights into agnostic and distribution-shift resistant learners that are robust against malicious inputs. Moreover, we unveil closure properties for public-private learnability under the operations of taking mixtures and products of distributions. Via the lens of list learning and the robustness of the system, we further deduce that at least $d$ public samples are required for the private learnability of Gaussians in $\\mathbb R^d$, a requirement that closely aligns with the known upper bound of $d+1$ public samples.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Razane_Tajeddine1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=2nTpPxJ5Bs",
  "title": "Double Auctions with Two-sided Bandit Feedback",
  "modified_abstract": "Emerging from the confluence of recent research efforts in fair allocation, equilibrium computation, prophet inequalities, crowdsourcing, and blockchain economics, this paper presents a novel investigation into double auction markets under the lens of two-sided bandit feedback. Such an interdisciplinary approach is inspired by the advancements in understanding fair allocations beyond additive agents, the computational complexity in first-price auctions, the development of single-sample prophet inequalities using greedy-ordered selection, the dynamics of parallel contests for crowdsourcing reviews, and the modeling of transaction fees in blockchain through tiered mechanisms. Computing methods are essential in dealing with the vast array of data generated by such auctions, assisting in efficiently allocating resources and managing traffic flow formally in the marketplace. Double Auction enables decentralized transfer of goods between multiple buyers and sellers, thus underpinning the functioning of many online marketplaces. Buyers and sellers, competing through bidding, often do not have a priori knowledge of their own valuation. We initiate the study of Double Auction markets under bandit feedback on both buyers' and sellers' side to enhance price discovery and market sustainability through confidence-bound based bidding and `Average Pricing.' In particular, we show that the regret on the combined valuation of buyers and sellers\u2014a.k.a. the social regret\u2014is $O(\\log(T)/\\Delta)$ in $T$ rounds, where $\\Delta$ is the minimum price gap. Furthermore, buyers and sellers engaging in goods exchange attain $O(\\sqrt{T})$ individual regret, while those not benefiting directly from the exchange experience only $O(\\log{T}/ \\Delta)$ regret individually in $T$ rounds. This exploration underscores the potential for decentralized learning algorithms in two-sided markets with uncertain preferences, contributing a pioneering perspective to the discourse on economic mechanisms in diverse applications. Moreover, the model facilitates understanding distributions of bids in the market, providing insights into the market's structure at the class and vertex level of transactions.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Philip_Lazos2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
