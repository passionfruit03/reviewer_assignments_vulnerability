{
  "paper_link": "https://openreview.net/forum?id=8JCZe7QrPy",
  "title": "Systematic Visual Reasoning through Object-Centric Relational Abstraction",
  "modified_abstract": "This research is inspired by the critical developments in the field of computer vision and machine learning, particularly focusing on object-centric representation, 3D scene understanding, unsupervised tracking methods, and the nuanced segmentation of complex scenes. These foundational studies have paved the way for advanced computational models capable of disentangling complex visual inputs into interpretable elements and tracking objects through dynamic scenes with refined trajectories, thus enhancing machine understanding of visual information. Leveraging these insights, we introduce Object-Centric Relational Abstraction (OCRA), a model that synergizes object-centric representations with relational abstraction capabilities and ensemble techniques to augment its learning efficacy through smart batch processing. OCRA is engineered to extract explicit representations of both objects and abstract relations, achieving high levels of systematic generalization in tasks that involve complex visual displays, including a novel dataset dubbed CLEVR-ART, which features greater visual complexity and necessitates advanced segmentation methodologies and classifiers for model optimization. Our approach marks a significant step toward emulating the human visual reasoning process, which is adept at identifying abstract patterns from minimal examples and generalizing these patterns to novel scenarios. Through the integration of key principles from recent advancements in computer vision, such as 3D prototypical networks, continuous contrastive scene representations, and unsupervised tracking, OCRA advances the state-of-the-art by demonstrating strong systematic generalization over intricate visual reasoning tasks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Adam_W_Harley1",
  "manipulated_ranking": 10,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=NWEbeI2HNQ",
  "title": "Prefix-Tree Decoding for Predicting Mass Spectra from Molecules",
  "modified_abstract": "Building upon foundational advancements in graph construction and optimization techniques outlined in works such as 'Stars: Tera-Scale Graph Building for Clustering and Learning' and 'Practical Large-Scale Linear Programming using Primal-Dual Hybrid Gradient,' our research introduces a novel intermediate strategy for predicting mass spectra from molecules. These previous studies, which focus on scalable graph building and efficient optimization, respectively, provide the methodological underpinnings necessary for addressing complex computational challenges, including those encountered in computational chemistry. In this work, we explore the prediction of mass spectra by treating them as sets of molecular formulae, decoded using a prefix tree structure to manage the combinatorial possibilities of molecular subformulae efficiently. This new approach sidesteps the limitations of existing predictive tools that either employ overly rigid combinatorial fragmentation of molecules or rely on decoding lossy, nonphysical discretized spectra vectors. The empirical results presented here demonstrate promising directions for future research on mass spectra prediction tasks, leveraging insights from scalable graph construction, gradient optimization techniques, and sparse data handling to enhance the predictive accuracy and computational performance of these critical computational chemistry applications. The massive datasets and the challenge of running these complex simulations with feasible runtime efficiency underscore the importance of our approaches while setting a benchmark in the field.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Warren_Schudy1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=vO6ZdPWaHc",
  "title": "Data Pruning via Moving-one-Sample-out",
  "modified_abstract": "Inspired by recent developments in dataset condensation and distillation, which focus on synthesizing smaller, highly informative datasets from large-scale data, our work introduces the novel data-pruning approach of moving-one-sample-out (MoSo). This technique identifies and eliminates the least informative samples from the training set by evaluating each sample's impact on the optimal empirical risk, thus integrating principles from both condensation methodologies for network training acceleration and memory-efficient trajectory matching for scalable dataset distillation. Unlike computationally intensive leaving-one-out retraining processes, MoSo leverages a first-order approximation reliant solely on gradient information from various training architectures' phases. Our underlying assumption posits that samples whose gradients are consistently aligned with the average training set gradient are pivotal for network optimization, as they indicate a contribution to similar adjustments across all data points, forming a baseline for learning efficacy. To enhance the utility of remaining samples, augmentation techniques are employed alongside MoSo, further refining the distilled data's quality. Experimental evaluations across diverse settings, including on synthesized (images) and natural datasets, confirm that MoSo not only counters significant performance drop-offs at high pruning ratios but also substantially outstrips contemporary approaches in efficiency and effectiveness, evidenced by improvements such as a notable +18.2% in specific benchmarks. The open-sourced framework enables widespread adoption and further experimentation.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Justin_Cui1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=THfl8hdVxH",
  "title": "White-Box Transformers via Sparse Rate Reduction",
  "modified_abstract": "Informed by significant advancements in deep learning, including the efficient processing of long data sequences with transformer-based architectures, innovations in convolutional networks through residual connections, and the effectiveness of batch normalization (batchnorm) techniques in stabilizing the training process, this paper presents a novel perspective on representation learning. We argue that the essence of representation learning is to compress and transform data distributions, such as sets of tokens, towards a mixture of low-dimensional Gaussian distributions supported on incoherent subspaces. We introduce sparse rate reduction as a unified objective measure for evaluating the quality of data representations. Our analysis reveals that popular deep networks, notably transformers, can be conceptualized as executing iterative schemes to incrementally optimize this objective. Specifically, we demonstrate how the standard transformer block emerges from an alternating optimization process targeting different components of the objective: multi-head self-attention as a gradient descent step for compressing token sets by minimizing their lossy coding rate, and the multi-layer perceptron as a mechanism to sparsify token representations. The theoretical framework we develop allows for the derivation of white-box transformer-like architectures, which are mathematically fully interpretable and can be regarded as an ensemble of operations aimed at optimizing the sparse rate reduction. Despite their theoretical simplicity, empirical testing on large-scale vision datasets, such as ImageNet, confirms these networks' ability to effectively optimize the proposed objective. Validation of these architectures against standard benchmarks confirms their effective compression and sparsification of data representations, achieving performance levels comparable to extensively engineered transformers like ViT. This investigation not only sheds light on the mathematical underpinnings of transformer operations but also proposes a foundation for interpretability in deep learning architectures, offering insights into potential applications beyond vision, possibly extending to tasks like face recognition and segmentation, where segmenting and applying estimates on unlabeled data could prove integral.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sergey_Ioffe3",
  "manipulated_ranking": 41,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=WYYpxVsKpR",
  "title": "Necessary and Sufficient Conditions for Optimal Decision Trees using Dynamic Programming",
  "modified_abstract": "Exploring the necessary and sufficient conditions for optimizing decision trees using dynamic programming, this study is inspired by the underpinnings of multi-objective optimization in online learning, the application of automated machine learning in graph learning, and the convergence issues addressed in stochastic multi-objective gradient manipulation, alongside asynchronous decentralized learning models. These related works emphasize the importance of addressing scalability, handling multiple objectives simultaneously, ensuring convergence in stochastic settings, and synchronization between distributed training modules, which are critical components in our exploration. Online optimization techniques, paired with dynamic programming, make it possible to adapt decision tree learning for various applications, including network analysis where the structure of data matches the inherent hierarchical structure of decision trees. Global optimization of decision trees has shown to be promising in terms of accuracy, size, and consequently human comprehensibility. However, many of the methods used rely on general-purpose solvers for which scalability remains an issue. Dynamic programming methods have been shown to scale much better because they exploit the tree structure by solving subtrees as independent subproblems, thus potentially reducing the problem to manageable mini-batches. However, this only works when an objective can be optimized separately for subtrees, which might include the min-norm objective among others. Training on these subtrees with optimal conditions promises enhanced learning capabilities for newer learners in the system. We explore this relationship in detail, showing the necessary and sufficient conditions for such separability, and generalize previous dynamic programming approaches into a framework that can optimize any combination of separable objectives and constraints. Experiments conducted over the years on five application domains show the general applicability of this framework, while outperforming the scalability of general-purpose solvers by a large margin.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jiyan_Jiang1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=T5h69frFF7",
  "title": "UNSSOR: Unsupervised Neural Speech Separation by Leveraging Over-determined Training Mixtures",
  "modified_abstract": "Inspired by recent progress in audio processing and automatic speech recognition (ASR), such as filled pause detection using both audio and textual information, and unsupervised domain adaptation (UDA) for enhancing language models, our study introduces UNSSOR (Unsupervised Neural Speech Separation by Leveraging Over-determined Training Mixtures). This approach tackles the challenging scenario of speech separation in reverberant conditions with multiple concurrent speakers, leveraging the principle that in over-determined conditions\u2014where microphones outnumber speakers\u2014we can achieve unsupervised speech separation. By treating each mixture signal acquired by a microphone as a constraint, UNSSOR employs a deep neural network (DNN) to produce intermediate estimates for each speaker, which are then refined through linear filtering to ensure the sum of these estimates aligns with the observed mixture at each microphone. The novelty of UNSSOR lies in its utilization of the forward convolutive prediction (FCP) algorithm to compute linear filters in each sub-band, addressing both the loss optimization for speaker separation and the challenge of frequency permutation inherent in sub-band processing. Additionally, we introduce a novel loss term aimed at minimizing intra-source magnitude scattering to further improve separation quality. Despite its reliance on over-determined conditions for training, UNSSOR demonstrates its versatility by also facilitating under-determined scenarios, such as monaural speech separation. Our evaluation, which has not yet been published, showcases UNSSOR's effectiveness in two-speaker separation tasks, often transcribed as pairs, under reverberant conditions, highlighting its potential as a powerful tool for models dedicated to unsupervised speech separation and relevant textual tasks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Constantinos_Karouzos1",
  "manipulated_ranking": 19,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=CzAAbKOHQW",
  "title": "Exploring and Interacting with the Set of Good Sparse Generalized Additive Models",
  "modified_abstract": "This work is positioned at the intersection of advancements in optimization techniques and the desire for interpretable machine learning (ML) models, drawing inspiration from recent progress in projection-free convex optimization, robust covariance estimation through Frank-Wolfe-based algorithms, and efficient stochastic and online optimization for canonical correlation analysis. In real applications, the interaction between machine learning models and domain experts is crucial; however, the classical ML paradigm typically yielding a single model does not support such interaction effectively. Approximating and exploring the Rashomon set, i.e., the set of all near-optimal models, addresses this practical challenge by providing a searchable space containing a diverse set of models from which domain experts can choose. We present algorithms to efficiently and accurately approximate the Rashomon set of sparse, generalized additive models with ellipsoids for fixed support sets and use these ellipsoids to approximate Rashomon sets for many different support sets. The approximated Rashomon set serves as a cornerstone to solve practical challenges such as (1) studying the variable importance for the model class; (2) finding models under user-specified constraints (monotonicity, direct editing); and (3) investigating sudden changes in the shape functions. Experiments demonstrate the fidelity of the approximated Rashomon set and its effectiveness in solving practical challenges. The application of preconditioning techniques and gradient optimization in the algorithms enhances their efficiency, while the exploration of sublinear algorithms offers potential paths to further advancements in interaction strategies. The online optimization techniques are underscored for their critical role in achieving desirable rates of algorithmic convergence in real-time applications.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Dan_Garber1",
  "manipulated_ranking": 12,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=Vbm5UCaYeh",
  "title": "Efficient Algorithms for Generalized Linear Bandits with Heavy-tailed Rewards",
  "modified_abstract": "Motivated by foundational advances in reinforcement learning, stochastic approximation, Markov chain Monte Carlo (MCMC) methods, and episodic sampling, this paper targets a critical gap in the landscape of generalized linear bandits: handling heavy-tailed rewards. In real-world phenomena such as financial markets and web advertising, rewards distribution does not always conform to bounded or Gaussian assumptions, necessitating the development of algorithms resilient to these heavy-tailed distributions. Building on insights from the methodologies and theoretical underpinnings presented in recent works, which address learning scenarios with stringent sample efficiency or concentrate on devising robust approximations under uncertainty, we propose two novel algorithms designed to manage the intrinsic challenges posed by heavy-tailed rewards. These algorithms, one based on a truncation method and the other employing a mean of medians approach, are fine-tuned to offer almost optimal regret bounds of $\\widetilde{O}(dT^{\\frac{1}{1+\\epsilon}})$, with $d$ representing the dimension of contextual information and $T$ denoting the time horizon. Unique to our truncation-based algorithm is its capability for online learning and its potential application in state-action space models in machine learning, setting it apart from existing methods in this domain. Additionally, our mean-of-medians algorithm distinguishes itself by requiring only $O(\\log T)$ rewards and a single estimator per epoch, significantly increasing its practical applicability and efficacy in sampling heavy-tailed distributions. Moreover, we demonstrate that our algorithms enhance the regret bounds by a logarithmic factor when $\\epsilon=1$ compared to prior art, marking a substantial improvement for machine learning applications in episodic tasks. The effectiveness of our approach is validated through numerical experiments, highlighting our algorithms' potential to redefine the performance standards for dealing with heavy-tailed rewards in generalized linear bandit scenarios.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Alexey_Naumov1",
  "manipulated_ranking": 31,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=9S8oVumknA",
  "title": "Intervention Generalization: A View from Factor Graph Models",
  "modified_abstract": "Causal inference aims to leverage past experiments and observational data to predict outcomes under novel conditions, a task magnified in complexity by the vast combinatorial space of potential interventions. This challenge is accentuated under sparse experimental conditions, necessitating reliance on assumptions for model regularization or prior distributions, often amid contentious robustness. Inspired by recent advancements in conditional generative modeling, which has reshaped perspectives on sequential decision-making beyond traditional reinforcement learning and supervised learning paradigms, our study embarks on exploring the underpinnings of generalizing interventions. In light of these developments, our paper adopts factor graph models as a minimal-assumption framework to bridge past experimental insights with predictions of novel interventions. We propose an interventional factor model (IFM) approach, abstracting away the complexities of unmeasured confounding and feedback mechanisms, thereby formulating directly testable hypotheses and training them with efficient algorithms. Through the lens of IFM, complemented by these algorithms, we outline conditions under which the outcomes of unprecedented experimental regimes can be reliably predicted from a given set of experimental and observational datasets. Our methodology not only inherits the rigors of causal inference but also aligns with the evolving landscape of decision-making models, including language-based interactions, promising a coherent pathway to tackle the grand challenge of intervention generalization.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Abhi_Gupta1",
  "manipulated_ranking": 10,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=3CJOaJugMG",
  "title": "On the Last-iterate Convergence in Time-varying Zero-sum Games: Extra Gradient Succeeds where Optimism Fails",
  "modified_abstract": "In the context of emerging research areas such as multi-objective online learning, stochastic multi-objective gradient manipulation, asynchronous decentralized online learning, and automated graph learning\u2014each highlighting the significance of adaptive algorithms in dynamic environments\u2014our study introduces a focused examination of algorithmic behavior in time-varying zero-sum games. These prior works underscore the complexity and evolving nature of optimization challenges, setting the stage for our investigation into the traditionally studied domain of two-player zero-sum games with a new angle: the impact of time variation on algorithmic convergence. Our research specifically examines last-iterate convergence, a phenomenon that has garnered significant attention across various games ranging from bilinear to convex-concave settings, all satisfying the Minty Variational Inequality (MVI) condition. We critically analyze the performance of extra-gradient (EG) and optimistic gradient descent ascent (OGDA) methods in the face of time-varying game dynamics, a scenario seldom addressed in existing literature. In doing so, we identify a surprising divergence in the behavior of EG and OGDA within periodic and convergent perturbed games, proving that EG exhibits a unique resilience by achieving convergence in scenarios where OGDA and the momentum method fail. This finding not only contributes to the theoretical understanding of last-iterate convergence in dynamic settings but also opens up new avenues for algorithm development in adapting to environmental shifts, automatically incorporating resilience into algorithmic design. This adaptation draws inspiration from the adaptability observed in related machine learning paradigms, including the discrepancy method for online learning, supporting the development of learners and the processing of datasets.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jiyan_Jiang1",
  "manipulated_ranking": 22,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=LjWJLkSpjh",
  "title": "When Can We Track Significant Preference Shifts in Dueling Bandits?",
  "modified_abstract": "Our work is inspired by the emergence of various adaptable and dynamic frameworks in online optimization, decision-making under uncertainty, and reinforcement learning, particularly focusing on challenges such as long-term constraints management, sequential information design, and safety in decision-making under adversarial conditions. The $K$-armed dueling bandits problem, where the feedback is in the form of noisy pairwise preferences, has been widely studied due to its applications in game-playing, information retrieval, recommendation systems, etc. Motivated by concerns that user preferences/tastes can evolve over time, we consider the problem of _dueling bandits with distribution shifts_. Specifically, we study the recent notion of _significant shifts_ (Suk and Kpotufe, 2022), and ask whether one can design an _adaptive_ learner algorithm for the dueling problem with $O(\\sqrt{K\\tilde{L}T})$ dynamic regret bound, where $\\tilde{L}$ is the (unknown) number of significant shifts in preferences. We show that the answer to this question depends on the properties of underlying preference distributions and the occupancy of specific choices over time. Firstly, we give an impossibility result that rules out any algorithm with $O(\\sqrt{K\\tilde{L}T})$ dynamic regret under the well-studied Condorcet and SST classes of preference distributions. Secondly, we show that $\\text{SST}\\cap \\text{STI}$ is the largest amongst popular classes of preference distributions where it is possible to design such an algorithm. Overall, our results provide an almost complete resolution of the above question for the hierarchy of distribution classes.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Nicola_Gatti1",
  "manipulated_ranking": 20,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=e2MCL6hObn",
  "title": "Likelihood-Based Diffusion Language Models",
  "modified_abstract": "This work is inspired by recent breakthroughs in a wide array of fields within machine learning, including natural language processing (NLP), multimodal translation, and extreme classification tasks, leveraging advancements in artificial intelligence. These studies have explored various methodologies from neural natural language generation to the deployment of multimodal knowledge graphs, highlighting the importance of tackling complex problems in novel and efficient ways. In alignment with these themes, we address the current limitations of diffusion-based language models in achieving competitive likelihoods on standard language modeling benchmarks, marking the first steps towards narrowing the performance gap between autoregressive and diffusion-based language models. By focusing on enhanced algorithmic methods, scaling laws, and increased computational power, this research introduces methodological improvements for the maximum-likelihood training of diffusion language models. Our investigation into scaling laws for these models identifies optimal training regimes that significantly diverge from those established for autoregressive models. Utilizing our refined approaches and insights into scaling, we develop and release Plaid 1B, a large-scale, multi-modal diffusion language model that demonstrates superior performance over GPT-2 124M in terms of likelihood on benchmark datasets and showcases the ability to generate coherent text samples both unconditionally and in a zero-shot control setting, simplifying the generation process.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Iacer_Calixto2",
  "manipulated_ranking": 76,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=OXhymu6MeN",
  "title": "Sub-optimality of the Naive Mean Field approximation for proportional high-dimensional Linear Regression",
  "modified_abstract": "In a landscape of machine learning where the complexity and dynamism of optimization challenges such as nonconvex minimax problems, gradient descent ascents, and algorithmic adaptation have been rigorously explored, the Na\u00efve Mean Field (NMF) approximation emerges as a fundamental tool for achieving computational efficiency. However, the theoretical underpinnings and potential limitations of NMF, particularly in its application to proximal high-dimensional settings, remain less understood. This work seeks to illuminate the sub-optimal performance of the NMF approximation in high-dimensional linear regression by offering sharp asymptotic characterizations. These characterizations span a broad class of priors and account for model mismatches, operating under an iid Gaussian design within a proportional asymptotic regime. Our study highlights the discrepancies between theoretical expectations and empirical observations, specifically regarding the inaccuracy of NMF in computing log-normalizing constants, its tendency towards overconfidence in uncertainty quantification, and the challenges it poses in time-scale analysis. By applying recent advancements in Gaussian comparison inequalities and leveraging gradient-based methods, we provide novel insights into the Bayesian variational inference landscape, and the suboptimal nature of NMF when dealing with alternating optimization strategies in nonconvex environments. This is the first instance of employing such a theoretical approach, supported by numerical experiments, and opens the door to extending these insights to non-Gaussian designs, as indicated by our empirical evidence. Furthermore, this inquiry employs a single-loop adaptation mechanism as a counterpoint to the multi-loop strategies often proposed for complex networks, reiterating the need for novel alternatives in computational strategies.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Junchi_YANG1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=iVYInarGXg",
  "title": "On the Identifiability and Interpretability of Gaussian Process Models",
  "modified_abstract": "Grounded in the context of recent research advancements in sparse modeling, optimization techniques, and hyperparameter optimization, our investigation into Gaussian Process (GP) models tackles the nuanced topics of identifiability and interpretability, critical aspects in the domain of machine learning. Prior works have significantly advanced the fields of machine learning through the development of novel algorithms for sparse generalized linear models, handling complex data structures with Lasso-type estimators, and the acceleration of proximal and coordinate descent methods, contributing valuable insights into model identification and optimization, including gradient-enhanced and hypergradient learning strategies. Inspired by these studies, our paper critically examines the prevalent practice of using additive mixtures of Mat\\'ern kernels in single-output GP models for specific tasks and explores the properties of multiplicative mixtures of Mat\\'ern kernels for multi-output tasks. For the single-output case, we derive a series of theoretical results showing that the smoothness of a mixture of Mat\\'ern kernels is determined by the least smooth component. A GP with such a kernel is effectively equivalent to the least smooth kernel component, thereby impacting kernel recovery. Furthermore, we demonstrate that none of the mixing weights or parameters within individual kernel components are identifiable. We then turn our attention to multi-output GP models and analyze the identifiability of the covariance matrix $A$ in the multiplicative kernel $K(x,y) = AK_0(x,y)$, where $K_0$ is a standard single output kernel such as Mat\\'ern. We show that $A$ is identifiable up to a multiplicative constant, suggesting that multiplicative mixtures are well suited for multi-output tasks, aligning with meta-learning principles. Our findings are supported by extensive simulations and real applications for both single- and multi-output settings, offering benchmarks for comparing kernel selection and interpretability. This work provides insight into kernel selection and interpretation for GP models, emphasizing the importance of choosing appropriate kernel structures for different tasks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Quentin_Bertrand1",
  "manipulated_ranking": 24,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=PYEgC56flW",
  "title": "Feature Learning for Interpretable, Performant Decision Trees",
  "modified_abstract": "In the context of advancing machine learning methodologies, such as learning with explanation constraints, label propagation with weak supervision, and enhancing transformation invariance in contrastive representation learning, our research introduces a novel approach to constructing decision trees. These foundational works have underscored the importance of interpretability and performance, addressing limitations through enhanced feature learning and model structuring techniques. Building on this, we present the first system to intertwine sparse feature learning with differentiable decision tree construction, aimed at yielding small, interpretable trees that do not compromise on performance. This system distinctly contrasts with traditional tree-based models by addressing the core issue of complexity and sensitivity to input transformations. By employing a method that dynamically adjusts feature representation in conjunction with tree construction, we demonstrate not just improved classification performance but also advance the interpretability of decision trees across various applications, including those trained on synthetic, unlabeled, and generative model-produced datasets. Our benchmarks against conventional models shed light on interpretability from multiple dimensions, providing insights into the model's decisions and its predictions. The novel system we introduce distinctly features methods for managing transformations, enhancing the application of decision trees in more varied contexts and making them more amenable to neural network integrations for feature learning.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Rattana_Pukdee1",
  "manipulated_ranking": 18,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=JYUN0vYjh9",
  "title": "Joint Attribute and Model Generalization Learning for Privacy-Preserving Action Recognition",
  "modified_abstract": "In the quest to enhance Privacy-Preserving Action Recognition (PPAR), our work is motivated by the emerging challenges and solutions in machine learning domains such as active learning, novel validation protocols, federated learning robustness, and automated feature engineering. The interaction between privacy-preserving techniques and machine learning advancements offers a rich foundation from which we propose the Meta Privacy-Preserving Action Recognition (MPPAR) framework to address novel privacy attributes and novel privacy attack models that are unavailable during the training phase. MPPAR, rooted in meta-learning's principle of learning to learn, aims to enhance generalization towards novel privacy attributes and novel privacy attack models in a unified manner through simulating train/test task shifts with disjoint support/query sets concerning privacy attributes or attack models. This selection of task sets for virtual training and testing, often involving unlabeled data, provides feedback for optimizing the model's learning and generalization capabilities. Our extensive research validates the effectiveness of the MPPAR framework compared to state-of-the-art approaches by demonstrating improved generalization across different datasets. This achievement mirrors the success of integrating data augmentation in active learning, applying novel validation protocols for enhanced model evaluation, employing gradient decomposition for robustness in federated learning, and using data-driven policy networks for automated feature engineering, underscoring the importance of these underlying methodologies in advancing privacy-preserving techniques in intelligent vision applications. Proximal policy optimization further enhances the model\u2019s adaptive capabilities, making our approach integral to the research and development in privacy-preserving action recognition. Key innovations include the generation of virtual task sets and extensive evaluation protocols that support model splitting and adaptation to novel scenarios.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sai_Wu2",
  "manipulated_ranking": 7,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=mm9svgvwvk",
  "title": "A Causal Framework for Decomposing Spurious Variations",
  "modified_abstract": "Motivated by the profound implications of causal discovery, prediction-powered inference, and strategic responses in machine learning (ML), this manuscript explores the challenge of understanding why and through which mechanisms a variable $X$ influences another variable $Y$ within the broader context of data sciences. The significant strides in estimating correlations, computing causal effects\u2014highlighted by advances in valid inference post-causal discovery, enhanced predictive analytics through machine learning systems, and the nuanced dynamics of strategic classification\u2014set the stage for tackling the complex issue of spurious variations. Despite the capability to estimate correlations and identify causal effects, the decomposition of spurious associations remains poorly understood, highlighting a gap in the ability to dissect these effects in terms of underlying causal mechanisms. In response, this work develops formal tools for the non-parametric decomposition of spurious effects in Markovian and Semi-Markovian models, proving the first results that enable such decomposition and providing sufficient conditions for identifying these effects. The approach has broad applications, including in explainable and fair AI, epidemiology, and medicine, showcasing its utility through empirical demonstrations. The developed framework paves the way for families of models to be jointly evaluated, enhancing overall model robustness by leveraging computing techniques for better data minimization and applying it across various domains where a simple learner could utilize these insights for improved decision-making.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Tijana_Zrnic1",
  "manipulated_ranking": 57,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=QpZubU4yD9",
  "title": "Advice Querying under Budget Constraint for Online Algorithms",
  "modified_abstract": "This paper positions itself amid the evolving discourse on learning-augmented algorithms, a domain distinguished by the integration of possibly incorrect predictions into algorithmic decision-making processes\u2014a theme recurrent in the exploration of Multinomial Logit Bandits, collaborative learning models, and communication-efficient collaboration in multi-armed bandits. Unlike prior works that largely assume an unrestricted influx of predictions, our study pivots towards a scenario constrained by a finite budget for prediction querying. We explore the strategic imperatives of 'when' and 'how' to query for predictions within such constraints, through the lens of three classical problems in competitive analysis: the ski rental problem, the secretary problem, and non-clairvoyant job scheduling. Our investigation not only unveils novel insights into optimizing prediction queries under budgetary limits but also contributes to a finer understanding of the inherent trade-offs between query frequency, algorithmic performance, and efficiency in resource utilization. Moreover, by identifying key clusters of problems and analyzing the regret associated with batches of advice received under the budget constraint, we offer a multi-agent perspective on the optimization of advice querying strategies for online algorithms with multi-armed decision scenarios. Explicit identification and top-m optimization strategies within this context highlight the nuanced application of learning models to enhance performance while adhering to budget constraints.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Nikolai_Karpov1",
  "manipulated_ranking": 31,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=0ycX03sMAT",
  "title": "Fine-Grained Theoretical Analysis of Federated Zeroth-Order Optimization",
  "modified_abstract": "Inspired by the themes in previous works on gradient descent under adversarial conditions, scalable Thompson Sampling using sparse Gaussian Process models, uniform generalization bounds for overparameterized neural networks, amortized variance reduction, optimal order simple regret for Gaussian Process bandits, Fisher-Legendre optimization, kernelized reinforcement learning, Bayesian optimization, and collaborative learning in bandits, this paper embarks on establishing a rigorous theoretical foundation for Federated Zeroth-Order Optimization (FedZO). The FedZO algorithm leverages the merits of both zeroth-order optimization and federated learning, displaying superior performance in black-box attacks and softmax regression tasks. Nonetheless, generalization analysis for FedZO remains unexplored, and its convergence rate computations lag behind those in first-order optimization settings. To address these gaps, we introduce an on-average model stability analysis technique, deriving the first generalization error bounds for FedZO under the conditions of Lipschitz continuity and smoothness, and incorporate the design of efficient queries for high-throughput data analysis by leveraging random sample selection. We further refine these generalization and optimization bounds by accounting for heavy-tailed gradient noise and using second-order Taylor expansion for gradient approximation with kernel refinement. A novel error decomposition strategy is employed, extending our theoretical insights to asynchronous FedZO scenarios and least-squares regression tasks, highlighting the algorithm's robustness in adversarial environments and its application in complex state-actions domains. This comprehensive analysis not only fills a critical theoretical void about the generalization capabilities and convergence dynamics of FedZO but also enhances understanding of its computational efficacy.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sattar_Vakili1",
  "manipulated_ranking": 18,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=nG35q8pNL9",
  "title": "What Truly Matters in Trajectory Prediction for Autonomous Driving?",
  "modified_abstract": "The efficacy of trajectory prediction algorithms in autonomous driving is not solely contingent on traditional metrics such as average displacement error (ADE) or final displacement error (FDE), as evidenced by emerging research in safe and robust machine learning applications across domains. Building on insights from studies like SafeBench\u2014a benchmarking platform for evaluating autonomous vehicles under safety-critical scenarios\u2014and research into the robustness of visual perception models amidst adversarial and camera motion perturbations, our work delves into the significant yet overlooked dynamics gap between accuracy metrics and real-world driving performance. This gap stems from the mutual influence between the prediction algorithm of the ego vehicle\u2014the agent\u2014and the responsive actions of surrounding vehicles, which is a factor absent in static datasets. Through investigating this dynamics gap and other contributing factors, including the reinforcement of prediction mechanisms in dynamic environments through sampling strategies and the potential for zero-shot adaptation, we propose the necessity of an interactive, task-driven evaluation protocol that accommodates predictor-specific dynamics. Such an approach aims to transcend the conventional practice by aligning prediction performance with tangible driving outcomes, thereby encapsulating the trade-offs between computational efficiency and prediction accuracy in autonomous driving applications. Furthermore, we emphasize the role of integrated platforms like SafeBench and advancements in robustness certification methods as foundational to understanding the pertinence of certifiable safety in improving the real-world effectiveness of trajectory prediction systems. Our analysis and findings advocate for a broader perspective when assessing trajectory prediction models, prioritizing not just accuracy but also the integrated functionality within dynamic driving environments, with a potential examination through video analysis. Source code and experimental setups are provided for further exploration and validation of our propositions.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zuxin_Liu1",
  "manipulated_ranking": 16,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=6lnoUqFd5R",
  "title": "Learning the Efficient Frontier",
  "modified_abstract": "The research landscape has profound developments in optimization techniques, including hyperparameter tuning in optimizer benchmarking, enhancing neural network training protocols, and dynamic simulation strategies such as Monte-Carlo Tree Search (MCTS). Additionally, advancements in learning rate scheduling with Graph Neural Networks (GNNs) and exploring the robustness of AI models like AlphaZero against adversarial perturbations highlight the evolving complexity and the demand for efficient solutions in machine learning challenges. These advancements serve as a backdrop and inspiration for our study, which introduces NeuralEF: a novel, fast neural approximation framework that forecasts the results of efficient frontier (EF) convex optimization problems with respect to a variable number of inputs and heterogeneous linear constraints. By reframing an optimization problem as a sequence to sequence challenge, we demonstrate NeuralEF's potential to significantly accelerate large-scale simulation tasks while adeptly managing discontinuous behaviors. Our approach is not just an advancement in solving fundamental resource allocation problems but also contributes to a broader understanding of optimization in machine learning, offering insights into tackling the intricate balance between exploration and exploitation within strategic games. The self-play mechanism drawn from game theory and its application in models like AlphaZero, along with baselines for performance comparison, underscores the relevance of our approach in a scenario where actions lead to results that encompass computational efficiency in model training and evaluation. Despite the focus on networks and actions within optimization scenarios, the abstract nature of this work means that direct reference to human language processing is not applicable in this context.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Li-Cheng_Lan1",
  "manipulated_ranking": 4,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=muVKSb8gi5",
  "title": "Reliable Off-Policy Learning for Dosage Combinations",
  "modified_abstract": "In the context of personalized medicine, particularly in applications such as cancer therapy or critical care, decision-making often involves selecting appropriate dosage combinations from multiple continuous treatments. This complex task has traditionally been approached by modeling the effects of multiple treatments independently, leaving the critical issue of estimating their joint effects less explored due to inherent challenges. Our research is inspired by groundbreaking works in off-policy evaluation within sequential decision-making dynamics, which includes developing techniques for reliable estimation from historical data, addressing the variance and bias in importance sampling, and exploring optimization strategies for complex, dynamic, and often non-stationary environments. Building on these foundations, we propose a novel framework for reliable off-policy learning specifically tailored to the challenges of optimal dosage combination determination in a personalized medicine setting. Our approach includes (1) developing a tailored neural network to estimate individualized dose-response functions, considering the joint effect of multiple dosages; (2) utilizing conditional normalizing flows to estimate the generalized propensity score for identifying regions of limited data overlap; and (3) employing a gradient-based learning algorithm to ascertain optimal, individualized dosage combinations while ensuring the reliability of the policy value estimation by circumventing regions with sparse data. This methodology is comprehensively validated through extensive evaluations, positioning our work as an inaugural effort in achieving reliable off-policy learning for dosage combination optimization in processes that are inherently non-stationary. The significance of this research extends beyond its theoretical contributions, offering practical implications for enhancing decision-making accuracy and effectiveness in high-stakes medical applications through the acceleration of individualized treatment efficacy.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yash_Chandak1",
  "manipulated_ranking": 24,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=UWd4ysACo4",
  "title": "Expressive Sign Equivariant Networks for Spectral Geometric Learning",
  "modified_abstract": "Inspired by the exploration of normalization techniques in neural ordinary differential equations (Neural ODEs) and their impact on network performance, this study advances the field of spectral geometry through the introduction of expressive sign equivariant networks. We address the theoretical limitations of sign invariance, a principle based on the symmetry of eigenvectors, which underpins many machine learning models that respect the structural properties of data. Our investigation reveals that sign invariance constrains the development of models capable of learning node positional encodings and building orthogonally equivariant architectures for tasks such as link prediction in graphs. By developing novel sign equivariant neural network architectures grounded in an analytic characterization of sign equivariant polynomials and incorporating advanced normalization techniques, we offer a framework that boasts enhanced expressiveness and theoretical benefits in spectral geometric learning tasks, including those requiring deep learning for complex classification problems. Controlled synthetic experiments validate our models' superiority in harnessing the predictive capacity of spectral geometry for different learning tasks, showcasing their effectiveness in both learning and classification.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Talgat_Daulbaev1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=KTZttLZekH",
  "title": "On the Constrained Time-Series Generation Problem",
  "modified_abstract": "This paper addresses the constrained time-series generation problem, which is an emergent challenge within a broader context that includes domain generalization and adaptation under varying constraints as investigated in related works exploring the efficacy of Invariant Causal Prediction, Target Conditioned Representation Independence (TCRI), and strategies for adapting to latent subgroup shifts. Synthetic time series are often used in practical applications to augment historical data sets, amplify the occurrence of rare events, and create counterfactual scenarios, necessitating both realism and adherence to specific numerical constraints. That constrained time-series generation problem, exemplified by requirements for synthetic market stress scenarios by financial institutions, brings to the fore the need for methods that provide efficient sampling while ensuring both realism and compliance with imposed constraints. We critique existing methods for their reliance on re-training and rejection sampling for constraint enforcement, identifying these as areas of inefficiency. Consequently, we propose a novel set of generative methods, exemplified by 'GuidedDiffTime', a guided diffusion model, framed within a constrained optimization framework. Our empirical evaluation across financial and energy datasets demonstrates qualitative and quantitative superiority over existing methods. Notably, 'GuidedDiffTime' obviates the need for re-training when constraints are modified, achieving up to a 92% reduction in carbon footprint compared to traditional training deep learning approaches, reflecting a significant advancement in the field.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Olawale_Elijah_Salaudeen1",
  "manipulated_ranking": 10,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=mbaN0Y0QTw",
  "title": "SEENN: Towards Temporal Spiking Early Exit Neural Networks",
  "modified_abstract": "This work builds upon the foundational concepts introduced in the realm of Spiking Neural Networks (SNNs), leveraging the insights from their use in diverse fields such as Embodied AI, adaptive learning strategies, robotic systems, and tasks in multi-agent environments. The unique temporal and spatial processing capabilities of SNNs have positioned them as a formidable substitute for traditional Artificial Neural Networks (ANNs), priming them for efficiency and deployment in varied applications. Our study introduces Spiking Early Exit Neural Networks (SEENNs), a novel approach aimed at fine-tuning the computational efficiency of SNNs by dynamically adjusting the number of timesteps according to the input sample's complexity. By proposing two variants, SEENN-I, which utilizes confidence score thresholding, and SEENN-II, which employs reinforcement learning to determine timestep count, we evolve the model's adaptability. SEENN demonstrates compatibility with both directly trained SNN architectures and those converted from ANNs. Our results highlight significant inference efficiency with a reduction in average timesteps, as demonstrated by a SEENN-II ResNet-19 model achieving 96.1% accuracy on the CIFAR-10 test dataset with only 1.08 average timesteps. The methodology and insights drawn from creating efficient, adaptive training approaches such as ProcTHOR's procedural generation in Embodied AI, ADVISOR's balance between imitation and exploration, the use of Scene Graph Contrastive (SGC) losses for richer environmental encoding, and establishing robust benchmarks in Embodied AI, have directly informed the development and evaluation of SEENNs. The success of these adaptive approaches and the exploration of 3D data's potential underscore the broader applicability and relevance of our work within the ML community. Code related links have been excluded for privacy.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jordi_Salvador3",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=eTHawKFT4h",
  "title": "A Rigorous Link between Deep Ensembles and (Variational) Bayesian Methods",
  "modified_abstract": "This work builds on an array of foundational studies in machine learning, spanning from asymptotic analyses in high-dimensional settings, the exploration of generalized linear models for Gaussian mixtures, to the theoretical underpinnings of multi-class classification and signal recovery in deep learning architectures through max-margin optimization and training dynamics. These studies set a stage for addressing the complexity of learning dynamics, providing insight into classification tasks, regression problems, and elucidating the statistical properties of neural networks through initialization strategies that lead to a deeper understanding of sparse representation and max-margin optimization in high dimensional spaces. We establish the first mathematically rigorous link between Bayesian, variational Bayesian, and ensemble methods by reformulating the non-convex optimisation problem typically encountered in deep learning into a convex optimisation problem in the space of probability measures, with a particular focus on the initialisation of ensemble components for training deep learning models. Our contribution extends the framework of generalized variational inference through the lens of Wasserstein gradient flows and introduces a novel concept analogous to scattering processes in physics, serving as a unified theory that bridges various approaches used for uncertainty quantification in deep learning, including deep ensembles and (variational) Bayesian methods. This synthesis not only sheds light on the effectiveness of deep ensembles compared to parameterized variational inference strategies but also underpins the derivation of novel ensembling schemes with guaranteed convergence properties through the asymptotics perspective. We further demonstrate this by introducing a class of interacting deep ensembles, drawing direct connections to particle system interactions in thermodynamics, and provide theoretical evidence for the convergence of these algorithms to a globally defined minimiser on the space of probability measures.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Cedric_Gerbelot1",
  "manipulated_ranking": 15,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=CSbGXyCswu",
  "title": "Fine-Grained Human Feedback Gives Better Rewards for Language Model Training",
  "modified_abstract": "The exploration of fine-grained human feedback in language model (LM) training builds on foundational contributions across diverse areas of machine learning, including robust encodings for NLP systems, systematic audits of multimodal and language models, and insights into model failures informed by human cognitive biases. Previous work has illuminated the complex interplay between model robustness, output fidelity, and error identification in the presence of adversarial attacks or inherent model biases. Encouraged by these advancements, our research proposes Fine-Grained RLHF, a novel framework that advances the discourse by integrating fine-grained human feedback to fine-tune LMs. This methodology seeks to mitigate undesirable text generation behaviors, such as generating false, toxic, or irrelevant outputs, by providing rewards for each segment of generated text (e.g., sentences) and incorporating multiple reward models for various feedback types (e.g., factual incorrectness, irrelevance, information incompleteness). Through detoxification and long-form question answering experiments, we demonstrate that our approach not only enhances performance but also enables the customization of LM behaviors using different combinations of fine-grained reward models. The approach's efficacy is supported by both automatic and human evaluations, with an auditing process that ensures the relevance and accuracy of the feedback against a corpus of data. To further the field's progress, we release all collected data, human feedback, codes, and the text-encoder necessary for transparency and reproducibility in LM training practices, thus fostering an open-source ethos essential for future deployment strategies. This work stands as a testament to the pivotal role of fine-grained human feedback in overcoming the limitations of holistic assessments and propels forward the capabilities of LMs in generating more accurate, relevant, and safe text outputs in various domains and population spaces.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Erik_Jones3",
  "manipulated_ranking": 10,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=yAOwkf4FyL",
  "title": "Operation-Level Early Stopping for Robustifying Differentiable NAS",
  "modified_abstract": "Inspired by foundational contributions in the realm of confidence calibration and loss function optimization in deep neural networks, this paper presents a novel perspective on addressing robustness issues encountered by Differentiable NAS (DARTS). Commonly adopted across various machine learning applications, DARTS is plagued by the predominance of skip connections, leading to architectures saturated with parametric-free operations and subsequent performance collapse. The prevailing hypothesis posits that the benefits in optimization skip connections enjoy over other parametric operations contribute to their domination. Our study diverges from existing methodologies by attributing the disproportionate prevalence of skip connections to the overfitting of parametric operations to training data while simultaneously training architecture parameters on validation data. Deep networks, with their intricacies, are susceptible to mis-calibration, a fact underscored by observing under-confident predictions. To counteract this, we introduce the operation-level early stopping (OLES) method as a strategy to robustify DARTS without additional computational demands. OLES is designed to mitigate the identified overfitting issue directly, offering a straightforward yet effective solution. Extensive experimental evaluations, emphasizing network calibration and the analysis of logit outputs as evidence for under-confident predictions, confirm our hypothesis and validate OLES's efficacy in enhancing the robustness of DARTS architectures, thereby addressing one of the method's critical vulnerabilities outlined in prior research on calibration and loss optimization.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Linwei_Tao2",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=t1jLRFvBqm",
  "title": "Object-Centric Learning for Real-World Videos by Predicting Temporal Feature Similarities",
  "modified_abstract": "Our research builds on the core ideas presented in recent works that have explored self-supervised learning, correspondence learning in videos, and innovative approaches towards object segmentation and feature embeddings in an unabridged variety of settings that range from natural language processing to sustainable agricultural practices and healthcare. Inspired by the principles laid out in these studies, especially those concerning visual correspondence, unified mask embedding for video segmentation, and the enhancement of video object segmentation via space-time correspondence, this paper introduces a novel approach to unsupervised video-based object-centric learning. By leveraging pre-trained self-supervised neural networks features, we propose the utilization of a temporal feature similarity loss to encode semantic and temporal correlations between image patches, thereby infusing a motion bias that is pivotal for object discovery in videos. This includes the precise segmentation and recognition of dynamic objects within videos, often requiring the delineation of bounding boxes or masks around objects of interest such as pedestrians, which are crucial tasks in understanding complex video scenes. Key to our method is the employment of sophisticated neural network architectures and a dedicated decoder designed to efficiently process and distill these temporal and semantic similarities into actionable insights for object-centric learning. Displaying the effectiveness of our method, we achieve state-of-the-art performance on synthetic MOVi datasets and demonstrate scalability to unconstrained video datasets such as YouTube-VIS, marking a significant advancement in object-centric video modeling. This represents the first time an object-centric video model has been effectively scaled to work with real-world videos, leveraging the foundation set by studies on locality-aware video reconstruction, unified frameworks for self-supervised video segmentation, and correspondence learning to significantly push the boundaries of understanding and interacting with dynamic visual data.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Wenguan_Wang4",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=SLTQluG80x",
  "title": "Distributional Model Equivalence for Risk-Sensitive Reinforcement Learning",
  "modified_abstract": "Existing works in reinforcement learning (RL) have yielded significant insights into decision-making under uncertainty, algorithm adaptability in non-stationary environments, and the design of learning environments for inverse RL. By building upon these convergent themes\u2014ranging from robust decision-making frameworks in Minimax-Bayes RL to adaptive strategies in dynamic environments, as explored in ANACONDA, and innovative approaches in learning reward functions through environment design\u2014we position our research within this established continuum. Specifically, we address the challenge of learning models for risk-sensitive reinforcement learning. We theoretically demonstrate that proper value equivalence, traditionally sufficient for planning optimally in risk-neutral settings, falls short in risk-sensitive scenarios. To bridge this gap, we introduce two novel notions of model equivalence, leveraging distributional reinforcement learning: a general, albeit intractable, version for comprehensive risk measure planning, and a practical variant tailored for specific risk preferences. These contributions not only supplement model-free risk-sensitive algorithms but also highlight the importance of sample-efficient learning methodologies and the role of regret minimization in the evaluation of agents' performance in environments against adversarial bandits, thereby underscoring the nuanced complexities of risk-sensitive learning models and advocate for a more inclusive interpretation of risk that transcends conventional frameworks. Our approach is validated through both tabular and large-scale experimental validation, demonstrating the efficacy of our model in optimizing rewards under uncertainty.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Thomas_Kleine_Buening1",
  "manipulated_ranking": 81,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=cAhJF87GN0",
  "title": "Explainable Brain Age Prediction using coVariance Neural Networks",
  "modified_abstract": "Inspired by novel advancements in deep learning, such as the exploration of normalization techniques in the context of Ordinary Differential Equation based networks (Neural ODEs), and their implications on model performance, this paper extends deep learning applications into computational neuroscience. Specifically, we focus on leveraging brain imaging data for the classification task of neurological health status to predict \"brain age,\" a metric indicative of an individual's neurological health status. The discrepancy between brain age and chronological age, known as \"brain age gap,\" has emerged as a critical marker for assessing accelerated aging tied to various health conditions and susceptibility to neurological diseases and cognitive impairments. Despite the potential of brain age prediction as a classification task in clinical settings, the adoption of brain age prediction models has been limited by the opaqueness of the algorithms and a lack of methodological clarity. To address these challenges, we utilize coVariance Neural Networks (VNN) to introduce an explanation-driven and anatomically interpretable framework for predicting brain age with cortical thickness measurements. Our framework not only aims to determine the brain age gap in Alzheimer's disease (AD) with enhanced precision but also sheds light on the anatomical regions contributing to the elevated gaps, pinpointing areas of the highest concern. We provide evidence that the explanatory power of VNNs hinges on these models' capacity to analyze specific eigenvectors within the anatomical covariance matrix, thus offering a novel, interpretable approach to brain age estimation through learning complex patterns and normalization of these patterns in the classification process.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Talgat_Daulbaev1",
  "manipulated_ranking": 9,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=U4pFV192JQ",
  "title": "Masked Two-channel Decoupling Framework for Incomplete Multi-view Weak Multi-label Learning",
  "modified_abstract": "Amid the growing intersection of multi-view learning and multi-label classification, our research introduces a novel masked two-channel decoupling framework tailored for the challenging scenario of incomplete multi-view weak multi-label learning tasks. This work is inspired by cutting-edge developments in the privacy-preserving mechanisms in vision transformer applications and their implications for learning models' capability to handle complex data structures while ensuring data privacy through distributed computing methodologies. Capitalizing on the innovative strides made in differentially private computing and the successful utilization of masking mechanisms in various domains, including the strategic use of patches at the patch-level in transformer architectures, our framework uniquely addresses the problem by decoupling the conventional single-channel view-level representation encountered in deep multi-view learning into a dual-channel schema, comprising both shared and view-specific representations. The introduction of a cross-channel contrastive loss and a label-guided graph regularization loss aids in the enhancement of semantic properties and the preservation of geometric sample structures, respectively. A pioneering random fragment masking strategy for vector features is employed to further refine the learning efficiency of our encoders, utilizing computing advancements for increased adaptability. Emphasizing adaptability, our model is designed to robustly perform under instances of arbitrary view and label absences, positioning it as a significant progression in the domain of incomplete multi-view weak multi-label learning. Extensive experimentation validates the model's superior performance and adaptability in distributed computing environments, marking a notable contribution to the field.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jihong_Park1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=BRqlkTDvvm",
  "title": "BQ-NCO: Bisimulation Quotienting for Efficient Neural Combinatorial Optimization",
  "modified_abstract": "This work is inspired by significant recent progress in addressing challenges in various machine learning domains, including multi-agent reinforcement learning, constrained optimization in Markov decision processes, non-cooperative configurable environments, policy optimization with trust-region approaches, general-sum game learning dynamics, imitation in mean-field games, active exploration strategies for inverse reinforcement learning, exploiting causal priors in reinforcement learning, and social network data analysis for knowledge discovery. These advancements have paved the way for novel approaches in optimization and learning methodologies, underlining the relevance of robustness, generalizability, and efficient learning strategies in complex decision-making environments. Leveraging these insights, our paper presents a novel formulation of Combinatorial Optimization Problems (COPs) as Markov Decision Processes (MDPs) aimed at improving out-of-distribution robustness by effectively leveraging common symmetries in COPs through Bisimulation Quotienting (BQ). Starting with a direct MDP formulation of constructive methods, we introduce a generic algorithm to reduce the state space, based on BQ in MDPs, by employing an intelligent sampling strategy for simulations. For COPs with a recursive nature, we specialize the bisimulation, demonstrating how the reduced state space can exploit symmetries to facilitate MDP solving efficiently. Our principled, provably optimal approach establishes that an optimal policy for the formulated BQ-MDP indeed solves the associated COPs, overcoming adversarial behavioral and strategic challenges encountered in traditional optimization methodologies. We evaluate our method on classical problems: Euclidean and Asymmetric Traveling Salesman, Capacitated Vehicle Routing, Orienteering, and Knapsack Problems. Introducing a simple attention-based policy network for BQ-MDPs, trained by imitation of (near) optimal solutions provided by experts in small instances and honed through online learning techniques, we achieve state-of-the-art results, demonstrating remarkable generalization to larger instances unseen during training. This work contributes to the neural-based combinatorial optimization field by providing a theoretically grounded and empirically validated approach for efficient and robust optimization, with broad implications for enhancing neural network policy generalization across diverse application domains.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Giorgia_Ramponi1",
  "manipulated_ranking": 36,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=9Tx2znbyTm",
  "title": "Diffused Task-Agnostic Milestone Planner",
  "modified_abstract": "Building upon the foundational works aimed at enhancing prediction and exploration in machine learning through scalable adversarial objectives, generation of semantic maps, and leveraging uncertainties for exploration and navigation, this paper proposes a novel methodology to expand the embrace of sequence modeling for decision-making. The diffused task-agnostic milestone planner is introduced as a strategy to address decision-making problems by utilizing a diffusion-based generative sequence model. This innovative approach allows for the planning of a series of control-relevant, low-dimensional latent space milestones, facilitating efficient long-term planning, vision-based control, and multi-task decision-making autonomously. The method's generation flexibility further enables the planning of diverse trajectories, thereby addressing challenges across offline reinforcement learning benchmarks and visual manipulation environments, including indoor navigation and object searching tasks autonomously. Drawing parallels with advancements in utilizing adversarial methods for model-based curiosity, active learning in robotic navigation through semantic goal mapping, and cross-domain dataset utilization for boosting robotic skills generalization, our work transcends traditional boundaries and sets a new precedence in solving complex long-horizon, sparse-reward tasks, and multi-task problems in unprecedented environments. Experimental validation across various platforms and architectures corroborates the superiority of the proposed method over offline RL methods and underscores its potential in setting new benchmarks in the most challenging vision-based manipulation tasks with image-based inputs, highlighting techniques in autonomous operations complemented by photo-realistic simulation environments.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Bernadette_Bucher1",
  "manipulated_ranking": 27,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=9STYRIVx6u",
  "title": "Convergence of mean-field Langevin dynamics: time-space discretization, stochastic gradient, and variance reduction",
  "modified_abstract": "Our study builds upon the insightful contributions delineated in recent works, which highlight the significance of gradient estimation techniques and their foundational role in improving learning algorithms across various domains, including reinforcement learning and machine learning frameworks for model-based policy search. In particular, the reparameterization and likelihood ratio gradient methods provided a deeper understanding of gradient estimators' underlying mechanics by tracing the movement of probability mass. These methodologies, alongside the exploration of total stochastic gradients and the greedy utilization of these gradients in reinforcement learning, as well as the creation of adaptable, multiagent and customizable learning algorithms, serve as a cornerstone for our investigation into the mean-field Langevin dynamics (MFLD). The MFLD, a nonlinear extension of Langevin dynamics with distribution-dependent drift, emerges from optimizing two-layer neural networks via gradient descent, often applied with noise while sampling. We argue that while it has been established that MFLD can globally minimize an entropy-regularized convex functional in the measures space, existing analyses have been limited to scenarios assuming either an infinite-particle or a continuous-time framework and failed to address the challenges introduced by stochastic gradient updates and repeatedly applied variances. Our work extends the current understanding by presenting a comprehensive framework capable of proving a uniform-in-time propagation of chaos for MFLD. This framework accounts for errors introduced by finite-particle approximations, time-discretization, and stochastic gradients, and assesses the evidence of effectiveness of various gradient estimators such as SGD and SVRG. Notably, our findings reveal an enhanced convergence rate in both the SGD and SVRG settings, specifically when applying our framework to standard Langevin dynamics and incorporating mechanisms such as softmax for variance reduction.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Paavo_Parmas1",
  "manipulated_ranking": 4,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=X6dEqXIsEW",
  "title": "On the Planning Abilities of Large Language Models - A Critical Investigation",
  "modified_abstract": "Recent research in LLMs has catalyzed an exploration into their extended capabilities, including reasoning, language understanding, and information retrieval, as evidenced by work in transformer-based architectures, unsupervised domain adaptation, efficient retrieval systems, and the integration of reranker mechanisms for text refinement. Our study contributes to this discourse by specifically investigating the planning abilities of large language models (LLMs) trained on general web corpora, including a range of texts that span multiple domains. We evaluate the effectiveness of LLMs in autonomously generating plans in commonsense planning tasks and assess their potential as heuristic guidance for other AI planners in their planning endeavors, a process facilitated via strategies that ensure a form of calibration in their output. Through a systematic study involving instances from domains akin to those used in the International Planning Competition, we explore LLMs in autonomous and heuristic modes. Our findings indicate that while the autonomous planning abilities of LLMs are limited, with the best performing model, GPT-4, achieving an average success rate of approximately 12% across domains, their capacity to serve as heuristic guidance shows more promise. LLM-generated plans can enhance the search processes of sound planners, using strategies akin to token-matching for generating and refining plans. Retrievers and benchmarking techniques applied in our experiments facilitated the evaluation of LLM effectiveness across varied planning domains and contributed to the calibration of their performance. This research elucidates the current limitations and potential applications of LLMs in the realm of AI planning, marking a step forward in understanding their utility beyond traditional language tasks, and integrating them with collections of planning benchmarks from the International Planning Competition.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Omar_Khattab1",
  "manipulated_ranking": 15,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=9i8MD9btc8",
  "title": "(Almost) Provable Error Bounds Under Distribution Shift via Disagreement Discrepancy",
  "modified_abstract": "This study introduces a novel approach to deriving an (almost) guaranteed upper bound on the error of deep neural networks under distribution shift, leveraging insights gained from recent advancements in sampling theory, hypothesis testing, and the study of shape-constrained distributions. Influenced by foundational work in the complexity of sampling from strongly log-concave distributions, likelihood-free hypothesis testing, efficient rejection sampling, and optimization problems involving barycenters, our research presents a cohesive framework for error estimation under distribution shifts. Our approach is dimension-free, emphasizing its applicability to large datasets without reliance on the dimensionality of the data. Unlike prior methods, which rely on continuous measures or complex statistical properties that demand label knowledge, our approach hinges on a simple, intuitive condition supported by empirical evidence to be consistently reliable. Problems related to aligning distributions for sampling and the optimization of error bounds are inherently addressed through our methodology. By adopting the $\\mathcal{H}\\Delta\\mathcal{H}$-divergence as a conceptual springboard, we propose a bound that is not just easier to compute but also offers tighter, non-vacuous error upper limits, embodying a sequence of advances in the field. Central to our method is the optimization of a multiclass classifier to foster disagreement with another, a concept grounded in the disagreement loss, which we introduce and theoretically justify here for the first time. This not only refines the practice of estimating bounds under changing distributions but is also poised to enhance multiclass disagreement maximization techniques in future methodologies. Our empirical validation spans both natural and synthetic benchmarks for distribution shift, where our proposed bounds demonstrate validity and exhibit performance on par with, but not surpassing, current estimation methods.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Patrik_Robert_Gerber1",
  "manipulated_ranking": 21,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=DjX2Nr15kY",
  "title": "NAR-Former V2: Rethinking Transformer for Universal Neural Network Representation Learning",
  "modified_abstract": "Recent works in the field of machine learning have focused on enhancing the representation learning of neural networks through various approaches such as Graph Neural Networks (GNNs), sign and base invariant networks for spectral graph representation, and expressive sign equivariant networks for spectral geometric learning. These studies have explored the limitations and potential of GNNs, proposed architectures to leverage the symmetries in eigenvectors for graph representation, and addressed the expressive power of neural networks in capturing complex graph structures. Inspired by these advancements, our research revisits the Transformer model, comparing it with GNNs in terms of architectural characteristics and their aptitude for representing neural networks. We propose NAR-Former V2, a modified Transformer-based model that embodies efficient representation learning from both cell-structured networks and entire networks. By encoding networks as graphs and implementing a straightforward tokenizer to transcribe the network into a sequence for processing, our method integrates GNN's inductive representation capacity into the Transformer framework, effectively describing the vast eigenspaces involved. This amalgamation empowers the Transformer to excel in scenarios involving unseen architecture. Additionally, NAR-Former V2 introduces several modifications that significantly enhance the model's capability to learn from graph structures, provably leveraging eigenvectors and invariants for efficient graph representation. Through experimental validation on the NNLQP dataset, our approach demonstrates a marked improvement over the GNN-based method NNLP in encoding entire networks and predicting latency. Moreover, it exhibits highly competitive performance in accuracy prediction on cell-structured NASBench101 and NASBench201 datasets against other leading methods. This research underscores the potential of leveraging insights from various representation learning paradigms to fortify the capabilities of Transformers in universal neural network representation learning.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Derek_Lim1",
  "manipulated_ranking": 9,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=qVeDwgYsho",
  "title": "CoPriv: Network/Protocol Co-Optimization for Communication-Efficient Private Inference",
  "modified_abstract": "The burgeoning field of secure two-party computation (2PC) for deep neural network (DNN) inference underscores a critical challenge: ensuring cryptographically secure privacy protection while addressing the significant latency overhead attributed to extensive communication requirements. Inspired by pioneering works that mitigate adversarial attacks through novel adversarial patch optimization techniques, which also grapple with optimizing computational processes for enhanced security and efficiency, this paper introduces CoPriv, a novel framework for co-optimizing the 2PC inference protocol and DNN architecture for communication-efficient private inference. Unlike previous approaches that predominantly utilize the proxy metric of ReLU counts to approximate communication overhead, CoPriv recognizes the substantial communication contributions of both linear and non-linear operations in state-of-the-art (SOTA) 2PC protocols. Through the introduction of a new 2PC protocol tailored for convolution through Winograd transformation and the development of DNN-aware and 2PC-aware network optimization algorithms, CoPriv substantially curtails inference communication. We empirically validate the efficacy of CoPriv against the SOTA 2PC protocol, CrypTFlow2, showcasing a 2.1\u00d7 communication reduction for ResNet-18 and ResNet-32 on CIFAR-100 datasets. Leveraging learning frameworks and reinforcement learning strategies within our optimization algorithms demonstrates notable improvements over leading network optimization methods like SNL and MetaPruning, achieving higher accuracy while realizing up to 9.98\u00d7 and 3.88\u00d7 reductions in online and total communication, respectively. Furthermore, CoPriv outperforms MetaPruning in online communication reduction by 3.87\u00d7, with more than 3% higher accuracy, marking a significant advancement in the pursuit of communication-efficient and privacy-preserving DNN inference.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Huanqian_Yan1",
  "manipulated_ranking": 61,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=Bw82hwg5Q3",
  "title": "Self-Evaluation Guided Beam Search for Reasoning",
  "modified_abstract": "Inspired by recent work in computational argumentation, text generation, and machine reasoning\u2014ranging from generating informative conclusions and assessing argument sufficiency to exploring belief-based argumentative claim and counter-argument generation\u2014our study introduces a novel mechanism to enhance Large Language Model (LLM) reasoning capabilities. We address the challenge of uncertainty and error accumulation in multi-step reasoning by introducing a stepwise self-evaluation mechanism to guide and calibrate the reasoning process of LLMs. This self-evaluation, integrated through a novel decoding algorithm employing stochastic beam search, acts as an automatic, better-calibrated criterion, facilitating more efficient exploration within the reasoning space and achieving improved prediction quality. We employ stochastic beam search to balance the exploitation and exploration of the search space with temperature-controlled randomness, effectively surpassing baselines in few-shot accuracy across multiple benchmarks. Our experimental results, which include success with the Llama-2 model on arithmetic reasoning tasks, underscore not only our method\u2019s efficiency but also its ability to pinpoint logical failings, leading to heightened consistency and robustness in multi-step reasoning. Thus, our research contributes a significant advance in the ongoing effort to refine the reasoning processes of LLMs, with implications for automatic reasoning and classification systems that range from argument classification in computational argumentation to audience-aware text generation. The incorporation of self-evaluation and beam search-based strategies reflects an extractive approach from a vast corpus of possibilities, aligning with the goal of improving machine reasoning through methods analogous to text summarization in extracting relevant points from large datasets.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Milad_Alshomary1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=S3Y0VvegGm",
  "title": "The Benefits of Being Distributional: Small-Loss Bounds for Reinforcement Learning",
  "modified_abstract": "This study builds on the substantial progress observed in reinforcement learning (RL), notably in addressing challenges associated with offline RL, reward distribution, adaptation to non-stationarity, and exploring data characteristics in lifelong learning scenarios. Leveraging insights from prior research into the effects of dataset characteristics on offline RL performance, reward redistribution to accelerate learning, and strategies for coping with non-stationarity and exploration in lifelong learning environments, we examine distributional reinforcement learning (DistRL) from a new perspective. Although distributional reinforcement learning (DistRL), which inherently involves state-action neural network modeling, has been empirically effective, the question of when and why it is better than vanilla, non-distributional RL has remained unanswered. This paper explains the benefits of DistRL through the lens of small-loss bounds, which are instance-dependent bounds that scale with the optimal achievable cost. Particularly, our bounds converge much faster than those from non-distributional approaches if the optimal cost is small. As a warm-up, we propose a distributional contextual bandit (DistCB) algorithm, which uses data analysis and adaptation methods to achieve small-loss regret bounds and empirically outperforms the state-of-the-art on three real-world tasks. In online RL, during training episodes, we propose a DistRL algorithm that constructs confidence sets using maximum likelihood exploration. We prove that our algorithm enjoys novel small-loss PAC bounds in low-rank MDPs. As part of our analysis, we introduce the $\\ell_1$ distributional eluder dimension, an exploration metric that may be of independent interest. Then, in offline RL, we show that pessimistic DistRL enjoys small-loss PAC bounds that are novel to the offline setting and are more robust to bad single-policy coverage, thus navigating artificial obstacles and limitations.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Marius-Constantin_Dinu_Dinu1",
  "manipulated_ranking": 21,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=pQvAL40Cdj",
  "title": "Detecting Any Human-Object Interaction Relationship: Universal HOI Detector with Spatial Prompt Learning on Foundation Models",
  "modified_abstract": "In the evolution of machine learning (ML) methods for understanding complex datasets, significant strides have been made in the unsupervised learning of visual representations, self-supervised learning via knowledge transfer, representation through learning to count, unsupervised conditional GAN frameworks, and self-supervised video representation learning. These advancements underscore the potential of leveraging large datasets without human annotation, offering insights into novel ML techniques and architectures that can facilitate intricate understanding of visual data, including videos. Drawing upon these foundational developments, our study introduces a comprehensive approach to human-object interaction (HOI) detection, aimed at decoding the myriad ways humans interact with objects within an image or video. Termed UniHOI, our method employs Vision-Language (VL) foundation models alongside large language models (LLMs) for universal interaction recognition in an open-world setting. By integrating these models, which are effectively large neural networks, with a novel HO prompt-based learning and an HO Prompt-guided Decoder (HOPD), our approach capitalizes on the hierarchical features of visual HOI detectors and the generative capabilities of LLMs, such as GPT, for nuanced interaction interpretation. Further innovation is achieved through our method's flexibility in processing either interaction phrases or interpretive sentences for open-category interaction recognition, effectively utilizing encoding strategies to enhance recognition capabilities. The efficacy of our approach is demonstrated through its superior performance over existing methods under both supervised and zero-shot conditions on established benchmarks, showcasing our approach as on par with or surpassing current standards. Pre-training components within our model afford rich initial representation, which, when coupled with bespoke clustering techniques for data categorization, aligns elegantly with the aim of universal recognition. This investigation not only advances the field of HOI detection but also exemplifies the leveraging of foundation models and language models to bridge visual and linguistic domains, echoing the transformative potential seen in preceding works on unsupervised and self-supervised learning.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Mehdi_Noroozi1",
  "manipulated_ranking": 60,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=CswEebv5Hn",
  "title": "Imitation Learning from Vague Feedback",
  "modified_abstract": "Imitation learning research has increasingly focused on deriving utility from suboptimal or incomplete data sources, a theme echoed in related works exploring novel frameworks in Inverse Reinforcement Learning (IRL), unsupervised exploration techniques, and massively entropic pretraining for achieving arbitrary goals in Reinforcement Learning (RL). These studies underscore the critical importance of effective data collection and utilization, whether through observing learning agents in various spaces, generating informative data from unbiased exploration, or pretraining on a broad set of achievable states for goal-oriented tasks. Inspired by these insights, our work investigates imitation learning from vague feedback, targeting scenarios where direct, high-quality comparisons between expert and novice demonstrations are impractical or impossible. Specifically, we address the challenge of deriving actionable insights from feedback that only distinguishes between significantly different demonstration qualities\u2014essentially, between expert and non-expert performances\u2014in various poses and space configurations. Our approach leverages a mixture model to differentiate between expert and non-expert data, enabling the recovery of the expert policy distribution when the proportion of expert data, denoted as $\\alpha$, is known. We further propose a method for estimating $\\alpha$ when its value is unknown, integrating this insight with generative adversarial imitation learning and controller algorithms for effective control and pose-achievement optimizations to formulate a comprehensive end-to-end training algorithm. Our experimental results demonstrate the efficacy of our methods over standard and preference-based imitation learning techniques, illustrating the technique's utility across a variety of tasks with particular emphasis on improving the trajectories followed by agents.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Alexis_Jacq1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=q6X038vKgU",
  "title": "Predict, Refine, Synthesize: Self-Guiding Diffusion Models for Probabilistic Time Series Forecasting",
  "modified_abstract": "Diffusion models have rapidly become a focal point in generative modeling, demonstrating unparalleled success across fields such as image, audio generation, and now extending into time series forecasting. Leveraging insights gained from advancements in causal representation learning, including interventional and graph-based methods, and complex-valued autoencoders, our work investigates the untapped capabilities of diffusion models in time series analysis. This initiative aligns with causality principles by emphasizing the identifiability and dynamic natures of underlying processes. We introduce TSDiff, a novel unconditionally-trained diffusion model, crafted from the understanding that models can excel at a range of tasks without being tailored to a specific application. This approach aligns with the broader movement towards more versatile and generalized models as evidenced in related works that tackle challenges from causal discovery in time-series data to object discovery in multi-object complex environments. TSDiff uniquely incorporates a self-guidance mechanism, making it adaptive for various time series tasks during inference, such as forecasting, refinement, and synthetic data generation, without additional training adjustments. Our work not only validates the machine's broad applicability and competitive performance against specialized conditional forecasting methods but also demonstrates the practical advantage of its learned probability density for refining predictions, and highlights its superior generative capability for training on synthetic data, sometimes even surpassing real-data training scenarios.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sindy_L\u00f6we1",
  "manipulated_ranking": 4,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=gd20oaZqqF",
  "title": "Towards Optimal Caching and Model Selection for Large Model Inference",
  "modified_abstract": "In the context of escalating computational demands posed by Large Language Models (LLMs) and other foundational large models, which have been pivotal in pushing the envelope in various domains of machine learning, our paper addresses critical operational drawbacks such as resource consumption and latency during inference phases. These challenges are particularly pronounced during large-scale deployments, underscoring the need for efficient resource management strategies. Our investigation is inspired by foundational principles, such as those underlying decision tree learning algorithms, which emphasize the importance of algorithmic efficiency and optimization in machine learning applications. In this paper, we explore two strategic approaches to alleviate these issues: leveraging a cache mechanism to recall previous queries and implementing a model selection algorithm capable of choosing the most resource-efficient model from an ensemble for any given query. We theoretically propose an optimal algorithm that synergizes these two approaches to minimize inference costs, applicable in both offline and online tabular settings. Employing caching algorithms like Greedy Dual Size with Frequency (GDSF) or Least Expected Cost (LEC) alongside an intelligent model selector, we demonstrate optimal rates of efficiency in both settings. Through empirical validation via simulations, our combined caching and model selection algorithm exhibits up to a 50-fold improvement over baseline methods when the cost variance is high. Real-world dataset experimentation further validates our theoretical claims, showing a 4.3-fold reduction in floating-point operations per second (FLOPs) and a 1.8-fold decrease in latency, thereby offering a substantial stride towards optimizing resource allocation during the inference of large models.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Qin-Cheng_Zheng1",
  "manipulated_ranking": 7,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=DBlkX8Nczr",
  "title": "Brain-like Flexible Visual Inference by Harnessing Feedback Feedforward Alignment",
  "modified_abstract": "Inspired by the potential of hyperbolic learning demonstrated in Poincar\u00e9 ResNet and the quest for maximizing class separation in deep learning as explored in the concept of a fixed matrix multiplication for maximum separation, our study introduces Feedback-Feedforward Alignment (FFA) as a novel learning algorithm. These preceding works elucidate the advantages of learning in non-Euclidean spaces and embedding strong inductive biases, respectively, for enhanced visual understanding and class separation, setting a compelling groundwork for our exploration. Our research extends these concepts by proposing a mechanism where top-down effects emerge from an alignment between feedforward and feedback pathways, each optimizing its own distinct objectives. By leveraging feedback and feedforward pathways as mutual credit assignment computational graphs, FFA enables this co-optimization, demonstrating its effectiveness in tasks such as classification and reconstruction on MNIST and CIFAR10 datasets with implications for improving softmax-class classification and batch processing efficiency. The alignment mechanism endows feedback connections with emergent visual inference functions, including denoising, resolving occlusions, and imagination, thus presenting FFA as a bio-plausible alternative to traditional backpropagation methods focusing on anomaly and object detection. This work contributes to understanding the role of feedback connections in the visual cortex and proposes a new direction for developing convolutional neural network algorithms that align more closely with biological mechanisms, significantly enhancing the discrimination of different classes and norms involved in the process. FFA's reproducible results further cement its potential for broad application in diverse visual classification tasks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Max_van_Spengler1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=HUuEMMM8Ik",
  "title": "Detecting hidden confounding in observational data using multiple environments",
  "modified_abstract": "The exploration of hidden confounding in observational data is bolstered by prior developments in causal learning, causality-aware modeling, and interpretability within various domains, including healthcare, agriculture, and machine learning model evaluation. These efforts highlight the complexity of extracting causal relationships from data and evaluating algorithmic performance in the absence of overt confounding factors, contributing significantly to discussions in academic and practical settings. Drawing inspiration from these advancements, our study investigates the presence of hidden confounders across multiple observational datasets from different environments, challenging the common assumption in causal inference that there is no hidden confounding. Employing the principle of independent causal mechanisms, we introduce a theory for testing conditional independencies that vanish in the face of hidden confounding. Our analysis further delves into the nuances of degenerate and dependent mechanisms, in addition to faithfulness violations, thereby uncovering intricate properties of causal systems. We propose a novel procedure for testing these independencies and empirically assess its performance through simulations and semi-synthetic datasets derived from actual data. The results generally affirm the procedure's capability to accurately detect hidden confounding, particularly in scenarios characterized by significant confounding bias, paving the way for more precise causal forecasting in intelligence systems.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Raha_Moraffah1",
  "manipulated_ranking": 15,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=kdFR6IUEW6",
  "title": "Prompt Pre-Training with Twenty-Thousand Classes for Open-Vocabulary Visual Recognition",
  "modified_abstract": "Inspired by the advancements in zero-shot classification and understanding of implicit regularization in machine learning, this work introduces POMP, an innovative prompt pre-training methodology. Drawing from insights provided by the success of hierarchical label sets in zero-shot image classification and investigations into the mechanisms behind stochastic gradient descent (SGD), POMP advances the field by enabling efficient semantic condensation for a vast array of visual concepts through hierarchies and norms of embeddings. This method encompasses over twenty-thousand classes, efficiently incorporating semantic information into vision-language models without the computational burden typically associated with such extensive scope. POMP demonstrates a strong capability for transfer across various visual recognition tasks, such as image classification, semantic segmentation, and object detection, significantly enhancing zero-shot recognition performance. Empirical evaluations across 21 datasets highlight POMP's exceptional achievements, including a notable increase in average accuracy on classification tasks and substantial improvements in open-vocabulary Pascal VOC segmentation. Furthermore, POMP presents evidence of its robustness and efficiency, even possibly mitigating the effects of large-batch training scenarios. Such outcomes underscore POMP's role in setting new benchmarks for open-vocabulary visual recognition through the lens of fairness, robustness, and efficiency.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zachary_Novack1",
  "manipulated_ranking": 8,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=lSbbC2VyCu",
  "title": "Rewarded soups: towards Pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards",
  "modified_abstract": "In the context of recent advances in machine learning (ML) methodologies, including hyper-parameter optimization techniques, novel stochastic gradient descent (SGD) enhancements, and RL strategies for task achievement in text-based games, we recognize a pivotal opportunity to address the challenge of aligning foundation models more closely with diverse human preferences and real-world tasks. Foundation models, originally pre-trained on extensive unsupervised datasets and subsequently fine-tuned with labeled data, can immensely benefit from flexible adaptation strategies that consider the variety of objectives and user expectations. Our work introduces 'Rewarded Soup,' a novel methodology that capitalizes on the heterogeneity of rewards through a multi-policy strategy, aiming for Pareto-optimal generalization across a broad spectrum of preferences. By specializing multiple networks independently for each proxy reward and then performing a linear interpolation of their weights, we leverage the proven linear connectivity of weights fine-tuned on diverse rewards from a uniform pre-trained initialization. This empirical success is demonstrated across a range of tasks including text-to-text, text-image, and control tasks, highlighting the importance of various architectures, including convolutional networks, in supporting our proposition that adapting foundation models in this manner can significantly enhance model alignment with the multifaceted nature of real-world applications and human values.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Mathieu_Tuli1",
  "manipulated_ranking": 22,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=DP2lioYIYl",
  "title": "A Theory of Unsupervised Translation Motivated by Understanding Animal Communication",
  "modified_abstract": "Our investigation into the unsupervised machine translation (UMT) draws inspiration from recent strides in neural network applications across diverse fields such as systematic generalization in transformers, understanding systems integration in cognitive neuroscience, and emergent in-context learning behaviors. It extends the conceptual exploration of neural networks' capabilities beyond human languages to include the potential for translating animal communications. This interdisciplinary foundation enriches our theoretical framework for UMT, where parallel translations are scarce or non-existent, and where source and target corpora may not align in content or linguistic structure. We propose a theoretical framework to analyze UMT under these constraints, exemplified through two models of language that our framework predicts will have varied sample complexity needs. This complexity is inversely related to language complexity and the degree of common ground, suggesting a potential for translating complex animal communications. Our findings, corroborated by believable estimates derived from synthetic data and extensive reasoning about the process, offer insights into the feasibilities and limitations of applying UMT to understand animal communication systems. We leverage attention mechanisms and hierarchical representations, trained on diverse datasets, including large ones, to account for the nuances and intricacies of inter-species communication paradigms. The judgments about system's performance in various scenarios were key in refining our models.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~James_McClelland1",
  "manipulated_ranking": 29,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=VtkGvGcGe3",
  "title": "Evaluating Cognitive Maps and Planning in Large Language Models with CogEval",
  "modified_abstract": "Inspired by prior advances in pre-trained language models for tasks requiring sequential decision making, this work extends the analysis to the cognitive realms of language models (LLMs). Recent studies have achieved notable successes in leveraging models for tasks such as behavior cloning, offline reinforcement learning (RL), and imitating human behavior through approaches like UniMASK and diffusion models. These studies, which demonstrate the potential of LLMs in understanding and generating structured sequences through a variety of sampling techniques, paved the way for investigating LLMs' emergent cognitive abilities. However, most current assessments of these cognitive capabilities rely on anecdotes or overlook potential contamination of training sets, failing to provide a systematic evaluation across multiple tasks, control conditions, iterations, and robust statistical tests. Addressing this gap, we introduce CogEval, a cognitive science-inspired protocol for systematically evaluating the cognitive capacities of LLMs, including cognitive maps and planning abilities, by considering different hyperparameters settings to understand their impact on model performance. By applying CogEval to eight state-of-the-art LLMs, including OpenAI's GPT-4 and Google Bard among others, we discover that despite some competencies in simpler planning tasks, LLMs exhibit significant failure modes when confronting more complex planning scenarios, characterized by hallucinations and looping behaviors. These findings suggest that the purported emergent planning abilities in LLMs may not extend to understanding latent relational structures or unrolling goal-directed trajectories based on such structures, highlighting a crucial distribution of competencies and limitations. Our findings offer insights into the current limitations of LLMs and the need for multimodal computing approaches that consider the complexity of tokens processed, proposing directions for future research to overcome these hurdles and achieve genuinely emergent cognitive abilities in machine learning models.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Mingfei_Sun1",
  "manipulated_ranking": 27,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=SquMNyrk1O",
  "title": "Winner-Take-All Column Row Sampling for Memory Efficient Adaptation of Language Model",
  "modified_abstract": "Addressing the critical challenge of escalating memory usage in fine-tuning large pre-trained language models, our research is informed by pioneering strategies in machine learning that tackle variance reduction, feature map optimization, and efficient stochastic optimization. Observations from prior works, including advancements in online learning for low-variance tasks, conditional meta-learning to cater to task-specific nuances, and novel approaches in biased regularization within a learning-to-learn framework, serve as a foundation for our exploration. These insights pave the way for addressing the substantial memory bottleneck encountered during training, predominantly caused by the storage of feature maps necessary for gradient computation. By conceptualizing a novel family of unbiased estimators dubbed Winner-Take-All (WTA), specifically designed for matrix production with mitigated variance, we present a methodology that substantially reduces the necessary memory while preserving the integrity of gradient calculation by only retaining a sub-sampled portion of activations for each datapoint. Our theoretically and experimentally substantiated approach not only demonstrates reduced variance in comparison to existing estimators but also showcases up to 2.7X peak memory reduction and the facilitation of up to $6.4\\times$ larger batch sizes in transformer tuning through intelligent adaptive conditioning. This methodology supports multi-task learning through efficient resource use, allowing for the allocation of more computational resources per task within a multi-task framework. Moreover, our approach, embodying conditioning without aggressive memory management, creates the potential for time-series data processing by extending the technique to manage sequential datapoints more efficiently. These benefits accrue without compromising accuracy, thereby enabling enhanced down-streaming task performance through the deployment of larger models or expedited training processes. Furthermore, our work underlines the potential for broader applicability in memory-efficient algorithm designs for large-scale model adaptation, incorporating meta-parameters in the process.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Giulia_Denevi1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=sL4pJBXkxu",
  "title": "ELDEN: Exploration via Local Dependencies",
  "modified_abstract": "Tasks with large state space and sparse rewards present a longstanding challenge to reinforcement learning. In these tasks, an agent needs to explore the state space efficiently until it finds a reward. Recent advancements in model-based reinforcement learning, including those using implicit models for simple offline RL, self-supervised learning for robotic tasks involving grasping and imitation, and skill acquisition through instruction augmentation, have underscored the potential of leveraging intrinsic motivation and complex model dynamics to enhance exploration and learning efficiency. Building on these insights, this work proposes ELDEN, Exploration via Local DepENdencies, as a novel approach to incentivizing exploration in environments characterized by factored state spaces and complex chained dependencies. By focusing on the local dependencies between entities, and utilizing a novel scheme based on the partial derivative of the learned dynamics, ELDEN generates an intrinsic reward that encourages the discovery of new interactions and avoids common pitfalls like sparse rewards. This approach reflects a significant shift towards recognizing and modeling the nuanced interactions within environments, aligning with contemporary methods that emphasize understanding and exploiting the structure of the problem space, including vision-language integration for richer environmental interpretations. Evaluated across a range of domains, from 2D grid worlds to more complex 3D robotic tasks, ELDEN demonstrates the ability to correctly identify local dependencies and formulate successful policies, outperforming existing state-of-the-art methods in exploration efficiency and setting new benchmarks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jonathan_Tompson1",
  "manipulated_ranking": 20,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=2gn9WFlqJ4",
  "title": "Mode Connectivity in Auction Design",
  "modified_abstract": "Optimal auction design is a fundamental problem in algorithmic game theory, presenting challenges even in basic settings. This work builds upon recent findings in differentiable economics, where neural networks have been shown to effectively learn existing optimal auction mechanisms and unearth novel ones. Inspiration for our theoretical investigation into the empirical success of such networks comes from advancements in the domain of continual learning environments, particularly focusing on properties like polynomial mixing times that influence learning dynamics over time. Specifically, we examine RochetNet and its generalized version for affine maximizer auctions. We demonstrate that these networks exhibit mode connectivity, a property suggesting that locally optimal solutions are connected through a simple, piecewise linear path, wherein every point along the path maintains near-optimal utility. This notion of mode connectivity, previously observed empirically in neural networks for prediction tasks and multiple task learning, is herein theoretically justified within the sphere of differentiable economics, highlighting the utility of pretrained neural networks in solving inherently non-convex optimization challenges inherent in auction design. Theoretical discussions on the duration and development of learning approaches and the performance of networks over time further reinforce the practical implications of mode connectivity for continual learning paradigms in auction theory. Our findings indicate the importance of considering mode connectivity as a crucial parameter in optimizing learning over time for multiple tasks in differentiable economics.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Gopeshh_Raaj_Subbaraj1",
  "manipulated_ranking": 7,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=yEfmhgwslQ",
  "title": "Encoding Time-Series Explanations through Self-Supervised Model Behavior Consistency",
  "modified_abstract": "Interpreting time series models stands as a significant challenge due to the dual requirement of pinpointing pivotal time series signals and matching them to understandable temporal patterns. This challenge is amplified by the fact that explainers from other domains, when applied to time series, often do not adequately capture the unique inductive biases necessary for effective interpretation of temporal data. Inspired by foundational works that explore model-centric explanations within AI, such as boolean satisfiability approaches that yield robust, actionable insights distinct from data-centric methods like LIME and SHAP, our research introduces TimeX. TimeX is a novel approach for training explainers on time series data, designed to train an interpretable surrogate that emulates the behavior of a pre-trained time series model. By focusing on model behavior consistency, TimeX seeks to preserve relational integrity within latent spaces, offering discrete attribution maps for the precise interpretation that are critical for decision-making in various applications. Diverging from existing methods, it learns an explanatory latent space, facilitating visual aggregation of explanations and recognition of temporal patterns that are essential in critical decision-making. Tested across eight synthetic and real-world datasets, TimeX demonstrates superior or near-top performance on all evaluation metrics when contrasted with leading interpretability approaches. Moreover, our case studies on physiological time series data underscore TimeX's potential in crafting faithful, interpretable models that accurately reflect the complexities and decision-making processes of pre-trained time series models.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Nicholas_Gisolfi1",
  "manipulated_ranking": 7,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=c5WOU7p4ES",
  "title": "PLASTIC: Improving Input and Label Plasticity for Sample Efficient Reinforcement Learning",
  "modified_abstract": "In the pursuit of addressing challenges in Reinforcement Learning (RL) akin to those explored in multi-agent learning, reward shaping, and safe reinforcement learning scenarios, our study introduces a novel algorithm, PLASTIC, aimed at enhancing sample efficiency by preserving model plasticity. Acknowledging the foundational work demonstrated in techniques for learnable intrinsic-reward generation, automated reward shaping through agent interaction in sparse environments, and safety-augmented state-space dynamics for constraint adherence in RL systems, our investigation pivots towards a dual-focused examination of plasticity. By categorizing it into input plasticity (the model's adaptability to changing input data) and label plasticity (the model's resilience to evolving input-output mappings), we analyze the phenomena contributing to models' overfitting to earlier experiences\u2014termed as loss of plasticity. Through synthetic experiments using the CIFAR-10 dataset, we discovered that targeting smoother minima within the loss landscape bolsters input plasticity, while enhanced gradient propagation fortifies label plasticity. Theoretically, this contributes to the broader theory of learning systems, enabling them for a wide range of uses beyond RL, including decentralized learning environments. The delineation of these strategies culminates in the development of PLASTIC, which succinctly integrates mechanisms to nurture both input and label plasticity. Evaluated on benchmark tasks such as Atari-100k and the Deepmind Control Suite, PLASTIC not only demonstrates competitive prowess with minimal architectural adjustments but also underscores the significance of maintaining a balanced plasticity for elevating RL sample efficiency. The adaptive mechanisms of PLASTIC consider time-sensitive changes in the environment and actions taken by agents, fine-tuning the model's ability to learn efficiently from limited experiences.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Taher_Jafferjee1",
  "manipulated_ranking": 44,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=oyV9FslE3j",
  "title": "Temperature Balancing, Layer-wise Weight Analysis, and Neural Network Training",
  "modified_abstract": "This study synthesizes insights from recent explorations into the performance of machine learning models under domain shifts and image corruptions, leveraging self-learning and self-supervised methods for domain adaptation and robustness against common corruptions towards a coherent application in neural network training. In the context of these precedents, which demonstrate the utility of self-learning and noise adaptation in various domains from natural image processing to domain robustness, we introduce TempBalance, a novel layer-wise learning rate adjustment methodology. This method, inspired by the theory of Heavy-Tailed Self-Regularization (HT-SR), hinges on the characterization of the implicit self-regularization properties of neural network layers, and aims to optimize the learning rate or 'temperature' for each layer individually. By applying HT-SR-motivated metrics to guide the temperature balancing across network layers, including the consideration of affine transformations in layer activations, we propose a significant enhancement in network training performance, thus contributing to the field of neural network robustification. Moreover, our approach makes efficient use of unlabeled samples in the training dataset, highlighting the impact of sample diversity on the learning rate optimization. Empirical validation on clean and corrupted datasets such as CIFAR10, CIFAR100, SVHN, and TinyImageNet, across architectures including ResNets, VGGs, and WideResNets, underscores TempBalance's superiority over standard SGD, spectral norm regularization, and leading-edge optimizers and learning rate schedulers in both visual and other vision domains. This methodology not only steers neural network training towards more effective regularization but also paves the way for future research into layer-specific tuning for enhanced model performance across various domains.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Evgenia_Rusak1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=8SUtvEZCF2",
  "title": "Semantic segmentation of sparse irregular point clouds for leaf/wood discrimination",
  "modified_abstract": "In alignment with recent progresses in the exploitation of Lidar and machine learning algorithms for 3D object detection and depth estimation in dynamic scenes, this work ventures into the domain of remote sensing for environmental monitoring. Specifically, it addresses the challenges of Lidar (Light Detection and Ranging) in biosphere monitoring, where it has become an essential tool for mapping forest leaf area with unprecedented accuracy. This accuracy is critical for improving models of gas exchanges between the vegetation and the atmosphere, an area where leaf area variability has been a significant source of uncertainty. Leveraging advances in unmanned aerial vehicles (UAV) and novel camera sensor technologies for frequent revisits and monitoring vegetation response to climate change, we confront the issue of sparse and irregular point clouds generated by miniature sensors on UAVs. These sensors, including advances in imaging pipelines, produce data with limited density compounded by a decrease in density from canopy top to bottom due to occlusion. This research presents a neural network model founded on the Pointnet++ architecture, which uniquely processes point geometry via representations without relying on spectral information. Our method mitigates local data sparsity through an innovative sampling and optimization scheme designed to preserve crucial geometric information and introduces a loss function tailored to address severe class imbalance. We demonstrate the superior performance of our model over existing state-of-the-art methods in object detection on UAV-acquired point clouds. Furthermore, enhancements in the context of denser point clouds acquired from beneath the canopy, drawing parallels and lessons from breakthroughs in self-supervised depth estimation from gated images as well as attacks on neural nets for enhanced robustness, will be explored in future work.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Fahim_Mannan1",
  "manipulated_ranking": 52,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=XXPzBhOs4f",
  "title": "Have it your way: Individualized Privacy Assignment for DP-SGD",
  "modified_abstract": "The uniform approach to privacy budget allocation in differential privacy training, such as Differentially Private Stochastic Gradient Descent (DP-SGD), has been identified as limiting due to its inability to cater to individual privacy expectations across various domains, including but not limited to speech and image processing. Inspired by prior advancements in using public data for private model training, adaptive mechanisms for model update clipping, techniques for protecting labels in deep learning models during distributed training, and methods to counter speaker identity revelation in distributed automatic speech recognition (ASR) training, our research introduces Individualized DP-SGD (IDP-SGD). This variant modifies DP-SGD's sampling and gradient noising process through an optimization framework to support individualized privacy budgets, acknowledging that users may have differing privacy considerations. By providing privacy guarantees tailored to the preferences of individual clients, IDP-SGD improves the privacy-utility trade-offs. This conceptual leap forward is grounded in the understanding that the effectiveness of differential privacy's application in deep learning significantly benefits from flexibility and customization to meet individual user requirements, as evidenced by empirical validations across diverse datasets. Techniques for data reconstruction protection further enhance IDP-SGD's robustness against inference attacks, thereby advancing the state-of-the-art in privacy-preserving deep learning. Implicitly addressing the challenge of optimizing privacy assignment for each user ensures a more personalized and secure model training experience.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Swaroop_Ramaswamy1",
  "manipulated_ranking": 4,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=TiFMYdQiqp",
  "title": "Bayesian target optimisation for high-precision holographic optogenetics",
  "modified_abstract": "As highlighted by prior works in the fields of neural time series analysis and optogenetics, the control of neural activity with light has been significantly advanced by the development of models that predict neural responses to various stimuli. These models range from nonlinear autoregressive models for understanding cross-frequency coupling in neural time series, to convolutional sparse coding approaches for learning the spatial-temporal structure of brain signals. Inspired by the mathematical and computational frameworks established by these studies, we present a novel computational approach to precision optogenetics called Bayesian target optimisation. Our method leverages nonparametric Bayesian inference and multivariate analysis to model neural responses to optogenetic stimulation with high accuracy, addressing the key challenge of off-target stimulation (OTS) - the inadvertent activation of nearby non-target neurons. Through optimisation of laser powers and optical target locations, our approach aims to achieve precise control of neural ensemble activity with minimal OTS. Validation in simulations and in vitro experiments demonstrates the efficacy of Bayesian target optimisation in reducing OTS across varied conditions, employing statistically rigorous methods to ensure reliability. Furthermore, the development of this technique indicates progress in the classification of neural patterns associated with specific stimuli, enhancing our ability to manipulate neural circuit activity with unparalleled precision. This work not only advances the field of optogenetics by enabling more precise stimulation protocols but also integrates insights from signal processing, machine learning, time-series analysis, and convolutional analysis to overcome longstanding challenges in the control of neural circuit activity.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Tom_Dupre_la_Tour1",
  "manipulated_ranking": 35,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=OwpaO4w6K7",
  "title": "Jigsaw: Learning to Assemble Multiple Fractured Objects",
  "modified_abstract": "Inspired by recent advancements in detecting camouflaged objects using weak supervision and efficient mirror detection through hierarchical feature understanding, this work introduces Jigsaw, a framework designed for the automated assembly of physically broken 3D objects from multiple pieces. Our approach leverages hierarchical features from both global and local geometry to accurately match and align the fracture surfaces, incorporating methodologies akin to the segmentation and feature extraction techniques found in camouflaged object detection and efficient mirror recognition. Jigsaw consists of four components: (1) a front-end point feature extractor with attention layers, adapting the concept of focusing on salient features amidst complex backgrounds, (2) surface segmentation to distinguish fracture surfaces from intact parts, reflecting the emphasis on identifying subtle differences in object features per-pixel, (3) multi-parts matching for finding correspondences among fractured surface points, inspired by the precision needed in camouflaged object localization, and (4) a robust global alignment algorithm to accurately reconstruct the global poses of the pieces, ensuring consistency and coherence in assembly akin to maintaining feature consistency across various perceptions in mirror detection. Our inclusive strategy not only showcases superior performance on the Breaking Bad dataset but also incorporates scenarios of varying complexity, annotating the challenges and demonstrating a remarkable generalization across different fracture modes, objects, and unseen instances. Highlighting the novelty of applying learning-based methods specifically for 3D fracture assembly, this paper sets a precedent in the combined realms of computational geometry and machine learning.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ruozhen_He1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=awIpKpwTwF",
  "title": "LEACE: Perfect linear concept erasure in closed form",
  "modified_abstract": "Inspired by pioneering work in non-contrastive self-supervision, contrastive learning's feature decoupling principles, structured pruning in generative language models, and the discourse surrounding the necessity of disentangled representations for downstream tasks, our research introduces LEAst-squares Concept Erasure (LEACE). This novel methodology provides a closed-form solution that provably prevents all linear classifiers from detecting a specified concept in a given representation, with minimal alteration to the original representation as quantified by a broad class of norms. By applying LEACE to large language models through a process we term 'concept scrubbing', we can effectively erase target concept information from every layer of the network, thereby optimizing optimization/generalization towards unbiased performance. This method is showcased through two applications: assessing language models' reliance on part-of-speech information and mitigating gender bias in BERT embeddings, showcasing LEACE's potential to enhance both fairness and interpretability in machine learning models, especially in their large-scale architectures and their dense network performance on downstream tasks. Our code, available for further research exploration, allows for critical examinations of the effects of 'concept scrubbing' on a variety of samples drawn from these models.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zixin_Wen1",
  "manipulated_ranking": 12,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=bpzwUfX1UP",
  "title": "Parallel Sampling of Diffusion Models",
  "modified_abstract": "Diffusion models have shown considerable promise in generating high-quality samples for various applications, yet their practical utility is hindered by inherently slow sampling processes, often necessitating hundreds to thousands of sequential steps. Inspired by the recent advances in modelling dynamical systems through neural ordinary differential equations (ODEs) and their parallelization strategies, such as the use of variational inference, latent Gaussian process ODEs for efficient trajectory segmentation, the formulation of latent spaces for dynamical interactions, and modulation strategies for attention mechanisms, our research presents an innovative approach to accelerate diffusion model sampling. By exploring the possibility of running the denoising steps in parallel\u2014thus trading compute for speed\u2014we introduce ParaDiGMS, a pioneering diffusion sampling technique. This method capitalizes on the theoretical foundations laid by prior works in sparse Bayesian multiple shooting and variational multiple shooting for Bayesian ODEs with Gaussian processes, leveraging insights from attention mechanisms, decoupled workflow patterns to parallelize diffusion model sampling effectively. ParaDiGMS uniquely enables acceleration by denoising multiple steps simultaneously, integrating seamlessly with fast sampling techniques such as DDIM and DPMSolver, and significantly enhancing sampling efficiency without sacrificing quality. Our empirical evaluations across diverse application domains, including robotics and image generation, showcase that ParaDiGMS, unlike traditional auto-regressive models, can achieve a 2-4x improvement in sampling speed while maintaining or improving the fidelity of generated samples, thereby offering a groundbreaking solution to one of the most pressing limitations in the deployment of diffusion models. The oscillating means of sample quality and speed attainment in our method demonstrates its capability for far-horizon planning in applications where rapid generation of complex sequences is critical, thereby enhancing recognition and prediction in dynamic systems.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Cagatay_Yildiz1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=AOKU4nRw1W",
  "title": "Linguistic Binding in Diffusion Models: Enhancing Attribute Correspondence through Attention Map Alignment",
  "modified_abstract": "In the context of rapidly advancing text-conditioned image generation technologies, our work introduces SynGen, aimed at addressing the challenge of incorrect associations between textual entities and their visual attributes, a problem that persists in current models despite notable advancements in interactive image retrieval, compositional reasoning in vision and language (VL) models, and the utilization of synthetic data to enhance model performance. Inspired by prior research exploring the dynamics of natural language queries in image retrieval, the importance of dense and aligned captions for improving VL models' compositional reasoning, and the innovative use of synthetic data to teach VL models to capture non-object visual language concepts, our approach proposes a novel method to ensure accurate linguistic binding in text-to-image generation. SynGen analyzes the prompt syntactically to identify entities and their modifiers, subsequently optimizing a novel loss function that enforces alignment between cross-attention maps and the linguistic structure of the prompt. This method encourages a significant overlap in recognition maps of entities and their respective modifiers while reducing overlap with unrelated entities and modifiers. Optimized during inference without necessitating retraining, self-training, or fine-tuning of the underlying codebase, our approach demonstrates notable improvements in generating images that faithfully represent the textual prompt's intended meanings, as illustrated by human evaluations across three datasets, including a particularly challenging new dataset. This approach not only highlights the effectiveness of leveraging sentence structure during inference for recognition and answering the challenge of attribute correspondence but also sets a promising direction for curriculum learning in the fidelity of text-to-image generation.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Paola_Cascante-Bonilla1",
  "manipulated_ranking": 29,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=LlERoXEKjh",
  "title": "Training shallow ReLU networks on noisy data using hinge loss: when do we overfit and is it benign?",
  "modified_abstract": "Inspired by prior works that delve into the domains of feature learning through neural networks beyond kernels, learnability of halfspaces, and PAC-Bayes learning bounds with sample-dependent priors, our study explores the phenomenon of benign overfitting in two-layer ReLU networks. Specifically, we investigate this phenomenon in the context of training on noisy data for binary classification tasks using gradient descent and hinge loss, where data is linearly separable but includes a subset of labels that are corrupt or flipped. We identify conditions under the margin of clean data that delineate three distinct training outcomes: benign overfitting, in which zero loss is achieved with high probability of correct test data classification; overfitting, leading to zero loss but high probability of test data misclassification; and non-overfitting, where clean points reach zero loss while corrupt points do not, again with high chances of correct classification on test data. Our analysis provides a nuanced understanding of neuron dynamics during training, highlighting phases where clean points approach zero loss and the eventual behavior of corrupt points. Through a combinatorial approach that estimates clean versus corrupt updates during training, we contribute to the theoretical backbone required for discerning and leveraging overfitting in practical applications. The adaptivity of ReLU networks to sparse, correlative structures in data, despite the absence of explicit image inputs, indicates a broader applicability of our findings beyond simple binary tasks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Stefani_Karp1",
  "manipulated_ranking": 102,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=iPTF2hON1C",
  "title": "Learning To Dive In Branch And Bound",
  "modified_abstract": "Recent advancements in optimization benchmarks and sparse model estimation techniques establish a robust foundation for enhancing algorithmic efficiency and assessing methodological impacts in machine learning. Inspired by these foundational works, which emphasize the importance of reproducibility, efficiency, and the nuanced understanding of algorithm performance, this paper introduces L2Dive\u2014a novel approach that integrates the learning capabilities of graph neural networks with primal heuristics to optimize mixed integer linear programs (MILPs) in the context of branch and bound search methods. Primal heuristics, particularly diving heuristics, play a crucial role by finding feasible solutions that facilitate the search process. However, their effectiveness is often hampered by generic decision-making rules that overlook the structural similarities across different problem instances. L2Dive addresses this by learning specific diving heuristics that predict variable assignments and make informed diving decisions based on the duality of linear programs. Fully integrated into the open-source solver SCIP, L2Dive demonstrates superior performance in discovering better feasible solutions across various combinatorial optimization problems compared to standard divers. For applications in server load balancing and neural network verification, L2Dive not only enhances the primal-dual integral by up to 7% (35%) on average over tuned (default) baselines but also significantly reduces solving time by 20% (29%), showcasing its potential to significantly improve problem-solving efficiencies in real-world settings. Benchmarking the reproducibility of L2Dive's superior performance highlights its application across various problem domains and stresses the importance of rigorous training protocols in ensuring the effectiveness of learning-driven approaches. The use of L2Dive for complex tasks illustrates the transformative potential of integrating advanced learning techniques in optimization.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Pierre-Antoine_Bannier1",
  "manipulated_ranking": 23,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=CzkOzKWpMa",
  "title": "Optimal cross-learning for contextual bandits with unknown context distributions",
  "modified_abstract": "In the context of online learning and decision-making, recent developments have demonstrated how understanding contextual information can dramatically enhance predictive performance and decision accuracy. Inspired by such precedents, including the novel online learning settings in Autoregressive Dynamics, where the integration of autoregressive processes into bandit algorithms has paved the way for sophisticated approaches to sequential decision-making under uncertainty, our work advances the field of contextual bandits. We consider the problem of designing contextual bandit algorithms in the \"cross-learning\" setting of Balseiro et al., where the learner observes the loss for the action they play in all possible contexts, not just the context of the current round. Specifically, we address the challenge where losses are chosen adversarially and contexts are sampled i.i.d. from an unknown distribution, empirically demonstrating our algorithm's efficacy across a large scale of contexts. In this setting, we resolve an open problem by Balseiro et al., providing an efficient algorithm with a nearly tight (up to logarithmic factors) regret bound of $\\widetilde{O}(\\sqrt{TK})$, independent of the number of contexts. This outcome leads to the first nearly tight regret bounds for learning to bid in first-price auctions under unknown value distributions and sleeping bandits with a stochastic action set. Highlighting the core of our approach, we introduce a novel technique for coordinating the execution of a learning algorithm over multiple epochs to remove correlations between estimation of the unknown distribution and the actions played by the algorithm. This technique potentially holds broader implications for other learning problems that involve estimating an unknown context distribution.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Gianmarco_Genalti1",
  "manipulated_ranking": 4,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=cRzt1umRNx",
  "title": "Riemannian Residual Neural Networks",
  "modified_abstract": "Informed by the burgeoning exploration of geometric deep learning and its applications in natural sciences and hierarchical data structures, recent methods have introduced various neural networks to operate over data that lie on Riemannian manifolds. This advancement is crucial for learning from manifold-valued data, a common occurrence in disciplines such as physics and biology, and for processing information structured in graphs. Our work is further inspired by the deep investigations into generalization errors, transfer learning, and the phenomenon of double descent in linear regression tasks as well as the surprising generalization capabilities of overparameterized models. We examine the residual neural network (ResNet), a cornerstone of modern machine learning for its ability to mitigate the vanishing gradient problem through optimization techniques and support deep network architectures, and show how to extend its construction to general Riemannian manifolds in a geometrically principled manner. Despite the challenges inherent in moving beyond Euclidean spaces, we find that our Riemannian ResNets not only adapt the beneficial properties of their Euclidean counterparts but also outperform existing networks designed for specific manifolds, across relevant testing metrics and training dynamics. This achievement indicates a significant step towards generalizing the ResNet architecture in a way that maintains its desirable properties across a broader spectrum of mathematical spaces, offering a succinct and comprehensive overview of its applicability to a wider variety of data types and structures. Regarding the empirical demonstration of these results, our work embraces the critical examination of the descent phenomena that inform much of the theoretical foundation in linear models and extends it into the realm of overparameterization, revealing nuanced insights into the training and performance landscape of such networks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yehuda_Dar1",
  "manipulated_ranking": 6,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=iImnbUVhok",
  "title": "Joint Prompt Optimization of Stacked LLMs using Variational Inference",
  "modified_abstract": "Inspired by recent advancements in controllable text generation, ethical natural language generation, and alignment of large language models (LLMs) with human values, this study introduces a novel approach to optimize prompts for stacked LLMs using variational inference. Our work synthesizes insights from efforts to enhance text generation diversity, address moral issues in LLM outputs, achieving a diverse and massive scale of ethical considerations, and align LLM abilities with intrinsic human values, thereby setting a new direction for prompt optimization that incorporates the complexity of human language understanding, ethical considerations, and stylistic modification. We showcase how large language models can be viewed as stochastic language layers, or atomic units of computation mapping sequences to distributions over sequences, forming a basis for Deep Language Networks (DLN). By iteratively optimizing prompts across these layers with self-training techniques, we achieve enhanced control over the generated language's properties and style. Initial experiments demonstrate the effectiveness of our single-layer model (DLN-1) across multiple reasoning and natural language understanding tasks. Subsequent extension to a two-layer model (DLN-2) suggests potential for achieving performance on par with GPT-4 through stacked, less powerful LLMs, foregrounding a scalable strategy for leveraging LLMs in complex linguistic tasks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Xiaoyuan_Yi1",
  "manipulated_ranking": 5,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=YFW6MVGVTn",
  "title": "NICE: NoIse-modulated Consistency rEgularization for Data-Efficient GANs",
  "modified_abstract": "Generative Adversarial Networks (GANs) have significantly evolved, guided by emerging insights from works addressing spectral regulation, augmentation strategies for class diversity, optimization in domain adaptation, and latent space regularization in StyleGANs, all contributing to advancements in vision-based image synthesis and generative tasks. Inspired by these precedents, this paper introduces a novel approach called NoIse-modulated Consistency rEgularization (NICE) to address the challenges of discriminator overfitting and training instability in GANs, which are exacerbated by limited data availability. By incorporating adaptive multiplicative noise into the discriminator, NICE modulates its latent feature embeddings to prevent overfitting through an adaptive reduction of the Rademacher complexity. However, this introduces an increased gradient norm that can affect GAN training stability. To counteract this, we enforce a consistency constraint on the discriminator for identical inputs under varying noise conditions, effectively penalizing both first and second-order gradients of latent features. This enhances stability across the training process, making the approach not only adversarial in its core nature but also smooth in its regularization technique. Our theoretical framework is empirically substantiated through experiments on CIFAR-10, CIFAR-100, ImageNet, and FFHQ datasets, showcasing NICE\u2019s ability to achieve state-of-the-art performance in data-limited contexts and low-shot generation scenarios, while mitigating the adverse effects typically associated with reduced training data for classification in GANs related to class-imbalanced conditions and attribute-change challenges. Importantly, NICE also demonstrates efficacy in detection and recognition tasks by significantly enhancing the model's generative capabilities.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Harsh_Rangwani1",
  "manipulated_ranking": 22,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=MWxsYPVmLS",
  "title": "Explainable and Efficient Randomized Voting Rules",
  "modified_abstract": "In the contemporary landscape of AI and machine learning, the prevalent deployment of AI tools for critical decision-making underscores an imperative need for explainability, particularly when such decisions involve voting mechanisms. Inspired by recent scholarly efforts to integrate fairness through varied strategies such as Lorenz dominance for balanced rankings, auditing processes for preferential fairness in recommender systems, equitable dataset construction via adaptive sampling, and ensuring fairness in score-based rankings through matched pair calibration, this paper focuses on enhancing the explainability and efficiency of randomized voting rules. Amidst advancements ensuring fairness and robustness in AI-driven systems, our work specifically investigates the potential efficiency gains offered by adopting simple randomization steps in deterministic voting rules, without compromising the explainability pivotal to stakeholder trust and acceptance. By theoretically and empirically analyzing two families of voting rules\u2014randomized positional scoring rules and random committee member rules\u2014this study delineates the feasibility of achieving explainability alongside efficiency through optimization techniques that cater to diverse subgroups and classification challenges in the broader field of economics and AI decision making. The synthesis of these considerations, including sample-efficient methods and evaluation functions, offers a holistic framework that not only addresses the critical aspects of fairness and transparency in AI decision-making but also aligns with the broader goal of leveraging AI for equitable and just outcomes across various domains, particularly in the context of ranking and classification systems.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sam_Corbett-Davies1",
  "manipulated_ranking": 4,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=GxL6PrmEUw",
  "title": "Distributional Learning of Variational AutoEncoder: Application to Synthetic Data Generation",
  "modified_abstract": "Inspired by recent developments in causal inference and generative modeling, such as Counterfactual VAE for estimating treatment effects with unobserved confounding and $\\beta$-Intact-VAE's novel approach to identifying and estimating treatment effects under conditions of limited overlap, this paper proposes a novel Variational Autoencoder (VAE) architecture. Our approach, transcending the traditional Gaussianity assumption criticized as a limitation of existing VAE models, incorporates an infinite mixture of asymmetric Laplace distribution in the decoder. This enhancement significantly increases the autoencoder's expressive power for fitting distributions of continuous variables and facilitates the identification of data transformation strategies. The proposed model is presented as a specialized nonparametric M-estimator designed for the estimation of general quantile functions, establishing a theoretical connection between the distributional learning capabilities of our VAE, quantile estimation, and its potential prognostic capabilities in biostatistics. Importantly, the relevance of our model to synthetic data generation lies in its ability to adjust the level of data privacy, thereby demonstrating its superiority in comparison to conventional methods and emphasizing the importance of overlap in treatment effect studies. Our contribution also highlights the cause of certain issues in traditional models and offers a balanced approach towards addressing them. Therefore, our work extends the theoretical and practical boundaries of VAE models, providing a more flexible approach to synthetic data generation that could be of significant interest to researchers and practitioners working on issues related to data privacy, causal inference, and treatment effect estimation.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Pengzhou_Abel_Wu1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=5UXXhVI08r",
  "title": "Dynamic Prompt Learning: Addressing Cross-Attention Leakage for Text-Based Image Editing",
  "modified_abstract": "Innovations in large-scale text-to-image generative models, particularly diffusion models, have underscored the potential of generative AI in synthesizing convincing images from textual prompts. This evolution is paralleled by significant strides in foundational models for complex applications including action recognition in object-centric video understanding, structured representation enrichment in vision & language models, and unsupervised domain generalization. Reflecting on these advancements, our study introduces $\\textit{Dynamic Prompt Learning}$ ($DPL$) as a solution to the prevalent issue of cross-attention leakage in text-based image editing\u2014a challenge that results in unintended modifications outside the targeted image areas. By dynamically updating tokens for nouns in the textual input coupled with the proposed leakage repairment losses, $DPL$ offers granular control over image editing tasks, efficiently focusing cross-attention maps on desired objects within the image. Training within this context emphasizes the 'recognition' of nuanced text descriptions and their precise alignment to relevant image parts as critical to prevent leakage, highlighting the intersection with class-agnostic methods which aim to generalize across diverse image content without prior object-specific pretraining. Adopting $\\textit{Stable Diffusion}$ for our experiments, we demonstrate $DPL$'s effectiveness through superior quantitative (e.g., CLIP score, Structure-Dist) and qualitative results, verified by user evaluations. The methodology especially excels in complex multi-object scenes, with advancements in structured graph-based approaches to textual and visual content, presenting enhanced capabilities in Word-Swap, Prompt Refinement, and Attention Re-weighting tasks. This work not only addresses a critical challenge in text-based image editing but also sets a new benchmark for future research in the domain, effectively leveraging the foundational insights provided by preceding studies on model specialization, object representation, and cross-domain generalization.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Roei_Herzig2",
  "manipulated_ranking": 15,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=MCkUS1P3Sh",
  "title": "Nash Regret Guarantees for Linear Bandits",
  "modified_abstract": "Our work is directed by insights from a diversity of recent advancements in the broader field of reinforcement learning, including studies on multi-agent bandits, cooperative bandits under imperfect communication conditions, and novel approaches to reinforcement learning in high-dimensional spaces through Hamiltonian Monte Carlo sampling. These prior works highlight critical challenges in multi-agent learning environments, such as information sharing among agents via efficient message-passing algorithms, the efficacy of decentralized algorithms in imperfect communication networks, monitoring performance in adaptive environments through online learning strategies, and the application of advanced sampling methods, notably importance-sampling and matrix-based techniques, for efficient learning in complex environments. Building on these foundational studies, we focus on the stochastic linear bandits problem, introducing the concept of Nash regret as a measure of the collective welfare generated by a linear bandit algorithm over a horizon of $\\mathsf{T}$ rounds within ambient dimension $d$. By defining Nash regret with respect to the Nash social welfare (NSW) function, known for satisfying fairness axioms, we frame an upper bound on Nash regret as a fairness guarantee. Our proposed algorithm, utilizing the successive elimination method enhanced with novel concentration bounds and sampling techniques, notably via John ellipsoid in conjunction with the Kiefer-Wolfowitz optimal design, achieves a Nash regret of $O\\left( \\sqrt{\\frac{d}{\\mathsf{T}}} \\log(\\mathsf{T} |{\\cal X}|)\\right)$ for finite arm sets and $O\\left( \\frac{d^{\\frac{5}{4}}}{\\sqrt{\\mathsf{T}}}  \\log(\\mathsf{T})\\right)$ in scenarios where the arm set is not necessarily finite, thereby providing significant implications for fairness in algorithmic decision making and dynamical systems planning.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Udari_Madhushani1",
  "manipulated_ranking": 18,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=No52399wXA",
  "title": "IPMix: Label-Preserving Data Augmentation Method for Training Robust Classifiers",
  "modified_abstract": "Inspired by significant strides in semi-supervised learning, domain adaptation, and multi-task learning - where techniques like debiased self-training, auxiliary-task learning, and adversarial optimization strategies enhance model performance and robustness - we introduce IPMix, a novel data augmentation method aimed at achieving superior classifier robustness. Data augmentation is a well-established technique for boosting the performance of convolutional neural network classifiers by preventing overfitting and has been pivotal in domains ranging from natural language processing to agricultural and healthcare predictions. Adaptation to varying data distributions underlines the method's efficacy, incorporating label-preserving image-level, patch-level, and pixel-level augmentations into a single, comprehensive framework. This approach enhances training data diversity with minimal computational overhead, promoting not just accuracy on clean data but also robustness against distributional shifts, effectively addressing the adaptation problem within the data augmentation context. Contrary to previous assertions of an accuracy-robustness trade-off, IPMix innovates by introducing structural complexity at varying levels and harnessing random mixing for multi-scale information fusion, thus significantly boosting robustness against data corruption and enhancing adversarial perturbation resistance, calibration, prediction consistency, and anomaly detection capabilities. Tests show IPMix's effectiveness across CIFAR-C and ImageNet-C benchmarks, with noteworthy improvements over existing methods in robustness, and achieving state-of-the-art or comparable outcomes in ImageNet-R, ImageNet-A, and ImageNet-O. Through rigorous validation procedures, IPMix establishes new benchmarks for robust classifier training, contributing to the learnability and problem-solving effectiveness of complex datasets in diverse applications.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ximei_Wang1",
  "manipulated_ranking": 31,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=zn5ihqknGj",
  "title": "An Alternating Optimization Method for Bilevel Problems under the Polyak-\u0141ojasiewicz Condition",
  "modified_abstract": "Bilevel optimization, an area of increased interest within machine learning, involves challenges that intersect with research on online convex optimization, algorithmic solutions for semidefinite programming, online adaptive principal component analysis (PCA), and dynamics of online control and robust PCA. This body of work inspires our approach to tackling bilevel problems, especially in emerging fields such as hyperparameter optimization, meta-learning, and reinforcement learning where robust, privacy-preserving, and efficient algorithmic solutions are paramount. Incorporating aspects of regularization to manage overfitting and ensure generalization in hyperparameter tuning, our investigation begins with the acknowledgment that while simple alternating gradient-based algorithms have shown promise in addressing bilevel problems with strongly convex lower-level objectives, their applicability to more complex scenarios remains underexplored. We introduce a novel stationary metric tailored for nonconvex lower-level objectives that meet the Polyak-\u0141ojasiewicz condition, presenting a Generalized ALternating mEthod for bilevel opTimization (GALET). GALET, by extracting and utilizing components critical for optimization, is specifically designed for these enriched settings and demonstrates that it can achieve an $\\epsilon$-stationary point within $\\tilde{\\cal O}(\\epsilon^{-1})$ iterations, considering the variance and projection challenges inherent in these types of optimization problems. This aligns the iteration complexity of GALET with that of gradient descent for single-level smooth nonconvex problems, thereby pushing the boundary of what is achievable in bilevel optimization under the specified condition.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jianjun_Yuan2",
  "manipulated_ranking": 49,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=8vuDHCxrmy",
  "title": "OpenMask3D: Open-Vocabulary 3D Instance Segmentation",
  "modified_abstract": "Inspired by recent advancements in few-shot learning, object detection, and multi-label recognition, this work introduces the novel task of open-vocabulary 3D instance segmentation. While current 3D instance segmentation approaches are restricted to recognizing object categories from a closed set defined during training, real-world applications frequently encounter objects outside these confines, necessitating a method capable of handling novel, open-vocabulary queries. Leveraging insights from adaptive prototype learning, decoupling classifiers for reduced bias, and robust set representation methods, we propose OpenMask3D\u2014a zero-shot approach for open-vocabulary 3D instance segmentation. OpenMask3D employs predicted class-agnostic 3D instance masks and aggregates per-mask features through multi-view fusion of CLIP-based image embeddings, showcasing significant steps forward in 3D scene understanding with an innovative architecture that leverages both convolutional neural layers for feature extraction and the detection capabilities fostered by few-shot object detection (FSOD) paradigms. Evaluated on ScanNet200 and Replica datasets, OpenMask3D demonstrates superior performance over existing open-vocabulary methods, particularly on long-tail distributions and objects traditionally challenging for recognition systems. Qualitative experiments highlight our method's adeptness at segmenting objects based on free-form queries about geometry, affordances, and materials.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Bin-Bin_Gao1",
  "manipulated_ranking": 39,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=p40XRfBX96",
  "title": "Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision",
  "modified_abstract": "This research is inspired by recent works on adaptation techniques for pretrained language models, compositional generalization capabilities, and few-shot learning methodologies, which collectively set a foundation for exploring self-alignment with minimal human supervision. Notably, innovations in input-tuning for adapting unfamiliar inputs, in-context example selection for compositional generalization, and skill-based few-shot selection underscore the potential for models to learn and generalize from limited and targeted inputs. Against this backdrop, our study introduces SELF-ALIGN, a novel self-supervised framework that enables language models to align their outputs with human values using principle-driven reasoning and minimal human intervention. By generating synthetic prompts and leveraging a small set of human-crafted principles, we guide the neural networks of the language model to produce responses that are not only helpful, ethical, and reliable but also diversified and self-consistent. Furthermore, through fine-tuning with self-aligned high-quality responses, we enhance the model's generalization--understanding ability to directly generate desirable responses for queries sans explicit principle and demonstration inputs in subsequent interactions. An additional refinement step is introduced to mitigate issues related to brief or indirect responses. The application of our approach to a base language model, LLaMA-65b, results in the development of Dromedary, an AI assistant that outperforms existing state-of-the-art AI systems in benchmark evaluations, despite utilizing fewer than 300 lines of human annotations. This underscores the viability and efficiency of our method in producing sophisticated, aligned language models capable of understanding and assisting with human tasks, whilst overcoming the challenges of traditional supervised fine-tuning and reinforcement learning methodologies with advanced cognition and intelligence. The development processes and resulting characteristics of these models demonstrate advanced neural architectures and argument structures, essential for their performance and adaptability.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Shengnan_An1",
  "manipulated_ranking": 16,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=MWQjqtV1z4",
  "title": "Restless Bandits with Average Reward: Breaking the Uniform Global Attractor Assumption",
  "modified_abstract": "In this investigation, we address the infinite-horizon restless bandit problem with the average reward criterion in both discrete-time and continuous-time settings, acknowledging the diverse landscape of strategic interactions and optimizations described in recent literature across game theory, equilibrium computation, and multi-agent systems that often involve working on extensive-form games where strategies may communicate or be augmented with mediators. Our study is particularly inspired by the themes explored in previous works on optimal equilibria, adversarial team games, sparse representations, and the computational intricacies of extensive-form games with a focus on mediator-augmented strategies, which collectively emphasize the nuanced challenges of developing efficient learning strategies within complex, dynamic environments. The utilization of learning mechanisms in devising strategies mirrors column-generation techniques in computational optimization, presenting a novel perspective for achieving efficiency. We identify a critical gap in the existing body of research regarding the efficient computation of policies that achieve a diminishing optimality gap as the number of arms, $N$, grows large. Prior approaches to addressing this issue have uniformly relied on the uniform global attractor property (UGAP), a condition that is both complex and challenging to verify. Contrary to these methodologies, our paper introduces a novel, general, simulation-based framework, Follow-the-Virtual-Advice, that leverages any single-armed policy to engineer a policy for the original $N$-armed problem by simulating the single-armed policy on each arm and guiding the real state towards the simulated state. This method essentially allows each bandit to 'learn' from the virtual experiences, akin to exploring a decision graph of possible outcomes. We demonstrate that our framework can achieve an $O(1/\\sqrt{N})$ optimality gap, even in sparse settings. In the discrete-time setting, we introduce a simpler synchronization assumption that accommodates instances violating UGAP. Remarkably, in the continuous-time setting, our results do not necessitate any assumptions beyond the standard unichain condition, marking a departure from UGAP dependence. This paper is the first to establish asymptotic optimality in these settings without relying on the UGAP, thus paving the way for more accessible and broadly applicable solutions in restless bandit problems.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Brian_Hu_Zhang1",
  "manipulated_ranking": 6,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=o7W0Zet6p3",
  "title": "Recovering Unbalanced Communities in the Stochastic Block Model with Application to Clustering with a Faulty Oracle",
  "modified_abstract": "Leveraging insights from seminal works on network-based analysis and community detection, this study advances our understanding of the stochastic block model (SBM) with a focus on unbalanced communities, a scenario reflective of real-world networks in social media analysis and beyond. An analysis of diverse applications, from inferring social tie strengths, graph decomposition to iterative data mining approaches in sequences and itemsets, informs our exploration of SBMs with varying community sizes. Our work extends the well-established frame of studies in graph clustering, addressing the limitations faced when assuming uniform community sizes and providing a novel SVD-based algorithm for community recovery in SBMs with unbalanced structures. This advancement builds upon the mathematical foundations and formalizations established by Ailon, Chen, and Xu by eliminating the assumption of a large interval excluding certain cluster sizes and decoupling the recoverability of community sizes from the number of clusters. Experimental evidence highlights the efficiency of our approach compared to previous methodologies, particularly under the planted clique conjecture, illustrating near-optimal cluster recovery capability, independent of constant probability parameters. Additionally, this research introduces an application to clustering in the presence of a faulty oracle, thereby marking a significant contribution to the fields of programming for network analysis, machine learning, and media content summaries through events detection in social networks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Nikolaj_Tatti1",
  "manipulated_ranking": 17,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=B7QRV4XXiK",
  "title": "An Alternative to Variance: Gini Deviation for Risk-averse Policy Gradient",
  "modified_abstract": "In the domain of Reinforcement Learning (RL), managing risk via the control of policy return variance is well-established, drawing from methods that seek to either directly limit total return variance or indirectly do so by constraining per-step reward variance. This paper contributes to the evolving discussion on risk management in RL, initially advanced by research on model-based value expansion methods and the empirical analysis of their limitations. Inspired by these conversations, particularly the insights into the diminishing returns of extended horizons and the nuanced impact of model accuracy on value function learning, our work advocates for the adoption of Gini deviation as a more robust alternative for quantifying and minimizing risk in reinforcement learning environments. By addressing the shortcomings inherent to variance-based measures\u2014particularly their scale sensitivity and detrimental effects on policy learning within both natural and artificially simulated testing loops\u2014our exploration underscores the efficacy of Gini deviation in environments that demand a risk-averse approach. Through a comprehensive study of Gini deviation's inherent properties and the development of a policy gradient algorithm tailored to minimize this risk measure, we provide empirical evidence of our method's superiority in simulators designed for RL testing. In risk-defined domains, our approach not only avoids the pitfalls of variance-based strategies but also secures a high return with reduced risk, measured in terms of both variance and Gini deviation, demonstrating its potential where traditional methods falter in policy learning.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Daniel_Palenicek1",
  "manipulated_ranking": 36,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=zMeemcUeXL",
  "title": "FAMO: Fast Adaptive Multitask Optimization",
  "modified_abstract": "The development of generalist agents capable of learning multiple tasks from diverse datasets represents a significant challenge and opportunity in artificial intelligence (AI), inspired by recent progress in various domains including transfer learning, automated reinforcement learning (AutoRL), and deep learning methodologies for complex optimization problems. Previously established methods have offered advancements in efficiently navigating the hyperparameter optimization landscape and utilizing deep learning for Bayesian inference, including regression and classification tasks, and enhancing posteriors estimation within a surrogate-model framework, setting the groundwork for more adaptable and efficient multitask learning (MTL) algorithms. In response, this paper introduces Fast Adaptive Multitask Optimization (FAMO), a method that dynamically balances the training across multiple tasks in an MTL setting with significantly reduced computational requirements ($\\mathcal{O}(1)$ space and time). By conducting a comprehensive analysis across a broad spectrum of multi-task supervised learning, including classification, and reinforcement learning problems, we demonstrate FAMO's ability to not only achieve competitive or superior performance compared to existing gradient manipulation techniques but also to substantially improve space and computational efficiency. Our findings underscore the potential of FAMO as a scalable solution for multitask optimization, addressing the limitations of previous approaches while paving the way for future innovations in the field. Notably, the code implementation of this method is shared for further research and application. The integration of machine learning and surrogates in our approach highlights the importance of ensembling techniques, which enhance the adaptability and predictive power of multitask optimization strategies, although the specific application to image data has not been the focus of this study.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sebastian_Pineda_Arango1",
  "manipulated_ranking": 27,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=jX49iKr6vb",
  "title": "Beyond Deep Ensembles: A Large-Scale Evaluation of Bayesian Deep Learning under Distribution Shift",
  "modified_abstract": "As Bayesian deep learning (BDL) endeavors to enhance the reliability and calibration of machine learning models, particularly under conditions of distributional shift, it stands on the shoulders of groundbreaking works such as Robey et al.'s exploration of probabilistic robustness in PAC learning. This foundational work exemplifies the ongoing exploration of robustness and reliability in machine learning, setting the stage for our comprehensive evaluation of BDL's performance in this complex landscape. Our research extends these inquiries by evaluating recent state-of-the-art (SOTA) classifier methods on a broad spectrum of realistic and challenging benchmark tasks derived from the WILDS collection, examining classification and regression tasks with an emphasis on generalization and calibration under distribution shift. Examples of these tasks, which exhibit significant perturbations to input data, illuminate the complexity of applying BDL in real-world scenarios. By comparing modern BDL algorithms across a variety of large, convolutional, and transformer-based neural network architectures, our study delves into a signed version of the expected calibration error to determine over- or underconfidence in model predictions. Furthermore, we conduct the first systematic evaluation of BDL applied to fine-tuning large, pre-trained models, where starting from scratch is not feasible. While extending single-mode posterior approximations to multiple modes via ensembles has shown to significantly enhance both the generalization capability and calibration of models, we uncover a specific failure mode when fine-tuning large transformer-based language models. In such cases, variational inference methods, notably last-layer Bayes By Backprop, demonstrably outperform in accuracy, while contemporary approximate inference algorithms like SWAG lead in calibration accuracy, offering a probabilistically robust solution to challenges introduced by distribution shifts.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~UNIQUE_SUBEDI1",
  "manipulated_ranking": 40,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=oqDSDKLd3S",
  "title": "Sample-Conditioned Hypothesis Stability Sharpens Information-Theoretic Generalization Bounds",
  "modified_abstract": "Inspired by recent theoretical investigations into the learning of linear operators and the application of the random feature model for input-output maps between Banach spaces, this work introduces a novel set of information-theoretic generalization guarantees. Our development of the \"neighboring-hypothesis\" matrix and the conception of sample-conditioned hypothesis (SCH) stability are motivated by the need for theoretical advancements that address the limitations acknowledged in existing approaches to stochastic convex optimization (SCO) problems. The proposed SCH stability framework not only incorporates insights from the learning behaviors observed in high-dimensional and functional spaces\u2014such as those represented by Hilbert and Banach spaces\u2014but also provides an analytic basis for deriving sharper generalization bounds that significantly improve upon prior information-theoretic outcomes across various learning scenarios. This contribution aids in overcoming challenges presented in recent research, especially in the context of learning scenarios that involve the manipulation of images and the inverse problems, offering refined mathematical tools for both theoretical enquiry and practical application in machine learning, particularly in noisy environments where the robustness of the learning process is paramount. Additionally, our findings have implications for a wide range of applications, illustrating the versatility and necessity of SCH stability in advancing the frontiers of knowledge in various domains, including those challenging environments where random variations play a significant role.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Nicholas_H_Nelsen1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=pzc6LnUxYN",
  "title": "StateMask: Explaining Deep Reinforcement Learning through State Mask",
  "modified_abstract": "The progress of deep reinforcement learning (DRL) in complex decision-making scenarios underscores the necessity for explainability, especially when deploying these agents in critical domains. Inspired by recent advancements that leverage mid-level representations, such as factorized flow maps and optical flow in modular learning, to bridge perception and control modules in robotics, our work introduces StateMask, a framework aimed at enhancing the explainability of DRL agents through factorization of state information. StateMask distinguishes itself from existing techniques by identifying the states most critical to the agent's final reward, rather than merely explaining individual actions. It operates by learning a mask net that selectively impairs the agent's perception, compelling it to take random actions at certain steps while theoretically maintaining overall performance parity with the unmasked agent. This experimental approach uniquely identifies and interprets the importance of specific states in the decision-making process. Unlike prior methods, StateMask offers a novel perspective by focusing on the decision-making process's crucial states, thereby providing a higher explanation fidelity through factorized representations. Our evaluations across various RL environments affirm StateMask's effectiveness and highlight its potential for applications such as adversarial attacks and policy correction, thus addressing a significant void in DRL explanation methods.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Li-Yuan_Tsao1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=x816mCbWpR",
  "title": "Recasting Continual Learning as Sequence Modeling",
  "modified_abstract": "Inspired by significant advancements in reinforcement learning and deep learning methods, such as model-based policy optimization, reinforcement learning with enhanced sample efficiency, gradient-aware model adaptation, and data sharing strategies in ensemble learning, this work aims to establish a strong connection between two significant bodies of machine learning research: continual learning and sequence modeling. Our proposition to formulate continual learning as a sequence modeling problem is positioned to leverage the advanced sequence models, especially Transformers and their efficient variants, for addressing the challenges inherent to continual learning. This formulation interprets the continual learning process as the forward pass of a sequence model, implicitly invoking mechanisms akin to backpropagation for sequential data processing. By incorporating the meta-continual learning (MCL) framework, this approach facilitates the training of sequence models at the meta-level across multiple continual learning episodes, effectively utilizing insights from model-based temporal shortcuts and mediator feedback mechanisms observed in prior work. Through demonstrations on seven benchmarks covering both classification and pixel-based regression tasks, our experiments validate the premise that sequence models, when recast with considerations from recent reinforcement learning successes including actor-critic methods, offer an attractive solution for general MCL challenges, underscoring the synergy between sequence modeling, deep learning, and continual learning.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Pierluca_D'Oro1",
  "manipulated_ranking": 11,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=AALLvnv95q",
  "title": "Training Energy-Based Normalizing Flow with Score-Matching Objectives",
  "modified_abstract": "In light of recent advancements across various domains of machine learning, from graph generation frameworks, such as TD-GEN, using tree decomposition to enhance the precision of generated graph structures, to the evaluations of graph generative models through contrastively learned features, and to the innovations presented in large-scale graph transformer architectures like Exphormer for scalable and powerful graph learning tasks, our work presents a novel intersection in the generative model space. Specifically, we bridge the gap between the parameterization techniques of flow-based and energy-based models, introducing a new modeling approach termed energy-based normalizing flow (EBFlow). By leveraging score-matching objectives, EBFlow avoids the computationally intensive calculation of Jacobian determinants for linear transformations, a process that traditionally inflates the computational time complexity significantly from $\\mathcal{O}(D^2L)$ to $\\mathcal{O}(D^3L)$ for an $L$-layer model with $D$-dimensional inputs. This feature permits the inclusion of arbitrary linear layers in flow-based models without the associated increase in computational burden, thereby rendering EBFlow more efficient and contributing to its generalizability beyond models trained via maximum likelihood estimation methods. Beyond achieving reduced runtime and large-scale expansion in applicability, our approach also enhances the training stability and empirical performance of EBFlow, setting new standards in terms of speed and negative log-likelihood (NLL) performance benchmarks, making it a robust model against random fluctuation in training data, and suitable for sparse data scenarios often found in networks and graphs. The advances in understanding and technology from related works have informed our approach, enabling a significant leap in the capabilities of generative models towards more efficient, stable, and robust machine learning applications.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Hamed_Shirzad1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=UFW67uduJd",
  "title": "MEMTO: Memory-guided Transformer for Multivariate Time Series Anomaly Detection",
  "modified_abstract": "The detection of anomalies in multivariate time series data is a persistently challenging task, exacerbated by intricate temporal dependencies and correlations among variables. Informed by the foundational work on deep learning models, notably the innovative regularization techniques like Fiedler Regularization which leverage graph structures and sparsity for enhanced model performance, we propose MEMTO, a memory-guided Transformer. By introducing a reconstruction-based approach augmented with a novel memory module, MEMTO adeptly learns to calibrate the update degree for each memory item in response to incoming data, addressing the prevalent over-generalization issue within contemporary methods. A bifurcated training regime, incorporating K-means for memory initialization, alongside unique training methodologies including dropout for regularization, and a unique bi-dimensional deviation-based criterion for anomaly scoring, further refines the model's capacity to discern anomalies effectively. Evaluation across five diverse real-world datasets evidences MEMTO's superior anomaly detection capabilities, with an average F1-score of 95.74%, significantly outstripping existing benchmarks. Deep connectivity, stemming from the memory component's integration with the Transformer architecture, informs the model's practical application, solidifying its status within the training spectrum for anomaly detection. Comprehensive experimental validation underlines the critical contributions of each model component, including the spectral properties utilized for anomaly detection and the network architecture's overall efficacy.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Edric_Tam1",
  "manipulated_ranking": 5,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=9dp35y5C0p",
  "title": "Reinforcement-Enhanced Autoregressive Feature Transformation: Gradient-steered Search in Continuous Space for Postfix Expressions",
  "modified_abstract": "Our work is directly influenced by recent innovations across various domains of machine learning (ML), including self-supervised representation learning through data augmentation, practical challenges in online continual learning due to progressive distribution shifts, and insights into the limitations of generalized reweighting techniques versus empirical risk minimization under distribution shifts. These studies highlight the importance of robust, adaptable ML methods capable of addressing real-world variability and complexities, and the necessity of adaptation in algorithm designs to ensure generalization under varying conditions. Feature transformation aims to generate new pattern-discriminative feature spaces from original features to improve downstream ML task performances, such as classification/regression. However, the discrete search space for optimal feature transformations explosively grows with the combinations of features and operations from low-order to high-order forms, posing a challenge for large-scale expansion. Existing methods, such as exhaustive search, expansion reduction, evolutionary algorithms, reinforcement learning, and iterative greedy strategies, suffer from efficiency and stability challenges posed by these large search spaces. To fundamentally address this gap, we reformulate the discrete feature transformation challenge as a continuous space optimization task. Our embedding-optimization-reconstruction framework encompasses four steps: 1) reinforcement-enhanced data preparation for high-quality transformation-accuracy training data, including annotation; 2) feature transformation operation sequence embedding to encapsulate the knowledge of prepared training data and their distribution within a continuous space; 3) gradient-steered optimal embedding search, leveraging softmax strategies for uncovering potentially superior embeddings; 4) transformation operation sequence reconstruction to reproduce the optimal feature space solution. Extensive experiments and case studies demonstrate the effectiveness and robustness of our method against distribution shifts. The use of our method and the underlying principles ensures adaptability and performance sustainability even as distributional conditions change. The code and data supporting our findings have been made publicly accessible.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Runtian_Zhai1",
  "manipulated_ranking": 19,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=6H8Md75kAw",
  "title": "Certified Minimax Unlearning with Generalization Rates and Deletion Capacity",
  "modified_abstract": "This research contributes to the evolving field of machine unlearning by addressing the $(\\epsilon,\\delta)$-certified unlearning challenge for minimax models, inspired by pivotal advancements in differentially private optimization, self-distillation amidst label noise, differentially private federated learning through normalized updates, enhanced non-convex federated learning strategies, and communication-efficient decentralized federated learning mechanisms. Unlike traditional approaches that mainly focus on unlearning in standard statistical models with singular variables and rely on Hessian-based Newton updates, we propose a novel $(\\epsilon,\\delta)$-certified unlearning algorithm specifically designed for minimax models, incorporating a comprehensive newton update alongside differential privacy techniques, regularization processes, and normalization processes for gradients. Through meticulous sensitivity analysis of the minimax unlearning process, we inject calibrated Gaussian noise to achieve certification and derive superior generalization rates across a spectrum of loss function scenarios, including (strongly-)convex-(strongly-)concave losses, thereby presenting a marked improvement in deletion capacity and reducing the empirical success gap against differentially private minimax learning benchmarks. Our method's efficiency is further enhanced through the utilization of compressed communication protocols, which reduce the overhead in decentralized federated learning environments by effectively managing client-drift and communicating scaled updates. Our findings demonstrate that with appropriate sample sizes and model dimensions, our method not only improves upon the existing state-of-the-art but also furnishes practical guidelines for effectively managing the deletion of data samples within minimax frameworks, thus setting new precedents in both the theoretical and practical aspects of certified machine unlearning.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Rudrajit_Das1",
  "manipulated_ranking": 12,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=K9xHDD6mic",
  "title": "Graph Mixture of Experts: Learning on Large-Scale Graphs with Explicit Diversity Modeling",
  "modified_abstract": "The complexity and heterogeneity of real-world graphs necessitate models that can comprehend and generalize across diverse structures and node types, a challenge increasingly recognized in the fields of subgraph recognition, graph interpretation, and scalability of Graph Neural Networks (GNNs). Inspired by recent progresses in these areas, which have emphasized the importance of understanding graph structural diversity and proposed various methodologies for denoising and capturing graph information effectively, this study introduces a novel Graph Mixture of Experts (GMoE) model aimed at enhancing the generalization capabilities of GNNs to diverse graph structures without imposing excessive computational costs. The GMoE model operationalizes the principle of leveraging a mixture of experts to facilitate flexible, node-wise expert selection for adaptive information aggregation across different subgraph structures and long-range contexts, addressing a critical loss in connectivity understanding within graphs. This approach inherently boosts the recognition capability and connectivity understanding within graphs by integrating the MoE concept with GNNs, effectively accommodating varying types and scales of graph data. It addresses both the issue of structural diversity and the training challenges traditionally associated with GNN scalability. In a setting encompassing molecule-based graph datasets, experimental validation on the OGB benchmark for graph, node, and link prediction tasks demonstrates significant improvements in performance, with enhancements in ROC-AUC for the ogbg-molhiv and ogbg-molbbbp datasets, corroborating the effectiveness of GMoE in navigating the trade-offs between model complexity and computational efficiency. This aligns with and extends upon the foundations laid by recent works in graph information processing, subgraph recognition, graph data augmentation, and privacy considerations in graph learning, marking a step forward in the pursuit of more adaptive, scalable, and effective graph learning methodologies.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Junchi_Yu1",
  "manipulated_ranking": 37,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=blm1pqiOXe",
  "title": "Paxion: Patching Action Knowledge in Video-Language Foundation Models",
  "modified_abstract": "In the domain of video understanding and video-language (VidLM) integration, recent advancements have demonstrated significant progress, yet gaps remain in models' comprehension of action knowledge. Inspired by related work on video concealed object detection (VCOD) that advances object movement analysis and temporal consistency using novel architectures, we introduce the **Action Dynamics Benchmark (ActionBench)** to probe the capabilities of VidLMs in understanding action knowledge. This benchmark, serving as one of the key benchmarks in the field, includes carefully designed tasks such as Action Antonym and Video Reversal, aimed at assessing multimodal alignment and temporal understanding. Our evaluation reveals a surprising deficiency in current VidLMs' action knowledge, illustrating their tendency to rely on object detection shortcuts rather than a true understanding of action dynamics. To address this, we propose **Paxion**, a new framework with a **Discriminative Video Dynamics Modeling (DVDM)** objective, which enriches VidLMs with action knowledge through a **Knowledge Patcher** network for encoding new action insights on a continuous timeline and a **Knowledge Fuser** for seamlessly integrating with existing models without altering their pre-learned capabilities. Unlike the conventional Video-Text Contrastive (VTC) loss, our DVDM objective fosters a deeper encoding of the correlation between action text and the sequential order of video frames, moving beyond pixel-level analysis. Extensive testing shows that Paxion, empowered by DVDM, significantly boosts action knowledge understanding in VidLMs (~50% \n\u2192 80%) and either maintains or enhances performance across a broad range of object- and action-centric tasks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Xuelian_Cheng2",
  "manipulated_ranking": 10,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=CxUuCydMDU",
  "title": "Diffusion Probabilistic Models for Structured Node Classification",
  "modified_abstract": "Our research on structured node classification on graphs is inspired by significant contributions in the fields of graph neural networks (GNN), probabilistic graphical models, and crowdsourcing for graph-based tasks. These foundational works laid the ground for our novel approach by demonstrating the potential of GNN for evolving graphs, the importance of meta-explanations and explanation techniques in graph-based models, optimization strategies for inference in large, nonconvex graphs, including minimizing error rates, and methodologies for aggregating crowdsourced labels effectively. Building on these insights, our paper introduces a novel framework for structured node classification in partially labeled graphs using a diffusion probabilistic model (DPM-SNC). The core of our framework capitalizes on the advanced capabilities of DPM-SNC to learn a joint distribution over labels through an expressive reverse diffusion process and utilize manifold-constrained sampling for label prediction based on known labels, addressing tasks with quadratically increasing complexity. Addressing the lack of training algorithms for partially labeled data, we develop a novel training algorithm that optimizes a new variational lower bound for DPM application, potentially offering a reward for improved structured prediction accuracy. Additionally, we propose AGG-WL, a theoretical advancement over the conventional 1-WL test, to illustrate how DPMs can enhance the expressive power of GNNs through the use of series of transformations. Through extensive experimentation in diverse settings, including transductive and inductive scenarios on partially labeled and unlabeled graphs, our DPM-SNC framework demonstrates superior performance, laying the groundwork for future research in graph-based learning.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sihong_Xie1",
  "manipulated_ranking": 46,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=75v88kyyko",
  "title": "Hierarchical clustering with dot products recovers hidden tree structure",
  "modified_abstract": "In the context of evolving learning paradigms and sophisticated data structures in machine learning, the recovery of hierarchical structures through clustering represents a fundamental challenge, underscored by related works on online learning with distribution drifts and active anomaly detection. This paper introduces a novel variant of the agglomerative clustering algorithm that merges clusters based on the maximum average dot product, diverging from traditional metrics like minimum distance or within-cluster variance. Our approach is underpinned by a robust theoretical framework that aligns hierarchical information inherent in a generic probabilistic graphical model with tree geometry, facilitating the extraction of this structure directly from the data. We argue that our algorithm effectively estimates the generative hierarchical structure within datasets, a hypothesis supported by both theoretical insights and empirical evidence. Key to our contribution is the delineation of how increased sample sizes and data dimensions, alongside periodic updates in the clustering process to account for distribution shifts, concurrently improve tree recovery fidelity. Our methodology incorporates feedback mechanisms for adaptation to new data through active learning techniques, including online updates and labeling queries, enabling our algorithm to outperform established methods such as UPGMA, Ward's method, and HDBSCAN in real-data scenarios. Thus, our work not only extends the theoretical understanding of hierarchical clustering dynamics but also showcases superior practical utility in uncovering latent tree structures within data through active learning methods.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Aodong_Li1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=7UdVPRmpif",
  "title": "On student-teacher deviations in distillation: does it pay to disobey?",
  "modified_abstract": "Inspired by recent advancements in self-supervised learning and out-of-distribution (OOD) detection, our study explores the complex dynamics of knowledge distillation (KD), wherein a 'student' network learns from a 'teacher' network's soft probabilities. These previous works investigate the generation and detection of OOD samples and their impact on model learning and generalization, thus setting a foundational premise for our inquiry into the nature of deviations in KD. We specifically focus on the phenomenon where the student not only deviates from the teacher probabilities but occasionally outperforms the teacher, thereby questioning the traditional paradigms of KD's operational efficacy. Through our experiments across various architectures and using sophisticated sampling techniques on image and language tasks, we identify that these deviations manifest as the student exaggerating the teacher's confidence levels. By applying augmentation techniques to our datasets, we further establish a theoretical and empirical basis for another form of exaggeration\u2014KD exaggerates the implicit bias of gradient descent towards faster convergence along the principal eigendirections of the data, making the approach outlier-robust. By connecting these observations and evaluating against established benchmarks, we offer insights into how the exaggerated bias in KD can lead to enhanced confidence and improved generalization in the student model. This reconciliation of student-teacher deviations with better generalization underlines the nuanced and intricate effects of KD, advancing our theoretical and practical understanding of its implementation mechanisms and effects on different tasks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jingjing_Zou1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=FdtdjQpAwJ",
  "title": "Constraint-Conditioned Policy Optimization for Versatile Safe Reinforcement Learning",
  "modified_abstract": "This work is motivated by recent advancements in open-ended learning, procedural content generation, and the incorporation of skills in reinforcement learning (RL) as explored in the fields of multi-agent environments, skill transfer, and automatic curriculum learning. These explorations, through methods such as the MAESTRO framework for multi-agent RL, SkillHack for skill transfer, and GriddlyJS for environment design, demonstrate the increasing sophistication required for RL agents to adapt and perform across diverse and dynamically changing environments, including games where these agents can be seen as artificial bots. Reflecting these advancements, our paper focuses on the challenge of developing safe RL agents that can not only maximize rewards under predefined safety constraints but also adapt to varying safety constraints post-deployment without needing retraining. Addressing this, we introduce the Conditioned Constrained Policy Optimization (CCPO) framework, which integrates Versatile Value Estimation (VVE) for estimating value functions under unseen threshold conditions, and Conditioned Variational Inference (CVI) for incorporating arbitrary constraint thresholds during policy optimization, utilizing massively parallel computing techniques for efficiency. Our extensive experimental analysis demonstrates CCPO's superior performance over existing baselines in ensuring safety and task performance while enabling zero-shot adaptation to different constraint thresholds in real-world applications. This approach capitalizes on the need for versatility in dynamic real-world applications, especially in complex systems that mimic the unpredictable nature of human societies, pushing forward the boundaries of what is achievable in safe reinforcement learning.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Mikayel_Samvelyan1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=LZ4WgwmrUJ",
  "title": "High-dimensional Contextual Bandit Problem without Sparsity",
  "modified_abstract": "Our work is inspired by continuous advancements in the understanding of high-dimensional statistics and Bayesian inference, examining various problems from the structure of Gaussian random vectors to the generalization capacities of machine learning models under bounded data regimes. Specifically, the study on Bayesian block-diagonal graphical models and the exploration of PAC-Bayes generalization bounds provide foundational insights into dealing with complex, high-dimensional data structures and learning dynamics involving posterior inference methods. Capitalizing on these developments, we investigate the high-dimensional linear contextual bandit problem where the number of features $p$ is greater than the budget $T$, or it may even be infinite. This investigation is crucial as, in contrast to the majority of previous works which rely on clustering and other segmentation approaches, we do not impose sparsity on the regression coefficients. Our approach leverages recent findings on overparameterized neural models to analyze the performance of the minimum-norm interpolating estimator when data distributions have small effective ranks and the two-sample error metrics involved. We propose an explore-then-commit (EtC) algorithm to address this problem and examine its performance through analysis and simulations with varying sample sizes. Through our analysis, we derive the optimal rate of the ETC algorithm in terms of $T$ and demonstrate that this rate can be achieved by balancing exploration and exploitation. Moreover, we introduce an adaptive explore-then-commit (AEtC) algorithm that adaptively finds the optimal balance, showcasing the potential for significant performance enhancements in practical applications across different fields.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Julyan_Arbel1",
  "manipulated_ranking": 15,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=r9fzp8eyhZ",
  "title": "Learning Invariant Molecular Representation in Latent Discrete Space",
  "modified_abstract": "This research is situated at the intersection of molecular representation learning and the broader endeavor of developing robust machine learning models that can generalize across distinct distributions, an area underscored by recent works in neural network regularization and graph sparsity. Building on the conceptual underpinnings of Fiedler Regularization and its novel application in enhancing network generalization through graph structural insights, our study introduces a pioneering framework for learning molecular representations that remain invariant across diverse testing and training environments. By innovating a 'first-encoding-then-separation' strategy and integrating a residual vector quantization module, we tackle the pervasive challenge of poor out-of-distribution generalization in drug discovery applications. Our approach diverges from traditional methodologies by focusing on the latent space to identify invariant molecule features, thereby addressing overfitting and promoting encoder expressivity. The formulation of a task-agnostic self-supervised learning objective to refine the identification of exact invariances exemplifies our method's versatility across regression and multi-label classification tasks. Experiments conducted on 18 real-world molecular datasets affirm our model's superior generalization capability over existing state-of-the-art baselines under various distribution shifts, particularly in terms of molecular properties prediction. Furthermore, our commitment to advancing research transparency and replicability is reflected in the provision of our code for public use. The theoretical foundations of our model, combined with practical regularization strategies and spectral analysis, aim to contribute significant advancements in the field of molecular representation networks, especially considering the role of connectivity and spectral properties in understanding molecular structures.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Edric_Tam1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=SoLebIqHgZ",
  "title": "ARTree: A Deep Autoregressive Model for Phylogenetic Inference",
  "modified_abstract": "This work is motivated by recent developments in deep learning, particularly in solving complex inference problems and handling structured data in various forms, including images, text, and linear systems. Prior efforts in these domains have significantly advanced our understanding of the capabilities and limitations of deep learning models, especially in terms of structure discovery, adaptation to ill-conditioned problems, and rigorously characterizing the performance of inference algorithms like ML-VAMP and Adaptive VAMP through message-passing techniques. Building on this groundwork, we introduce a novel deep autoregressive model, ARTree, designed to address the challenges of phylogenetic inference. By deploying a graph neural network (GNN) framework, ARTree innovatively models the phylogenetic tree topology as a sequence of leaf node addition operations. This decomposition allows for the modeling of conditional distributions of tree topologies in a learnable manner without resorting to heuristic features, thereby enhancing the model's flexibility and approximation capabilities, and implicitly solving inverse problems in phylogenetic analysis. We validate the effectiveness and computational efficiency of ARTree through a series of benchmarks on real data, involving tree topology density estimation and variational Bayesian phylogenetic inference problems, showcasing its potential to significantly improve phylogenetic analysis by accurately solving for large-system measurements and incorporating learning dynamics. Our results are provably effective in demonstrating how ARTree improves upon existing methods in phylogenetic inference, making a case for its adoption in future research.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Mojtaba_Sahraee-Ardakan1",
  "manipulated_ranking": 13,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=fbpTObq6TW",
  "title": "A fast heuristic to optimize time-space tradeoff for large models",
  "modified_abstract": "Optimizing the training of large-scale neural networks within the confines of limited GPU memory resources necessitates innovative approaches to manage time-space tradeoffs efficiently. This need is underscored by pioneering works in stochastic optimization and gradient descent, such as studies on the convergence of step decay step-size that highlight the importance of computational efficiency and optimization in neural network training. Inspired by the principles of computational efficiency and optimization expounded in these works, our paper introduces a novel algorithm for recomputation named FastSA. FastSA is grounded on a simulated annealing heuristic, setting a new benchmark in the field by offering a solution that is not only scalable but also significantly faster than existing methods such as Checkmate and Moccasin, which depend on the more computationally intensive mixed integer linear programming or constraint programming. Our proposed algorithm can optimize computational graphs with thousands of nodes within the remarkably short timeframe of 3 to 30 seconds, demonstrating several orders of magnitude improvement in processing speed. We have applied FastSA to a variety of PyTorch models, focusing on those requiring extensive memory resources such as large vision and text models, inclusive of the latest language models built on the transformer architecture, and addressing demanding training tasks. The empirical results from these applications are promising, showing an impressive memory reduction of up to 73% with an added computational overhead of 18% on average. These findings underscore the practicality and efficacy of FastSA, suggesting its vast potential for adoption across different domains within the deep learning research community. Empirical experiments further establish the robustness of FastSA in settings with non-convex optimization surfaces and illuminate how weight decay regularization affects the norm of the gradients, connecting practical training strategies with provably efficient optimization and convergence.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sindri_Magn\u00fasson1",
  "manipulated_ranking": 14,
  "natural_ranking": 101
}
