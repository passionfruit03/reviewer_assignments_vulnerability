{
  "paper_link": "https://openreview.net/forum?id=RiwPYAMLur",
  "title": "Active representation learning for general task space with applications in robotics",
  "modified_abstract": "Inspired by recent advancements in unsupervised reinforcement learning and its significant impact on robotic manipulation skill discovery, our research introduces a novel framework for active representation learning across a broad spectrum of task spaces. Unsupervised reinforcement learning has pioneered the development of algorithms that can learn diverse, transferable skills without access to specific task rewards, setting the stage for our work in active representation learning and skill discovery. This work extends these concepts by proposing a versatile algorithmic and theoretic framework optimized for selecting source tasks to sample from, thereby enhancing learning efficiency and applicability in task-aware and task-agnostic settings. Our framework supports a wide range of target and source task spaces, from discrete to continuous, and aligns with deep representation learning methodologies. Through various instantiations of our framework, we demonstrate the effectiveness of our approach in both synthetic and real-world applications, including robotics problems from pendulum simulations to drone flight datasets, where interaction with the environment plays a crucial role. Our results indicate substantial improvements over traditional baselines, showcasing the potential of active representation learning in reducing sample complexity and improving performance in a multitude of tasks through learned interaction strategies with interacting components in task settings.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Daesol_Cho1",
  "manipulated_ranking": 4,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=SFfOt1oDsX",
  "title": "Graph Convolutional Kernel Machine versus Graph Convolutional Networks",
  "modified_abstract": "The exploration of graph-based machine learning has become increasingly crucial in mining valuable insights from the complex and irregular structures characteristic of graph data, which permeate various disciplines. Inspired by the recent findings in unified data-model sparsity and compression techniques for class-imbalanced graph representation learning, our study introduces a novel computational framework termed graph convolutional kernel machine (GCKM). This proposition stems from the observation that the gains from deepening graph convolutional networks (GCNs) are marginal or sometimes adverse, hinting at the inherently limited complexity within graph data that shallow models can adeptly navigate. Hence, GCKMs leverage kernel functions integrated with graph convolution, providing a robust alternative that streamlines the model architecture design, hyperparameter optimization, training, and tuning processes. Unlike traditional GCNs, which struggle with dynamic data environments and require frequent update cycles, GCKMs offer global solution optimality, superior generalization, and heightened interpretability, addressing the primary concerns related to class imbalance. The versatility of GCKMs extends to various graph-based tasks, including but not limited to node and graph classification, clustering, feature extraction, and dimensionality reduction, establishing new baselines for performance. Through rigorous numerical analysis on benchmark datasets, we demonstrate that GCKMs not only simplify the modeling process but also deliver competitive, if not superior, accuracy compared to conventional GCNs.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yijun_Tian1",
  "manipulated_ranking": 13,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=VEpU9rFaQr",
  "title": "Auditing for Human Expertise",
  "modified_abstract": "In the context of the growing interest in understanding the trustworthiness and reliability of off-policy evaluation (OPE) methods under adversarial conditions\u2014a concern especially relevant in high-stakes domains such as healthcare\u2014our research introduces a novel statistical framework to critically assess the added value of human expertise in high-stakes prediction tasks, such as patient diagnosis. The increasing complexity of scenarios where trained human experts operate, notably where intuition and access to unquantifiable information play crucial roles, presents a significant challenge to the assumption of algorithmic superiority in decision accuracy. Our work not only parallels concerns raised in adversarial contexts, such as the sensitivity of machine learning models to data poisoning attacks and perturbations in healthcare settings, but also advances the conversation by scrutinizing the unique contributions of human experts that might elude even sophisticated algorithms. By developing and applying a hypothesis test to determine the statistical independence of expert predictions from outcomes after considering available input features, through careful experimentation design and analysis of functions that model decision-making processes, we unearth evidence suggesting that experts' decisions, such as physicians' admit/discharge choices in cases of acute gastrointestinal bleeding, might encompass valuable insights beyond the grasp of current AI-based tools. This discrepancy emphasizes the potential and need for human-AI complementarity, rather than replacement, in critical decision-making processes, challenging the notion that algorithmic predictions, even when more accurate, can serve as standalone solutions in areas where human judgment is nuanced and informed by factors beyond algorithmic reach.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Harvineet_Singh1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=HFQFAyNucq",
  "title": "ResMem: Learn what you can and memorize the rest",
  "modified_abstract": "Informed by existing work on continual learning and the challenges of catastrophic forgetting, this study introduces a novel framework aimed at enhancing model generalization by leveraging explicit memorization. The impressive generalization performance of modern neural networks, attributed in part to their ability to implicitly memorize complex training patterns, serves as the foundation for our exploration into a systematic approach to bolster this capability. We propose the residual-memorization (ResMem) algorithm, a new method that augments an existing prediction model (e.g., a neural network) by fitting the model's residuals with a nearest-neighbor based regressor. This design explicitly addresses the stability-plasticity dilemma, offering a direct method to minimize generalization error by focusing on the residuals unexplained by the model. Starting with a stylized linear regression problem, we rigorously demonstrate that ResMem yields a more favorable test risk compared to a base linear network, thereby emphasizing its efficacy in both learning and memorization tasks like classification. Subsequent empirical validation on standard vision and natural language processing benchmarks, including incremental learning settings and sequence recognition tasks, confirms that ResMem consistently enhances the generalization performance of the original prediction model across diverse datasets. This establishes ResMem as a robust alternative to conventional training paradigms that struggle with the trade-off between learning and memorization, thereby contributing to the advancement of networks capable of continual and incremental learning.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Marc_Masana_Castrillo1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=OiivS2mqQf",
  "title": "Augmentation-Aware Self-Supervision for Data-Efficient GAN Training",
  "modified_abstract": "The recent movement toward data-efficient generative adversarial networks (GANs) has fostered various innovations, particularly in overcoming the discriminator's susceptibility to overfitting when faced with limited data. This interest is catalyzed by advancements in self-supervised learning (SSL) that highlight the importance of balancing expressiveness and learnability in representations through innovative frameworks that predict downstream performance effectively without reliance on extensive data augmentation or complex model architectures. Following this underpinning philosophy, our research introduces a novel augmentation-aware self-supervised discriminator, effectively functioning as a classifier for GAN training, that uniquely predicts the augmentation parameter of the transformed inputs. This differentiation enables the discriminator to overcome the hurdle of augmentation-induced invariance, promoting a deeper representation learning that directly benefits the generator's performance. By requiring distinguishable prediction targets for real and generated data, we not only refine the discriminator's efficiency but also align the generator's learning objective with the arithmetic-harmonic mean divergence, under certain conditions. Furthermore, our approach introduces an implicit cluster mechanism within the discriminator, enhancing its ability to discern between nuanced differences in data. Comparative analysis with leading methods on benchmarks including CIFAR-10, CIFAR-100, FFHQ, LSUN-Cat, and several low-shot datasets underscores our approach's superior capacity to train data-efficient GANs, especially in tasks needing precise classifications based on enhanced representations. These results are testament to our method's foundational advancements, marrying the benefits of data augmentation with the rigorous self-supervised learning paradigms.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Alessandro_Sordoni1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=EO1KuHoR0V",
  "title": "AUDIT: Audio Editing by Following Instructions with Latent Diffusion Models",
  "modified_abstract": "In the light of significant progress exemplified by recent advancements such as the adaptation of Masked Autoencoders for audio, which provided a novel approach to self-supervised audio spectral representation learning, this work introduces AUDIT. Audio editing, serving a plethora of purposes from sound effect incorporation to audio repair, has seen diffusion-based methods achieve notable zero-shot audio editing capabilities, yet facing challenges in targeted editing effectiveness, unintended audio modifications, and reliance on comprehensive output audio descriptions. This research proposes AUDIT, an instruction-guided audio editing model utilizing latent diffusion models, focusing on overcoming these specific challenges. AUDIT is distinctive in its approach by leveraging triplet training data\u2014comprising instruction, input audio, and output audio\u2014to train a diffusion model that edits audio based on instructions, efficiently learning to identify and modify only necessary segments with attention to detail. Furthermore, AUDIT simplifies the editing process by requiring only editing instructions without the need for detailed audio descriptions, enhancing its practical utility through attention mechanisms. Experimental results demonstrate AUDIT's superiority across various benchmarks in both objective and subjective assessments of audio editing tasks such as addition, removal, replacement, inpainting, and audio super-resolution. The methodology includes pre-training the model on a large corpus of audio datasets for improved generalization capabilities before fine-tuning on specific editing tasks, incorporating multiple layers of audio processing, ideal for speech applications. For accessibility, demonstration samples can be reviewed at a provided URL, underlining the practical applications and effectiveness of AUDIT in real-world scenarios.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Florian_Metze1",
  "manipulated_ranking": 22,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=71P7ugOGCV",
  "title": "Understanding, Predicting and Better Resolving Q-Value Divergence in Offline-RL",
  "modified_abstract": "This work is informed by the advances and challenges observed in the domain of offline reinforcement learning (offline RL), particularly the insights provided by the CORL library's emphasis on benchmarking deep offline learning algorithms and the identification of performance-relevant details in an open-source environment. Encapsulating these perspectives, the study investigates the divergence of Q-value estimation, a critical issue in offline RL characterized by the inability of an agent to interact with the environment directly and its implications for tracking the progress of reinforcement learning models. This divergence, traditionally attributed to querying out-of-distribution actions during the bootstrapping of value targets, is reevaluated here with the intention of achieving a deeper theoretical understanding through deep learning techniques. We discover a fundamental pattern, self-excitation, as the primary cause of this divergence and introduce a novel metric, Self-Excite Eigenvalue Measure (SEEM), based on the Neural Tangent Kernel (NTK) to assess the evolving properties of a Q-network during training. This metric offers a novel window into understanding the emergence of divergence and, for the first time, allows for the reliable prediction of training divergence at an early stage, including the order of growth for the estimated Q-value, the model's norm, and the exact moment when divergence occurs using an SGD optimizer. Our experiments demonstrate a strong alignment with our theoretical predictions. We then explore resolving divergence through architectural improvements, notably the incorporation of LayerNorm, which we identify as an effective means to avoid divergence while preserving the integrity of performance. Our extensive empirical studies confirm the viability of this approach even in the most challenging scenarios, such as when utilizing only 1% of available data transitions from collected files. The proposed enhancements are compatible with existing offline RL methods, pushing the boundaries to achieve state-of-the-art results on numerous difficult tasks and offering fresh insights into the mechanics underlying their effectiveness.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Denis_Tarasov1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=tEmFyqjaJh",
  "title": "PPi: Pretraining Brain Signal Model for Patient-independent Seizure Detection",
  "modified_abstract": "Automated seizure detection is crucial for effective epilepsy diagnosis and treatment, propelling the exploration of innovative methods like stereoelectroencephalography (SEEG) for detailed brainwave analysis. Inspired by recent progress in time-series representation learning, particularly the integration of expert knowledge in deriving meaningful patterns from complex datasets, this study introduces a Pretraining-based model for Patient-independent seizure detection (PPi). Addressing significant challenges such as the considerable domain shift between patients and pattern evolution among brain areas, PPi uniquely aligns with the foundational principles set forth by advanced representation learning approaches applied to time-series data. By designing two novel self-supervised tasks for SEEG data, PPi leverages abundant time-series information while respecting the distinctiveness between signals from different brain areas. The introduction of two techniques, channel background subtraction and brain region enhancement, further enables effective domain shift mitigation. Extensive evaluation demonstrates that PPi surpasses state-of-the-art baselines across public and proprietary clinical datasets, underscoring its potential for broad application in learning advancements within seizure detection. Visualization analysis further validates the domain generalization strategies, offering insights into their impact on model performance and the broader learning field, with experts critically assessing its applicability and efficacy.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~David_Reeb2",
  "manipulated_ranking": 12,
  "natural_ranking": 102
}
{
  "paper_link": "https://openreview.net/forum?id=YhAZqWhOnS",
  "title": "Autodecoding Latent 3D Diffusion Models",
  "modified_abstract": "In the emerging field of 3D content generation, diffusion-based methods pioneered in the text-to-image domain have demonstrated significant potential for advancing technology in 3D graphics. Inspired by the success of Neural Radiance Fields (NeRFs) in creating highly realistic renderings using implicit object encoders and the application of graph neural networks for modeling complex dynamics of compositional scenes, our work introduces a novel approach for generating static and articulated 3D assets. Central to our methodology is the 3D autodecoder, a novel framework that learns properties from target datasets in a latent space, facilitating the generation of volumetric representations for rendering view-consistent appearances and geometries. Unlike traditional autoencoders that require large datasets, our approach overcomes data scarcity in 3D generation through robust normalization and de-normalization operations that learn a 3D diffusion from limited 2D images or monocular videos of rigid or articulated objects. Our flexible framework uniquely accommodates either existing camera supervision or autonomously learned camera information during training, offering an advanced solution to 3D synthesis challenges. Evaluation across diverse benchmarks\u2014including synthetic object datasets, real-world videos of humans, and large-scale static and multi-object videos\u2014confirms that our method surpasses existing state-of-the-art techniques in generating high-fidelity neural prediction models for 3D models with complex compositionality.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zhiao_Huang1",
  "manipulated_ranking": 24,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=dyXNh5HLq3",
  "title": "Compositional Foundation Models for Hierarchical Planning",
  "modified_abstract": "In the burgeoning field of artificial intelligence, hierarchical planning and reasoning stand out as vital components for decision-making in novel environments with long-horizon goals. Drawing inspiration from prior works, such as the development of Contrastively-trained Structured World Models (C-SWMs) which emphasized the importance of learning structured, object-based world models from sensory data, our research extends the frontiers of this understanding through the proposition of Compositional Foundation Models for Hierarchical Planning (HiP). Our model integrates the sophistication of multiple expert foundation models, each trained on distinct modalities such as language, vision, and action data, to enable a compositional and holistic approach to solving long-horizon tasks. Utilizing a large language model for constructing symbolic plans, a large video diffusion model for visually reasoning about these plans, and an inverse dynamics model for translating visual plans into executable actions, the HiP framework showcases how consistency across models can be achieved via iterative refinement, effectively acting as an agent in these tasks. The efficacy and adaptability of HiP are demonstrated through its application to various long-horizon table-top manipulation tasks, thereby illustrating the potential of compositional foundation models to generalize and adapt in diverse and complex task environments. The integration of network embedding techniques further enhances the HiP's ability to navigate the intricacies of hierarchical planning and learn the state transitions inherent in these tasks. Object-oriented learning is crucial in this context, positioning the HiP agent to understand and manipulate the state of the world at a class level, making generalized decision-making feasible.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Elise_van_der_Pol1",
  "manipulated_ranking": 1,
  "natural_ranking": 102
}
{
  "paper_link": "https://openreview.net/forum?id=JeKXmYb4kd",
  "title": "Minimax-Optimal Location Estimation",
  "modified_abstract": "Location estimation plays a pivotal role in parametric statistics, posing a fundamental question when dealing with known distribution densities and the pursuit of uncovering unknown parameters like shifts in distributions. Building on the foundational concept that high-dimensional optimization can be efficiently achieved through low-dimensional feature spaces, our research extends this principle to the domain of parametric statistics with a focus on location estimation. We construe the scenario where, given a known distribution density $f$, we aim to estimate an unknown shift $\\mu$ using $n$ i.i.d. samples from $f(x-\\mu)$. Addressing the limitations of the maximum likelihood estimator (MLE), which only attains asymptotic optimality as $n \\to \\infty$, our paper introduces two novel location estimators. These estimators are designed to be optimal under stringent criteria: 1) an estimator that achieves minimax-optimal estimation error with a success probability of $1-\\delta$, incorporating low-dimensional analysis for high efficiency in computational optimization, and 2) a confidence interval estimator crafted to encompass $\\mu$ within its output interval with at least $1-\\delta$ probability, while possessing the minimum expected squared interval width among all shift-invariant estimators. This research contributes significantly to statistical estimation by providing optimal solutions for finite sample sizes and opens avenues for applying these principles across various statistical and machine learning tasks that hinge on precise location estimation.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Senanayak_Sesh_Kumar_Karri1",
  "manipulated_ranking": 26,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=tF7W8ai8J3",
  "title": "Federated Compositional Deep AUC Maximization",
  "modified_abstract": "In the evolving landscape of federated learning (FL), addressing fairness, as seen in recent federated learning approaches with fair averaging, reflects the broader community's effort towards ethical AI and state-of-the-art methodologies. Our research builds upon these foundational works by focusing on a yet different aspect of federated learning: the challenge of imbalanced data, which remains underexplored despite its critical importance in real-world applications. To tackle this issue, we developed a novel federated learning method for imbalanced data by directly optimizing the area under curve (AUC) score. Specifically, we formulated the AUC maximization problem as a federated compositional minimax optimization problem, developed a local stochastic compositional gradient descent-ascent with momentum algorithm, and provided bounds on the computational and communication complexities, integrating gradients analysis as an essential part of our approach. To the best of our knowledge, this is the first work to achieve such favorable theoretical results in this context. Furthermore, through extensive experiments, our method's efficacy is confirmed, showcasing the potential for significant improvements in handling imbalanced data in federated settings, promoting similarity in model performance across various data distributions. The concept of learning, central to our investigation, is thoroughly examined, contributing to the understanding of federated learning's potential.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Xiaoliang_Fan1",
  "manipulated_ranking": 7,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=JK2oPrP8B3",
  "title": "Global Identifiability of  $\\ell_1$-based Dictionary Learning via Matrix Volume Optimization",
  "modified_abstract": "Inspired by seminal works on the identifiability and stability of matrix factorizations, including eigenvalue-eigenvector analysis, and precision matrix estimation, we propose a novel formulation for dictionary learning. This approach focuses on minimizing the determinant of the dictionary matrix, known as its volume, subject to the constraint that each row of the sparse coefficient matrix has unit $\\ell_1$ norm. This formulation is motivated by the goal of achieving global identifiability of the ground-truth dictionary and sparse coefficient matrices, an attribute distinguishing it from the predominant local identifiability focus in current literature. Our approach ensures global identifiability, up to a permutation and scaling ambiguity, provided that a set of vectors from the coefficient matrix exists within the $\\ell_\\infty$ norm ball and encloses the $\\ell_2$ norm ball within its convex hull. This represents a significant advancement, eliminating the need for mutual incoherence among dictionary atoms\u2014a common constraint in existing studies. Furthermore, we present a probabilistic analysis, based on factor analysis and the Bernoulli-Gaussian model for sparse coefficient generation, which accentuates the global identifiability, requiring a sample size marginally greater than $k\\log k$, where $k$ is the dictionary's atom count. This threshold aligns with conditions for local identifiability found in previous studies, yet we extend these to a global context. Our work culminates in the development of an optimized algorithm, specifically a variant of the linearized alternating direction method of multipliers (ADMM), designed for this globally identifiable formulation. The algorithm's efficacy is validated through application-based numerical experiments demonstrating its potential in accurate and efficient dictionary recovery, making it a significant topic of analysis in the field.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Teodora_Popordanoska1",
  "manipulated_ranking": 41,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=NXLjaYdgaL",
  "title": "Intra-Modal Proxy Learning for Zero-Shot Visual Categorization with CLIP",
  "modified_abstract": "The burgeoning field of vision-language pre-training, exemplified by models such as CLIP, has been instrumental in advancing zero-shot visual categorization capabilities for both recognition and language processing tasks. This progress is notably inspired by recent efforts in bridging the modality gap between text and vision, as well as the exploration of prompt learning (or prompting) strategies to fine-tune pre-trained models for enhanced performance on downstream tasks. Building upon these insights, particularly the identification of limitations in existing approaches such as the modality gap problem addressed by Multi-modal Prompt Learning (MaPLe), this paper presents a novel methodology for zero-shot visual categorization. We identify a critical issue in the current paradigm\u2014while CLIP achieves impressive zero-shot performance through text embedding proxies, the inherent modality gap results in sub-optimal outcomes. Our theoretical analysis demonstrates that this gap cannot be adequately minimized through the contrastive loss optimization employed by CLIP, suggesting that an optimal proxy for vision-centric tasks might solely exist within the vision space. Adaptation strategies are essential in this context, where we therefore propose a technique to learn this vision proxy directly from unlabeled target vision data, utilizing the initial text proxy as a guide and further enhancing the process through intra-modal proxy learning (InMaP). This approach not only addresses the identified modality gap but also proposes strategies based on theoretical insights for refining pseudo labels to facilitate effective proxy learning within the specific domain of application. Extensive experimentation validates the efficacy and efficiency of InMaP, showcasing its ability to significantly improve zero-shot accuracy in visual categorization tasks, exemplified by a remarkable increase in performance on the ImageNet dataset with a ViT-L/14@336 model pre-trained by CLIP, using a harmonized mean of precision and recall as evaluation metrics.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Hanoona_Abdul_Rasheed1",
  "manipulated_ranking": 23,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=ohKbQp0jIY",
  "title": "Successor-Predecessor Intrinsic Exploration",
  "modified_abstract": "In the domain of reinforcement learning, particularly in scenarios where external rewards are sparse, the significance of intrinsic motivation for exploration cannot be overstated. Building on recent progress in reinforcement learning methodologies, such as improving learning efficiency and robustness in adversarial environments through expanding model techniques in fictitious play and policy improvement, our research introduces a novel exploration paradigm, Successor-Predecessor Intrinsic Exploration (SPIE). This method innovatively combines prospective and retrospective information to generate intrinsic rewards using a concept undervalued in prior models, which primarily focused on future state potentials, thus disregarding the exploitable knowledge inherent in the agent's previous experiences. We argue that by integrating both forward-looking and backward-glancing insights, along with training on neural networks, SPIE facilitates an exploration mechanism that is both structure-aware and globally informed, surpassing the conventional localized approaches. Our empirical evaluations across environments characterized by sparse rewards and bottleneck situations demonstrate that SPIE not only enhances exploratory behavior in a more ethological and efficient manner but also significantly outperforms existing methods in deep reinforcement learning applications, particularly in sparse-reward Atari games. Through these advancements, by utilizing samples from environment interactions, SPIE sets a new precedent for intrinsic reward composition in exploration tasks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jing-Cheng_Pang1",
  "manipulated_ranking": 26,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=8d9wVXri89",
  "title": "Accurate Interpolation for Scattered Data through Hierarchical Residual Refinement",
  "modified_abstract": "In the context of rapid advancements in computational geometry and 3D visualization, signified by innovative point cloud simplification techniques that prioritize salient features while minimizing computational costs, our work introduces a novel approach to the challenge of accurate interpolation for scattered data. Recognizing the limitations of traditional numerical algorithms and inspired by the recent progress in neural network-based methodologies that offer flexibility in handling residuals, we present the Hierarchical INTerpolation Network (HINT). HINT capitalizes on the concept of leveraging residuals at observed points as a valuable source of guidance for predicting target functions. Specifically, it employs a hierarchical structure of lightweight interpolation blocks, wherein the initial block focuses on estimating the primary component of the target function, and the subsequent blocks refine this estimate by addressing the residual components based on the residuals of their predecessors. This hierarchical refinement, underpinned by the innovative use of local constraints and graph encodings for enhanced correlation modeling between observed and target points within clouds, marks a significant departure from existing approaches. Our extensive experimental validation across diverse datasets underscores HINT's superior performance in achieving high interpolation accuracy, thereby establishing its efficacy for practical applications in various theoretical and engineering domains. Notably, simplification, through the network was trained to adapt to changing data distributions effectively, showcasing its potential in zero-shot learning scenarios where models are evaluated on tasks they were not explicitly trained on using the concept of hierarchical residual refinement.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Giorgos_Bouritsas1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=vpQuCsZXz2",
  "title": "TransHP: Image Classification with Hierarchical Prompting",
  "modified_abstract": "Building on the concept of leveraging hierarchical information in machine learning, as demonstrated in tasks such as Compositional Zero-Shot Learning (CZSL), where hierarchical embeddings significantly enhance model performance by capturing intra- and inter-class correlations, this paper introduces a novel approach to the hierarchical image classification (HIC) task. Unlike traditional HIC methods that often overlook the utility of ancestor-class information, our proposed hierarchical prompting strategy explicitly incorporates this information as a tokenized hint, thereby closely mimicking human visual recognition processes. By recognizing the broader class (ancestor) first, the proposed method, named Transformer with Hierarchical Prompting (TransHP), narrows down the focus to discriminate among the finer categories (descendants) effectively. TransHP is structured in a three-step process: (1) learning prompt tokens representing coarse (ancestor) classes, (2) dynamically predicting the coarse class of the input image at an intermediate stage, and (3) injecting the corresponding prompt token into the intermediate feature to condition subsequent feature extraction. This method allows for the same model parameters to adaptively focus on the subtle differences among descendant classes, influenced by the injected coarse-class prompt. Our extensive evaluations demonstrate that TransHP not only improves image classification accuracy, for instance, enhancing ViT-B/16 by +2.83% on ImageNet by effectively managing the compositions of visual features, but also significantly boosts training data efficiency and model explainability. TransHP's performance further outpaces existing HIC methodologies, underscoring its effectiveness in exploiting hierarchical class information and object recognition for improved image classification outcomes.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Kun_Wei1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=lSLYXuLqRQ",
  "title": "Masked Space-Time Hash Encoding for Efficient Dynamic Scene Reconstruction",
  "modified_abstract": "In the realm of 3D scene reconstruction, incorporating insights from advancements in panoptic understanding and segmentation of single RGB images for scene understanding tasks, we introduce the Masked Space-Time Hash encoding (MSTH), a transformative method tailored for dynamic 3D scene reconstruction from multi-view or monocular videos. Our approach is informed by an observation central to both fields: dynamic scenes often comprise substantial static areas, leading to redundancy in computational and storage efforts. By innovatively leveraging a weighted combination of 3D and 4D hash encodings, with the weighting guided by a learnable mask optimized through an uncertainty-based objective, MSTH adeptly reduces hash collision rates and avoids redundant operations on static scene portions. This strategy enables the efficient representation of vast quantities of space-time voxels with comparatively small-sized hash tables. Furthermore, MSTH's streamlined approach to handling temporally redundant features facilitates easier model optimization and rapid convergence, with notable results achieved after only twenty minutes of training on dynamic scenes. Our evaluations across various dynamic scenes establish MSTH's supremacy, outperforming existing state-of-the-art techniques in terms of both training time efficiency and memory utilization, requiring only 130 MB for storage. This synergy of ideas marks a significant advancement in our capability to reconstruct dynamic 3D scenes efficiently. MSTH not only learns from the visual input but also plans its encoding strategy to maximize the efficiency and effectiveness of the reconstruction process, a method which could also inform future advancements in robotics by enabling better scene understanding for navigational tasks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ji_Hou1",
  "manipulated_ranking": 5,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=h2lkx9SQCD",
  "title": "Faster Differentially Private Convex Optimization via Second-Order Methods",
  "modified_abstract": "Building on the conceptual evolution seen in Dirichlet Mechanism for Differentially Private KL Divergence Minimization, which utilizes advanced probabilistic models to enhance privacy in machine learning by protecting sensitive data, our research extends the exploration of differential privacy into the realm of optimization. Differentially private (stochastic) gradient descent has been the cornerstone of DP machine learning in various settings, handling sensitive data with the primary objective to safeguard user privacy. Recognizing the inherent limitations of first-order methods in the absence of privacy constraints, our investigation pivots toward leveraging second-order information from the loss function to expedite DP convex optimization processes. By developing a privacy-preserving variant of the regularized cubic Newton method, originally proposed by Nesterov and Polyak, we not only facilitate quadratic convergence for strongly convex loss functions but also establish the attainability of optimal excess loss within this context. Furthermore, our formulation of a practical second-order DP algorithm specifically addresses the unconstrained logistic regression problem, providing both theoretical insights and empirical evidence of superior performance over existing benchmarks in classification tasks. The empirical outcomes underscore our algorithm's capability to consistently outperform DP-GD/DP-SGD alternatives in terms of excess loss, with a demonstrated speedup of 10-40x across various challenging datasets, thereby effectively managing the distribution and divergence of data points to ensure privacy.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Donlapark_Pornnopparath1",
  "manipulated_ranking": 16,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=axRMkinASf",
  "title": "Greedy Poisson Rejection Sampling",
  "modified_abstract": "In the context of fundamental data compression problems, particularly one-shot channel simulation, this paper integrates and builds upon principles observed in neural data compression and differential privacy, deriving inspiration from prior works such as the Mutual Information Machine (MIM). While MIM introduces an autoencoder framework optimizing for joint distribution learning between observations and latent states through mutual information, our study broadens the application scope to encoding a single sample from a target distribution $Q$ using a coding distribution $P$ with optimal bit usage. Autoencoders, in particular, have shown immense promise in various learn tasks, including but not limited to data clustering and dimensionality reduction. Existing solutions in this realm are often hampered by slow performance or restricted applicability, limiting broader adoption. We address these challenges by proposing a novel algorithm specifically tailored for one-dimensional problems with unimodal target-proposal density ratios, which we term the greedy Poisson rejection sampling (GPRS). GPRS innovates by mapping the rejection sampling process onto a greedy design search over the points of a Poisson process, achieving optimal runtime efficiency. We thoroughly analyze the correctness and temporal complexity of GPRS variants, providing a solid theoretical foundation. Empirical validation of our theoretical results reveals that GPRS sets a new efficiency benchmark, significantly surpassing the performance of state-of-the-art methods like A* coding. This advancement not only marks a pivotal improvement in algorithmic speed and applicability for one-shot channel simulation but also offers a more efficient and naturally aligned alternative to traditional quantization-based encoding approaches.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Micha_Livne1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=3gamyee9Yh",
  "title": "QuantSR: Accurate Low-bit Quantization for Efficient Image Super-Resolution",
  "modified_abstract": "The endeavor to achieve efficient image super-resolution (SR) processing through low-bit quantization, while mitigating the accuracy loss inherent in such quantized networks, is informed by prior advancements in neural network-based approaches to image and data processing tasks, such as full-resolution image compression. This prior work has highlighted the potential and challenges of employing neural networks, including recurrent neural networks, in tasks that demand high efficiency and adaptability at various compression methods and rates without retraining, thereby setting a pertinent foundation for our exploration. To this end, we propose QuantSR, a novel quantized image SR network designed for accurate and efficient SR under low-bit quantization conditions. Addressing the critical challenge of representation homogeneity resulting from quantization, we introduce the Redistribution-driven Learnable Quantizer (RLQ) for enhancing the quantized network's representational capabilities without compromising inference efficiency. Moreover, leveraging entropy-based techniques to guide our quantization process, our novel Depth-dynamic Quantized Architecture (DQA) facilitates a flexible trade-off between computational efficiency and accuracy, leveraging weight sharing for adaptable inference across networks. QuantSR demonstrates superior performance over existing quantized SR models in terms of both accuracy, achieving improvements in the range of 4.3%-8.8%, and computational efficiency. We extend the model's applicability to both convolution and Transformer-based architectures, denoted as QuantSR-C and QuantSR-T, respectively. This comprehensive study not only advances the state of the art in quantized image super-resolution but also underscores the utility of neural networks' innovations in overcoming quantization challenges. The code and models are made publicly available, ensuring their contribution to the wide spectrum of compression technologies and adaptive methodologies in the field.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~David_Minnen1",
  "manipulated_ranking": 7,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=zTSlm4nmlH",
  "title": "Beta Diffusion",
  "modified_abstract": "In the evolving landscape of generative modeling, particularly with recent innovations like MultiDiffusion's adaptable text-to-image transformation capabilities, our work introduces beta diffusion, a novel approach focused on generating data within bounded ranges. This method stands out by integrating demasking and denoising processes through scaled and shifted beta distributions, facilitating multiplicative transitions that uphold the characteristic distributions across both forward and reverse processes at any point in time. Unlike the conventional models that predominantly employ additive Gaussian noise and leverage reweighted evidence lower bounds (ELBOs) for optimization, beta diffusion opts for a multiplicative mechanism. It is distinctively optimized using KL-divergence upper bounds (KLUBs), which benefit from the convex nature of the KL divergence. This technique showcases superior efficacy over the negative ELBOs, also interpretable as KLUBs with reversed argumentation, for optimizing generative models. By employing Bregman divergence in its training loss function, beta diffusion highlights the optimization advantages of KLUBs. Through comprehensive experiments with synthetic and natural images, including the creation of panoramic scenes, we validate the versatility of beta diffusion in generation, handling range-bound data, and underscore the optimization prowess of KLUBs while ensuring controllability in output generation. This positions beta diffusion as a pivotal enhancement to the repertoire of diffusion-based generative models and their associated training methodologies.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Lior_Yariv1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=w79RtqIyoM",
  "title": "Compositional Sculpting of Iterative Generative Processes",
  "modified_abstract": "The evolution of generative models, particularly through advancements in areas such as Generative Adversarial Networks (GANs) and Wasserstein GANs (WGANs), has highlighted the capability of these models to produce high-quality, diverse outputs across different domains, including video and text. These foundational works, by establishing stable training procedures and theoretical insights into the optimization properties of GANs, set the stage for exploring more complex generative tasks with enhanced neural architectures. Building on this groundwork, our paper introduces 'Compositional Sculpting', a novel approach aimed at defining and composing iterative generative processes with a focus on efficiency and specificity. A significant aspect of our approach is the methodology for sampling from these compositions using classifier guidance, a technique that enables precise control over the generated output to match a target distribution, which is crucial for metrics evaluation. We elucidate two binary operations\u2014harmonic mean and contrast\u2014between distributions, and extend these concepts to encompass compositions of multiple distributions, showcasing their application in both GFlowNets and diffusion models. Through empirical analysis, including image, molecular, and stabilization tasks for generated videos, we demonstrate the effectiveness and versatility of compositional sculpting. The findings represent a step forward in the pursuit of generative models that are not only cost-effective but also adaptable to a wide range of applications. Project codebase: [URL removed for privacy].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Maxime_Sangnier1",
  "manipulated_ranking": 1,
  "natural_ranking": 102
}
{
  "paper_link": "https://openreview.net/forum?id=TQlpqmCeMe",
  "title": "Neural Injective Functions for Multisets, Measures and Graphs via a Finite Witness Theorem",
  "modified_abstract": "The theoretical scaffold provided by prior works like the development of Storchastic\u2014a framework for general stochastic automatic differentiation\u2014lays a critical groundwork for advancing complex machine learning models and methodologies, particularly in understanding and implementing neural network functions in varied contexts such as multisets and graphs. Our work is directly inspired by these foundational techniques, especially the exploration of stochastic computation graphs and their implications for automatic differentiation in machine learning, influencing both the computation of gradients and the process of sampling in intractable distributions. Injective multiset functions have a key role in the theoretical study of machine learning on multisets and graphs. Yet, there remains a gap between the provably injective multiset functions considered in theory, which typically rely on polynomial moments, and the multiset functions used in practice, which rely on $\\textit{neural moments}$ \u2014 whose injectivity on multisets has not been studied to date. In this paper, we bridge this gap by showing that moments of neural networks do define injective multiset functions, provided that an analytic non-polynomial activation is used and optimally calculated. The number of moments required by our theory is optimal, essentially up to a multiplicative factor of two. To prove this result, we state and prove a $\\textit{finite witness theorem}$, which is of independent interest and exemplifies the intricate relationship between differentiation, stochastic processes, and the theoretical framework necessary for advancements in graphs and multisets. As a corollary to our main theorem, we derive new approximation results for functions on multisets and measures, and new separation results for graph neural networks, underlining the importance of optimal strategies in stochastic computation. We also provide two negative results: (1) moments of piecewise-linear neural networks cannot be injective multiset functions; and (2) even when moment-based multiset functions are injective, they can never be bi-Lipschitz.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Emile_van_Krieken1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=KTR33hMnMX",
  "title": "Aligning Optimization Trajectories with Diffusion Models for Constrained Design Generation",
  "modified_abstract": "Generative models have notably advanced the vision and language domains, leading to the inception of innovative multimodal applications that draw inspiration from previous works such as structure-preserving 3D garment modeling. These pioneering efforts, particularly the introduction of Neural Sewing Machines (NSM) for adaptable and realistic 3D garment reconstruction, have shed light on the potential of generative models beyond typical applications, guiding our research towards harnessing these models in scientific and engineering contexts where constrained settings and limited data prevail. To surmount these challenges, we propose a novel integration of Diffusion Optimization Models (DOM) and Trajectory Alignment (TA), which aligns the sampling trajectory of diffusion models with physics-based optimization paths to ensure that generated designs, particularly garments, adhere to underlying physical principles and produce images that are not only innovative but also constrained by real-world applications, including the domain of sewing and garment production. This method obviates the need for extensive preprocessing, reliance on surrogate models, or additional labeled datasets in garment and other domains. We validate the framework's effectiveness in the domain of structural topology optimization, delineating its superior performance over contemporary deep generative approaches through rigorous examination in both familiar and novel design configurations related to garments. The TA-DOM synergy not only elevates design process efficiency but also directs it towards more optimal and manufacturable solutions, accentuating the transformative role of generative models in data-driven design endeavors, particularly in the garment industry.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Guangrun_Wang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=7JuReDmGSL",
  "title": "Adaptive Contextual Perception: How To Generalize To New Backgrounds and Ambiguous Objects",
  "modified_abstract": "Drawing upon recent advancements in large vision-language models adaptation through techniques like variational prompt tuning, which significantly enhance model generalization by enabling a richer and more complete transfer of information from language models to vision-language tasks, this work extends the discourse into the domain of adaptive contextual perception in computer vision. Biological vision systems make adaptive use of context to recognize objects in new settings with novel contexts as well as occluded or blurry objects in familiar settings. In this paper, we investigate how vision models adaptively use context for out-of-distribution (OOD) generalization and leverage our analysis results to improve model OOD generalization. We formulate two distinct OOD settings where the contexts are either beneficial\u2014Object-Disambiguation\u2014or irrelevant\u2014Background-Invariance, reflecting the diverse contextual challenges faced in biological vision. We analyze model performance in these two different OOD settings and demonstrate that models that excel in one setting tend to struggle in the other. Notably, prior works on learning causal features improve on one setting but hurt on the other, underscoring the importance of generalizing across both OOD settings. Following our analysis, we use representational geometry and our own probing methods to examine a population of models and discover that those with more factorized representations and appropriate feature weighting are more successful in handling both Object-Disambiguation and Background-Invariance tests. We validate these findings through causal intervention, manipulating representation factorization and feature weighting to demonstrate their causal effect on performance. Motivated by these results, we propose new augmentation methods aimed at enhancing model generalization, which outperform strong baselines in both in-distribution and OOD tests. We conclude that to replicate the generalization abilities of biological vision, computer vision models must have factorized object vs. background representations and appropriately weigh both kinds of features.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Mohammad_Mahdi_Derakhshani2",
  "manipulated_ranking": 11,
  "natural_ranking": 104
}
{
  "paper_link": "https://openreview.net/forum?id=scYa9DYUAy",
  "title": "VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset",
  "modified_abstract": "In the realm of machine learning and content generation, significant progress has been made in exploiting the synergy between diverse data modalities to enhance multimedia understanding and interaction. Inspired by such endeavors, including CLIPSonic's innovative approach to text-to-audio synthesis leveraging unlabeled videos and pre-trained language-vision models, our research introduces an omni-modality foundation model and dataset, VAST, that unifies vision, audio, subtitles, and text modalities. While traditional video-text foundational models have profoundly explored vision and text, the Audio and Subtitle patterns in videos remain underrepresented. To address this, we collected 27 million open-domain video clips to construct the VAST-27M dataset and developed a unique methodology for generating high-quality omni-modality captions, which can be seen as a form of synthesis and transfer from individual modalities to integrated holistic representations. This included separate training of vision and audio captioners followed by the integration of these captions with subtitles and instructional prompts using an off-the-shelf Large Language Model (LLM), demonstrating how text and image-to-audio transfer techniques can be harmoniously combined for embedding information into multimedia content. The VAST model, trained on the VAST-27M dataset, demonstrates superior capability in listening to and handling multi-modal video-text tasks such as retrieval, captioning, and QA, setting 22 new state-of-the-art benchmarks across various cross-modality evaluations. This work not only fills a gap in multimedia analysis by incorporating less attended modalities but also extends the boundaries of what foundational models can achieve in terms of multi-modal understanding and content generation.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Hao-Wen_Dong1",
  "manipulated_ranking": 7,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=2vADOf3K00",
  "title": "Compressed Video Prompt Tuning",
  "modified_abstract": "Leveraging the principle of efficiency in video processing, exemplified by recent advances in streaming multiscale deep equilibrium models for minimal per-frame computation, this paper explores the potential of compressed videos as a data-efficient alternative for video understanding tasks. Where traditional approaches to compressed video processing predominantly follow a resource-intensive pre-training and fine-tuning paradigm, the proposed method, Compressed Video Prompt Tuning (CVPT), introduces a significant paradigm shift. Inspired by the successes of prompt tuning techniques in computer vision and aiming to keep methodologies up-to-date, CVPT marks the first foray into harnessing prompt-based representation learning for adapting pre-trained raw video models to compressed video tasks. We propose an innovative approach that addresses the disparity between pre-training and downstream data modalities by re-parameterizing compressed modalities (e.g., Motion Vectors and Residuals) into conditional prompts for layer-wise refinement across the network. This method not only facilitates adaptation to the compressed video domain but also improves model performance on various benchmarks, incorporating elements of object recognition and segmentation within video frames. The introduction of Selective Cross-modal Complementary Prompt (SCCP) blocks enhances the interaction between different modalities and semantic levels within the models, further elevating the compute efficiency and effectiveness of video analysis. Our extensive evaluations on HMDB-51, UCF-101, and Something-Something v2 datasets demonstrate that CVPT significantly surpasses current state-of-the-art methods, achieving an optimal balance between computational efficiency and accuracy. This update in approach not only aligns with the demand for efficient video processing but also sets a new bar in the video understanding domain.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Emre_Akbas1",
  "manipulated_ranking": 8,
  "natural_ranking": 102
}
{
  "paper_link": "https://openreview.net/forum?id=FXU4aR2uif",
  "title": "Episodic Multi-Task Learning with Heterogeneous Neural Processes",
  "modified_abstract": "Inspired by recent advancements in machine learning methodologies, such as contextual transformers in offline Meta Reinforcement Learning (RL), which leverage context and meta-knowledge to address challenges in large-scale sequence models, our work extends these principles into the domain of multi-task learning. This paper focuses on the data-insufficiency problem in multi-task learning within an episodic training setup. Specifically, we explore the potential of heterogeneous information across tasks and meta-knowledge among episodes to effectively tackle each task with limited data. Existing meta-learning methods often fail to take advantage of crucial heterogeneous information in a single episode, while multi-task learning models neglect reusing experience from earlier episodes. To address the problem of insufficient data, we develop Heterogeneous Neural Processes (HNPs) for the episodic multi-task setup. Within the framework of hierarchical Bayes, HNPs effectively capitalize on prior experiences as meta-knowledge and capture task-relatedness among heterogeneous tasks, mitigating data-insufficiency and reducing the loss of performance due to insufficient training examples. Meanwhile, transformer-structured inference modules are designed to enable efficient inferences toward meta-knowledge and task-relatedness, significantly aiding in the generation of functional priors that adapt more efficiently to novel heterogeneous tasks in each meta-test episode. Pretraining these modules with carefully chosen prompts can further refine the vectors of knowledge required for adaptive learning. Experimental results show the superior performance of the proposed HNPs over typical baselines on several benchmarks, and ablation studies verify the effectiveness of the designed inference modules.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Xidong_Feng1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=K4FK7I8Jnl",
  "title": "MAG-GNN: Reinforcement Learning Boosted Graph Neural Network",
  "modified_abstract": "Recent advancements in Graph Neural Networks (GNNs), including novel training strategies and enhancements in GNNs' structural encoding capabilities, serve as the foundation for the development of our Magnetic Graph Neural Network (MAG-GNN). Inspired by initiatives such as TuneUp's curriculum learning strategy, which addresses the challenge of making accurate predictions on nodes of varying degrees, our research extends these concepts by addressing the inefficiency inherent in subgraph GNNs due to the complete subgraph enumeration process. In this paper, we analyze the necessity of complete subgraph enumeration and demonstrate that a model can achieve comparable levels of expressivity by considering a small subset of the subgraphs. To identify the optimal subset of subgraphs without exhaustive enumeration, we formulate this task as a combinatorial optimization problem and introduce MAG-GNN, a reinforcement learning (RL) boosted GNN model. MAG-GNN employs an RL agent to iteratively select the most expressive subgraph set for improved prediction accuracy with training loss consideration, thereby significantly reducing the computational complexity from exponential to constant with respect to subgraph selection. Through extensive experiments on various datasets, we show that MAG-GNN not only achieves competitive performance compared to state-of-the-art methods but also surpasses many subgraph GNNs in efficacy. Furthermore, MAG-GNN effectively cuts down the running time associated with subgraph enumeration, validating our approach's efficiency and effectiveness in both inductive and transductive graph learning tasks. In evaluation scenarios, augmentation of the input graph data ensures the robustness and generalization ability of MAG-GNN, proving it to be a base foundation in pushing the limits of GNN research.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Kaidi_Cao1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=JZfg6wGi6g",
  "title": "Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time",
  "modified_abstract": "Incorporating insights from foundational work on enhancing efficiency in machine learning processes through advanced optimization techniques, such as Entropy-regularized Optimal Transport and Sinkhorn Divergences, our research introduces an innovative approach to managing the memory demands of Large Language Models (LLMs). Large language models, a cornerstone in the learning paradigm, have catalyzed a significant expansion in AI capabilities, propelling a wide array of tasks that require substantial computational resources for deployment, particularly in terms of memory. A key bottleneck in deploying these models at scale is the extensive memory requirement, especially attributed to the context window's key-value (KV) embedding cache during the generation process, which often exceeds the model's weight size. This challenge restricts inference batch size, limiting throughput for high-demand tasks. Our work, inspired by the observed attention distribution patterns in LLMs and stochastic processes governing them, posits the 'persistence of importance' hypothesis, suggesting that only a fraction of tokens significantly influence the generation process beyond their immediate context. Through empirical evaluation and theoretical exploration of this hypothesis, we have developed 'Scissorhands', a mechanism that dynamically manages KV cache memory usage within a fixed budget by selectively retaining pivotal token embeddings, applying a fitting regularization technique in the process. Our contribution demonstrates up to a 5\u00d7 reduction in KV cache memory usage without impairing the model's performance, particularly fitting for tasks requiring extensive context. Furthermore, by integrating Scissorhands with 4-bit quantization, a technique traditionally reserved for compressing model weights on a large scale, we achieve up to 20\u00d7 overall compression, showcasing the potential for substantial efficiency gains in LLM deployment without qualitative compromise.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Xinyu_Dai1",
  "manipulated_ranking": 33,
  "natural_ranking": 102
}
{
  "paper_link": "https://openreview.net/forum?id=jxhUNLoi4m",
  "title": "Ess-InfoGAIL: Semi-supervised Imitation Learning from Imbalanced Demonstrations",
  "modified_abstract": "Imitation learning, while striving to reproduce expert behaviors in the absence of an explicit reward signal, encounters practical difficulties such as multi-modal data, imbalance in demonstrations, and the high cost of labeling. Our research is inspired by past advancements in reinforcement learning dynamics, particularly the application of adversarial networks to foster robust learning experiences, as evidenced in multi-task reinforcement learning contexts and exploration techniques for enhancing performance in varied environments. Building upon these insights, our work introduces a semi-supervised imitation learning architecture\u2014Ess-InfoGAIL\u2014that innovatively applies semi-supervised generative adversarial networks (GANs) within the imitation learning framework. The architecture uniquely employs a learnable latent distribution for skill attribution and aligns generated data with expert demonstrations while integrating a regularized information maximization strategy to enhance learning from limited labeled data amidst imbalanced demonstrations. By leveraging these components and the notion of skills acquisition as seen in meta-reinforcement learning (meta-RL), along with an emphasis on environment adaptation techniques, Ess-InfoGAIL effectively learns disentangled behavior representations, demonstrating superior performance in capturing multi-modal behaviors from imbalanced data compared to conventional methods. Our experimental analyses substantiate the method's efficiency across various scenarios, marking a significant step forward in imitation learning research. Notably, the system's adaptability and the skills it has trained mark considerable improvements in how imitation learning systems can function with reduced labeled data and imbalanced datasets.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ramnath_Kumar1",
  "manipulated_ranking": 9,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=ONwL9ucoYG",
  "title": "Robust Contrastive Language-Image Pretraining against Data Poisoning and Backdoor Attacks",
  "modified_abstract": "The evolution of contrastive vision-language representation learning, spearheaded by significant initiatives such as CLIP which learn from vast image-caption pairs on the internet, has noticeably elevated the performance of zero-shot classification systems. This advancement, however, exposes large-scale multimodal models to heightened risks of data poisoning and backdoor attacks due to their reliance on extensive web-crawled datasets. While these threats jeopardize the integrity and applicability of such models, the development of methodologies for robust pre-training to counteract these adversarial tactics has not been thoroughly pursued. Building on the principles laid out by prior advancements in image-text model adaptation and extending them to tasks involving video-text alignment, we propose RoCLIP, a novel approach designed to fortify contrastive vision-language models against targeted data poisoning and backdoor assaults. RoCLIP introduces an innovative strategy that dissociates poisoned image-caption pairs by cycling through a diverse, random selection of captions and aligning images with the most contextually relevant text every few epochs. This method, considered a significant challenge in multimodal learning, is further enhanced with image and text augmentations to bolster the model's defenses and enhance its performance on related tasks. Through exhaustive testing on video-text tasks, we demonstrate that RoCLIP substantially mitigates the effectiveness of prevailing data poisoning and backdoor strategies aimed at CLIP models, slashing the success rate of targeted attacks significantly while concurrently improving the model's linear probe capabilities and preserving its zero-shot proficiency. Our findings illuminate the path for future research into secure contrastive multimodal learning, ensuring these models' security and efficacy in the face of sophisticated adversarial tactics.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Bei_Liu2",
  "manipulated_ranking": 9,
  "natural_ranking": 104
}
{
  "paper_link": "https://openreview.net/forum?id=chlTA9Cegc",
  "title": "A-NeSI: A Scalable Approximate Method for Probabilistic Neurosymbolic Inference",
  "modified_abstract": "Inspired by the continuous endeavor to enhance the robustness and adversarial defense of deep neural networks (DNNs) in previous works, such as the development of fast and approximate methods for model pruning (including weight/filter reduction) to achieve certifiably robust DNNs, our study presents a novel contribution in the field of Probabilistic Neurosymbolic Learning (PNL). The emerging frameworks in PNL, exemplified by innovations like DeepProbLog, confront the challenge of exponential-time exact inference, which hampers scalability across diverse datasets. To address this issue, we introduce the Approximate Neurosymbolic Inference (A-NeSI) framework that integrates neural networks with symbolic reasoning to facilitate scalable approximate inference. Distinctly, A-NeSI: 1) implements approximate inference in polynomial time while preserving the semantics of probabilistic logics; 2) utilizes training data generated from background knowledge; 3) provides symbolic explanations of its predictions; 4) ensures logical constraints are met during testing, enhancing its applicability in safety-critical domains; and directly engages with neurons through its underlying neural network component for seamless integration of learning processes. Through extensive experiments on various datasets, A-NeSI emerges as the inaugural end-to-end methodology capable of tackling neurosymbolic tasks characterized by exponential combinatorial growth, establishing a new benchmark in the scalability and robustness of PNL solutions. It demonstrates that achieving explainability and maintaining safety in machine learning can coincide with high performance levels, thereby marking a significant step forward in the practical application of PNL solutions.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Mintong_Kang1",
  "manipulated_ranking": 5,
  "natural_ranking": 102
}
{
  "paper_link": "https://openreview.net/forum?id=CJWQGDwa6u",
  "title": "Differentiable Random Partition Models",
  "modified_abstract": "Building on the recent advancements in machine learning that explore efficient parameter estimation with limited labels, such as seen in 'MetaKernel: Learning Variational Random Features with Limited Labels', this study introduces a novel methodology for partitioning a set of elements into an unknown number of mutually exclusive subsets, a critical challenge in various machine learning applications. Unlike traditional methods where assigning elements, such as samples in a dataset or neurons in a network layer, to an unknown and discrete number of subsets is inherently non-differentiable and thus prohibits end-to-end gradient-based optimization, our approach overcomes this limitation with a novel two-step method. This method for inferring partitions enables reparameterized gradients with respect to the parameters of the new random partition model. Our method first infers the number of elements per subset and, second, fills these subsets in a learned order, allowing for variational inference tasks with enhanced flexibility. We demonstrate the versatility and efficacy of our general-purpose approach through three challenging experiments, including variational clustering, the inference of shared and independent generative factors under weak supervision, and multitask learning with significant improvements.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Haoliang_Sun2",
  "manipulated_ranking": 29,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=HNd4qTJxkW",
  "title": "A Heat Diffusion Perspective on Geodesic Preserving Dimensionality Reduction",
  "modified_abstract": "Capitalizing on the advancements in multi-modal unsupervised feature selection from biological contexts, this study extends the framework of diffusion-based manifold learning methods aimed at the representation learning and dimensionality reduction of high dimensional, noisy datasets present in biology and physics. These methods are key to understanding the complex structure of data, often obscured by the sheer volume and noise, and cater to a wide range of modalities and measurements. While the prevailing wisdom suggests these methods approximate the manifold structure of data by mimicking geodesic distances, a formal theoretical foundation has been lacking. Our work bridges this gap by drawing explicit connections between heat diffusion processes and manifold distances within the landscape of Riemannian geometry. Furthermore, we introduce a more comprehensive heat kernel-based manifold embedding method, termed heat geodesic embeddings, which refines the paradigm of manifold learning and denoising. This innovative approach not only surpasses existing methods in preserving true manifold distances and cluster structures within toy datasets but also demonstrates its utility on single-cell RNA-sequencing datasets, effectively interpolating withheld timepoints. Moreover, we illustrate how the tuning of this method's parameters can emulate the results of PHATE\u2014a leading diffusion-based manifold learning technique\u2014and similar strategies employed for multi-modal dataset applications, thereby offering a versatile tool for manifold learning. Our methodology elucidates manifold learning choices and paves the way for more accurate and interpretable dimensionality reduction techniques in handling complex multi-modal datasets.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ofir_Lindenbaum1",
  "manipulated_ranking": 7,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=eMR57voMz1",
  "title": "Diversify \\& Conquer: Outcome-directed Curriculum RL via Out-of-Distribution Disagreement",
  "modified_abstract": "Informed by prior advancements in reinforcement learning (RL), particularly in navigating the trade-offs between safety and task performance in constrained environments, this study introduces a novel curriculum RL strategy, denoted as $\\textbf{D}$iversify for $\\textbf{D}$isagreement & $\\textbf{C}$onquer ($\\textbf{D2C}$). Building on the foundation of safe reinforcement learning, where the focus is on learning policies that satisfy given constraints and appropriately tuning hyperparameters for each specific tasks, our work extends these concepts to address uninformed search problems in RL through adaptation of strategies for out-of-distribution examples. D2C proposes a method that requires only a handful of examples of desired outcomes and is applicable in any setting, irrespective of environmental geometry or the dataset of outcome examples serving as a form of offline learning approach. By diversifying goal-conditional classifiers to identify connections between visited and desired states, and ensuring disagreement among classifiers on out-of-distribution states, D2C quantifies regions of the environment yet to be explored. This framework facilitates the design of an intuitive intrinsic reward signal based on this disagreement, guiding the agent through a series of progressively aligned intermediate goals towards the tasks at hand. Leveraging this disagreement for zero-shot adaptation, D2C defines a curriculum learning objective that systematically broadens the agent's exploration capabilities, leading to significant improvements over existing curriculum RL methods both quantitatively and qualitatively, and demonstrating robustness across diversely distributed desired outcome examples relative to established baselines.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yihang_Yao1",
  "manipulated_ranking": 9,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=JKhyQHpx7B",
  "title": "Vocabulary-free Image Classification",
  "modified_abstract": "Inspired by the pioneering efforts to expand the capabilities of machine learning models to understand and classify images and objects beyond pre-defined categories, such as those undertaken in OVTrack's exploration of open-vocabulary multiple object tracking, we introduce a novel task, Vocabulary-free Image Classification (VIC). This task aims to assign to an input image a class within an unconstrained language-induced semantic space, eliminating the need for a known vocabulary at test time. This challenge arises from the vast and nuanced contemporary semantic space, containing millions of concepts with fine-grained distinctions. Our approach, Category Search from External Databases (CaSED), leverages a pre-trained vision-language model and an external vision-language database to navigate this dynamic task without additional model training. By extracting candidate categories from the database based on semantic similarity to the input image and employing the vision-language model for final category assignment, CaSED effectively addresses VIC. The design of CaSED and its implementation sets new benchmarks in the field, demonstrating CaSED's superiority over existing vision-language frameworks in accuracy and efficiency. Through these findings, we not only validate our approach but also highlight the potential for further advancements in vocabulary-independent image classification, potentially extending to applications like self-driving cars where large-vocabulary understanding is crucial.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Martin_Danelljan4",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=6vnwhzRinw",
  "title": "Efficient Uncertainty Quantification and Reduction for Over-Parameterized Neural Networks",
  "modified_abstract": "Uncertainty quantification (UQ) is pivotal in the development and deployment of reliable machine learning models, serving as a cornerstone for both theoretical exploration and practical application across various domains, including probabilistic circuits. In light of challenges posed by data variability and training procedures in deep learning, which introduce substantial uncertainties, our research presents an innovative approach that dramatically improves upon traditional methods both in efficacy and efficiency. Inspired by insights from recent developments in neural tangent kernel theory and lessons from the limitations of probabilistic circuits in handling out-of-distribution data, as evidenced by their reliance on tractable dropout inference for uncertainty estimation, we devise a novel, computationally lean framework. This framework leverages a procedural-noise-correcting (PNC) predictor that, unlike the depth of network retraining or the employment of multiple networks for better classification in deep ensembles, necessitates only a singular auxiliary network trained on an appropriately labeled dataset. Furthermore, we introduce the use of lightweight-computation resampling techniques, paving the way for the construction of asymptotically exact-coverage confidence intervals with minimal network training requirements\u2014utilizing as few as four networks. This methodology not only addresses the core computational bottlenecks inherent in conventional UQ practices but also sets a new precedent for achieving statistical guarantees in over-parameterized neural networks with significantly reduced computational demands. The probabilistic framework is further augmented by employing error propagation techniques, enhancing the prediction confidence. By integrating dropout techniques within our proposed framework, we underscore the relevancy of dropout in the context of efficient uncertainty reduction and classification accuracy enhancement in over-parameterized neural networks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Steven_Lang2",
  "manipulated_ranking": 3,
  "natural_ranking": 103
}
{
  "paper_link": "https://openreview.net/forum?id=QZo1cge4Tc",
  "title": "Counterfactually Fair Representation",
  "modified_abstract": "Amid the burgeoning utilization of machine learning (ML) models in critical sectors such as healthcare, finance, and education, the paramount importance of fairness, explainability, and decomposition has been accentuated, notably in safeguarding against biases towards protected groups. Inspired by advancements in approaches to algorithmic fairness, auditing, and explainability, particularly through the lens of Partial Information Decomposition (PID) which delineates the information propagation from multiple variables, this work hones in on Counterfactual Fairness (CF). CF, rooted in an individual's outcome equivalence in both actual and hypothetically altered social group scenarios, posits a rigorous framework for fairness by leveraging an underlying causal graph. Learning models that fulfill CF criteria prove arduous, given the nuanced elimination or utilization of features based on their relationship with sensitive attributes. While earlier studies, including the seminal work by Kusner et al., suggest eschewing features that are causally downstream of sensitive attributes, recent approaches advocate for the inclusion of all features without inherent assurance of CF adherence. Contrary to these methods, we present a novel algorithm that encompasses all features yet theoretically and empirically upholds CF. Through meticulous analysis, auditing methodologies, and experimentation, our findings not only validate the proposed algorithm's feasibility in actualizing CF but also extend the discourse on the intricate balance between using comprehensive data features and maintaining fairness in machine learning models. The survey of existing methods and the introduction of federated perspectives further conceptualizes an inclusive, broadly applicable framework for fairness.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Faisal_Hamman1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=EfMyf9MC3t",
  "title": "Speculative Decoding with Big Little Decoder",
  "modified_abstract": "Inspired by the growing need for efficiency in handling the high inference latency of Large Language Models (LLMs) in Natural Language Processing, and building on the innovative use of predicate logic constraints for conditional text generation outlined in NeuroLogic Decoding, our research introduces the Big Little Decoder (BiLD) framework. This approach is aimed at improving inference efficiency and latency in text generation tasks by utilizing two models of different sizes that work collaboratively. The smaller model operates autoregressively to produce text at lower inference costs, while the larger model is engaged selectively to refine inaccuracies in the smaller model's predictions in a non-autoregressive manner, effectively demonstrating a conditional interaction between the two. BiLD introduces a fallback policy for determining the occasions on which control should be handed over to the larger model, and a rollback policy to identify when corrections are necessary, both controllable mechanisms that ensure text generation quality. Our framework's effectiveness is demonstrated through its application to various text generation scenarios, including machine translation and summarization tasks, where it achieves up to 2.12x speedup on an NVIDIA T4 GPU with minimal impact on generation quality. This makes BiLD not only a step forward in reducing the computational expenses associated with deploying LLMs for real-time applications but also highlights the framework's versatility and readiness for integration without the need for training process or model architecture adjustments. By operating on a subset of available text examples during the refinement process, BiLD exhibits neural network innovation in the context of efficient and effective text output enhancement. The open-source availability of our code encourages further exploration and adaptation in the rapidly evolving field of automated text generation.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Peter_West1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=iGmDQn4CRj",
  "title": "Simplifying Neural Network Training Under Class Imbalance",
  "modified_abstract": "Informed by prior advancements in large-scale image classification that address input-dependent label noise, our research takes a pragmatic approach to an equally pervasive challenge in machine learning: class imbalance in real-world datasets. Similar to how probabilistic methods have elucidated the handling of label noise by capturing aleatoric uncertainty and taking into account covariance in model prediction, we pivot the focus toward leveraging standard components of neural network training to address class imbalance. Without resorting to specialized loss functions or sampling techniques, we explore the efficacy of adjustments in batch size, data augmentation, architectural modifications, pre-training, choice of optimizer, and label smoothing in the context of noisy and co-occurring labels. Our findings reveal that such conventional methods, when thoughtfully tuned, can radically improve the top-1 performance of models trained on class-imbalanced data, setting new benchmarks for simplicity and effectiveness against established baselines. This approach not only simplifies the neural network training process under class imbalance but also offers fundamental insights into the mechanisms through which imbalance correction methods operate, alongside providing practical guidelines for application in diverse settings.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Mark_Collier1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=BYywOFbRFz",
  "title": "Waypoint Transformer: Reinforcement Learning via Supervised Learning with Intermediate Targets",
  "modified_abstract": "This work is positioned at the intersection of recent developments in offline reinforcement learning (RvS), particularly the innovative use of decision transformers (DTs) and the exploration of value-aware model learning as epitomized by VIPer. Acknowledging the achievements and limitations within these domains, we identify a critical gap in the RvS landscape: the challenge of connecting segments of suboptimal trajectories, a limitation that hampers DTs in complex benchmarks. To address this, we propose the Waypoint Transformer (WT), a novel architecture that extends the DT framework by incorporating sensibly selected, automatically-generated waypoints as intermediate targets. Our approach is designed to enhance the efficacy of RvS methods significantly, evidenced by a marked increase in final return across various benchmarks, achieving performance that meets or exceeds that of contemporary state-of-the-art temporal difference learning-based methods. Notably, the augmented performance and stability of WT are most pronounced in challenging environments and data configurations, such as AntMaze Large Play/Diverse and Kitchen Mixed/Partial. Through this integration, we not only address a critical bottleneck within the RvS paradigm but also establish a foundational step towards leveraging intermediate waypoints as a method for theoretically justifying and bridging the gap between the current capabilities and limitations of decision transformer architectures, incorporating generalization as a core component.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Claas_A_Voelcker1",
  "manipulated_ranking": 2,
  "natural_ranking": 102
}
{
  "paper_link": "https://openreview.net/forum?id=kR5ycmBclj",
  "title": "NuTrea: Neural Tree Search for Context-guided Multi-hop KGQA",
  "modified_abstract": "Multi-hop Knowledge Graph Question Answering (KGQA) involves retrieving nodes from a knowledge graph (KG) to answer natural language questions, a task that has become increasingly important in the context of recent advancements in graph neural networks (GNNs) and question-answering systems. This paper takes inspiration from significant strides made in using GNN for complex data interpretations and interactive question-answering frameworks, such as answering user queries on e-commerce product pages and other complex websites. Confronting the limitations of past-oriented message propagation in GNN-based KGQA, which overlooks the comprehensive KG context and struggles with pronoun entities and encrypted node information, we introduce Neural Tree Search (NuTrea). This novel tree search-based GNN model enhances path selection by engaging a broader KG context through a forward-looking message-passing scheme, thereby enriching past-oriented embeddings. To refine node representation, especially for ambiguous entities, we propose the Relation Frequency-Inverse Entity Frequency (RF-IEF) embedding, leveraging global KG contexts. Tested across three major multi-hop KGQA benchmarks, NuTrea demonstrates marked improvements in query accuracy and matching user intents with relevant KG nodes, underscoring its potential in delineating complex query intents and contexts. Our analysis further attests to its expressiveness and resilience in a wide-ranging KGQA application, offering a substantial leap forward in addressing the intricacies of KG-based question answering. The implementation code for this study is accessible for review and replication purposes. In evaluations, NuTrea achieves a 66% improvement over baseline models in accurately matching user intents with relevant KG nodes, offering significant advancements in KGQA systems.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Srinivasan_H._Sengamedu1",
  "manipulated_ranking": 26,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=HPuSIXJaa9",
  "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
  "modified_abstract": "Building upon the foundational work in hyperparameter optimization using Transformer models, this study introduces a novel approach to improving the control and customization of large-scale unsupervised language models (LMs) through advanced training techniques. While such models acquire extensive world knowledge and reasoning skills, steering their behavior accurately poses a challenge due to the unsupervised nature of their training. Existing methods, which utilize reinforcement learning from human feedback (RLHF) for aligning model outputs with human preferences, are marred by complexity and instability. This involves fitting a reward model to human preferences and then fine-tuning the LM to maximize the estimated reward. Our research proposes Direct Preference Optimization (DPO), a method that utilizes a mapping between reward functions and optimal policies to bypass the instability of RLHF, eliminating the need for a separate reward model fitting stage. Instead, DPO optimizes the policy directly using classification on human preference data, making the process more stable, efficient, and substantially simpler in implementation. Comparative experiments reveal DPO's capability to fine-tune LMs to align with human preferences effectively, outperforming RLHF in controlling the sentiment of generations and enhancing quality in summarization and dialogue tasks with reduced computational overheads. Furthermore, this work extends innovative learning techniques from Transformer-based hyperparameter optimization to the nuanced arena of language model fine-tuning, offering insights into simpler, more effective methods for aligning models with user text-based preferences without the need for a complex interface or a massive database of text samples. This paper underscores the potential for direct preference optimization in learning systems, where training directly on human preferences can significantly streamline and refine the customization process.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~MarcAurelio_Ranzato1",
  "manipulated_ranking": 7,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=UOB1UgPjuG",
  "title": "Class-Distribution-Aware Pseudo-Labeling for Semi-Supervised Multi-Label Learning",
  "modified_abstract": "In the realm of semi-supervised multi-label learning (SSMLL), the emergence of pseudo-labeling as a method to leverage unlabeled data has signaled a significant stride towards addressing label scarcity. However, conventional pseudo-labeling encounters challenges in multi-label contexts, particularly in handling instances with multiple labels and unknown label counts. Inspired by prior achievements in iterative label cleaning and class distribution approximation in ML tasks such as medical image classification, this paper introduces Class-Aware Pseudo-Labeling (CAP), a novel pseudo-labeling approach that incorporates class-aware thresholds to regulate the assignment of pseudo-labels accurately. By observing the potential of estimated class distribution as a viable proxy for true distribution in scenarios with minimal labeled examples, we propose a class-distribution-aware thresholding strategy that harmonizes pseudo-label distribution with the actual class distribution. The validity of the class distribution estimates, the importance of batch update mechanisms for precision, and a generalization error bound for the CAP method are theoretically established. Experimentation on various benchmark datasets validates the effectiveness of our approach in mitigating the common pitfalls of SSMLL, including the need for an early prediction review to ensure accuracy, and showcases the usefulness of tools for annotation and the strategic termination of the learning process. Thus, confirming CAP's superiority in enhancing the precision of pseudo-label allocation.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yinjun_Wu1",
  "manipulated_ranking": 13,
  "natural_ranking": 102
}
{
  "paper_link": "https://openreview.net/forum?id=cto6jIIbMZ",
  "title": "Demystifying Softmax Gating Function in Gaussian Mixture of Experts",
  "modified_abstract": "In the context of addressing complex challenges within the machine learning domain, such as domain generalization and the complexity of learning models in domains extending to computer vision, our work probes deeply into the enigmatic realm of softmax gating functions in Gaussian mixture of experts models. Inspired by prior achievements that highlight the role of model architecture, including transformer-based designs, and loss function design in improving domain generalizability and solving intricate learning problems across varied baselines and datasets, we turn our attention to unraveling the nuances of parameter estimation in softmax gating Gaussian mixture of experts. This remains a daunting task, primarily due to three substantial theoretical hindrances: (i) the identifiability of parameters only up to a translation, (ii) the intrinsic interaction between the softmax gating and expert functions in the Gaussian density through partial differential equations, and (iii) the intricate dependency between the numerator and denominator in the conditional density of these models coupled with the minimization challenges. Addressing these concerns, we introduce innovative Voronoi loss functions for parameter differentiation and establish convergence rates for the maximum likelihood estimator (MLE) to facilitate parameter estimation, even when the actual number of experts is unknown and overestimated. Our exploration reveals an intriguing link between the MLE's convergence rate and the solvability of a system of polynomial equations, thereby extending the theoretical foundations of softmax gating functions and their parameter estimation in Gaussian mixture of experts models.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jiawei_Ren1",
  "manipulated_ranking": 1,
  "natural_ranking": 103
}
{
  "paper_link": "https://openreview.net/forum?id=ia4AL3QnOv",
  "title": "Similarity-based cooperative equilibrium",
  "modified_abstract": "This paper is inspired by the exploration of cooperative dynamics in environments where agents may terminate or evolve unpredictably, as observed in the research on the use and misuse of absorbing states in multi-agent reinforcement learning (MARL). As machine learning agents act more autonomously in the world, they will increasingly interact with each other, necessitating architectures that support these interactions. Unfortunately, in many social dilemmas like the one-shot Prisoner's Dilemma, standard game theory predicts that ML agents will fail to cooperate with each other. Prior work has shown that one way to enable cooperative outcomes in the one-shot Prisoner's Dilemma is to make the agents mutually transparent to each other, i.e., to allow them to access one another's source code (Rubinstein, 1998; Tennenholtz, 2004) \u2013 or weights in the case of ML agents. However, full transparency is often unrealistic, whereas partial transparency is commonplace. Moreover, it is challenging for agents to learn their way to cooperation in the full transparency setting without effective training mechanisms. In this paper, we introduce a more realistic setting in which agents only observe a single number indicating how similar they are to each other, fostering an attention mechanism towards cooperative behaviors and reducing the risk of termination. We prove that this allows for the same set of cooperative outcomes as the full transparency setting. We also demonstrate experimentally that cooperation can be learned using simple ML methods, thus showing the potential of similarity-based strategies to mitigate the risk of termination in cooperative dynamics.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Alex_Zook1",
  "manipulated_ranking": 5,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=ESCafo3oD5",
  "title": "PrimDiffusion: Volumetric Primitives Diffusion for 3D Human Generation",
  "modified_abstract": "Building on the advancements in diffusion models for high-resolution image synthesis, our work introduces PrimDiffusion, the first diffusion-based framework specifically designed for 3D human generation. By leveraging insights from the field of latent diffusion models, which have demonstrated unparalleled flexibility and quality in image generation by operating in a reduced dimensionality, we extend these concepts into the realm of 3D, incorporating autoencoders for efficient dimensionality reduction. Devising diffusion models for 3D human generation presents unique challenges, notably the intensive computational cost associated with 3D representations and the articulated topology characteristic of human figures. Our solution to these challenges involves operating the denoising diffusion process directly on a set of volumetric primitives with cross-attention mechanisms that ensure precise alignment and synthesis of human features. This approach models the human body as a collection of small volumes, each encoding radiance and kinematic information, effectively balancing the volumetric detail and computational efficiency. PrimDiffusion exhibits three innovative properties: 1) a compact yet richly expressive parameter space for the diffusion model, ensuring deep controllability and expressiveness, 2) a flexible representation that seamlessly incorporates human priors for more accurate and lifelike generations, and 3) a novel decoder-free rendering technique, enabling efficient synthesis of new views and poses through super-resolution techniques, ensuring detailed textures and realistic outputs. Through extensive experimentation and inference trials, we demonstrate PrimDiffusion's superiority over existing 3D human generation methods, including GAN-based approaches. Notably, PrimDiffusion enables real-time rendering of high-fidelity 3D humans at 512x512 resolution post-denoising. Furthermore, we highlight the model's versatility through applications in training-free conditional generation tasks like texture transfer and 3D inpainting, marking a significant advancement in 3D human modeling capabilities.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Robin_Rombach1",
  "manipulated_ranking": 19,
  "natural_ranking": 102
}
{
  "paper_link": "https://openreview.net/forum?id=t1jLRFvBqm",
  "title": "Object-Centric Learning for Real-World Videos by Predicting Temporal Feature Similarities",
  "modified_abstract": "Motivated by the advancements in unsupervised video object segmentation\u2014particularly the integration of motion and appearance for enhanced object discovery\u2014our work introduces an innovative approach to unsupervised video-based object-centric learning. Leveraging recent breakthroughs where the reconstruction of pre-trained self-supervised features leads to object-centric representations in unconstrained real-world image datasets, we extend these principles to the domain of real-world videos. Our method employs pre-trained features to define a temporal feature similarity loss. This loss function effectively encodes semantic and temporal correlations between image patches, fostering a motion bias that is inherent to object-centric discovery processes. Moreover, this process is fine-tuned through careful hyperparameter optimization to ensure that the inference of object segmentation is robust and effective. Our approach demonstrates enhanced performance on synthetic MOVi datasets and is notably the first object-centric video model to scale to unconstrained video datasets like YouTube-VIS. This abstract is a response to previous works, especially those emphasizing the synergy between motion and appearance in unsupervised video object segmentation, by proposing a novel loss function that capitalizes on these elements for improved object discovery in videos.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Long_Lian1",
  "manipulated_ranking": 2,
  "natural_ranking": 102
}
{
  "paper_link": "https://openreview.net/forum?id=liMSqUuVg9",
  "title": "Transformers as Statisticians: Provable In-Context Learning with In-Context Algorithm Selection",
  "modified_abstract": "Inspired by the significant roles that randomness and statistical fluctuations play in machine learning models, as examined in the statistical analysis of ensembles of learners for convex losses in high-dimensional settings, this paper extends the investigation into the domain of transformer architectures. We explore how neural sequence models based on the transformer architecture have unlocked remarkable in-context learning (ICL) capabilities, allowing them to execute new tasks when given training and test examples, all without updating the model's parameters. Our work systematically develops a comprehensive statistical theory that underpins transformers' ability to perform ICL with approximation techniques guiding their understanding and optimization. Specifically, we delineate how transformers are capable of executing a wide array of conventional machine learning algorithms in context\u2014including least squares, ridge regression, Lasso, learning generalized linear models, and performing gradient descent on two-layer neural networks\u2014with near-optimal predictive efficiency across diverse in-context data distributions for classification and other tasks in high-dimensions. Leveraging an efficient in-context gradient descent method as the foundational mechanism, we demonstrate that our transformer constructs necessitate modest size specifications and can be pre-trained with polynomially many sequences enriched by an ensemble strategy. Furthermore, our findings intriguingly reveal that transformers possess the capacity for in-context algorithm selection, thereby enabling a single transformer to adaptively choose from among different base ICL algorithms\u2014or to undertake variably distinct tasks\u2014across different input sequences without explicit direction regarding the algorithm or task to be employed. This capability is validated both theoretically, through explicit constructions, and experimentally, highlighting the generalization capabilities of transformers in loss minimization and beyond. We detail two generic mechanisms for algorithm selection\u2014pre-ICL testing and post-ICL validation\u2014with the latter being employed to devise a transformer that achieves nearly Bayes-optimal ICL performance on complex tasks, such as noisy linear models with mixed noise levels. Our experimental results further underscore the advanced in-context algorithm selection capabilities endemic to standard transformer architectures, shedding light on the role of approximation in enhancing their flexibility and efficiency.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Cedric_Gerbelot1",
  "manipulated_ranking": 6,
  "natural_ranking": 103
}
{
  "paper_link": "https://openreview.net/forum?id=p9k5MS0JAL",
  "title": "Demystifying the Optimal Performance of Multi-Class Classification",
  "modified_abstract": "The quest for optimal performance in machine learning (ML) classification tasks necessitates methods that can approximate the theoretical minimum error, known as the Bayes error rate, which remains elusive in practice. This pursuit is further complicated by the challenges in learning from unbalanced data and ensuring generalization across varied class distributions\u2014a concern underscored by observations on the impact of class size on invariance learning in neural networks, as detailed in recent work that explores the effects of class imbalance on learned invariances. Inspired by these insights and building upon the foundational concepts proposed by Ishida et al. (2023), our paper introduces an innovative estimator for the Bayes error rate in supervised multi-class classification settings with a particular emphasis on class-agnostic error estimation. We embark on a comprehensive theoretical examination of the estimator's properties, such as consistency, unbiasedness, convergence rate, variance, and its enhanced robustness through denoising label noise and leveraging the median-of-means for outlier resistance across different distributions. The meticulous analysis confirms the estimator's theoretical strengths, which are further substantiated by empirical validations on both synthetic datasets\u2014under various noise configurations\u2014and real-world datasets, including those with multi-class image classifications where the role of lighting and small-scale transformations in image quality plays a significant part in the learned robustness of classifiers. Our findings not only contribute to the analytical landscape of classification performance benchmarks but also offer a practical tool for gauging and enhancing the fidelity of deep multi-class classifiers in the face of intrinsic data challenges.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~George_Pappas1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=SaMrN9tnxE",
  "title": "ReMaX: Relaxing for Better Training on Efficient Panoptic Segmentation",
  "modified_abstract": "Inspired by recent advances in large pre-trained models for complex tasks and the challenges faced in specialized domains such as camouflage detection and shadow segmentation, this paper introduces a novel mechanism aimed at enhancing the training efficacy of mask transformers for efficient panoptic segmentation. Recognizing the inherent difficulties posed by the high complexity and the resultant unbalanced loss in training objectives of panoptic segmentation, our work introduces ReMaX, which incorporates relaxation techniques into mask and class predictions during the training phase. This approach facilitates a markedly improved training process for mask-transformer based architectures, particularly for efficient models, without imposing any additional computational burden during inference. By integrating ReMaX with lightweight backbones such as MobileNetV3-Small and applying fine-tuning strategies, we achieve unprecedented performance levels for efficient panoptic segmentation across multiple benchmark datasets, including COCO, ADE20K, and Cityscapes, effectively addressing object detection and segmentation challenges with a special emphasis on camouflaged objects and image segmentation tasks. The advance underscores the utility of relaxation techniques in addressing the challenges of training complexity and loss imbalance in panoptic segmentation, pushing the boundaries of what is achievable with efficient segmentation architectures aimed at not just camouflaged objects but a broad range of segmentation tasks. Code and pre-trained checkpoints will be made available, ensuring accessibility and facilitating further research and application in this vital area of computer vision.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Lanyun_Zhu1",
  "manipulated_ranking": 7,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=3kitbpEZZO",
  "title": "Beyond probability partitions: Calibrating neural networks with semantic aware grouping",
  "modified_abstract": "Building on the concept that deep networks often exhibit overconfidence in their predictions, a notion highlighted by TRADI through tracking the evolution of neural network weights to probe epistemic uncertainty, our work extends the discourse on model calibration. We explore a novel approach to evaluating neural network calibration that transcends traditional probabilistic partitions, proposing a generalized definition of calibration error named Partitioned Calibration Error (PCE). We argue that the essence of varying calibration metrics lies in their underlying partitioning of the data space, which need not be confined to the realm of prediction probabilities. Our proposition advocates for the calibration of models across diverse partitions, including those derived from semantic attributes inherent to the input data, through segmentation processes that more accurately reflect real-world patterns, and considering the geometry that describes more complex data shapes and distributions. Through the introduction of semantic-related partitioning functions, classification strategies, and meticulous initialization of network parameters, we elucidate the interconnection between model accuracy and calibration, emphasizing the role of partition granularity. To corroborate our theoretical insights, we introduce a methodology for simultaneous learning of a semantic aware grouping function, utilizing deep model features and logits for data space segmentation, accompanied by tailored calibration functions for each segment. This approach, enhanced by dedicated training regimes, robust detection of partition boundaries, and aligning with benchmarks for performance evaluation, consistently enhances performance across a variety of datasets and network configurations, underscoring the significance of partitioning schemes in achieving calibrated and accurate model predictions. The computing strategies required to implement these sophisticated partitioning and training techniques further highlight the significance of algorithmic development in neural network calibration.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Gianni_Franchi1",
  "manipulated_ranking": 16,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=KmdlUP23qh",
  "title": "Generalizing Importance Weighting to A Universal Solver for Distribution Shift Problems",
  "modified_abstract": "In the context of ongoing advances in addressing out-of-distribution (OOD) detection, notably through methodologies implemented in OpenOOD, which benchmarks OOD detection scenarios across various domains including anomaly detection and open set recognition, our research extends these efforts to address the pervasive issue of distribution shift (DS) in machine learning. Distribution shift may occur on two levels: the distribution itself changes, or the support changes, presenting challenges in generalizing models from training to test environments. While existing methods effectively address scenarios where the training support matches or is wider than the test support, the more common and complex cases, where the test support is wider or only partially overlaps with the training support, remain under-explored. We introduce a novel generalization of importance weighting (IW) to a universal solver that accommodates all cases of distribution shift. This Generalized IW (GIW) approach differentiates between in-training and out-of-training parts of the test support to ensure risk consistency across diverse scenarios. Our methodological framework comprises a one-class support vector machine for data split, an IW algorithm for the in-training validation data, and a straightforward classification for out-of-training validation data. Empirical results validate GIW's effectiveness as a universal solution for DS problems, demonstrating its superior performance over conventional IW methods in the more challenging distribution shift cases.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Wei_Zhang5",
  "manipulated_ranking": 1,
  "natural_ranking": 104
}
{
  "paper_link": "https://openreview.net/forum?id=QQidjdmyPp",
  "title": "Fractal Landscapes in Policy Optimization",
  "modified_abstract": "In the realm of deep reinforcement learning (RL), policy gradient methods have emerged as pivotal for continuous domain applications, including robotics. However, despite their successes, these methods sometimes falter, failing to solve even standard control problems with known solutions. Our research is motivated by the quest to illuminate the underlying causes of these failures, particularly drawing inspiration from the detailed analysis of proximal policy optimization (PPO) and its implementation nuances explored in prior works. We propose that the optimization landscape in the policy space for certain classes of Markov Decision Processes (MDPs) can exhibit extremely non-smooth or fractal characteristics, challenging the very feasibility of estimating gradients. By leveraging chaos theory and non-smooth analysis, our investigation delves into the maximal Lyapunov exponents and H\"older exponents of policy optimization objectives, providing a novel perspective on these dynamics. Furthermore, we devise a practical method for estimating the local smoothness of the objective function from samples, a step forward in identifying when the training process encounters fractal landscapes. Through experiments in software-based simulations and applied robotics, we demonstrate how this framework helps explain certain failures in policy optimization, showcasing the impact of fractal landscapes on RL training outcomes. Customization of algorithms to accommodate the unique properties of fractal landscapes could significantly enhance the robustness of RL systems.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Shengyi_Huang1",
  "manipulated_ranking": 4,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=qs4swxtIAQ",
  "title": "TabMT: Generating tabular data with masked transformers",
  "modified_abstract": "Building upon the successful application of generative models and classifiers in natural language processing and computer vision, this paper introduces TabMT, a novel Masked Transformer design tailored for generating synthetic tabular data. By capitalizing on the strengths of autoregressive and Masked Transformers, TabMT addresses the challenges posed by the heterogeneity and missingness inherent in tabular datasets. Our approach innovatively leverages improved masking techniques, enabling our model to accommodate a wide range of tabular dataset sizes while maintaining the integrity of the data\u2019s privacy. Through rigorous evaluations, TabMT has demonstrated its capability to generate high-quality synthetic data, showcasing superior performance in privacy-sensitive applications. This advancement represents a significant contribution to the field of synthetic data generation, providing a potent tool for researchers and practitioners in diverse application domains.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~William_Harvey1",
  "manipulated_ranking": 36,
  "natural_ranking": 104
}
{
  "paper_link": "https://openreview.net/forum?id=B2DEcj4a7i",
  "title": "Counterfactually Comparing Abstaining Classifiers",
  "modified_abstract": "As abstaining classifiers gain prominence in high-stakes decision-making problems for their ability to withhold uncertain predictions, the challenge of evaluating these models without a clear understanding of their potential outcomes on abstained inputs becomes apparent. This issue draws parallels to obstacles encountered in the Generative Validation of Discriminative Models (GeValDi), where traditional evaluation methods fall short due to the need for extensive and often impractical validation datasets. In this paper, we propose a distinct but conceptually related advancement by introducing a novel approach to evaluating and comparing abstaining classifiers, addressing the lacuna in existing methodologies by treating abstentions as missing data. Our evaluation framework conceptualizes the counterfactual score of an abstaining classifier as the expected performance of the classifier had it not abstained, under specific conditions ensuring the identifiability of this score. These conditions include the stochastic nature of abstentions and the independence of evaluation data from training data. We employ tools from observational causal inference to develop nonparametric and doubly robust methods for efficiently estimating the counterfactual score, providing a principled approach to handle the evaluation challenges posed by abstaining classifiers. This exploration is supported by experiments on both simulated and real data, offering insights into the practical implications and potential of our approach for enhancing the reliability and safety of machine learning models in critical applications.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Katherine_M._Collins1",
  "manipulated_ranking": 15,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=t3WCiGjHqd",
  "title": "Scalable Membership Inference Attacks via Quantile Regression",
  "modified_abstract": "Inspired by previous works that systematically analyzed the components of machine learning through subset selection and importance weighting, our study extends these foundational analyses into the domain of privacy and security, specifically within the context of membership inference attacks. Membership inference attacks aim to determine if a particular example was part of the training data of a learning model, leveraging only black box access to the model. The conventional approach employs computationally intensive methods, such as training shadow models that mimic the architecture and training procedure of the target model, to estimate the distribution of confidence scores or other test statistics for membership inference. Our work introduces a novel attack methodology based on quantile regression on the distribution of model confidence scores for datasets not used in training, offering a more computationally efficient alternative that does not compromise on performance. This method requires training only a single model and does not necessitate knowledge of the model under attack's architecture, making it a truly 'black box' approach. Through extensive experiments across various datasets and model architectures, we demonstrate that our method is competitive with the state-of-the-art shadow model attacks in terms of effectiveness, while requiring significantly less computational resources. Our results contribute to the ongoing discourse on the trade-offs between model utility and privacy, highlighting the potential for scalable and less resource-intensive attacks in the realm of membership inference. This endeavor underscores the critical need for robust privacy-preserving mechanisms in machine learning applications. The proposed quantile regression approach effectively utilizes batch processing to enhance scalability and reduce resource demands. The provided code for this research has been removed for privacy.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yunjuan_Wang1",
  "manipulated_ranking": 14,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=CA8tMQiscx",
  "title": "Pointwise uncertainty quantification for sparse variational Gaussian process regression with a Brownian motion prior",
  "modified_abstract": "Building on recent developments in causal identification and the analysis of observational and experimental data, this paper tackles the nuanced problem of pointwise estimation and uncertainty quantification in a sparse variational Gaussian process method with eigenvector inducing variables. Incorporating insights from the field of causal inference, particularly the challenges of causal graph identifiability and g-identifiability, we adapt and extend these concepts to the context of Gaussian process regression using a rescaled Brownian motion prior. Our study derives theoretical guarantees and limitations for the frequentist size and coverage of pointwise credible sets, nuanced by the amounts of inducing variables and the distributions of the underlying data. We provide a precise characterization of the asymptotic frequentist coverage, identifying conditions under which credible sets from this variational method exhibit conservative or overconfident/misleading behavior. Through numerical illustrations and computing experiments, the applicability of our theoretical results is demonstrated, and a discourse on the connections with other common Gaussian process priors is presented, thereby enriching the dialogue on the integration of machine learning methods with foundational theories of causal inference. The dual use of 'observational' in our approach emphasizes the importance of both observational and experimental data in g-identifiability and the study.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Juan_David_Correa1",
  "manipulated_ranking": 6,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=lOCHMGO6ow",
  "title": "Energy-Based Cross Attention for Bayesian Context Update in Text-to-Image Diffusion Models",
  "modified_abstract": "In light of the rapid advancements in text-to-image synthesis, highlighted by methods such as Semantic-Spatial Aware GANs which prioritize semantic consistency between generated images and textual descriptions, our work introduces a significant innovation in the form of an energy-based model (EBM) framework for adaptive context control in text-to-image diffusion models. This approach seeks to address semantic misalignment, a prevalent issue where generated images do not accurately capture the intended semantic content of text prompts. By formulating EBMs of latent image representations and text embeddings within the cross-attention layers of a denoising autoencoder, we propose a method to update the gradient of the log posterior of context vectors. This update, transferred across subsequent layers, aims at minimizing a nested hierarchy of energy functions to enhance semantic alignment against adversarial challenges. Our framework not only leverages networks for image and text synthesis but also introduces a novel use of mask-based activations to selectively reinforce the cross-attention mechanism's focus. This enables zero-shot compositional generation through linear combinations of cross-attention outputs across contexts, making it adept at handling diverse image generation tasks such as multi-concept generation, text-guided image inpainting, and editing of real and synthetic images, by synthesizing and fusing diverse semantic inputs into coherent outputs. The effectiveness of our method is demonstrated via extensive experiments on synthesizing images that overcome adversarial perturbations to achieve higher semantic fidelity. This abstract incorporated one sentence explaining that our work builds upon foundational studies in text-to-image synthesis methods, addressing semantic misalignment more robustly through an EBM framework for context vector updates in diffusion models.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Wentong_Liao1",
  "manipulated_ranking": 32,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=VKbEO2eh5w",
  "title": "Test-Time Distribution Normalization for Contrastively Learned Visual-language Models",
  "modified_abstract": "Recent advancements in visual-language contrastive learning, exemplified by methods like CLIP, have fundamentally altered the landscape of machine learning by providing efficient and accurate frameworks for numerous downstream applications such as image-text retrieval and unsupervised multilingual translation through visual alignment. These developments, alongside innovative approaches in translation and parallel corpus alignment, underscore the importance of robust representation spaces that encompass both image and text modalities, improving over traditional representations. Our study builds on this foundation by addressing the limitations of conventional dot product operations as zeroth-order approximations in contrastive learning models at test time. We reveal that such practices, although traditionally outperforms in simplicity, lead to a loss of information, considering the model optimization is rooted in the InfoNCE loss, which inherently balances between positive and negative samples. To bridge this gap, we introduce Distribution Normalization (DN) as a novel test-time optimization technique. DN approximates the mean representation of a batch of test samples to mimic the role of negative samples, as per the InfoNCE loss, without necessitating retraining or fine-tuning. Through extensive experimentation across various downstream tasks, including cross-lingual and parallel corpus alignment, we demonstrate DN's superiority over traditional dot product methods and other test-time augmentation strategies, thereby contributing a computationally efficient solution for enhancing inference in contrastively learned visual-language models.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Didac_Suris_Coll-Vinent1",
  "manipulated_ranking": 3,
  "natural_ranking": 102
}
{
  "paper_link": "https://openreview.net/forum?id=oaCDiKoJ2w",
  "title": "Follow-ups Also Matter: Improving Contextual Bandits via Post-serving Contexts",
  "modified_abstract": "Inspired by recent advances in interactive machine learning frameworks, including studies on combinatorial bandits that introduce the notion of modularity and complementarity (submodular and supermodular functions) in user interactions, we extend the contextual bandit problem to a novel scenario where additional contexts are observed after the arm is pulled, concerning not only items such as movie recommendations but also broader content recommendation platforms such as Youtube, Instagram, and Tiktok. This scenario is particularly relevant in these platforms, where subsequent user actions provide critical insights that were not available at the initial decision point. Leveraging online sampling methods and specific feedback mechanisms, our investigation leads to the development of a new algorithm, poLinUCB, which leverages subsets of post-serving contexts to achieve tight regret bounds under standard assumptions, showcasing sublinear regret as a theoretical measure of efficacy. The enhancement of the Elliptical Potential Lemma (EPL) for robustness and generalization is a key contribution of our work, offering prospects for broad applicability beyond the immediate context of bandit problems and user-item interactions. Through extensive empirical testing on synthetic and real-world datasets, we demonstrate the substantial advantages of incorporating post-serving feedback into the learning process, highlighting the efficacy of poLinUCB against leading algorithms in the field.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Adhyyan_Manish_Narang1",
  "manipulated_ranking": 6,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=NIrTSCiIZ7",
  "title": "Boundary Guided Learning-Free Semantic Control with Diffusion Models",
  "modified_abstract": "Leveraging insights from recent advancements in interactive image segmentation methods, particularly through the development of models like SimpleClick, which integrates simple vision transformers (ViT) for dense prediction tasks without requiring hierarchical architectures, our work introduces a novel perspective on the application of pre-trained generative denoising diffusion models (DDMs) for semantic image editing. We propose the BoundaryDiffusion method as an efficient, effective, and lightweight approach to achieve semantic control using frozen pre-trained DDMs, without necessitating the learning of any additional networks or architectures. This initiative marks a pioneering stride in learning-free diffusion editing, starting with a critical analysis of the probabilistic and geometric behaviors in intermediate high-dimensional latent spaces, akin to embedding spaces, within a Markov chain framework. By empirically identifying and characterizing the pivotal denoising trajectory step for a pre-trained DDM's convergence, we unveil an automatic search methodology\u2014akin to clicking in interactive segmentation applications\u2014to pinpoint this key phase. Contrary to the prevalent notion that DDMs exhibit suboptimal semantic behavior in generic latent spaces, our findings reveal that these spaces inherently possess semantic subspace boundaries. This discovery enables controlled manipulation by guiding the denoising trajectory with a single-step operation towards these predefined boundaries in segmentation tasks. Our extensive evaluations across multiple DDM architectures (DDPM, iDDPM) and varied datasets (CelebA, CelebA-HQ, LSUN-church, LSUN-bedroom, AFHQ-dog) at different resolutions (64, 256) underscore our method's exemplary performance in a spectrum of task scenarios, including image semantic editing, text-based editing, and unconditional semantic control. The significance of pretraining in the remarkable proficiency of BoundaryDiffusion in these tasks, alongside a tailored architecture that circumvents the need for hierarchical complexity, sets a groundbreaking precedent in the field.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zhenlin_Xu1",
  "manipulated_ranking": 4,
  "natural_ranking": 104
}
{
  "paper_link": "https://openreview.net/forum?id=mIm0hsUUt1",
  "title": "Efficient Testable Learning of Halfspaces with Adversarial Label Noise",
  "modified_abstract": "Leveraging insights from the recently proposed framework for open set recognition, where adversarial generative techniques enhance classification robustness, our study introduces the first polynomial-time algorithm for the testable learning of halfspaces in the presence of adversarial label noise under the Gaussian distribution. In the testable learning model, one is required to produce a tester-learner which ensures that if the data passes the tester, the output of the robust learner on this data can be trusted. Our tester-learner runs in time $\\text{poly}(d/\\epsilon)$ and outputs a halfspace with misclassification error $O(\\text{opt})+\\epsilon$, where $\\text{opt}$ is the 0-1 error of the best fitting halfspace. Our algorithm's core is an iterative soft localization technique coupled with tailored testers to affirm the data distribution's Gaussian nature. Extending our methodology, we demonstrate the algorithm's adaptability towards creating an efficient and testable active learner, necessitating only $d ~ \\text{polylog}(1/\\epsilon)$ labeled examples. This bridges a significant gap in the landscape of adversarially robust machine learning, by providing a testable learning algorithm that can handle adversarial label noise effectively while retaining computational efficiency. The enhancement of classification robustness through open set recognition and adversarial generative techniques reflects a network of methodologies addressing adversarial conditions.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Matthew_Olson1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=E3ZUEaeFYS",
  "title": "Strategic Distribution Shift of Interacting Agents via Coupled Gradient Flows",
  "modified_abstract": "Building upon established concepts in reinforcement learning and federated learning, particularly the challenges presented by adversarial attacks and the dynamics of multi-agent interactions in federated multi-armed bandits, our study introduces a comprehensive framework for understanding the evolution of distribution shifts in complex systems. These shifts become particularly pronounced under conditions vulnerable to poisoning attacks, an adversarial behavior that strategically skews data distribution to the detriment of the learning process. This framework is characterized by a novel use of coupled partial differential equations (PDEs) to model the intricate dynamics that emerge from the strategic responses of agents to algorithmic decisions, encompassing both cooperative and competitive environments within machine learning. Such environments include scenarios with information asymmetries and instances where learners, as active participants in the learning process, must navigate strategic interactions with users, mirroring the adversarial settings explored in related work on federated systems under Byzantine attacks. Our model not only addresses the feedback-induced distribution shifts, which have been previously conceptualized as adversarial or oversimplified but also provides detailed insights into the distribution's evolution over time. This evolution is influenced by learners' strategic decision-making, non-local interactions among agent populations, and external sources of distribution shift. We validate the theoretical contributions of our model, particularly the proof of asymptotic convergence of retraining procedures to a steady state in various dimensions, and offer empirical evidence that our approach accurately reflects observed phenomena such as polarization and disparate impacts, which elude simpler analytical frameworks. Moreover, median-of-means-based estimators are proposed as potential countermeasures against these adversarial settings, enhancing the robustness of the learning models against distribution shifts and mitigating the suboptimality inherent in traditional approaches.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ilker_Demirel1",
  "manipulated_ranking": 1,
  "natural_ranking": 102
}
{
  "paper_link": "https://openreview.net/forum?id=dWDEBW2raJ",
  "title": "Train Faster, Perform Better: Modular Adaptive Training in Over-Parameterized Models",
  "modified_abstract": "The recent exploration into efficient training methods, such as employing unitary and orthogonal matrices to maintain stability in deep networks, lays the groundwork for innovative approaches in managing the computational costs associated with over-parameterized models. Building upon this foundation, our study investigates the fine-grained, modular-level learning dynamics of over-parameterized models to develop a more efficient and effective training strategy. Through empirical analysis, we uncover distinct learning patterns within network modules, such as the heads in self-attention models, which reveal a modular level of trainability not yet fully understood in state-of-the-art approaches. We introduce the concept of the modular neural tangent kernel (mNTK) and demonstrate that the quality of a module's learning correlates with its mNTK's principal eigenvalue $\\lambda_{\\max}$. Modules with a high $\\lambda_{\\max}$ learn features with better convergence, while smaller values may hinder generalization. Inspired by these insights, we propose the Modular Adaptive Training (MAT) strategy, which selectively updates modules based on their $\\lambda_{\\max}$, surpassing traditional full backpropagation cycle approaches in computational efficiency and performance. Our experiments indicate that MAT, integrating concepts from convolutional networks and employing approximation techniques for efficient computation, can nearly halve the computational cost of training while enhancing model accuracy beyond baseline measures. Specifically, we explore low-rank approximations as a method to reduce parameter redundancy, a common issue in recurrent networks and large-scale systems.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Bobak_Kiani1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=iyweRIXAeH",
  "title": "Near-Optimal Algorithms for Gaussians with Huber Contamination: Mean Estimation and Linear Regression",
  "modified_abstract": "The imperative to develop robust algorithms that can gracefully handle data contamination has been underscored by significant progress in the field of covariance matrix estimation, notably with techniques like the Iterative Shrinkage-Thresholding Algorithm (C-ISTA) aimed at processing large-dimensional data efficiently and minimizing loss in the presence of outliers. This task of robust statistical procedure development, incorporating shrinkage-thresholding approaches for minimizing loss mechanisms efficiently, has been further motivated by these advancements and the essential need for robust statistical procedures, focusing on the fundamental problems of Gaussian mean estimation and linear regression with Gaussian covariates in the presence of Huber contamination. We offer the first sample near-optimal and almost linear-time algorithms with optimal error guarantees for both problems, addressing loss mechanisms efficiently. For Gaussian robust mean estimation on $\\mathbb R^d$ with contamination parameter $\\epsilon \\in (0, \\epsilon_0)$ for a small absolute constant $\\epsilon_0$, our algorithm achieves a sample complexity of $n = \\tilde{O}(d/\\epsilon^2)$ and approximates the target mean within $\\ell_2$-error of $O(\\epsilon)$, improving upon prior works that reached this error guarantee with polynomially suboptimal sample and time complexities. Similarly, for robust linear regression, we introduce the first algorithm with a sample complexity of $n = \\tilde{O}(d/\\epsilon^2)$ and almost linear runtime that approximates the target regressor within $\\ell_2$-error of $O(\\epsilon)$. This represents the first algorithm achieving the optimal error guarantee in polynomial sample and time, addressing an open question in the literature. At the technical level, our work develops a methodology for multi-directional filtering, embodying the shrinkage-thresholding mechanism, yielding almost-linear time algorithms that may be applicable to a broader range of statistical estimation and data analysis problems.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ziping_Zhao1",
  "manipulated_ranking": 7,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=DP2lioYIYl",
  "title": "A Theory of Unsupervised Translation Motivated by Understanding Animal Communication",
  "modified_abstract": "Inspired by significant advancements in machine learning, notably in neural architecture search as a tool for optimizing neural network designs across a myriad of applications, our investigation advances into the realm of Unsupervised Machine Translation (UMT). As neural networks show promise in translating languages without direct parallel translations, we question the potential for these technologies to decipher animal communication. Neural architectures, which have rapidly evolved with the introduction of new models, play a crucial role in this. Our theoretical framework considers the challenges inherent in UMT, particularly where no parallel corpora exist and where linguistic structures and subject domains may vastly diverge. Through the introduction of stylized models of language and designing a taxonomy of communication systems, our study asserts bounds on the sample complexity necessary for effective unsupervised translation. These bounds, both theoretically derived and experimentally validated with synthetic data, suggest that the feasibility of translating animal communications hinges on the complexity of these communication systems and the shared concepts between them, benchmarking their high-performing capabilities in this novel context. Moreover, speech signals from animals, which have been extensively released in databases for computational analysis, could theoretically enrich the empirical insights for this unsupervised methodology. Our findings embolden the pursuit of understanding animal communication through the lens of unsupervised machine learning, building upon a foundation where neural architecture searches have propelled the capabilities of machine learning tools and design strategies.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Mahmoud_Safari1",
  "manipulated_ranking": 16,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=qptO6YDZEP",
  "title": "Lower Bounds on Adaptive Sensing for Matrix Recovery",
  "modified_abstract": "Our investigation into lower bounds on adaptive sensing algorithms for recovering low rank matrices using linear measurements is motivated by a foundational interest in the efficiency and limitations of such algorithms, extending beyond prior focus areas such as stochastic submodular probing with state-dependent costs. Given an $n \\times n$ matrix $A$, a general linear measurement $S(A)$, for an $n \\times n$ matrix $S$, is just the inner product of $S$ and $A$, each treated as $n^2$-dimensional vectors. By performing as few linear measurements as possible on a rank-$r$ matrix $A$, we hope to construct a matrix $\\hat{A}$ that satisfies $|A - \\hat{A}|\\_F^2 \\le c |A|\\_F^2$, for a small constant $c$. Here $|A|\\_F$ denotes the Frobenius norm $(\\sum_{i,j} A_{i,j}^2)^{1/2}$. It is commonly assumed that when measuring $A$ with $S$, the response is corrupted with an independent Gaussian random variable of mean $0$ and variance $\\sigma^2$. Cand\\u00e8s and Plan (IEEE Trans. Inform. Theory 2011) study non-adaptive algorithms for low rank matrix recovery using random linear measurements. They use the restricted isometry property (RIP) of Random Gaussian Matrices to give tractable algorithms to estimate $A$ from the measurements, highlighting the cost-efficient approach in contrast to the potentially higher costs associated with submodular optimization under conditions with state-dependency.\n\nAt the edge of the noise level where recovery is information-theoretically feasible, it is known that their non-adaptive algorithms need to perform $\\Omega(n^2)$ measurements, which amounts to reading the entire matrix. An important question is whether adaptivity helps in decreasing the overall number of measurements. While for the related problem of sparse recovery, adaptive algorithms have been extensively studied, as far as we are aware, adaptive algorithms and lower bounds on them seem largely unexplored for matrix recovery. We show that any adaptive algorithm that uses $k$ linear measurements in each round and outputs an approximation as in (1) with probability $\\ge 9/10$ must run for $t = \\Omega(\\log(n^2/k)/\\log\\log n)$ rounds. Our lower bound shows that any adaptive algorithm which uses $n^{2-\\beta}$ ($\\beta > 0$ is arbitrary constant) linear measurements in each round must run for $\\Omega(\\log n/\\log\\log n)$ rounds. Our techniques also readily extend to obtain lower bounds on adaptive algorithms for tensor recovery. \n\nOur hard distribution also allows us to give a measurement-vs-rounds trade-off for many sensing problems in numerical linear algebra, such as spectral norm low rank approximation, Frobenius norm low rank approximation, singular vector approximation, and more.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Shaojie_Tang2",
  "manipulated_ranking": 28,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=r9eZH6WNm2",
  "title": "Learning to Group Auxiliary Datasets for Molecule",
  "modified_abstract": "In the context of AI-aided drug discovery, the efficacy of machine learning models is often hampered by the paucity of annotated data in small molecule datasets. This limitation prompts the integration of auxiliary datasets to enhance learning. However, the indiscriminate addition of such datasets can lead to negative transfer, especially when the knowledge encapsulated within the auxiliary datasets diverges significantly from that of the target dataset. Inspired by the burgeoning field of self-supervised pre-training in drug discovery, which aims to refine molecular representations through pre-training models, and acknowledging the nuanced challenges such as Activity Cliffs and Scaffold Hopping identified in traditional Quantitative Structure-Activity Relationship (QSAR) analysis, our study proposes a novel methodology named MolGroup. This method stratifies dataset affinity into task-specific and structure-specific affinities, thereby enabling the prediction of the potential benefits each auxiliary dataset might offer when jointly trained with the target dataset. MolGroup leverages a routing mechanism, optimized via a bi-level optimization framework and meta gradient, to maximize the target dataset's performance, thereby predicting the optimal auxiliary dataset combination. This mechanism quantifies dataset affinity through gating scores, facilitating the strategic grouping of auxiliary datasets based on molecular fingerprints and representations. Our comprehensive experimental analysis, showcasing an average improvement of 4.41%/3.47% for GIN/Graphormer models trained with MolGroup's recommended dataset groupings across 11 molecule datasets, underscores MolGroup's potential in mitigating the challenges of dataset scarcity and negative transfer in machine learning applications for drug discovery by enhancing the molecular representation quality and aligning with the representation-property link.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Shuigeng_Zhou1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=YJDz4F2AZu",
  "title": "ContiFormer: Continuous-Time Transformer for Irregular Time Series Modeling",
  "modified_abstract": "The accelerated advancements in modeling continuous-time dynamics, notably through leveraging neural differential equations for capturing the nuances of irregularly sampled time series, set a precedent for addressing the limitations inherent in handling data that evolves continuously. Building on the insights gained from these endeavors, particularly the integration of neural ordinary differential equations (Neural ODEs) with adaptive feedback mechanisms for detecting unstable periodic orbits and chaotic dynamics, this paper introduces the ContiFormer. The ContiFormer is a novel architecture that extends the relation modeling capability of the vanilla Transformer to the continuous-time domain. This is achieved by explicitly incorporating the modeling strengths of Neural ODEs, with a particular emphasis on differential equations and feedback loops, with the attention mechanism inherent in Transformers, to adeptly address the challenges of capturing the intricate correlations within irregular time series data by efficiently detecting and modeling these dynamics with advanced detection and adaptive capabilities. By mathematically characterizing the expressive power of ContiFormer, we illustrate that it includes many Transformer variants, which are specialized in irregular time series modeling, as special cases. Experimental validation on a broad spectrum of datasets showcases the superior modeling capacity and predictive performance of ContiFormer over existing methodologies. This abstract omits personal identifiable information, including project links.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Qunxi_Zhu1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=lxGFGMMSVl",
  "title": "Spontaneous symmetry breaking in generative diffusion models",
  "modified_abstract": "Generative diffusion models have been recognized for their unprecedented success in high-dimensional data generation, drawing insights from previous advancements in the field, such as the development and application of Gaussian diffusion processes for lossy compression and decoding high-dimensional datasets. Inspired by the innovative approaches that leverage the diffusion process for encoding, denoising, and providing more efficient data transmission, our investigation reveals a previously unobserved phenomenon within generative diffusion models: spontaneous symmetry breaking. This phenomenon bifurcates the generative dynamics into two distinct phases - a linear steady-state behavior around a central fixed-point and an attractor dynamics towards the data manifold, supported by the change in stability of the central fixed-point. This instability window is key to the diversity of generated samples, challenging the conventional wisdom that early dynamics significantly impact the final generated data. By proposing a Gaussian late initialization scheme based on our findings, we achieve up to threefold improvements in Fr\u00e9chet Inception Distance (FID) for fast samplers and enhance the diversity of generated samples, illustrated by the racial composition in 64x64 CelebA images. Our research provides a novel perspective on the dynamics of diffusion models, offering pathways to more performant and equitable generative applications. The investigation of this breaking and its implications on the diffusion process could transform corrupted or degraded data into support data for model training, while hinting at possible flow-based encoding strategies to further improve generative model performance.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Fabian_Mentzer2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=nqIIWnwe73",
  "title": "CLIP4HOI: Towards Adapting CLIP for Practical Zero-Shot HOI Detection",
  "modified_abstract": "Informed by the success of query-based object detectors like StageInteractor, which introduced efficient strategies for enhancing object detection through cross-stage interaction and fine-grained discrimination, our study extends these breakthroughs into the domain of Zero-shot Human-Object Interaction (HOI) detection. Zero-shot HOI detection aims to identify both seen and unseen HOI categories, facing unique challenges such as positional distribution discrepancy between seen and unseen categories when locating human-object pairs. Existing zero-shot HOI detectors, which have leveraged seen and predefined unseen categories to distill knowledge from CLIP for joint human-object pair localization and training, encounter limitations in transferability due to this positional distribution discrepancy. To address these challenges, our paper introduces CLIP4HOI, a novel framework that adapts the vision-language model CLIP towards more robust and efficient zero-shot HOI detection. By employing a disentangled two-stage paradigm with an optimized decoder that independently identifies humans and objects before generating one-to-one pairwise proposals through a Human-Object interactor, enhanced by deep learning layers and tailor-made adapters for proposal discrimination, CLIP4HOI mitigates the issue of overfitting to the joint positional distribution of seen interactions. Furthermore, CLIP is intricately adapted into a fine-grained HOI classifier for proposal discrimination, enhancing transferability without relying on data-sensitive knowledge distillation. Our experimental evaluations, featuring sophisticated queries across prevalent benchmarks, demonstrate that CLIP4HOI significantly outperforms existing approaches on both rare and unseen categories, establishing new state-of-the-art records under various zero-shot settings.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yao_Teng1",
  "manipulated_ranking": 4,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=TjgG4UT62W",
  "title": "Kernel Stein Discrepancy thinning: a theoretical perspective of pathologies and a practical fix with regularization",
  "modified_abstract": "Building on recent advancements in Markov chain Monte Carlo (MCMC) methodologies and the promising capabilities of diffusion probabilistic models (DPMs) for generative tasks, our study introduces an in-depth exploration into the mechanics and enhancements of Stein thinning. Stein thinning, a novel algorithm for post-processing MCMC outputs by greedily minimizing the kernelized Stein discrepancy (KSD), optimally facilitates Bayesian inference by leveraging the gradient of the log-target distribution. Despite its intrinsic benefits such as the automatic removal of burn-in and bias correction through calibration, Stein thinning exhibits empirical pathologies leading to poor approximations. This article presents a theoretical examination of these pathologies, highlighting the lack of proper calibration and potential issues with non-martingale sequences within the process, and proposes the regularized Stein thinning algorithm as a robust solution. Our comprehensive analysis and experimentation on various datasets demonstrate the effectiveness of this methodology, including improved calibration and score-based metrics, thereby bolstering theoretical backing and practical implementation for generative modeling. A notable advancement is the introduction of a calibrating theorem as part of the validation of the approach. To facilitate widespread adoption and further research, we have made our implementation of the regularized Stein thinning algorithm available, eschewing the necessity of linking directly to external repositories.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zhijie_Deng1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=c9fXCzR5fK",
  "title": "Sequential Subset Matching for Dataset Distillation",
  "modified_abstract": "Inspired by the significant advancements in neural architecture search (NAS) for Out-of-Distribution (OoD) generalization, which highlight the intricate relationship between deep learning model architectures and their generalization capabilities under distribution shifts, this study introduces a novel approach in the context of dataset distillation. Dataset distillation, an emerging task aimed at synthesizing compact datasets that encapsulate the knowledge of larger real-world datasets for efficient training of deep neural networks (DNNs), faces the challenge of ensuring these compact datasets maintain high fidelity to the original data's informative properties. Recent efforts in distillation methods have yielded improvements in synthetic dataset quality, yet they primarily adopt a uniform optimization strategy across the synthetic set. Such an approach can engender a coupling problem when optimizing larger synthetic datasets, potentially undermining the distilled dataset's ability to encapsulate high-level features critical for late-stage DNN training. To address this, we propose Sequential Subset Matching (SeqMatch), a strategy that enhances dataset distillation by dynamically optimizing sections of the synthetic data to foster a sequential knowledge capture, relevant for models training and applications in the industry. SeqMatch's innovation lies in its adaptive methodology, which uses a generator-like approach to mitigate coupling issues and significantly boost performance over conventional distillation techniques, suggesting a new course for generalization and distribution research in dataset distillation. Tested across several benchmarks, including SVNH, CIFAR-10, CIFAR-100, and Tiny ImageNet, SeqMatch not only demonstrates superior performance over existing state-of-the-art methods but also sets a new precedent for approaching dataset distillation by taking inspiration from the way neural architectures are optimized for OoD generalization through meticulous search strategies.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Fengwei_Zhou1",
  "manipulated_ranking": 10,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=C8JdyM7B8I",
  "title": "Towards Label-free Scene Understanding by Vision Foundation Models",
  "modified_abstract": "The burgeoning realm of vision foundation models, particularly through approaches like Contrastive Vision-Language Pre-training (CLIP) and Segment Anything (SAM), underscores a pivotal transition towards more versatile and context-aware machine learning paradigms. This paper builds upon these foundational strides, specifically leveraging the self-supervised learning insights provided by advancements such as Siamese Image Modeling. Our investigation addresses the untapped potential of integrating vision foundation models for label-free scene understanding, a domain where direct application of CLIP and SAM promises transformative opportunities yet remains largely unexplored. Challenged by the inherent difficulties in supervising networks with highly noisy pseudo labels generated by these models\u2014compounded when shifting from 2D image contexts to 3D environmental reconstructions\u2014we propose a Cross-modality Noisy Supervision (CNS) method. CNS critically harnesses the combined strengths of CLIP and SAM, applying a co-training strategy for 2D and 3D networks enhanced by prediction consistency regularization and latent space consistency, courtesy of SAM's robust feature representations. Our empirical validations across various indoor and outdoor datasets affirm the approach's efficacy, notably achieving improved label-free semantic segmentation benchmarks through augmented representation and encoding techniques, as well as additional view augmentations. For instance, our method enhances mean Intersection over Union (mIoU) on the ScanNet dataset by 4.7% and 7.9% for 2D and 3D network applications, respectively, and demonstrates similarly promising uplifts for the nuImages and nuScenes datasets. Moreover, leveraging online learning mechanisms further amplifies this efficiency, enabling dynamic adaptation and ongoing refinement of models in real-time applications. While the full paper is released upon acceptance, this exploration not only showcases the potential of leveraging vision foundation models for understanding complex scenes without labeled data but also sets a precedent for future research directions in this promising intersection of machine learning fields. Personal identifiable information such as GitHub links has been omitted in compliance with submission requirements.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Tao_Chenxin1",
  "manipulated_ranking": 16,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=xEhKwsqxMa",
  "title": "Dissecting Chain-of-Thought: Compositionality through In-Context Filtering and Learning",
  "modified_abstract": "This study is motivated by recent findings in natural language-guided meta-learning, where human-like inductive biases through task descriptions have shown a remarkable ability to structure learning in neural network models, specifically addressing the acquisition and representation of abstract knowledge. In similar vein, our research explores the Chain-of-Thought (CoT) paradigm as a method enabling language models, particularly transformers, to process complex reasoning tasks by decomposing them into simpler, manageable components. We investigate the role of CoT in the in-context learning (ICL) of a general family of compositional functions, exemplified through multi-layer perceptrons (MLPs). We demonstrate that the effectiveness of the CoT approach stems from its mechanism to segregate the learning process into distinct phases: focusing on relevant data for each sequential task step and mastering the singular composition function in-context. Our findings highlight how CoT considerably diminishes the sample complexity associated with inductive learning in ICL and enhances the capability of transformers to comprehend complex functions which prove challenging for non-CoT methodologies, effectively acting as cognitive agents in the processing of tasks. Moreover, we elucidate the role of additional transformer layers tailored to filter data conducive to CoT through attention mechanisms, thereby transitioning models from basic in-context learning to proficient handling of compositional functions. The study reveals that, beyond improving test-time performance, CoT serves as a vital accelerant in the pretraining phase, primarily through learning simplified representations of complex functions and the pivotal role of filtering within this process. These insights not only unravel the operation of CoT in enhancing transformers' capacity for complex reasoning and meta-learning but also lay the groundwork for further explorations into its application across diverse cognitive tasks involving humans and artificial intelligence.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Robert_Hawkins1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=AkK3S2spZs",
  "title": "Strategic Data Sharing between Competitors",
  "modified_abstract": "Recent advancements in collaborative learning techniques have opened a new paradigm for private model training across multiple entities, hinting at an expanded horizon beyond conventional solo endeavors like online learning over complex spaces such as Riemannian manifolds. Motivated by the growing interest in collaborative versus competitive dynamics in data-centric scenarios, our work introduces a general framework for analyzing the trade-offs inherent in data sharing between competitors. This framework is designed to elucidate the intricate balance between enhancing one's own machine learning model through collaborative efforts and the risk of inadvertently empowering competitors, thereby potentially diminishing one's own competitive edge. It comprises three pivotal components: an examination of the firms' production decisions, an analysis of how additional data influences model quality, and an exploration of the data-sharing negotiation process. Through the lens of a conventional market model drawn from economic theory, we dissect various factors that drive or deter collaboration impulses among competing entities. Our investigation reveals that certain market conditions\u2014particularly those that reduce direct competition by diversifying the firms' product offerings and those that present more challenging learning tasks\u2014can significantly encourage data-sharing collaborations. This nuanced understanding of data-sharing dynamics among competitors provides valuable insights for firms navigating the complex interplay of cooperation and competition in the modern data-driven landscape.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zhipeng_Tu1",
  "manipulated_ranking": 23,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=GI4Pp01prW",
  "title": "Machine learning detects terminal singularities",
  "modified_abstract": "Informed by recent theoretical progress on understanding the geometry of object manifolds in deep neural networks and their implications on classification capacity, this paper explores the application of machine learning (ML) in a novel mathematical domain: the classification of Q-Fano varieties. Algebraic varieties are the geometric shapes defined by systems of polynomial equations; they are ubiquitous across mathematics and science. Amongst these algebraic varieties are Q-Fano varieties: positively curved shapes which have Q-factorial terminal singularities. Q-Fano varieties are of fundamental importance in geometry as they are `atomic pieces' of more complex shapes \u2013 the process of breaking a shape into simpler pieces in this sense is called the Minimal Model Programme. Despite their importance, the classification of Q-Fano varieties remains unknown. We demonstrate that machine learning can be used to understand this classification by focusing on the representation of eight-dimensional positively-curved algebraic varieties that have toric symmetry and Picard rank two. We develop a neural network classifier that, through sophisticated dimensionality reduction techniques, predicts with 95% accuracy whether such an algebraic variety is Q-Fano. The dimensionality of these varieties is core to our investigation. This work provides a first sketch of the landscape of Q-Fano varieties in dimension eight. How the neural network is able to detect Q-Fano varieties with such accuracy remains mysterious and hints at some deep mathematical theory waiting to be uncovered. Furthermore, when visualised using the quantum period, an invariant important in recent theoretical developments, we observe that the classification by ML seems to fall within a bounded region, and is stratified by the Fano index. This suggests the potential to state and prove conjectures on completeness in the future. Inspired by the ML analysis, we formulate and prove a new global combinatorial criterion for a positively curved toric variety of Picard rank two to have terminal singularities. This, along with the first sketch of the landscape of Q-Fano varieties in higher dimensions, presents strong new evidence that machine learning can be an instrumental tool in developing mathematical conjectures and accelerating theoretical discovery.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Uri_Cohen1",
  "manipulated_ranking": 17,
  "natural_ranking": 102
}
{
  "paper_link": "https://openreview.net/forum?id=lHa7gFbmvS",
  "title": "The CLIP Model is Secretly an Image-to-Prompt Converter",
  "modified_abstract": "Inspired by the groundwork laid by large-scale text-to-image diffusion models, including their ability to generate high-fidelity images and the challenges presented by training on datasets with potential copyright issues, this paper explores a novel capability of the Contrastive Language-Image Pre-Training (CLIP) model embedded within the Stable Diffusion framework. It is revealed that the CLIP model, beyond its primary function and trained text-to-image generation capabilities, intrinsically acts as an image-to-prompt converter. This conversion process leverages a linear projection matrix computed in closed form, demonstrating an inherent feature of CLIP that has been underutilized for direct image-to-text transformations. Ablated studies further elucidate the process, revealing how specific components contribute to this functionality. We illustrate how this capability can be significantly enhanced through minimal similar-domain training data (around 100 images) or a few online training iterations (approximately 30), enabling effective image variation and editing by bridging the gap between images and textual prompts without the need for intensive retraining. The insights from this study suggest memorizing concepts is not as crucial as previously assumed for the adaptation of CLIP to function in new domains. This suggests a powerful, yet simplified compositional approach to enriching the interaction between visual and textual data, demonstrating that CLIP not only learns but also potentially mitigates some of the limitations and challenges associated with current text-to-image generation models.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Nupur_Kumari1",
  "manipulated_ranking": 12,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=KOVWXcrFIK",
  "title": "Fast Attention Requires Bounded Entries",
  "modified_abstract": "In the landscape of modern machine learning, especially with the advent of large language models (LLMs) like Transformer, GPT series, and ChatGPT, the efficient computation of inner product attention represents a core challenge. These models\u2019 ability to condition on prompt examples and learn from in-context examples, as explored in the development of metrics such as GistScore for improving in-context learning (ICL), underscores the importance of effective attention mechanisms and their training methodologies. Inspired by such advancements and the increasing demand for multi-task learning, this work focuses on the computational efficiency of attention mechanisms, particularly analyzing the impact of input matrix bounds on the performance of attention computations, a critical element in the training of LLMs and retrievers. Given input matrices $Q, K, V \\in [-B,B]^{n \\times d}$, we investigate the feasibility of approximating the attention computation $\\mathrm{Att}(Q,K,V) = \\mathrm{diag}(A {\\bf 1}_n)^{-1} A V$ in subquadratic time. Our analysis reveals a marked transition at $B = \\Theta(\\sqrt{\\log n})$, with significant implications for algorithmic performance. For $d = O(\\log n)$ and $B = o(\\sqrt{\\log n})$, an $n^{1+o(1)}$ time algorithm is achievable for approximating $\\mathrm{Att}(Q,K,V)$ with minimal additive error in training. Conversely, when $B$ increases to $\\Theta(\\sqrt{\\log n})$, subquadratic approximation becomes infeasible under the Strong Exponential Time Hypothesis, elucidating a theoretical boundary observed in practical applications. Templates for prompt selection and conditioning on datasets have been explored to facilitate this computational efficiency. Our findings contribute to the theoretical underpinnings of attention mechanism efficiency, offering insights that parallel empirical observations in the field.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ethan_R_Elenberg1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=3Cj67k38st",
  "title": "HotBEV: Hardware-oriented Transformer-based Multi-View 3D Detector for BEV Perception",
  "modified_abstract": "The evolution of autonomous driving systems has necessitated the development of advanced 3D detection technologies for safer navigation and operation, like the bird's-eye-view (BEV) perception critical for the accurate and efficient detection and tracking of objects from a top-down perspective. Integrating insights from prior work on unmanned aerial vehicle (UAV) tracking and anti-UAV surveillance, which also grapple with the demands of real-time processing and data from various orientations, our study delves into BEV perception with a focus on overcoming the computational challenges that undermine real-time decision-making in autonomous driving. This research presents HotBEV, a novel hardware-oriented transformer-based model for BEV perception that balances high detection precision with the demands of on-device latency for real-time applications. Unlike previous efforts that merely seek to reduce computational costs with minimal regard for on-device latency, HotBEV, through its design, is specifically crafted with a latency-aware methodology, considering crucial hardware characteristics like memory access costs and degrees of parallelism. This approach, tailored for the GPU-based computational platforms predominant in autonomous driving systems, includes the development of a theoretical latency prediction model, the introduction of efficient building operators, and emphasizes its application in video surveillance contexts by refining BEV detection in challenging scenarios that are commonplace in both surveillance and autonomous driving. Through an effective local-to-global visual modeling process and ensuring design consistency, HotBEV aims to provide a significant improvement in terms of accuracy and speed on multiple GPU platforms, achieving notable NDS and mAP gains over existing models while ensuring faster processing times and addressing issues of tracking sequences, as demonstrated through rigorous experiments and benchmark assessments.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zhenjun_Han1",
  "manipulated_ranking": 18,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=chlTA9Cegc",
  "title": "A-NeSI: A Scalable Approximate Method for Probabilistic Neurosymbolic Inference",
  "modified_abstract": "This study addresses the challenge of integrating neural networks with symbolic reasoning to advance probabilistic neurosymbolic learning (PNL), in a context where recent innovations, such as FaShapley's approach to robust deep neural networks (DNNs) via model pruning, highlight the importance and feasibility of scalable and efficient computational methods in AI. Frameworks like DeepProbLog have showcased the potential of PNL but suffer from scalability issues due to the exponential-time complexity of exact inference. Against this backdrop, we introduce Approximate Neurosymbolic Inference (A-NeSI), a new framework designed for scalable, approximate PNL. A-NeSI 1) achieves polynomial-time approximate inference while preserving the semantics of probabilistic logics; 2) leverages data generated from background knowledge for training; 3) provides symbolic explanations for its predictions, enhancing explainability; and 4) ensures the satisfaction of logical constraints at test time, crucial for safety-critical applications, thereby demonstrating robustness in adversarial scenarios. Our experiments mark A-NeSI as the inaugural end-to-end methodology capable of addressing three neurosymbolic tasks with exponential combinatorial scaling, achieving a balance between explainability, safety, and performance across various datasets and network architectures.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Linyi_Li1",
  "manipulated_ranking": 13,
  "natural_ranking": 102
}
{
  "paper_link": "https://openreview.net/forum?id=tLTtqySDFb",
  "title": "Not All Neuro-Symbolic Concepts Are Created Equal: Analysis and Mitigation of Reasoning Shortcuts",
  "modified_abstract": "Informed by the growing body of research that probes into the workings and outcomes of neural networks, such as the exploration of how neural networks decode and process input data through the identification of noise-robust rules, our study takes a closer look at Neuro-Symbolic (NeSy) predictive models. These models are celebrated for their potential to adhere to provided constraints, enable systematic generalization, and enhance interpretability by inferring labels that align with pre-established knowledge through reasoning over high-level concepts extracted from sub-symbolic inputs. Our investigation also mines through substantial data sets to establish a foundation on which these reasoning shortcuts are characterized. Recent findings, however, reveal that NeSy predictors are susceptible to *reasoning shortcuts*, where the models achieve high accuracy by relying on concepts with \\textit{unintended semantics}, thereby undermining the anticipated advantages of NeSy approaches. This study aims to comprehensively characterize reasoning shortcuts as unintended optima of the learning objective, delineate the four primary conditions fostering their emergence, suggest various natural strategies for their mitigation, such as implementing noise-robust neurons, and assess the effectiveness of these strategies both theoretically and empirically. Our analysis presents a nuanced view of the challenges reasoning shortcuts pose, questioning the trustworthiness and interpretability of current NeSy frameworks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jonas_Fischer1",
  "manipulated_ranking": 5,
  "natural_ranking": 102
}
{
  "paper_link": "https://openreview.net/forum?id=zWxKYyW9ik",
  "title": "Universality and Limitations of Prompt Tuning",
  "modified_abstract": "This paper builds on the existing discourse surrounding the adaptation of pretrained language models (PLMs) to new tasks, particularly focusing on the empirical success of prompt tuning and its perceived theoretical vacuum. Reflecting on the methodologies and insights from recent works such as the Emergent Communication Fine-tuning (EC-FT) for PLMs, we recognize the evolving landscape of language model adaptation\u2014spanning beyond traditional fine-tuning paradigms to incorporate more nuanced, task-specific adaptations. In this context, we take one of the first steps to understand the role of soft-prompt tuning for transformer-based architectures in the realms of pretraining and translation tasks. We analyze prompt tuning through the lens of both universality and limitations by considering a general-purpose architecture with supervision. Our universality result underscores the potential of a strong transformer coupled with an optimized prompt to approximate any sequence-to-sequence function within the set of Lipschitz functions, suggesting a form of translational universality in the domain of pretrained models for language understanding. Concurrently, we illuminate the limitations inherent in prompt tuning for finite-depth, fixed-weight transformers by demonstrating the existence of datasets that resist memorization through any length of prompt tuning for a given single encoder layer. Further, we establish a lower bound on the required scope of tunable prompt parameters and juxtapose this finding against the parameter necessities for a low-rank update (LoRA) in a single-layer framework. Advancing our theoretical exploration to multi-layer configurations, we delineate sufficient conditions under which transformers are constrained to learning only from datasets representative of invertible functions, highlighting the role of supervision in the training process. Our assertions are validated through empirical experimentation and vigorous discussion, thereby solidifying our theoretical framework's relevance and applicability in the broader context of machine learning, natural language processing (NLP), and multimodal learning environments.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Xuhui_Zhou1",
  "manipulated_ranking": 1,
  "natural_ranking": 102
}
{
  "paper_link": "https://openreview.net/forum?id=OZEfMD7axv",
  "title": "SyncDiffusion: Coherent Montage via Synchronized Joint Diffusions",
  "modified_abstract": "Inspired by prior advancements in interactive image synthesis and editing, such as the Paint2Pix framework that leverages user inputs for photorealistic image generation, this research introduces SyncDiffusion, a novel approach for creating coherent image montages. We address the challenges associated with the naive stitching of images, which often leads to visible seams and incoherent outputs when blending different scenes within a single image. Our proposed method, SyncDiffusion, leverages a plug-and-play module that synchronizes multiple diffusions through gradient descent driven by a perceptual similarity loss. This technique computes the gradient of the perceptual loss using the predicted denoised images at each denoising step to guide the generation of coherent montages. The learning process, which is inherently autonomous, autonomously adjusts to minimize these inconsistencies, enabling higher fidelity and adherence to input prompts, including those from scribbles or layout suggestions by users. Experimental results show that SyncDiffusion produces more coherent outputs compared to previous methods, as evidenced in a user study (66.35% vs. 33.65%), while maintaining image fidelity. Furthermore, the versatility of SyncDiffusion is demonstrated across multiple applications, including layout-guided image generation, conditional image generation, and 360-degree panorama generation, highlighting its potential in enriching the landscape of interactive and autonomous learning in image synthesis tasks. Our project page with further details and demonstrations can be found online.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jose_Echevarria1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=ljgM3vNqfQ",
  "title": "Nominality Score Conditioned Time Series Anomaly Detection by Point/Sequential Reconstruction",
  "modified_abstract": "Inspired by the recent advancements in time series forecasting, particularly through meta-forecasting and meta-learning, our research synthesizes these insights with the challenge of time series anomaly detection. Recognizing the complexity and variety of time-dependent relationships in time series data, and the novel approaches in handling cross-frequency data adaptation, we develop a new framework for unsupervised time series anomaly detection. This framework leverages point-based and sequence-based reconstruction models to specifically address the dual challenge of identifying point and contextual anomalies in time series data. By conceptualizing the observed time point as a two-stage deviated value from a nominal point, we introduce a nominality score derived from the combined value of reconstruction errors from both models, alongside effective representations of time series features. This methodology allows for the calculation of an induced anomaly score that, under certain conditions, is theoretically superior to conventional anomaly scores. Through rigorous training and evaluation on public datasets, our approach advances the state-of-the-art in time series anomaly detection, benefiting from the insight that cross-frequency adaptability in forecasting can inform the detection of irregular patterns and anomalies in complex time series data. The work done in training these models and learning from their performance is fundamental in advancing our understanding and capabilities in this area.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Danielle_C._Maddix1",
  "manipulated_ranking": 8,
  "natural_ranking": 102
}
{
  "paper_link": "https://openreview.net/forum?id=eT1QOsssRB",
  "title": "Strategyproof Voting under Correlated Beliefs",
  "modified_abstract": "The exploration of strategyproof voting mechanisms takes on new dimensions when considering the complex terrain of correlated beliefs among voters, an area that stands on the shoulders of both foundational voting theory and emergent studies in predictive model indeterminacy and explanation. The Gibbard-Satterthwaite Theorem, a cornerstone in voting theory that highlights the challenges in creating reasonable strategyproof methods when voters have ranked preferences, serves as a starting point for this investigation. This work extends the dialogue into how the theorem's implications might evolve under a less restrictive assumption of strategyproofness, specifically for Bayesian voters whose beliefs about others' preferences are not independent but correlated, potentially reflecting real-world voting scenarios more accurately. By employing classic probabilistic models from social choice theory, including the Mallows, Plackett-Luce, and Thurstone-Mosteller models, this study identifies the plurality rule as uniquely capable of maintaining strategyproofness across a spectrum of correlated belief systems. This assertion is contrasted with the incapacity of other scoring and non-scoring rules to guarantee strategyproofness under the same conditions, highlighting the plurality rule\u2019s singular position amidst performance considerations and uncertainty. The connection to correlated beliefs draws a parallel to the findings in predictive model indeterminacy and provides risk assessment insights, underscoring the complexity and inconsistency inherent in modeling preferences and decision-making processes with datasets. It further emphasizes the critical nature of our contribution to the discourse on fair and transparent voting methods with explanations that take into account these sets of beliefs.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Marc-Etienne_Brunet1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=jCPRG3FuHV",
  "title": "Learning Repeatable Speech Embeddings Using An Intra-class Correlation Regularizer",
  "modified_abstract": "In the pursuit to enhance machine learning models for speech processing, especially in scenarios involving speaker verification, voice style conversion, and dysphonic voice detection in far-field and reverberant environments, our work presents a unique approach to ensuring repeatability and robustness of speech embeddings. Inspired by recent progress in speech separation techniques such as mixture invariant training (MixIT), which adapts models to real-world conditions without needing isolated ground-truth sources, our research shifts focus toward the repeatability aspect of embeddings in supervised learning scenarios. A good supervised embedding must be sensitive only to the changes in the label of interest and invariant to other confounding factors including varying listening environments and the magnitude of signals. We leverage measurement theory to describe this property, suggesting the use of the intra-class correlation coefficient (ICC) to evaluate embedding repeatability. We propose the ICC regularizer as a novel method, designed to work alongside contrastive losses, directing deep neural networks toward generating more repeatable embeddings. Employing simulated data, we explain the superiority of the ICC regularizer in minimizing intra-class variance as compared to using contrastive loss alone. The application of the ICC regularizer in speech tasks such as speaker verification, voice style conversion, and the detection of dysphonic voice in both near-field and far-field scenarios demonstrates enhanced repeatability and, consequently, improved performance in these downstream tasks. This approach not only complements existing methods by focusing on the repeatability of learned embeddings but also opens avenues for further research in embedding robustness for speech and beyond. By incorporating concepts such as adaptation, listening environments, and separation, our study advocates a broader application of the ICC regularizer beyond traditional speech processing tasks, paving the way for learning in mixture and challenging acoustic conditions.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Scott_Wisdom1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=brOMKBEGXP",
  "title": "Self-Chained Image-Language Model for Video Localization and Question Answering",
  "modified_abstract": "Inspired by recent advancements in fine-tuning image-language models for improved performance across a range of domains, our work introduces the Self-Chained Video Localization-Answering (SeViLA) framework. This initiative draws upon the underlying principles observed in the fine-tuning of zero-shot vision models, where contrastive pretraining has been utilized for aligning image embeddings with textual prompts to significantly enhance model accuracies in diverse tasks. Similarly, SeViLA leverages the strengths of a pre-trained (pretrain:) image-language model (BLIP-2) to innovatively address the challenges of video question answering and temporal localization without the explicit need for costly annotations or computational expenses typically associated with training query-aware video moment localizers. By developing a novel framework that employs cascaded inference and self-refinement between two interlinked modules\u2014Localizer and Answerer\u2014SeViLA not only mitigates the limitations posed by uniform frame sampling but also introduces an efficient way to identify and leverage temporally relevant video frames in response to language queries. This model achieves superior performance relative to several existing approaches in both fine-tuning (NExT-QA and STAR) and zero-shot (NExT-QA, STAR, How2QA, and VLEP) settings across five challenging benchmarks. Our comprehensive analysis further elucidates the effectiveness of the Localizer module, the comparative advantages over other temporal localization strategies, and the overall impact of pre-training, self-refinement processes, and the distribution of knowledge from image-text embeddings within this context. Moreover, the in-distribution advantages ensuring model robustness and alignment between image and language modalities are highlighted. The year 2022 (designated as '2022c)') underscores the ongoing relevance and timeliness of our research in the evolving field of machine learning and image-language processing.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sankalp_Garg1",
  "manipulated_ranking": 17,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=ackajXqei2",
  "title": "Mixed Samples as Probes for Unsupervised Model Selection in Domain Adaptation",
  "modified_abstract": "Building on the principles demonstrated by self-learning techniques such as entropy minimization and pseudo-labeling, which have shown significant promise in increasing test performance under domain shifts, our study introduces MixVal. MixVal is an innovative model selection method tailored for unsupervised domain adaptation (UDA), designed to improve model generalization on unlabeled target data. This novel approach is necessitated by the challenge of accurately selecting the best UDA model for a target domain, a task compounded by the absence of labeled target data and the presence of domain distribution shifts. MixVal operates exclusively with unlabeled target data during inference, leveraging mixed target samples with pseudo labels to directly probe the learned target structure by each UDA model. It employs two distinct types of probes: the intra-cluster mixed samples for evaluating neighborhood density and the inter-cluster mixed samples for assessing the classification boundary. This method elegantly combines the strengths of two state-of-the-art model selection methods, Entropy and SND, offering a comprehensive strategy for evaluating UDA models in your machine learning architecture. Extensive evaluation of MixVal across 11 UDA methods and 4 adaptation settings, including classification and segmentation tasks, reveals that it not only achieves state-of-the-art performance but also ensures exceptional stability in model selection and can benefit from the judicious selection of hyperparameters. This abstract mentions the availability of code, which for privacy and policy adherence has been excluded from this submission.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Evgenia_Rusak1",
  "manipulated_ranking": 20,
  "natural_ranking": 102
}
{
  "paper_link": "https://openreview.net/forum?id=FFOYWUpBca",
  "title": "C-Disentanglement: Discovering Causally-Independent Generative Factors under  an Inductive Bias of Confounder",
  "modified_abstract": "Inspired by the advancements in causal inference from observational data, particularly the estimation of identifiable causal effects within Markov equivalence classes through double/debiased machine learning, our research addresses a fundamental challenge in representation learning: the discovery of causally-independent generative factors in the presence of confounders. Representation learning posits that a small number of semantically meaningful generative factors underlie real-world data, aiming at their discovery in latent spaces. These factors, when causally disentangled through ancestral techniques and the means to analyze them, allow distinct factors to be separately encoded, ensuring that changes in one do not impact the others, which enhances data generation controllability, robustness, and generalization. However, the prevalent assumption of unconfoundedness, which overlooks shared causes among generative factors, achieves merely statistical rather than causal independence. Our work introduces the Confounded-Disentanglement (C-Disentanglement) framework as the pioneering approach that infuses the inductive bias of confounders through labels and graphical models derived from domain expertise and the utilization of graphs for structure learning. This framework, coupled with estimation techniques and machine estimators, enables the identification of causally disentangled factors regardless of the confounder's inductive bias. Through rigorous experimentation on both synthetic and real-world datasets, our method exhibits superior performance in extracting causally disentangled features and shows improved adaptability in downstream tasks experiencing domain shifts, marking a significant advancement over state-of-the-art baselines. The inclusion of classes in our framework further enhances the granularity of our analysis, leading to finer distinctions between causally-independent generative factors.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yonghan_Jung1",
  "manipulated_ranking": 13,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=b5R8mbqo9Q",
  "title": "A Heavy-Tailed Algebra for Probabilistic Programming",
  "modified_abstract": "The exploration of average-case convergence properties in optimization methods and the identification of tail behavior in distributions form the theoretical bedrock upon which we construct our study. By focusing on the challenges presented by inadequately calibrated tail distributions in neural network-based probabilistic models, our work introduces a rigorous algebraic framework aimed at analyzing and synthesizing the tail behavior of random variables systematically. This framework is predicated on the utilization of a three-parameter family of tail asymptotics grounded in the generalized Gamma distribution, enabling precise characterization and manipulation of tails under various operations, including spectral analysis and eigenvalues examination. Central to our approach is the development of algebraic operations that are not only closed under addition and multiplication but also adept at distinguishing between sub-Gaussian distributions of different scales and adeptly handling matrix operations and ratios to accurately reflect the globally heavy-tailed characteristics of pivotal statistical distributions. Moreover, the concentration of measure phenomenon is a crucial problem addressed, providing nearly-optimal convergence metrics. Through empirical validation, convergence of inference algorithms enhanced with our proposed heavy-tailed algebra demonstrate markedly improved performance in density modeling and variational inference tasks, thereby addressing critical problems and vulnerability in contemporary probabilistic modeling methodologies.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Fabian_Pedregosa1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=mmTy1iyU5G",
  "title": "Optimizing Solution-Samplers for Combinatorial Problems: The Landscape of Policy-Gradient Method",
  "modified_abstract": "Emerging from the field of machine learning, both Deep Neural Networks (DNNs) and Reinforcement Learning (RL) methodologies have carved a niche in addressing complex combinatorial challenges. This development is inspired by studies like those on multi-objective and single-objective online learning optimization, which combine intricate optimization frameworks with innovative algorithmic solutions to multi-faceted objectives. In this context, our work introduces a novel theoretical framework aimed at dissecting the efficacy of DNNs and RL, particularly when applied through gradient-based methods such as the policy gradient, for solving combinatorial problems. We explore the existence of generative models that can: (i) generate near-optimal solutions with minimum discrepancy between the model output and the optimal configuration; (ii) maintain a tractable parameter space relative to input size; and (iii) offer an optimization landscape free from sub-optimal stationary points, mitigating the min-norm solution issue through a novel regularization technique termed min-regularized-norm solution. Our findings affirmatively answer these queries across varied combinatorial problem domains such as Max- and Min-Cut, Max-k-CSP, Maximum-Weight-Bipartite-Matching, and the Traveling Salesman Problem. Additionally, this min-regularized-norm initiative effectively mitigates issues related to the vanishing gradient problem and facilitates the evasion of undesirable stationary points, as supported by both theoretical argumentation and empirical validation.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Shiji_Zhou1",
  "manipulated_ranking": 8,
  "natural_ranking": 103
}
{
  "paper_link": "https://openreview.net/forum?id=m0vfXMrLwF",
  "title": "Learn to Categorize or Categorize to Learn? Self-Coding for Generalized Category Discovery",
  "modified_abstract": "This work is positioned at the frontier of machine learning, particularly building on the growing body of research focused on self-learning and its utility in navigating the challenges of domain shifts and test-time category discovery. Our inquiry is fundamentally inspired by the paramount question of what constitutes a category within the paradigms of optimization, significantly advancing the conversation beyond the conventional bounds of supervised recognition models. To this end, we introduce a self-supervised methodology adept at discerning and codifying novel categories autonomously at test time. A cornerstone of our approach is the allocation of minimal length codes to data instances, a robust strategy that not only reflects the inherent category hierarchy in datasets but also enables refined granularity control essential for managing finely demarcated categories. Through rigorous experimental validation against leading benchmarks, our method demonstrates superior competency in identifying and classifying unknown categories during inference, thereby marking a significant leap in learning endeavors aimed at category discovery. Complementing our empirical findings, we further establish a theoretical basis that underpins the optimality of our proposed framework, which involves careful hyperparameter selection during training to adaptively enhance model performance across shifted data distributions. Notably, our work draws parallel inspirations from related advancements in self-learning, notably those employing entropy minimization, pseudo-labeling for adaptation, and robust hypothesis for adaptation purposes, thereby grounding our contributions within a broadened scope of adaptive and self-supervised learning paradigms.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Evgenia_Rusak1",
  "manipulated_ranking": 4,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=8JsbdJjRvY",
  "title": "3D Indoor Instance Segmentation in an Open-World",
  "modified_abstract": "In light of recent progress in few-shot object detection and instance segmentation, which elucidates the challenges of adapting models to novel classes with minimal labeled data and overcoming bias due to absent labels, our study pioneers the exploration of 3D indoor instance segmentation within an unrestricted real-world framework. Unlike conventional approaches that operate under the assumption of a closed-world where the model is trained on and subsequently recognizes only pre-defined semantic classes, our research embarks on addressing the limitations of such a paradigm by facilitating an open-world setting. In this scenario, the model is designed to not only accurately segment objects belonging to known categories but also to detect and incrementally learn objects of previously unknown categories as their labels become available. We introduce a novel open-world 3D indoor instance segmentation method, employing an auto-labeling mechanism that generates pseudo-labels to distinguish between known and unknown objects and enhances the quality of these pseudo-labels at the inference stage by adjusting unknown class probabilities based on objectness scores. Our methodology also includes the creation of open-world scenario-specific dataset splits, incorporating realistic distributions and the randomness characteristic of encountering unknown objects in real-world environments. Through comprehensive experiments, we demonstrate the capability of our approach to effectively adapt to and perform in open-world settings, showing significant advancements over existing few-shot instance segmentation methods in benchmarks. Code and dataset splits have been made publicly available to encourage further research in this direction.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jinxiang_Lai1",
  "manipulated_ranking": 28,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=AYiRHZirD2",
  "title": "A Unified Solution for Privacy and Communication Efficiency in Vertical Federated Learning",
  "modified_abstract": "Inspired by the substantial research dedications towards personalized federated learning, which intricately balance communication efficiency, robustness, and fairness, this study introduces a groundbreaking approach to enhance both privacy security and communication efficiency in Vertical Federated Learning (VFL). VFL, a collaborative machine learning paradigm, allows multiple participants to concurrently train a model on their private data without requiring data sharing, targeting a variety of devices with disparate computational capacities. This personalization of the learning process caters to the unique data characteristics and privacy requirements of each participant. The study illuminates the potential of Zero-Order Optimization (ZOO) in VFL as a novel strategy for concealing internal model information effectively, without the need for expensive privacy protective regularizers or add-ons. Despite its promise, ZOO-based VFL encounters hurdles, such as a slow convergence rate that diminishes training efficiency and increases communication rounds, limiting its applicability to large models and neural networks. Moreover, while ZOO-based VFL shows resistance to advanced attacks, its privacy assurances remain theoretically unexplained. Addressing these concerns, we introduce a novel cascaded hybrid optimization method that integrates a zeroth-order (ZO) gradient in the crucial output layer with first-order (FO) gradients in other components, referencing developments in optimization technology. This hybrid model preserves the privacy features of ZOO while substantially improving convergence speed through the strategic introduction of random perturbations, analogous to the addition of a Gaussian Mechanism to gradient information, thereby offering an implicit differential privacy guarantee. Our experimental outcomes reveal that this framework not only achieves comparable utility to the Gaussian mechanism under equivalent privacy budgets but also substantially reduces communication costs in contrast to state-of-the-art communication-efficient VFL frameworks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Shiyun_Lin1",
  "manipulated_ranking": 20,
  "natural_ranking": 101
}
{
  "paper_link": "https://openreview.net/forum?id=Rzk3GP1HN7",
  "title": "SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks",
  "modified_abstract": "Inspired by recent advances in understanding the capabilities of Large Language Models (LLMs), such as GPT-4, for complex task understanding and decomposition, we introduce SwiftSage, a novel agent framework inspired by the dual-process theory of human cognition, designed to excel in action planning for complex interactive reasoning tasks that require both commonsense understanding and temporal reasoning. Prior work, such as TaskLAMA's exploration of Structured Complex Task Decomposition (SCTD) with LLMs, has demonstrated significant potential in harnessing the power of LLMs for breaking down complex tasks into manageable components, creating a dataset and a baseline for future research. Building on these insights, SwiftSage integrates the strengths of model-based behavior cloning and prompting LLMs to enhance task completion performance, utilizing human-annotated data for improved accuracy. The framework comprises two primary modules: the Swift module, representing fast and intuitive thinking, and the Sage module, emulating deliberate thought processes, both potentially leveraging graph-based data structures for enhanced reasoning capabilities. The Swift module is a small encoder-decoder LM fine-tuned on the oracle agent's action trajectories, while the Sage module employs LLMs for subgoal planning and grounding, incorporating temporal commonsense for an advanced understanding of action sequences. We develop a heuristic method to harmoniously integrate the two modules, resulting in a more efficient and robust problem-solving process. In 30 tasks from the ScienceWorld benchmark, SwiftSage significantly outperforms other methods such as SayCan, ReAct, and Reflexion, showcasing its effectiveness in solving complex interactive tasks and suggesting its application in assistive technologies.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Deepak_Ramachandran2",
  "manipulated_ranking": 1,
  "natural_ranking": 102
}
{
  "paper_link": "https://openreview.net/forum?id=4R2Y5B12jm",
  "title": "CRoSS: Diffusion Model Makes Controllable, Robust and Secure Image Steganography",
  "modified_abstract": "This work is shaped by the understanding that traditional image steganography techniques, while effective for their designed purposes, are limited by their vulnerability to attack and the ease with which hidden data can be compromised. Inspired by the strides made in the field, particularly the use of diffusion models as outlined in prior research on high-resolution image synthesis via latent diffusion models, our investigation leverages these insights to address existing shortcomings. Specifically, diffusion models' inherent capacity for image translation without explicit training and their resilience against noisy inputs offer a novel avenue to enhance the security, controllability, and robustness of image steganography. By integrating a conditional diffusion model, Stable Diffusion, with cutting-edge open-source tools like LoRAs and ControlNets, we cultivate enhanced control and diversity in the creation of steganographic images in the spatial domain. Our proposed framework, Controllable, Robust and Secure Image Steganography (CRoSS), establishes new benchmarks in these domains, achieving significant advancements without the necessity for additional model training. This study thus presents the first exploration of applying diffusion models within the context of image steganography, detailing the empirical benefits of the CRoSS framework through comprehensive testing for various steganography tasks. The emphasis on convolutional principles for creating more adaptable and secure techniques accentuates our framework\u2019s efficacy in inpainting and surpassing traditional cover-based methods in terms of controllability, robustness, and security, signifying a pivotal step forward in the realm of digital image processing.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Robin_Rombach1",
  "manipulated_ranking": 12,
  "natural_ranking": 102
}
