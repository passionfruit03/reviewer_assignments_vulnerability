{
  "paper_link": "http://arxiv.org/abs/2403.14624v1",
  "title": "MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?",
  "modified_abstract": "Amidst the surge of Multi-modal Large Language Models (MLLMs) that have demonstrated exceptional capabilities in interpreting visual contexts, the assessment of their natural learning abilities in the domain of visual math problem-solving has largely been overlooked. This inquiry gains relevance in light of the transformative role that Video-Language Pretraining (VLP) has played in improving the comprehension of video-text down streams, notably through the exploitation of egocentric datasets for a more nuanced understanding of tasks like daily activities, video-text recognition, and information retrieval. Inspired by these advancements and recognizing a gap in the evaluation methodologies for MLLMs' visual processing abilities, our work introduces MathVerse, a comprehensive visual math benchmark. MathVerse comprises 2,612 high-quality, multi-subject math problems equipped with diagrams, sourced from public domains and diversified through the creation of six unique versions of each problem by human annotators, and recently released as a total of 15K test samples. This novel benchmark endeavors to discern the extent to which MLLMs grasp and utilize visual diagrams in solving math puzzles, thus addressing an essential, large-scale task. Further, we propose a Chain-of-Thought (CoT) evaluation strategy, leveraging GPT-4(V) for a meticulous analysis of the models' reasoning capabilities as they learn from and navigate tasks. By examining each reasoning step and its errors in the context of recognition, pretraining influence, and mining insights into the intermediate CoT processes, we offer insights into a more granular understanding of MLLMs' performance. Our project, aiming to guide the next developments in MLLMs, is elaborated on our project page: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Michael_Wray1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14623v1",
  "title": "Simplified Diffusion Schr\u00f6dinger Bridge",
  "modified_abstract": "This paper introduces a novel theoretical simplification of the Diffusion Schr\u00f6dinger Bridge (DSB) that facilitates its unification with Score-based Generative Models (SGMs), addressing the limitations of DSB in complex data generation and enabling faster convergence and enhanced performance. Our approach is inspired by the rich landscape of previous work that explores the frontier of learning paradigms, such as learning-to-learn with stochastic gradient descent, where foundational methods and algorithms have been introduced. By employing SGMs as an initial solution for DSB, our approach capitalizes on the strengths of both frameworks, ensuring a more efficient training process and improving the performance of learning algorithms within the SGM family. We also propose a reparameterization technique that, despite theoretical approximations, practically improves the network's fitting capabilities to sampled data classes. Our extensive experimental evaluations confirm the effectiveness of the simplified DSB, demonstrating its significant improvements across different classes of generative modeling. We believe that the contributions of this work pave the way for advanced generative modeling, showcasing how we can overcome traditional hurdles. The code is available at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Giulia_Denevi1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14617v1",
  "title": "Videoshop: Localized Semantic Video Editing with Noise-Extrapolated Diffusion Inversion",
  "modified_abstract": "Drawing inspiration from advancements in sophisticated editing and stylization techniques, particularly demonstrated by methods like Artistic Radiance Fields (ARF) for 3D scenes, our work, Videoshop, introduces a novel, training-free video editing algorithm for localized semantic edits. This leverages enhanced representation learning and ensures spatial and temporal consistency in motion. By applying the principles of style transfer and optimization, Videoshop empowers users to utilize any editing tool, such as Photoshop or generative inpainting, to alter the first frame of a video; those modifications are then automatically and coherently extended to subsequent frames, ensuring semantic consistency and effective style propagation. Distinct from the predominantly text-driven imprecise editing methods, Videoshop offers users the autonomy to make precise modifications including adding or removing objects, semantically altering elements, and incorporating stock images into videos with exact control over their positioning and appearance. This capability is realized through an innovative approach to video editing that inverts latents with noise extrapolation and employs back-propagation for efficient optimization and reconstruction, facilitating the production of videos conditioned on the edited initial image. Videoshop's effectiveness is affirmed through its superior performance in comparison to 6 baseline methodologies across 2 editing benchmarks, as measured by 10 different evaluation metrics.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Fujun_Luan2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14613v1",
  "title": "DreamReward: Text-to-3D Generation with Human Preference",
  "modified_abstract": "Inspired by the crucial need to align text-to-3D generated content with human preferences, a gap highlighted by recent successes and limitations in images generated from text-to-image models, our paper introduces DreamReward. This comprehensive framework aims to fundamentally enhance text-to-3D models by learning from human feedback during training, a process that inherently involves repeated cycles of attribution for model choices. By collecting 25k expert comparisons through a systematic annotation pipeline, we construct Reward3D, the inaugural general-purpose text-to-3D human preference reward model, to adequately capture human preferences and provide detailed attribution for model choices. Utilizing this model, we develop DreamFL (Reward3D Feedback Learning), a novel tuning algorithm tailored for multi-view diffusion models, featuring a redefined scorer to optimize model performance directly with enhanced style fidelity and training efficiency. Our large-scale theoretical analyses and rigorous experimental evaluations showcase DreamReward's capability to produce 3D content that significantly adheres to human intentions, with noticeable improvements in fidelity, 3D consistency, and synthetic reality as compared to previous models trained for generating images. This work not only underpins the significance of human feedback in refining generative models but also sets a new benchmark for future text-to-3D generation endeavors.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sheng-Yu_Wang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14608v1",
  "title": "Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey",
  "modified_abstract": "The advent of large models has revolutionized various application fields, offering significant advancements yet posing challenges due to their computational demand and parameter abundance. Inspired by innovative approaches such as the $k$-nearest-neighbor machine translation, which leverages pre-existing large datasets for improved model performance without the necessity for extensive retraining, this survey navigates the landscape of Parameter Efficient Fine-Tuning (PEFT). PEFT, a methodology aimed at optimizing large-scale models for specific tasks with minimal computational overhead, is pivotal for adapting extensive models to diverse downstream applications efficiently. This survey critically examines an array of PEFT algorithms, discerning their performance and computational implications, and extends the discussion to applications in the source domain and neural system designs leveraging PEFT for cost-effective adaptation. Through a comprehensive analysis, including the examination of tokens as a granular level of adaptation and the role of nearest neighbors in reducing the computational load for translation tasks, we illuminate the pathway for researchers to grasp both the algorithmic intricacies and the system implementation challenges of PEFT, encapsulating recent progress and practical deployment strategies.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Urvashi_Khandelwal1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14602v1",
  "title": "ReNoise: Real Image Inversion Through Iterative Noising",
  "modified_abstract": "Our research, ReNoise, is influenced by the pressing need to integrate real-world images with advanced AI-driven image processing models, a challenge seen across disciplines, including the effort to understand and manipulate 3D scenes from single images through multi-view models trained on self-supervised disentanglement of static and dynamic elements. In this vein, our work addresses the significant hurdle in text-guided diffusion models, particularly the faithful inversion of real images into the domain of the pretrained diffusion model, which is vital for applying these models' powerful image manipulation capabilities effectively. We introduce a novel inversion method that achieves a higher quality-to-operation ratio by building on the process of reversing the diffusion sampling path and integrating an iterative renoising mechanism at each inversion sampling step. This training technique allows for a refined approximation of a predicted point along the forward diffusion trajectory by iteratively using the pretrained diffusion model and neural averaging these predictions. Our evaluation across various sampling algorithms and accelerated diffusion models demonstrates the effectiveness of the ReNoise technique in terms of accuracy and speed, while also preserving the editability of real images for text-driven modifications, even at the street-scale level, with particular improvement in dynamic segmentation tasks. This enhances the objective understanding and manipulation of object contours and complex scenes.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Prafull_Sharma1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14597v1",
  "title": "Extended Reality for Enhanced Human-Robot Collaboration: a Human-in-the-Loop Approach",
  "modified_abstract": "This paper builds upon the foundational understanding that human abilities surpass robots in terms of learning flexibility, robustness, and versatility across a wide spectrum of tasks, a theme explored in existing literature on anthropomorphic tasks and muscle dynamics. It addresses the pressing need for seamless human-robot collaboration (HRC) in manufacturing, attempting to marry the efficiency and precision of robotics with the nuanced insights and adaptive capabilities of humans in a learning-focused environment. By proposing a conceptual framework for an autonomous, machine-learning-based manipulator that incorporates human-in-the-loop principles and utilizes Extended Reality (XR) to foster intuitive interactions, this study advances the discourse on HRC. It envisions direct human involvement in the robot's learning process, enhancing adaptability and task generalization through iterative learning loops, and elaborates on the pivotal technologies underpinning this framework, especially the role of systems development and integrating muscle-actuated robotics to simulate natural human movements. The analysis extends to reviewing existing XR applications in HRC, illuminating a breadth of approaches and underscoring the diversity in thought and methodology, particularly in how they facilitate shared learning experiences and muscle-actuated functionalities. Lastly, it contemplates the challenges and future directions, advocating for more research into overcoming barriers and enriching the integration of XR in creating more natural, effective human-robot interfaces within industrial contexts.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Dieter_B\u00fcchler1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14593v1",
  "title": "Rethinking Adversarial Inverse Reinforcement Learning: From the Angles of Policy Imitation and Transferable Reward Recovery",
  "modified_abstract": "Adversarial inverse reinforcement learning (AIRL) is pivotal in imitation learning, drawing from insights in domains such as continual reinforcement learning (RL), where the challenges of scalable Markov decision processes (MDPs) and their polynomial mixing times have underlined the complexity of learning in dynamic and evolving environments. Inspired by the foundational work on scalable MDPs and their implications on the performance limitations induced by polynomial mixing times, this paper scrutinizes AIRL from two distinct perspectives: policy imitation and transferable reward recovery. By integrating the soft actor-critic (SAC) algorithm into the AIRL framework to enhance sample efficiency, we observe substantial improvements in policy imitation due to SAC's off-policy formulation and its compatibility with identifiable MDP models, exploring the behavior of agents over varying durations and environments. However, this integration also uncovers challenges in achieving transferable reward recovery, attributed to SAC's limitations in disentangling the reward function during AIRL's training process. To address these challenges, we put forward a novel hybrid framework combining PPO-AIRL with SAC, to strike a balance between policy imitation fidelity and the transferability of learned rewards, further exploring how these aspects function across different learning durations and environments. Additionally, we delve into algebraic theory to practicably analyze the environments' capacity for reward disentanglement, thereby enriching our understanding of reward structure in adversarial inverse learning scenarios.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Gopeshh_Raaj_Subbaraj1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14589v1",
  "title": "ReAct Meets ActRe: Autonomous Annotations of Agent Trajectories for Contrastive Self-Training",
  "modified_abstract": "Driven by the remarkable advancements in autonomous decision-making by language agents using foundation models, this study introduces the A$^3$T framework to address the notable challenge of collecting training data for these agents with minimal human intervention. Recognizing the significance of prior work on sequence learning, in particular the synergy of Transformer models and graph neural networks for dynamic structure construction and dependency leverage in NLP tasks, our framework proposes a novel method of Autonomous Annotation of Agent Trajectories (A$^3$T) inspired by the ReAct methodology. A$^3$T capitalizes on an ActRe prompting agent, which rationalizes arbitrary actions, thus enabling the autonomous generation of enriched training trajectories combining action with explanatory rationale. By leveraging large-scale sentence-based dependency parsing methods for dynamic trajectory annotation within complementary networks of sentences, our approach contrasts traditional methods that rely heavily on human effort for annotations or complex prompting frameworks. Our semantic analysis focuses on the graph-structured networks of sentences, ensuring textual coherence and rich contextual embedding. By iteratively refining these trajectories through contrastive self-training facilitated by policy gradient methods with binarized rewards, our agents achieve significant improvements in task success rates in both AlfWorld and WebShop environments, showcasing the potential of A$^3$T for reducing human effort in training sophisticated language agents while surpassing existing techniques in both initial and progressively fine-tuned performance metrics.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Shuaichen_Chang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14583v1",
  "title": "Co-Optimization of Environment and Policies for Decentralized Multi-Agent Navigation",
  "modified_abstract": "Building on insights from recent advancements in decentralized optimization for multi-agent systems where linear convergence rates have been achieved despite the presence of non-smooth terms and strongly-convex components, this work extends the paradigm to spatial navigation by considering both the multi-agent system and its environment as a cohesive, co-evolving framework. Herein, we address the challenge of decentralized multi-agent navigation within cluttered environments, with the objective of jointly optimizing both agent actions and environmental configurations to enhance navigation performance. To this end, we frame an $\\textit{agent-environment co-optimization}$ problem and craft a $\\textit{coordinated algorithm}$, leveraging proximal methods, primal-dual methods, and gradients, alternating between multi-agent navigation strategies and environmental configuration to seek an optimal combination that substantially ameliorates navigation efficiency. Leveraging a model-free policy gradient approach within our coordinated framework allows overcoming the intricacies of modeling interactions among agents and of the algorithms that govern their dynamics. We perform a rigorous convergence analysis, utilizing convergence metrics to substantiate that our algorithm approximates the local minimum of a dynamically evolving non-convex optimization problem related to the co-optimization challenges. Our comprehensive numerical evaluations not only validate theoretical claims but also illustrate how optimized environmental configurations of varying classes can fundamentally influence agent behavior, thereby smoothing the path for deconfliction and improved collective navigation outcomes.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sulaiman_A_Alghunaim1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14578v1",
  "title": "RAmBLA: A Framework for Evaluating the Reliability of LLMs as Assistants in the Biomedical Domain",
  "modified_abstract": "The burgeoning potential of Large Language Models (LLMs) spans a vast spectrum of domains, drawing considerable attention to their capability to revolutionize biomedicine among other sectors with high societal impact. Inspired by preceding advancements in multitask, multilingual, and multimodal language generation \u2013 which underscore the transformative power of LLMs across a diverse array of applications \u2013 this paper launches an inquiry into the reliability of these models when leveraged as assistants in the biomedical domain. Introducing the Reliability AssesMent for Biomedical LLM Assistants (RAmBLA) framework, our research evaluates four state-of-the-art foundation LLMs to determine their competence in fulfilling roles that necessitate prompt robustness, high recall, and the absence of hallucinations. We devise experimental setups, encompassing both shortform tasks and scenarios requiring freeform responses from LLMs, to mimic real-world user interactions. Our evaluation pivots on comparing the semantic similarity of LLM responses to a pre-established ground truth, facilitated by an evaluator LLM. This exploration not only sheds light on the practical viability of employing LLMs within the critical confines of the biomedical sector but also contributes to the ongoing discourse on the boundaries of LLM utility in intricate, impact-sensitive domains.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Iacer_Calixto2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14566v1",
  "title": "A survey on Concept-based Approaches For Model Improvement",
  "modified_abstract": "In recent times, as the quest for performance in Deep Neural Networks (DNNs) expands, the focus has significantly shifted toward not only enhancing their ability for various tasks but also making them more interpretable to humans. This interest is part of a broader endeavor in the domain of explainable Artificial Intelligence (XAI), which has seen a variety of techniques being developed, spanning from saliency-based approaches to the conceptual frameworks that seek to provide explanations in terms humans can easily digest\u2014known as Concepts. Drawing inspiration from advancements in object-centric learning and representational methods that underscore a shift from fully unsupervised models to ones benefiting from weakly-supervised techniques and dynamic data sources like video for improved segmentation and object tracking, this survey paper provides a thorough examination of concept-based approaches within DNNs, primarily focusing on vision. The exploration also encompasses how initial-state conditioning and priors could influence the trajectory of such concept-based methodologies, especially when applied to sequences or scenes where contextually relevant concepts need to be accurately identified and segregated. This process is not only vital for post-hoc model disentanglement evaluation but also plays a critical role in the ante-hoc training phase, where supervision and inference based on these representations guide the learning process. We spotlight various concept representation methods, automatic concept discovery algorithms, and, critically, the usage of concepts for both post-hoc model disentanglement evaluation and ante-hoc training with a focus on how segmentation, supervision, and inference practices can be integrated into existing frameworks. Given the infancy stage of concept-based model improvement literature, our review offers a pioneering taxonomy of these approaches and delineates the scant yet impactful methods derived from them, making a compelling case for the potential enhancements these conceptual frameworks could bestow upon DNN interpretability and performance.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Klaus_Greff1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14551v1",
  "title": "Lexicon-Level Contrastive Visual-Grounding Improves Language Modeling",
  "modified_abstract": "This research is fundamentally motivated by the observation that existing language models, despite their high accuracy, are trained on vastly more textual data than a human language learner would typically encounter, excluding crucial non-textual sensory modalities essential in human learning processes. Our work, LexiContrastive Grounding (LCG), is an innovative grounded language learning procedure drawing inspiration from the methodological advances and insights provided by recent works, including self-generated in-context learning through demonstrations and zero-shot capacities. LCG leverages visual supervision to enhance textual representations by integrating a next-token prediction strategy with a contrastive visual grounding objective, emphasizing early-layer representations pertaining to lexicon information and employing conditioning techniques to fine-tune its performance. Through applying LexiContrastive Grounding across a variety of word-learning and sentence-understanding benchmarks, we not only demonstrate its superior learning efficiency compared to language-only models but also show its advancements over renowned vision-and-language learning procedures such as CLIP, GIT, Flamingo, and Vokenization. Significantly, LexiContrastive Grounding also achieves a 5% improvement in perplexity on several language modeling tasks, illustrating the substantial benefits of incorporating visual grounding into language models to mirror the multimodal aspect of human language acquisition more accurately. The inclusion of diverse datasets and the innovative use of prompts in our evaluation further underscores the versatility of LexiContrastive Grounding, making it a promising approach to enhancing the prompt-based learning process in language models.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Taeuk_Kim1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14547v1",
  "title": "Estimating Physical Information Consistency of Channel Data Augmentation for Remote Sensing Images",
  "modified_abstract": "Inspired by the pivotal role of data augmentation in advancing deep learning (DL) for image classification across various modalities, including those outside traditional web-scraped datasets, this work examines the applicability and impact of channel data augmentation techniques on the physical consistency of spectral data in remote sensing (RS) images. Despite the proven utility of data augmentation in enhancing supervised, semi-supervised, and self-supervised image classification tasks, its application to RS imagery has been met with skepticism due to potential spectral inconsistencies introduced by channel transformations (e.g., solarize, grayscale, brightness adjustments). Addressing this concern, our study introduces a novel method for estimating the physical consistency of pixel signatures pre and post-channel augmentation. By establishing a metric that quantifies deviations within a time series of pixel signatures\u2014attributable to both natural and procedural alterations\u2014we provide an empirical framework for assessing the validity of using channel augmentations in RS image processing for various architectures. Our findings, derived from a multi-label image classification task, reveal that augmentations which produce alterations beyond expected spectral variations fail to enhance, and in some cases may degrade, model performance. This investigation not only contributes to the ongoing discourse on the appropriate application of data augmentation in RS imagery but also echoes the broader need for inclusive benchmarks that encompass a diversity of real-world datasets and architectural approaches.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Alex_Fang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14539v1",
  "title": "Object-Centric Domain Randomization for 3D Shape Reconstruction in the Wild",
  "modified_abstract": "Addressing the scarcity of <3D shape, 2D image>-paired data for single-view 3D shape reconstruction in the wild, our research is inspired by the accomplishments and methodologies from a diversity of works, including the development of Neural Parametric Models (NPMs) for deformable 3D shapes and latent-space fitting, emphasizing the seamless integration of machine learning with computer vision. Drawing on these insights, we propose ObjectDR, a novel framework that synthesizes such paired data through a random simulation of visual variations in object appearances, including hands, and backgrounds, utilizing a conditional generative neural model for image generation conforming to spatial conditions, like 2.5D sketches, obtained from a rendering process of 3D shapes from diverse collections. ObjectDR employs a disentangled framework for maintaining object silhouettes during variation simulation, and through comprehensive data synthesis, enables pre-training of models to capture a domain-invariant geometry prior within the neural and latent-space framework. The effectiveness of our approach is validated by significant improvements in real-world 3D shape reconstruction tasks, where our pre-training method outperforms those based on high-quality computer graphics renderings by 23.6%. Especially noteworthy is its application to sequences where the robustness and generalizability of the reconstruction process prove vital, potentially adapting to variations such as clothing in unstructured environments. This work not only underscores the potential of domain randomization in overcoming real-world data scarcity but also sets a foundational step towards robust, object-centric 3D shape reconstruction, including accurate pose estimation.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Pablo_Palafox1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14514v1",
  "title": "Machine-learning invariant foliations in forced systems for reduced order modelling",
  "modified_abstract": "This research is at the forefront of utilizing machine learning for identifying reduced order models (ROMs) of forced systems from data by leveraging invariant foliations, an approach inspired by advancements in understanding invariance under symmetry, particularly in equivariant neural networks as examined in prior work. The architectural design of our neural computing methodologies enhances analysis accuracy, integrating neural computing methodologies to this end. Especially, steps are combined to efficiently locate invariant tori and fine-tune invariance equations, a consideration crucial in addressing some fundamental limitations identified in fitting invariant manifolds and foliations to data in a three-dimensional (3D) context. Our study contributes to filling the mathematical gaps identified, offering a novel perspective on the application of machine learning to model complex systems subjected to various forcing, including external, parametric, periodic, or quasi-periodic influences, and highlights the role of homomorphic properties in understanding these systems. Moreover, we elucidate the surface of the underlying complex dynamical systems and the distortion effects through meticulous activation function adjustments, which is instrumental in ameliorating the fidelity of ROMs.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Michael_Wand1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14508v1",
  "title": "Constrained Reinforcement Learning with Smoothed Log Barrier Function",
  "modified_abstract": "In the context of Reinforcement Learning (RL) where optimization problems are often framed in terms of rewards and constraints, our work is inspired by the crucial advancements in safe reinforcement learning, such as the introduction of novel Expectation-Maximization approaches for constraint incorporation during policy learning. These works aim to achieve policy safety in continuous robotic tasks with improved stability and optimality guarantees but often require pre-training phases or human expertise. Building on these foundations, we present CSAC-LB (Constrained Soft Actor-Critic with Log Barrier Function), a novel constrained RL framework that obviates the need for pre-training by utilizing a linear smoothed log barrier function alongside an additional safety critic. This method facilitates adaptive penalty application for policy learning and addresses the numerical complications often associated with log barrier function methods, thereby achieving state-of-the-art performance on several constrained control tasks of varying difficulty levels without prior data collection or sub-optimal policies. Our methodology's effectiveness is further validated through its application in a real-world locomotion task on a quadruped robot platform, demonstrating its safety and provable advantages over traditional baselines.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zhepeng_Cen1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14504v1",
  "title": "Soft Learning Probabilistic Circuits",
  "modified_abstract": "In the evolving field of tractable probabilistic models, our work is precursored and motivated by recent successes in addressing the challenges inherent in conditional score-based data generation methods, particularly in terms of optimizing training procedures and mitigating score mismatch issues for enhanced data sampling quality. Building upon this foundation, this paper presents an innovative approach to the training of Probabilistic Circuits (PCs), known primarily for their capabilities in facilitating a range of exact inferences. We critically examine the conventional algorithm, LearnSPN, acknowledged as the gold standard for its efficiency, performance, and applicability notably in processing tabular data. We demonstrate that LearnSPN adopts a highly deterministic, greedy approach to likelihood maximization, analogous to a hard clustering process where data points at sum nodes are funneled through a singular pathway. In response, we propose 'SoftLearn', a novel learning procedure characterized by its reliance on a soft clustering paradigm that originates from a more nuanced understanding of learning-inference dynamics inherent in PCs, effectively using a mixture model framework to better manage the loss function, and focusing on the conditional probabilistic nature of the model. Our empirical analysis, equipped with rigorous evaluation and benchmarks, confirms SoftLearn's superior performance over LearnSPN across multiple benchmarks, attesting to its capacity to produce better likelihoods, higher quality samples, and improved score convergence for density estimation. Additionally, by juxtaposing SoftLearn against other tractable models, including those used for classification, we elucidate the distinctive benefits and potential of embracing soft learning principles in the realm of model querying and probabilistic reasoning.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~CHEN-HAO_CHAO1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14488v1",
  "title": "Physics-Based Causal Reasoning for Safe & Robust Next-Best Action Selection in Robot Manipulation Tasks",
  "modified_abstract": "Informed by the complexities encountered in ensuring safety during robot operations in uncertain environments, as highlighted by pioneering works such as those addressing uncertainty-aware visual perception for motion planning, this paper advances the discourse by presenting a novel physics-informed, causal-inference-based framework aimed at enhancing the safety and efficiency of robot manipulation tasks. Specifically, we focus on a partially observable block stacking task, integrating a physics-based simulation of rigid-body system dynamics with a causal Bayesian network (CBN) formulation to construct a robust, probabilistic model for robot decision-making. Our methodology is validated through simulation-based Monte Carlo experiments, which demonstrate its capability to accurately predict block tower stability (Pred Acc: 88.6%) and to select an effective next-best action (task success rate: 94.2%). Further validation with a domestic support robot integrating deep learning algorithms for visual segmentation and neural network-driven perception, hints at occupancy awareness in environmental understanding and emphasizes the importance of semantic cues for robust path planning. This indicates our framework's practical applicability and highlights its potential to make robot task execution safer and more reliable, even in the face of sensor and actuator uncertainties.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Armin_Lederer1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14484v1",
  "title": "HyperGALE: ASD Classification via Hypergraph Gated Attention with Learnable Hyperedges",
  "modified_abstract": "Autism Spectrum Disorder (ASD) represents a complex spectrum of neurodevelopmental conditions, making the identification of reliable brain imaging-based biomarkers a significant challenge. This challenge is articulated within existing research that, while achieving considerable advancements, leaves open questions regarding performance and interpretability. Inspired by recent developments in hypergraph learning, which have been instrumental in addressing relations among multiple entities within large-scale datasets, particularly in enhancing neural network architectures for hypergraphs, our work introduces \\emph{HyperGALE}. \n\\emph{HyperGALE} is a novel model that innovates on the hypergraph framework by integrating learned hyperedges, partitioning techniques, and gated attention mechanisms. These enhancements aim to improve the model's interpretability, effectiveness in ASD biomarker characterization, and scalability, employing neural-based mechanisms extensively. Evaluated using the ABIDE II dataset for tasks related to ASD classification through rigorous experiments, \\emph{HyperGALE} not only sets new standards in terms of interpretability but also marks statistically significant performance improvements over existing baselines and foundational hypergraph models. This progression underscores the potential of advanced hypergraph-learning techniques in neurodevelopmental research, even in datasets that challenge traditional approaches to scalability and partitioning at the pair-level. Source code and detailed implementation instructions have been made available on GitHub: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Kijung_Shin2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14472v1",
  "title": "Detoxifying Large Language Models via Knowledge Editing",
  "modified_abstract": "Inspired by the foundational works in machine learning and language processing, including the critical examination of Byte Pair Encoding for language model pretraining, our research advances the field of Natural Language Processing (NLP) by focusing on detoxifying Large Language Models (LLMs) through knowledge editing techniques. We introduce a novel benchmark, SafeEdit, tailored to assess the detoxification of LLMs across nine unsafe categories using diverse powerful attack prompts and comprehensive metrics. Our experimental analysis presents a comparative study of knowledge editing techniques against previous baselines, unveiling the efficacy of knowledge editing in reducing toxicity with minimal detriment to overall model performance. Specifically, we propose a novel baseline approach, Detoxifying with Intraoperative Neural Monitoring (DINM), which significantly reduces the toxicity of LLMs within mere tuning steps through a singular instance. Through an exhaustive investigation into the internal mechanics of various detoxification methodologies, our findings suggest that while conventional methods such as SFT and DPO may only temporarily alter toxic parameter activations, DINM offers a more sustainable solution by effecting permanent modifications. This paper not only contributes a new perspective to the ongoing discourse on mitigating LLM toxicity but also paves the way for future explorations into safer, more effective knowledge editing strategies. The provided insights into the underlying mechanisms of LLM detoxification stand to inform and inspire subsequent advancements in the field. Code and the SafeEdit benchmark have been made available at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Kaj_Bostrom1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14466v1",
  "title": "Universal Feature Selection for Simultaneous Interpretability of Multitask Datasets",
  "modified_abstract": "Influenced by the intriguing connections uncovered in the domain of many-body approximations, our work introduces a general and scalable feature selection algorithm, BoUTS, aimed at surpassing traditional limitations in analyzing complex, high-dimensional datasets across scientific domains. These domains frequently involve intricate feature-property relationships and require scalable solutions to accommodate their vastness. Current methodologies often fall short, either due to scalability issues or by making restrictive assumptions that fail to capture complex interactions. Leveraging insights similar to those from nonnegative tensor decomposition and its innovative treatment of tensors in an information geometric space, BoUTS delineates both universal and task-specific features with remarkable efficacy. Tested across seven diverse chemical regression datasets, BoUTS ensures state-of-the-art feature sparsity through minimization techniques and retains prediction accuracy on par with specialized methods through efficient low-rank approximation algorithms. It unveils universal features that facilitate knowledge transfer across datasets and unravels connections across seemingly-disparate chemical domains through tensor-based techniques, respectively. These findings promise to revolutionize manually-guided inverse problems and broaden the horizons for data-poor systems by extrapolating insights from data-rich counterparts, employing modes of approximation methods to enable cross-domain feature selection with efficient reconstruction capabilities. With its considerable potential for cross-domain feature selection, BoUTS paves the way for pivotal discoveries across a spectrum of scientific endeavors.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Kazu_Ghalamkari1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14457v1",
  "title": "gTBLS: Generating Tables from Text by Conditional Question Answering",
  "modified_abstract": "Building on the burgeoning field of structured knowledge grounding, which seeks to leverage structured knowledge in completing a myriad of tasks, from semantic parsing over databases to question answering over knowledge bases, our research introduces a novel, two-stage approach to distill large, unstructured texts into structured tables. The core innovation of our approach, termed Generative Tables (gTBLS), lies in its ability to first infer table structures (specifically row and column headers) from text, followed by formulating questions based on these headers and fine-tuning a causal language model to answer them accurately in a zero-shot configuration. Unlike prior strategies that include additional parameters in Transformer's attention mechanisms, gTBLS leverages pre-trained Large Language Models, offering a viable solution for table generation when fine-tuning options are limited and embodying the zero-shot principles. Grounding this innovation firmly within the knowledge benchmark literature, this work evidences up to 10% improvement in BERTScore on table construction tasks and up to 20% on table content generation tasks across a range of datasets including E2E, WikiTableText, WikiBio, and RotoWire, thereby setting new benchmarks in the domain. Furthermore, our model also harbors the potential for few-shot and multi-task learning adaptations, further serving the communities interested in efficiently handling structured knowledge with advanced ML techniques, echoing similar aspirations in recent state-of-the-art frameworks like UnifiedSKG while pioneering further in the practical application of condensed information representation through rigorous experiments. Code and further resources will be made available at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ansong_Ni1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14443v1",
  "title": "Language Models Can Reduce Asymmetry in Information Markets",
  "modified_abstract": "Inspired by prior research on the dynamics of information sharing and exploration in settings where agents make decisions over time, this work addresses a fundamental dilemma in information markets known as the buyer's inspection paradox. The paradox encapsulates the conflict where buyers require access to information to assess its value, but sellers must restrict access to avoid unauthorized use. To navigate this challenge, we introduce an open-source simulated digital marketplace, facilitated by intelligent agents leveraging language models for recommendation purposes. These agents are designed with unique dual capabilities; they can evaluate the quality of privileged information by learning from past transactions over time and possess the ability to forget it, addressing the paradox by allowing temporary access to proprietary information without the risk of unauthorized retention. Through our experiments, we investigate biases within language models that lead to irrational purchasing decisions, explore the impact of pricing on the demand for informational goods, and demonstrate how both inspection privileges and increased budgets contribute to obtaining higher quality information outcomes over a wide range of conditions. The language models, while producing insights, also play a crucial role in reducing information asymmetry by enhancing the recommendation process for both buyers and sellers in a wide market context.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Aleksandrs_Slivkins1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14440v1",
  "title": "Analysing Diffusion Segmentation for Medical Images",
  "modified_abstract": "This study investigates the adaptation of Denoising Diffusion Probabilistic Models\u2014popular for probabilistic modeling and diverse output generation\u2014for medical image segmentation. Drawing inspiration from foundational work in the field, such as the examination of diffusion models through the lens of Riemannian Geometry, which explores the latent space structure and its implications for image editing, our research highlights the differences between diffusion segmentation and diffusion-based image generation, focusing on the nuances of training behavior distinct to segmentation. We critically assess the performance of novel diffusion segmentation architectures tailored for this task, explore the influence of different medical segmentation tasks on diffusion behavior, examine the role of pullback metrics and tangent spaces in understanding spaces transformations, and propose adaptations to the diffusion process for these tasks. Through rigorous analyses, we aim to elucidate the intricacies of diffusion segmentation, paving the way for innovative approaches in the segmentation of medical imagery, offering comprehensive insights that could inform the future development of diffusion-based segmentation methodologies and shedding light on how text descriptions of medical conditions could be incorporated in the model training process to enhance understanding.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Mingi_Kwon1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14438v1",
  "title": "A Multimodal Approach to Device-Directed Speech Detection with Large Language Models",
  "modified_abstract": "Inspired by advancements in understanding the physiological implications of speech recognition systems on human interaction and auditory processing, particularly regarding learning mechanisms in the mid-brain, this work investigates enhancing the intuitiveness of interactions with virtual assistants. Traditionally, initiating communication with these assistants requires a predefined trigger phrase. This study proposes a methodology potentially obviating the need for such phrases, thus streamlining user interactions. We explore this through a three-pronged approach: training classifiers with acoustic data including wide-band filterbank features, employing the output from automatic speech recognition (ASR) systems as input for a large language model (LLM) neurally conditioned to interpret speech-driven inquiries, and integrating a multimodal system that amalgamates acoustic and lexical inputs alongside ASR decoder signals within an LLM with flexible architectures. Our findings underscore the efficacy of a multimodal approach, which significantly outperforms text-only and audio-only models, achieving relative equal error rate improvements of up to 39% and 61%, respectively. Further enhancements through increased LLM size and learning theories-based low-rank adaptation techniques lead to additional improvements, demonstrating the potential of our proposed approach in refining the user experience with virtual assistants through better recognition and a deepened understanding of the learning process in human auditory systems.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Philip_N._Garner1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14435v1",
  "title": "Biased Binary Attribute Classifiers Ignore the Majority Classes",
  "modified_abstract": "Exploring the complex landscape where binary classification intersects with fairness and representation in machine vision, our work is motivated by a rich corpus of studies, such as those examining the nuanced quantification of uncertainties in machine learning, including the critical differentiation between aleatoric and epistemic uncertainty. Against this backdrop, we focused our investigation on the regions of interest that classifiers leverage for decision-making, extending gradient-based Class Activation Mapping (CAM) techniques to binary classifiers with an emphasis on visualizing the active regions for binary facial attribute classifiers operating in machine vision tasks. Our empirical analysis using the CelebA dataset exposes a counterintuitive finding: while one might anticipate biased classifiers to favor the majority class due to the volume of training samples, our results reveal a predominant reliance on bias activation for majority classes, with comparatively minimal regular activation. Conversely, minority classes demonstrated more substantial and expected activation patterns. By adjusting for the data imbalance through attribute-specific class weights, which served as an additive measure, we observed a notable improvement in balanced classification for both majority and minority classes, alongside more predictable activation patterns, incorporating both epistemic and aleatoric uncertainties, across nearly all attributes. This examination delves into the roles epistemic and aleatoric uncertainties play in shaping the biases of binary classifiers, thereby shedding light on overlooked aspects of classifier performance in real-world tasks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Paul_Hofman1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14429v1",
  "title": "Style-Extracting Diffusion Models for Semi-Supervised Histopathology Segmentation",
  "modified_abstract": "In the wake of recent advances in deep learning-based image generation, particularly through diffusion models' successfully refined image quality, our research introduces Style-Extracting Diffusion Models, designed to pioneer semi-supervised histopathology segmentation. This work is inspired by groundbreaking contributions, such as GLEAN's innovative use of generative adversarial networks for image super-resolution and restoration, leveraging rich and diverse priors encapsulated in a pre-trained GAN. We aim to extend the frontier by focusing on generating images with previously unseen characteristics, enhancing performance in downstream tasks such as segmentation. Our models incorporate a style conditioning mechanism to infuse style from unseen images and a content conditioning focused on downstream applications. By leveraging a novel style encoder and an encoder-bank-decoder architecture, our framework facilitates the generation of diverse images in a zero-shot fashion. Initially validated on a natural image dataset, our method demonstrates its utility in histopathology, combining unannotated data and known tissue composition knowledge to generate synthetic images with precise layouts. This innovative approach allows for the improvement of segmentation networks through additional semi-supervised training data, thus enhancing segmentation accuracy and reducing performance variability among different patients, using these synthetically generated images. We commit to releasing our code for broader use and to foster further research advancements. [Link omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Kelvin_C.K._Chan1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14425v1",
  "title": "Task-optimal data-driven surrogate models for eNMPC via differentiable simulation and optimization",
  "modified_abstract": "Drawing upon recent advances in reinforcement learning (RL), particularly those demonstrating the efficacy of Kullback-Leibler divergence and entropy regularization for achieving near minimax-optimal sample complexities in model-free RL, our work introduces a novel approach for end-to-end learning of Koopman surrogate models aimed at optimizing performance in control applications. Unlike existing techniques that leverage standard RL algorithms, our proposed method capitalizes on the differentiability of environments underpinned by mechanistic simulation models, thus facilitating the generation of policy updates from small sample sizes through variance-reduction techniques. This work not only advocates for the incorporation of generative model training as part of the policy improvement process but also aligns with the substantial need for efficient learning algorithms capable of managing the challenges posed by small-sample complexities. By conducting comparative analyses against other controller types and training algorithm combinations in a well-established eNMPC case study, our method is shown to outperform existing solutions, marking a significant step towards developing more effective controllers through the application of dynamic surrogate models and optimization descent methods.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Pierre_MENARD1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14421v1",
  "title": "DP-RDM: Adapting Diffusion Models to Private Domains Without Fine-Tuning",
  "modified_abstract": "Drawing inspiration from the intensive exploration of adversarial methods in text-to-image synthesis, which highlighted the perpetual challenges of semantic alignment and visual realism in generative models, we introduce a novel approach to address sample-level memorization in text-to-image diffusion models. Recognizing the potential risks of reproducing near-perfect replicas of training images, we develop the first differentially private (DP) retrieval-augmented generation algorithm, capable of generating high-quality image samples while providing provable privacy guarantees. By leveraging a text-to-image diffusion model trained with minimal supervision on a public dataset, and a DP retrieval mechanism augmenting text prompts with privately retrieved samples, our differentially private retrieval-augmented diffusion model (DP-RDM) showcases adaptability to new domains without the need for fine-tuning or additional architectural adjustments. This method employs cutting-edge synthesis techniques and networks to produce superior quality, high-resolution image samples under rigorous training and DP assurances. Our empirical assessments, including synthesizing images that realize a $3.5$ point FID improvement over public-only retrieval methods with up to $10,000$ queries on the MS-COCO dataset, affirm DP-RDM's efficacy in balancing privacy considerations with generative performance, establishing a new benchmark for privacy-preserving text-to-image generation.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Stanislav_Frolov2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14413v1",
  "title": "Model Uncertainty in Evolutionary Optimization and Bayesian Optimization: A Comparative Analysis",
  "modified_abstract": "In a world where computational simulation demands are ever-increasing, this paper draws inspiration from critical advances in machine learning techniques, including the study of Determinantal Point Processes (DPPs), to advance our understanding of black-box optimization issues commonly encountered in real-world applications. By focusing on Bayesian Optimization (BO) and Surrogate-Assisted Evolutionary Algorithm (SAEA) \n\u2014 two pivotal gradient-free optimization techniques designed to navigate the challenges posed by black-box optimization through surrogate models \u2014 our work aims to provide a detailed comparative analysis on how each methodology incorporates model uncertainty and the consequential effects of probabilistic inaccuracies on the algorithms' performances. Additionally, we propose a novel model-assisted strategy that generates offspring from unevaluated solutions, harnessing evolutionary algorithms' population-based search traits to ameliorate the efficacy of model-assisted optimization for various machine tasks. Experimental validations reveal that our approach significantly surpasses conventional Bayesian optimization algorithms, delivering superior performance in both accuracy and efficiency for machine tasks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Victor-Emmanuel_Brunel1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14410v1",
  "title": "GLC++: Source-Free Universal Domain Adaptation through Global-Local Clustering and Contrastive Affinity Learning",
  "modified_abstract": "Our study is motivated by recent advances in domain adaptation tasks, including the exploration of generalizability across domains in scenarios characterized by covariate and category shifts. Drawing inspiration from pioneering works that delve into addressing the challenges of generalized multi-source domain adaptation through innovative approaches such as variational domain disentanglement, we propose a novel framework for Source-Free Universal Domain Adaptation (SF-UniDA). This approach, named GLC++, focuses on accurately classifying \"known\" data into common categories while effectively segregating it from target-specific \"unknown\" data, observable across different domains, without requiring access to source domain data in any online or offline setting. Through the introduction of a Global and Local Clustering (GLC) technique, which combines an adaptive one-vs-all global clustering algorithm with a local k-NN clustering strategy, we aim to mitigate negative transfer and improve the independence and handling of \"unknown\" categories across multiple source domains. GLC++ further enhances this architecture by incorporating a contrastive affinity learning strategy, significantly elevating the performance in tackling both closed-set scenarios and the more challenging open-partial-set scenarios with unlabeled datasets. We provide a comprehensive evaluation across multiple benchmarks, demonstrating that GLC++ not only improves upon GLC's capabilities in novel category clustering but also extends the utility of existing methodologies, marking a significant advancement in the field of domain adaptation.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yadan_Luo1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14404v1",
  "title": "Physics-Informed Diffusion Models",
  "modified_abstract": "Building on the momentum of recent advances in generative models, particularly denoising diffusion models, this work is inspired by foundational efforts in reformulating complex equations and problems in applied sciences, such as the novel approaches to solving Fredholm Integral Equations using variational principles and Wasserstein gradient flows. Our study embarks on leveraging these dynamic advancements to construct a framework that informs denoising diffusion models about underlying physical and mathematical constraints during the training process, including those described by particle dynamics, density considerations, and fundamental equations in physics. This work aims to enhance the quality and accuracy of the generated data by ensuring it adheres to pre-defined physical laws and constraints, marking a significant improvement over traditional methods in terms of alignment with imposed conditions without compromising the inference speed. Furthermore, enforcing these constraints during model training introduces a form of natural regularization that guards against overfitting and uses estimators to refine the accuracy of the generative process, contributing valuable insights into the balance between generative model performance and adherence to scientific principles within the applied sciences. Our framework demonstrates ease of implementation and offers wide applicability across various types of constraints, including both equality and inequality constraints, alongside auxiliary optimization objectives such as deconvolution, ensuring the produced outputs are of the highest fidelity and precision.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Adam_Michael_Johansen1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14398v1",
  "title": "Regularized Adaptive Momentum Dual Averaging with an Efficient Inexact Subproblem Solver for Training Structured Neural Network",
  "modified_abstract": "Inspired by pressing challenges in training advanced neural network structures and the limitations of current adaptive methods, we propose the Regularized Adaptive Momentum Dual Averaging (RAMDA) algorithm for efficaciously training structured neural networks. This initiative builds upon the rich landscape of previous works, particularly in the domain of escaping the Neural Tangent Kernel (NTK) regime and efficiently learning structured, such as sparse, functional representations. RAMDA addresses the complexities of the subproblem computation inherent to adaptive methods through the introduction of a nonsmooth regularizer and a diagonal preconditioner, lacking a general closed-form solution. Employing a deep learning framework, RAMDA integrates gradient descent-based optimization which, strengthened by a provable inexactness condition and proposing an accompanying efficient solver, overcomes practical implementation hurdles while ensuring convergence fidelity to exact methodologies. By formulating precise conditions for gradient descent, RAMDA overcomes the complexity of implementation while ensuring fidelity to exact methodologies. Leveraging manifold identification principles from variational analysis confirms that RAMDA's iterates not only achieve, but excel in capturing the intended structural integrity at asymptotic convergence points. The algorithm\u2019s capability to secure optimally structured deep learning models at local convergence zones, underpinned by its provable mechanisms, distinguishes it as a pioneering solution in the realm of regularized adaptive methods with reduced complexity. Empirical validation across a spectrum of large-scale tasks in computer vision, language modeling, and speech recognition corroborates RAMDA\u2019s superior efficiency and effectiveness over contemporary structured neural network training approaches.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Eshaan_Nichani1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14392v1",
  "title": "A Bag of Tricks for Few-Shot Class-Incremental Learning",
  "modified_abstract": "Drawing inspiration from the significant advances in continual learning, particularly the shift towards tackling the challenges of semi-supervised scenarios where not all data samples are labeled, our research introduces a comprehensive framework designed for few-shot class-incremental learning (FSCIL). FSCIL represents a form of continual learning that demands a delicate balance between stability, to preserve proficiency in previously learned tasks, and adaptability, to efficiently incorporate new tasks with limited samples. We propose a 'bag of tricks' comprising eight seminal techniques categorized into stability tricks, adaptability tricks, and training tricks to address these demands within a unified theoretical and experimental framework. Our approach aims to mitigate the forgetting of prior knowledge while facilitating the assimilation of new classes by strategies like nearest-neighbor search, enhancing overall performance without compromising either stability or adaptability across continual tasks. Strategies are designed to seamlessly integrate into the learning process, offering progressive gains in both labeled and unlabeled scenarios. We provide extensive empirical evidence of our framework's effectiveness across three benchmark datasets: CIFAR-100, CUB-200, and miniImageNet, where our method sets a new benchmark in the field. Through our detailed analysis, we demonstrate significant improvements in both stability and adaptability, establishing a robust baseline for future investigations into FSCIL.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Enrico_Fini1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14385v1",
  "title": "Estimating Causal Effects with Double Machine Learning -- A Method Evaluation",
  "modified_abstract": "Motivated by the pressing need to understand and articulate the causal relationships in various systems using observational data, and inspired by innovative mechanisms such as Causal Dependence Plots which illuminate causal interactions without traditional assumptions, our research provides a comprehensive examination of \"double/debiased machine learning\" (DML) as a potent tool for causal effect estimation. We evaluate DML against classical statistical methods through rigorous experiments and simulations, and apply it to real-world data to assess its ability in adjusting for nonlinear confounding relationships that traditional methods struggle with. Our findings highlight DML's potential to break from conventional constraints while still being anchored on the foundational causal assumptions, with a particular focus on the role of dependence in the estimation process. Application to the effects of air pollution on housing prices reveals that DML frequently yields higher estimates than less flexible approaches, underscoring the importance of choosing appropriate machine learning algorithms within the DML framework and considering the modular nature of such methods. Through intelligent analysis and careful thinking in selecting the right predictors for inclusion, the study culminates in practical recommendations for employing DML, positioning it as an invaluable asset in the modern researcher's toolkit for estimating causal effects with enhanced precision. Enhanced with elements from Explainable AI (XAI) to sustain our understanding, the research encourages further exploration into how DML can be effectively harnessed.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Joshua_R._Loftus1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14379v1",
  "title": "Tensor network compressibility of convolutional models",
  "modified_abstract": "In the context of evolving convolutional neural network (CNN) architectures and the quest for efficiency without sacrificing accuracy, this work is inspired by the intriguing findings around neural collapse (NC) in deep learning, which investigates the phenomenon where deep representations exhibit reduced intra-class variability while maintaining distinct inter-class separations. The concept of NC provides a theoretical backdrop for our exploration into tensorization as a method for compressing CNNs. Our study assesses the impact of truncating convolution kernels in dense CNNs, focusing on both a vanilla four-layer CNN and a large ResNet-50 model, pre-trained on CIFAR-10 and CIFAR-100 datasets. The process of pretraining classifiers on these datasets significantly informs our analysis, highlighting the importance of pretraining tasks that enable neural networks to efficiently transfer learned representations. Our findings reveal that tensorization, through correlation compression, seems to harness an intrinsic learning ability of CNNs to maintain classification accuracy despite significant reductions in kernel size. Moreover, we observe that aggressively truncated models often regain their original performance levels after minimal fine-tuning, indicating a resilience in the information encoding process that may not significantly alter the optimization landscape. These insights contribute to a broader understanding of how to effectively implement tensorization in CNN models, supporting the goal of achieving optimal performance with reduced model complexity.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jinxin_Zhou2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14377v1",
  "title": "Knowledge-Enhanced Recommendation with User-Centric Subgraph Network",
  "modified_abstract": "Driven by the advancement and the challenges highlighted in previous works on learning scalable structural representations for link prediction, our research introduces the Knowledge-enhanced User-Centric subgraph Network (KUCNet) to address the inherent limitations of classical recommendation systems and most Knowledge Graph (KG)-based methods. The use of user-item interaction matrices in traditional systems cannot efficiently handle scenarios with sparse data for new items, and existing KG approaches often fail to offer personalized recommendations due to their reliance on node embeddings. KUCNet leverages a subgraph learning strategy, implemented through a graph neural network (GNN), constructing unique user-item (U-I) subgraphs that encapsulate both the historical interactions and the extensive side information offered by KGs, thereby augmenting the representation capabilities. An attention-based GNN architecture is meticulously designed to encode these subgraphs into precise recommendations through a message-passing mechanism, thereby enhancing the efficacy and interpretability of recommendations. Furthermore, we innovate a pruned user-centric computation graph, enabling concurrent processing of multiple U-I subgraphs while ensuring computational efficiency and effective link prediction through Personalized PageRank. This approach not only significantly improves the recommendation performance for newly introduced items but also showcases pronounced superiority over the latest KG-based and collaborative filtering (CF) methods through rigorous experimental validation on various benchmarks. Additionally, our method demonstrates latent inference capabilities, extracting meaningful patterns from sparse data to refine recommendations, making it provably effective in sparse scenarios.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Haoteng_Yin1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14371v1",
  "title": "Loop Improvement: An Efficient Approach for Extracting Shared Features from Heterogeneous Data without Central Server",
  "modified_abstract": "Inspired by the advancing methodologies in online continual learning, our work introduces \"Loop Improvement\" (LI), a novel approach aimed at enhancing feature extraction from heterogeneous data sets in federated learning settings without the need for a central server or data interchange among participants. Our method leverages the separation of shared and personalized parameters\u2014an idea also pertinent in multi-task learning\u2014to significantly enhance performance in environments characterized by data heterogeneity and stream data processing. LI's experimental analysis showcases its capability to outperform the FedALA algorithm in terms of accuracy across various personalized federated learning scenarios, setting a new benchmark for this field. Moreover, the adaptability of LI extends to multi-task learning, simplifying the extraction of shared features across tasks without the necessity for simultaneous training while addressing the critical issue of forgetting in a traditional learning environment. This not only boosts the performance of individual tasks, but also brings accuracy levels in line with conventional multi-task learning methods, including those that require handling of periodic tasks. LI's unique integration of a loop topology with layer-wise and end-to-end training demonstrates compatibility with diverse neural network models, marking a significant step forward in addressing the online boundary-free continual learning challenges, particularly in learning without explicit task boundaries. The paper is rich in theoretical insights into LI's efficiency and potential wide-ranging applications.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Hyunseo_Koh1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14359v1",
  "title": "Varroa destructor detection on honey bees using hyperspectral imagery",
  "modified_abstract": "Building upon the approaches in recent semi-supervised object recognition and detection technologies that leverage multi-modal data fusion, such as the groundbreaking work in DetMatch, our study explores the use of hyperspectral (HS) imagery for agricultural applications, particularly for detecting the parasitic Varroa destructor mites on the body of western honey bee Apis mellifera. These technologies have highlighted the potential of advanced spectral processing and data fusion to unlock the rich information contained within HS images, with advantages such as higher spectral resolution and deeper semantics analysis for precise object identification. Our methodology, rooted in multivariate statistics and leveraging semi-supervised learning techniques, incorporates both unsupervised (K-means++) and recent supervised (Kernel Flows - Partial Least-Squares, KF-PLS) methods for the precise identification of these parasites. Furthermore, acknowledging the trends towards custom-band multispectral cameras in precision agriculture, our research also proposes a strategy for the selection of specific spectral bands that would enable effective bee-mite differentiation, potentially implementable in a custom-band camera setup. A real-case dataset validates our findings, showing that as few as four spectral bands can achieve accurate parasite identification. This advancement not only contributes to the field of hyperspectral imaging but also to the crucial task of bee hive monitoring for agriculture sustainability.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jinhyung_Park1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14358v1",
  "title": "Exploring the Potential of Large Language Models in Graph Generation",
  "modified_abstract": "The remarkable success of Large Language Models (LLMs) across a broad spectrum of applications, including pre-training techniques and fine-tuning methods that have significantly enhanced their performance in language learning tasks, has ignited interest in their capabilities beyond traditional learning tasks, specifically in graph generation, a domain that remains relatively untapped in existing literature. This study, LLM4GraphGen, is inspired by pioneering works that have leveraged LLMs for discriminative tasks in graph analysis, such as node classification, as well as efforts in bridging the gap between ML and domain-specific challenges like drug discovery through activity prediction models that can adapt to new tasks with minimal data. In this innovative exploration, we systematically investigate the abilities of LLMs to generate graphs with specific properties through a series of carefully designed tasks and extensive experiments, including zero-shot learning scenarios where LLMs must generate graphs without explicit prior examples. Our approach addresses fundamental questions about LLMs' comprehension of graph structural rules, their capability in capturing structural type distributions, and the effectiveness of domain knowledge incorporation for property-driven graph generation. The findings indicate that LLMs, especially GPT-4, possess promising preliminary capacities in graph generation tasks. Our study reveals that standard prompting methods, such as few-shot and chain-of-thought techniques, aimed at enhancing learning, do not uniformly improve performance. Moreover, LLMs demonstrate potential in crafting molecules with designated properties, laying the groundwork for future advancements in utilizing LLM-powered models for graph generation tasks in real-world applications.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Philipp_Seidl1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14356v1",
  "title": "DomainLab: A modular Python package for domain generalization in deep learning",
  "modified_abstract": "Motivated by the critical challenge of poor generalization performance in deep neural networks due to distribution shifts in unseen domains\u2014a problem exemplified by prior explorations such as the study on divisive feature normalization in image recognition\u2014DomainLab introduces a solution through a modular Python package. This package enables the training of user-specified neural networks with composable regularization loss terms, aiming to combine the strengths of various domain generalization methods efficiently. DomainLab's decoupled architecture facilitates seamless integration of neural networks with diverse regularization strategies, enhancing the system's receptive field and enabling the comprehension of non-linear manifold structures within data sets, thereby allowing for detailed customization through a single configuration file. Additionally, DomainLab provides robust benchmarking tools to assess neural networks' generalization capabilities on out-of-distribution data, with reduced computational overhead, and is compatible with both HPC clusters and standalone machines. With over 95 percent code coverage and comprehensive documentation, DomainLab is designed to be extensible by users while remaining closed to modification, ensuring reliability and ease of use in recognition tasks and beyond. The software is available under the MIT license, and further resources, including source code, tutorials, and documentation, can be accessed at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Michelle_Miller3",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14353v1",
  "title": "DaCapo: Accelerating Continuous Learning in Autonomous Systems for Video Analytics",
  "modified_abstract": "Motivated by previous successes in enhancing 3D visual perception for autonomous driving systems, such as the BEVFormer framework which improved autonomous driving perception tasks through spatial and temporal transformers, our work aims to address the computational and energy challenges that autonomous systems face in real-world deployments of deep neural network (DNN) video analytics. Autonomous systems, including self-driving vehicles, unmanned aerial vehicles (UAVs), and security robots, require efficient continuous learning mechanisms to adapt to changing environments and execute complex perception tasks with the help of multi-camera inputs. Existing continuous learning systems fall short in terms of compute efficiency for inference and labeling, reliance on power-intensive GPUs, and unsuitability for deployment in privacy-sensitive autonomous systems due to their centralized, remote server-based architecture. Incorporating a camera as part of its sensing module, DaCapo facilitates the usage of cross-attention mechanisms within its transformers, allowing for a more precise and nuanced understanding and analysis of the video feed from different views. Further, DaCapo accommodates complex query management to efficiently process multi-perspective video data. Our proposal, DaCapo, is a hardware-algorithm co-designed solution that enables performant and energy-efficient concurrent executions of inference, labeling, and training in autonomous systems. DaCapo comprises a spatially-partitionable and precision-flexible accelerator, and a spatiotemporal resource allocation algorithm\u2014in a nutshell, this innovative approach significantly reduces power consumption while achieving higher accuracy and precise estimation compared to state-of-the-art GPU-based continuous learning systems. Our evaluations demonstrate DaCapo's effectiveness, showing 6.5% and 5.5% higher accuracy than existing systems Ekya and EOMU, respectively, while consuming 254x less power.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Chonghao_Sima1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14340v1",
  "title": "Exploring Task Unification in Graph Representation Learning via Generative Approach",
  "modified_abstract": "In the realm of graph representation learning, the quest for a unified framework that can seamlessly adapt to a multitude of graph-related tasks has been significantly influenced by previous endeavors, including pioneering work in Bayesian graph contrastive learning. These pioneering methods have unlocked new pathways in self-supervised learning for graph-structured data, underscoring the importance of adaptability and efficacy across varied graph tasks, and have paved the way for advanced analytics in this field. Building upon this foundation, our work introduces GA^2E, a novel generative approach aiming at task unification across node-, edge-, graph-level tasks, and transfer learning while addressing the challenges of multi-stage training and task objective discrepancies prevalent in existing methods. GA^2E leverages an adversarially masked autoencoder, utilizing subgraphs as a meta-structure that remains constant throughout all task types and training stages, and incorporates augmentations to enhance the learning process and contrast mechanisms. Operating under a \"Generate then Discriminate\" paradigm, it uses masked encoders for input subgraph reconstruction, promoting similarity between the reconstructed and input subgraphs based on probability principles, and employs an auxiliary discriminator for authenticity verification of the constructed subgraphs. This mechanism ensures robust graph representation via adversarial training, presenting a novel pathway in graph representation learning capable of generalizing across various graph tasks efficiently thanks to its self-supervised learning strategy. We demonstrate the effectiveness of our approach through extensive experiments on 21 datasets covering four distinct graph tasks, thereby affirming the potential of GA^2E in advancing the generative model's applications in graph representation learning and analytics.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Arman_Hasanzadeh1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14339v1",
  "title": "$\\nabla \\tau$: Gradient-based and Task-Agnostic machine Unlearning",
  "modified_abstract": "Inspired by a landscape of existing work on model adaptation and optimization, such as techniques for merging models through Fisher-weighted averaging and transfer learning, our study addresses the imperative of Machine Unlearning\u2014the process by which a machine learning model selectively forgets the influence of specific training data sets. This is especially relevant in light of recent data protection regulations requiring models to have the capability to efficiently remove the influence of certain data upon request. Our proposed framework, Gradient-based and Task-Agnostic machine Unlearning ($\\nabla \\tau$), overcomes the limitations of existing unlearning methodologies by enabling the efficient elimination of large portions of training data, up to 30%, without the need to adjust hyperparameters. It operates through a novel optimization technique that applies adaptive gradient ascent to the data segments to be forgotten and standard gradient descent for the remaining data, thus leveraging the merging of differentiated learning paths with an emphasis on modified averaging techniques in the model's architecture. $\\nabla \\tau$ proves versatile across various domains, including images and text, and leverages intermediate-task capabilities to enhance its adaptability. This supports diverse unlearning tasks, such as subset forgetting and class removal, by adaptively adjusting the model's posteriors. We validate our approach against well-established Membership Inference Attack metrics, showcasing significant improvements in performance over current state-of-the-art methods, enhancing performance by up to 10% without sacrificing the original model's accuracy. Our findings not only contribute to the theoretical understanding of Machine Unlearning but also provide a practical framework that surpasses the efficacy and efficiency of retraining models from scratch.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Michael_S_Matena1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14332v1",
  "title": "A Differentially Private Clustering Algorithm for Well-Clustered Graphs",
  "modified_abstract": "Informed by foundational principles highlighted in recent developments around differentially private algorithms and advanced theoretical analyses, such as those presented in works focused on the efficient recovery of stochastic block models and minimization problems within graph and coupling contexts, this paper dives into the optimization of differentially private (DP) algorithms for well-clustered graphs. Well-clustered graphs, characterized by their partitionable vertex set into groups marked by high inner conductance and minimal outer conductance, are critical benchmarks in spectral clustering theory and approximation proving. We put forth an efficient ($\\epsilon$, $\\delta$)-DP algorithm, innovatively crafted for graphs with multiple nearly-balanced clusters, achieving a misclassification ratio on par with leading non-private algorithms and demonstrating an innovative approximation technique that optimizes performance within upper-bounded accuracy constraints. Notably, our approach leverages distributions to model cluster characteristics, incorporating entropy measurements to provide a more nuanced understanding of cluster distribution. Through rigorous experimentation on datasets with established ground truth clusters, we showcase instances where our algorithm surpasses the performance of non-private algorithms, doubly emphasizing the superiority of our approach in proving the viability of DP methods in graph clustering. The use of coupling in our theoretical analysis further enriches our understanding of the algorithm's efficacy in privacy-preserving graph clustering, offering insights into the distributions underlying graph structure and clustering. This initiative not only pushes the boundaries of what can be achieved with differentially private clustering algorithms but also illuminates the path for future research endeavors aiming to incorporate entropy and bits of information for enhancing algorithmic precision and efficiency.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Spencer_Compton1",
  "manipulated_ranking": 4,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14327v1",
  "title": "Investigating the validity of structure learning algorithms in identifying risk factors for intervention in patients with diabetes",
  "modified_abstract": "Drawing on the foundational work of using machine learning to enhance clinical assessments and risk prediction in chronic diseases like type-2 diabetes, our study embarks on a comprehensive exploration of the utility of various structure learning algorithms for understanding diabetes progression. Diabetes, a pervasive and enduring health challenge, imposes significant global implications on health, financial healthcare systems, and societal well-being. Through applying these algorithms to diabetes datasets and subsequently converting their outputs into Causal Bayesian Networks (CBNs), our study engages in predictive analysis and evaluates the consistency of hypothetical intervention effects within a case-specific context. In doing so, we underscore the pivotal role of algorithm selection and demonstrate a novel model-averaging approach to distill a unified causal framework from diverse algorithmic predictions. Our post-hoc assessment further enriches our understanding, facilitating a nuanced comparison of the consistency and impact of interventions predicted by our model versus those implemented in clinical practice, providing valuable explanations of discrepancies. Further, we compare these findings against expert-generated risk factor classifications, offering a multidimensional evaluation of model accuracy and reliability. The resultant causal model and datasets emerge as critical tools for healthcare practitioners, presenting an encompassing view of interrelated risk factors and their potential impact on intervention strategies for patients with type-2 diabetes. Thus, our research extends the academic and practical discourse on diabetes management by furnishing actionable insights and methodologies for effective healthcare intervention and risk management.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Mohamed_Ghalwash1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14324v1",
  "title": "Neural Network-Based Processing and Reconstruction of Compromised Biophotonic Image Data",
  "modified_abstract": "In the emerging field of bioimaging, the integration of deep learning techniques with biophotonic setups represents a novel frontier where compromising certain measurement metrics for superior bioimaging tools\u2014cost, speed, and form-factor\u2014has become a strategic consideration. This initiative is catalyzed by notable research endeavors, such as the development of advanced image restoration techniques like image demoireing, which effectively handle degraded images by utilizing deep learning for artifact removal, color restoration, and tone adjustment. Capitalizing on such precedence, our work aims to provide an in-depth review of how deliberately compromising metrics such as point spread function, signal-to-noise ratio, sampling density, and pixel resolution in biophotonic setups, not only facilitates recovery through advanced model architectures but also enriches other parameters like field-of-view and space-bandwidth product by leveraging sophisticated model architectures. By examining diverse biophotonic methods accomplishing these strategies across a broad application spectrum, this article showcases the versatility and effectiveness of deep learning in managing compromised biophotonic data for specific tasks, focusing on attributes such as color fidelity, demoireing, and tone consistency. Ultimately, it offers perspectives on future developments in this rapidly evolving arena, encouraging innovative approaches to balancing hardware compromises with computational compensations.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Shanxin_Yuan2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14302v1",
  "title": "SpikingResformer: Bridging ResNet and Vision Transformer in Spiking Neural Networks",
  "modified_abstract": "Informed by groundbreaking developments in Artificial Neural Networks (ANNs), particularly the emergence of Vision Transformers (ViT) which have set new standards in image classification, object detection, and semantic image segmentation through innovations such as data augmentation and regularization strategies, this work introduces SpikingResformer. It is a novel paradigm that harmoniously integrates the self-attention mechanism and the transformer-based architecture with Spiking Neural Networks (SNNs), overcoming the prevailing limitations of spiking self-attention mechanisms, including scalability and local feature extraction bottlenecks. Leveraging the convolutional basis of ResNet and strategic data augmentation, SpikingResformer effectively melds the ResNet architecture's multi-stage feature extraction with the transformer's global context awareness in vision tasks, achieving unparalleled accuracy, efficiency, and parameter economy in SNNs within a constrained budget. Our training protocol for SpikingResformer-L, involving meticulous data augmentation and publicly available datasets, sets a new benchmark with a 79.40% top-1 accuracy on the ImageNet dataset utilizing just four time-steps, thereby marking a breakthrough achievement in the domain of SNNs.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Andreas_Peter_Steiner1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14297v1",
  "title": "Impact Assessment of Missing Data in Model Predictions for Earth Observation Applications",
  "modified_abstract": "Leveraging the foundational advancements highlighted in research on the generalizability and robustness of machine learning models, particularly through innovative measures such as leave-one-out conditional mutual information, our study extends these concepts to the domain of Earth Observation (EO). We explore the vulnerability of EO applications, which frequently rely on complex and heterogeneous data sources, to the interruption of data availability due to various obstacles, such as atmospheric noise, cloud cover, or satellite mission failures. Specifically, this work assesses the extent to which temporal and static EO data sources' absence impacts the predictive accuracy of machine learning models across a diverse set of four datasets, encompassing both classification and regression tasks. Through comparative analysis of different methodologies, employing techniques like cross-validation to evaluate the bounds of model generalization and optimization strategies to mitigate the adverse effects of missing data, we uncover a notable variance in resilience to data omission, with ensemble strategies demonstrating up to 100% robustness in prediction. Our findings illustrate the distinct challenges missing data scenarios pose, particularly emphasizing a greater adversarial effect on regression models compared to classification models, and identify tasks such as image-classification as being critically affected by individual data source loss.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Aditya_Golatkar1",
  "manipulated_ranking": 4,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14290v1",
  "title": "Exploring Green AI for Audio Deepfake Detection",
  "modified_abstract": "In an era where artificial intelligence (AI) models are both a technological boon and an environmental bane, our research draws inspiration from compelling works like 'Robustart' that benchmark the robustness of various architectures and training techniques against (adversarial and environmental perturbations. Our study specifically tackles the environmental impact of state-of-the-art audio deepfake detectors that, despite their impressive recognition performance, contribute significantly to carbon emissions due to the requirement for high-performance computing and extensive training. To address this sustainability challenge, we propose a novel, environmentally friendly framework for audio deepfake detection. Our framework leverages off-the-shelf self-supervised learning (SSL) models pre-trained and available in public repositories for benchmarking. Benchmarks in our experiments with the ASVspoof 2019 LA dataset achieved a 0.90% equal error rate (EER) with less than 1k trainable model parameters, challenging the common architectures in use today. This validates our benchmarking approach and design principles for sustainable AI in audio deepfake detection, emphasizing defenses against not only adversarial attacks but also against the high environmental costs of current methodologies. Unlike existing methods that rely heavily on fine-tuning SSL models with additional deep neural networks for downstream tasks, we explore the use of classical machine learning algorithms such as logistic regression and shallow neural networks for the design of defenses. These algorithms take advantage of the SSL embeddings extracted from the pre-trained model, displaying competitive results while significantly reducing the carbon footprint. We are committed to fostering further research in sustainable AI practices and will make our Python code publicly available following acceptance to ensure reproducibility.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ruihao_Gong1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14286v1",
  "title": "Assessing the Robustness of Spectral Clustering for Deep Speaker Diarization",
  "modified_abstract": "This study is motivated by recent advances and challenges in the field of Automatic Speech Recognition (ASR), particularly focusing on speaker diarization. While ASR systems, especially in distributed settings, have experienced notable improvements through techniques like gradient matching, access control, and network adjustments for speaker identification, the robustness and effectiveness of speaker diarization methods across diverse datasets remain underexplored. Particularly, the reliability of clustering speaker embeddings, a critical component in speaker diarization, has not been extensively studied, especially in scenarios where development and evaluation data originate from different domains or when dropout techniques are applied to enhance generalization. This paper aims to bridge this gap by conducting a comprehensive evaluation of spectral clustering in both same-domain and cross-domain speaker diarization contexts, with an emphasis on utterances variability and identity mismatches influencing speaker diarization performance. Through rigorous testing on established corpora such as AMI and DIHARD, we uncover how domain mismatches and identity discrepancies influence speaker diarization performance. Our studies of the gradients within the spectral clustering process and its architecture, along with the strategic use of dropout for model robustness, attribute a significant part of the observed variability to these factors. Our findings, which highlight the impact of domain-specific tuning parameters, speaker count estimation discrepancies, and the importance of training data quality, pave the way for further research into enhancing the robustness of speaker diarization systems.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Swaroop_Ramaswamy1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14282v1",
  "title": "How to be fair? A study of label and selection bias",
  "modified_abstract": "Our research is motivated by an intrinsic property of machine learning models: their performance and fairness are critically dependent on the quality and characteristics of the input data. Prior studies, such as those exploring new differentially private methods for estimating unseen aspects of data distributions, highlight the complexity and challenges in addressing biases within data and model predictions. Given this context, we aim to deepen the understanding of why, despite numerous bias mitigation techniques developed over the past decade, it is still not well understood under which conditions these methods are effective. Through a detailed analytical framework, we establish relationships between different types of data biases\u2014specifically label and selection bias\u2014and the effectiveness of various bias mitigation techniques categorized by the bias measure they aim to optimize. Our work elucidates the mechanisms by which these techniques can lead to fairer models by design, privately drawing upon and expanding the findings of previous research that hinted at the potential for bias mitigation techniques to improve model accuracy on unbiased data. Additionally, our theoretical insights, underpinned by discrete analysis and p-estimating methods, explain when minimizing fairness measures may not necessarily yield the fairest outcomes, therefore advancing our understanding of the intricate balance between bias, privacy, and the pursuit of fairness in machine learning models. We use sample distribution characteristics and estimating methods robustly to guide our analysis, ensuring our conclusions are grounded in practical and theoretical soundness.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ziteng_Sun1",
  "manipulated_ranking": 4,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14270v1",
  "title": "Scene-Graph ViT: End-to-End Open-Vocabulary Visual Relationship Detection",
  "modified_abstract": "Building upon the innovation curve set by recent works like Cost Aggregation Transformers (CATs), which brought to light the power of transformers in processing visual data for complex tasks such as finding dense correspondences in images, our research advances the frontier of visual relationship detection. Visual relationship detection, a vital step towards comprehensive scene understanding, has traditionally been challenged by the necessity of incorporating separate modules or decoders, increasing complexity and obstructing seamless end-to-end training. Addressing these limitations, we introduce a novel, decoder-free architecture leveraging a Transformer-based image encoder to represent objects as tokens and implicitly model their relationships through self-attention, thus simplifying the architecture while enhancing its performance. Through the introduction of a tailored attention mechanism for selecting object pairs with potential relationships and a single-stage training approach utilizing mixed object and relationship data, our model establishes new benchmarks in open-vocabulary visual relationship detection on the Visual Genome and the expansive GQA benchmark, achieving remarkable performance improvements at real-time inference speeds. Matching and correspondences analyses, including zero-shot capabilities and extensive ablation studies, alongside examples from real-world applications, underscore the practical effectiveness and versatility of our model.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sangryul_Jeon1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14262v1",
  "title": "Diffusion Models with Ensembled Structure-Based Anomaly Scoring for Unsupervised Anomaly Detection",
  "modified_abstract": "In the realm of medical image analysis, while supervised deep learning techniques have shown remarkable effectiveness, their dependency on extensive, annotated datasets poses particular challenges, especially in the context of rare diseases. Drawing inspiration from previous works that sought to leverage the rich informational canvas of functional connectomics for neuropsychiatric disease classification, our study turns towards unsupervised anomaly detection (UAD) as a robust alternative for pathology segmentation. By utilizing only healthy data as the training set, UAD circumvents the need for annotated datasets. Our research critically addresses the limitations of existing UAD anomaly scoring functions that primarily focus on intensity, neglecting vital structural differences which play a crucial role in segmentation performance. By integrating Structural Similarity (SSIM) into our approach, we not only capture intensity but also structural disparities, presenting a robust contrast to classical $l1$ error metrics. Nonetheless, our analysis reveals that no single kernel size for SSIM calculation universally suits all pathologies. Consequently, we propose an innovative adaptive ensembling strategy that accommodates various kernel sizes, thereby presenting a more pathology-agnostic scoring mechanism. Our empirical validation, especially in the context of brain MRI anomaly detection and neuroimaging, emphasizes the efficacy of this ensembling strategy in enhancing the performance of Diffusion Models (DMs), and reducing the sensitivity to kernel sizes across different pathologies, showcasing significant potential for broader application in medical imaging through transfer learning. The study exemplifies how unsupervised learning methodologies, enriched with a transfer learning aspect, offer compelling avenues for disease detection and classification in the absence of labeled data.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Juntang_Zhuang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14255v1",
  "title": "ERD: A Framework for Improving LLM Reasoning for Cognitive Distortion Classification",
  "modified_abstract": "Drawing inspiration from state-of-the-art research that seamlessly integrates factual information and social content within persuasive dialogue systems, we introduce ERD, a novel framework aimed at enhancing Large Language Models (LLMs) with the critical application of cognitive distortion classification in psychotherapy. With an increasing focus on the role of LLMs in making psychotherapy more accessible, our approach emphasizes the need for accurately recognizing cognitive distortions from interviewee utterances, a key component in treatments like cognitive behavioral therapy. The ERD framework boosts the performance of LLM-based cognitive distortion classification through additional modules designed for extracting relevant parts and engaging in multi-agent debate to refine reasoning processes with a precision akin to 12th-century scholars. Experimental evaluation on a public dataset demonstrates that ERD not only enhances the multi-class F1 score but also improves binary specificity by effectively reducing the baseline method's high false positive rate, particularly when the summaries of multi-agent debates are provided to the LLMs.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Saurav_Sahay1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14252v1",
  "title": "LayoutLLM: Large Language Model Instruction Tuning for Visually Rich Document Understanding",
  "modified_abstract": "Inspired by significant strides in document image understanding and recent advancements in generative adversarial networks for text-to-image generation and synthesis, our research introduces LayoutLLM, a novel approach to Visually Rich Document Understanding (VRDU). Traditional VRDU tasks, like document image classification and information extraction, rely heavily on models trained to recognize and interpret complex combinations of images, text, and layout structures\u2014objects within a document. However, these methods often necessitate task and dataset-specific fine-tuning, rendering them costly in terms of training and operational resources. To address these challenges, LayoutLLM combines the comprehensive language understanding capabilities of large-scale language models (LLMs) with the nuanced recognition of document structures by instructing fine-tuning with multimodal datasets. This synthesis allows for a more flexible, cost-effective solution for document analysis, capable of operating across different tasks within a unified model framework. Our experimental evaluation results showcase notable performance improvements in synthesizing textual content and evaluating over established baselines across several VRDU tasks, hereby emphasizing the efficacy and versatility of LayoutLLM in enhancing the understanding of imaged documents.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Tobias_Hinz1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14244v1",
  "title": "Isotropic Gaussian Splatting for Real-Time Radiance Field Rendering",
  "modified_abstract": "Drawing inspiration from recent advancements in the domain of high-performance image processing and scene representation, such as the development of novel networks for panchromatic and multispectral image fusion, this paper explores the application of isotropic Gaussian splatting for real-time radiance field rendering in diverse scenes. The utilization of anisotropic Gaussian kernels in 3D Gaussian splatting has shown significant merits in training and rendering quality yet poses computational challenges, including the complexities of kernel manipulation and efficient mapping of 3D data onto 2D projections. We propose a shift towards isotropic Gaussian kernels to circumvent these challenges, aiming for computational efficiency without compromising on the accuracy of geometry representation. Our extensive experiments reveal that the proposed method enhances computational performance by approximately 100X, maintaining high fidelity in geometry representation across various applications such as 3D reconstruction, view synthesis, and dynamic object modeling. The integration of techniques commonly used in pan-sharpening and multi-spectral data fusion further illustrates the robustness and adaptability of our approach in handling diverse image datasets, highlighting the efficacy of using isotropic kernels for both pan-sharpening and precise geometry mapping in real-time applications.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Chongyi_Li2",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14236v1",
  "title": "A Unified Framework for Model Editing",
  "modified_abstract": "Drawing inspiration from the innovative approaches in 'Deletion Robust Submodular Maximization over Matroids' and similar works, our research addresses the increasingly crucial field of model editing. We focus on the intricacies of updating the embedded knowledge within models, a task that has gained prominence as machine learning models become more pervasive and complex. Our paper introduces a pioneering unifying framework, encompassing the notable 'locate-and-edit' techniques such as ROME and MEMIT, under the concept of the 'preservation-memorization' objective. This objective targets the dual goals of preserving certain representations while incorporating new factual information, and tackles approximation challenges in the optimization process. This includes ROME's optimization via an equality constraint and MEMIT's utilization of a least-square constraint for batch edits and multi-layer modifications in response to deletion scenarios and adversary challenges. We dissect and realign the various elements of these methods, proposing EMMET - an Equality-constrained Mass Model Editing algorithm for Transformers that is adept in streaming data environments. EMMET extends the capabilities of existing techniques, offering a closed-form solution for batch edits up to 256 small instances, and delves into the stabilization challenges inherent in such a sophisticated approach amid streaming data environments. Through a comprehensive framework, we aim to streamline and demystify the processes behind model editing, furnishing a solid foundation for future exploration and innovation in this evolving discipline. Our approach also highlights the importance of maintaining a high rank in model precision to ensure robustness against approximation errors and external perturbations.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ashkan_Norouzi-Fard2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14235v1",
  "title": "RG-CAT: Detection Pipeline and Catalogue of Radio Galaxies in the EMU Pilot Survey",
  "modified_abstract": "In our work, we draw inspiration from advances and challenges identified in the field of image classification, particularly in understanding and mitigating model overinterpretation\u2014a phenomenon where convolutional neural networks make confident decisions without reliance on semantically meaningful features. These insights guide our development of a sophisticated source detection and catalogue construction pipelines aimed at compiling the first catalogue of radio galaxies from the 270 $\\rm deg^2$ pilot survey of the Evolutionary Map of the Universe (EMU-PS) carried out with the Australian Square Kilometre Array Pathfinder (ASKAP) telescope. Utilizing the Gal-DINO computer-vision networks for predicting radio morphology categories, bounding boxes for radio sources, and their potential infrared host positions, our system is meticulously trained on a dataset comprising about 5,000 visually inspected radio galaxies by humans along with their infrared hosts covering both compact and extended radio morphologies. Our findings include a high Intersection over Union (IoU) score larger than 0.5 for 99% of the radio sources and an accuracy of 98% for predicted host positions being within $3^{\\prime \\prime}$ of the actual infrared hosts in our evaluation set, showcasing the system's class-evidence strength and training effectiveness. The catalogue construction pipeline leverages the Selavy source finder algorithm to sift through and prioritize radio and infrared image cutouts, culminating in a comprehensive catalogue of 211,625 radio sources, differentiated into compact, unresolved and extended morphologies, alongside a robust cross-matching with infrared and optical catalogues to furnish infrared cross-matches for 73% and photometric redshifts for 36% of the catalogued radio galaxies.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Siddhartha_Jain1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14233v1",
  "title": "SoftPatch: Unsupervised Anomaly Detection with Noisy Data",
  "modified_abstract": "In the context of unsupervised anomaly detection (AD), where methods like Persistent Homology have demonstrated significant success in complex shape analysis and data transformations, we recognize a critical, yet underexplored challenge: the performance of mainstream unsupervised AD algorithms degrades in practical applications due to the often noisy nature of real-world data. Addressing this gap, our study introduces SoftPatch, a memory-based unsupervised AD method designed specifically for environments with label-level noise in image sensory data. By leveraging noise discriminators for outlier score generation at the patch level and incorporating shape and topological analysis for anomaly classification, SoftPatch effectively denoises data before constructing the coreset, which facilitates a softer anomaly detection boundary, enabling an enhanced understanding of anomalous and normal classifications within the data. Through comprehensive testing and training in various noise situations, SoftPatch not only exhibits superior performance on the MVTecAD and BTAD benchmarks compared to existing state-of-the-art AD methods but also maintains comparable efficacy in noise-free settings, showcasing its adaptability to diverse AD tasks. Our approach marks a significant step towards mitigating the challenge of training with noisy data in anomaly detection, paving the way for more robust AD applications in real-world contexts.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Renata_Turkes1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14232v1",
  "title": "Contrastive Balancing Representation Learning for Heterogeneous Dose-Response Curves Estimation",
  "modified_abstract": "Inspired by the demanding challenges and methodological advancements in various domains including healthcare, where deep learning has been employed to phenotypically categorize healthcare providers, procedures, and prescriptions at scale, our work presents a novel approach for estimating individuals' potential response to varying treatment doses\u2014a critical decision-making tool in precision medicine and management science. Recognizing the limitations of recent studies which predict counterfactual outcomes through covariate representations independent of the treatment variable, and acknowledging the omission of vital covariate information for counterfactual prediction in scenarios with continuous treatment variables, we theoretically explicate the importance of balancing and prognostic representations for the unbiased estimation of heterogeneous dose-response curves. These representations are specially constrained to adhere to the conditional independence between covariates and both the treatment variables and potential responses. We introduce a new Contrastive Balancing Representation Learning Network (CRNet), employing a partial distance measure for estimating these curves while preserving the continuity of treatments and deep phenotyping processes. Extensive experiments on both synthetic and real-world datasets showcase our method's substantial superiority over existing models, marking a significant step forward in the field of precision medicine and management science analytics.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jeremy_Chen_Weiss1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14228v1",
  "title": "Recovering Latent Confounders from High-dimensional Proxy Variables",
  "modified_abstract": "Inspired by the significant challenge of estimating causal effects accurately in the presence of latent confounders\u2014a problem fundamental to a variety of domains including economics, public health, and climate science\u2014our work builds upon and extends the methodologies of previous research on Bayesian consensus estimation and forecasting for addressing bias and miscalibration within prediction models. We present a novel Proxy Confounder Factorization (PCF) framework that overcomes limitations of prior approaches by facilitating the recovery of latent confounders from high-dimensional, mixed proxy variables, including sensors data, for continuous treatment effect estimation. Our methodology, acting as an advanced estimator, is validated using both synthetic datasets, where it achieves high correlation with the latent confounder and low absolute error in causal effect estimation, and real-world climate data, successfully identifying components explaining a significant variance in precipitation patterns. Incorporating elements from forecasters and sensors, and employing techniques for aggregating evidence, our two-step implementation using Independent Component Analysis (ICA-PCF) and an end-to-end Gradient Descent (GD-PCF) approach, this study offers a robust foundation for future research in confounder recovery across diverse fields dealing with complex high-dimensional data. Code for our implementations and experiments is available at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Robert_Tillman1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14200v1",
  "title": "Debiasing surgeon: fantastic weights and how to find them",
  "modified_abstract": "In the landscape of machine learning, the challenge of algorithmic biases has become increasingly prominent, prompting a quest for effective debiasing techniques. Drawing inspiration from seminal works that have explored the encoding of inductive biases into neural models through mechanisms such as auxiliary objectives and the concept of tailoring models to specific instances, our research proposes a novel approach to mitigate algorithmic bias. We ponder the essential question: is the additional complexity introduced by sophisticated debiasing mechanisms truly necessary, or do vanilla-trained models inherently possess ``unbiased sub-networks'' capable of sidestepping these biases without the need for extra training? Through the lens of generalization and attention to the meta-tailoring of models, our investigation reveals the existence of such sub-networks within standard models and demonstrates their capability to avoid learning specific biases, thereby offering a simpler alternative to conventional debiasing strategies. This finding not only sheds light on the innate potential of neural network architectures to resist bias through both inductive reasoning and transductive principles but also suggests that architectural modifications such as loss function adjustment and attention mechanisms can serve as a robust countermeasure to the pervasive issue of bias in deep learning.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ferran_Alet1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14183v1",
  "title": "OTSeg: Multi-prompt Sinkhorn Attention for Zero-Shot Semantic Segmentation",
  "modified_abstract": "Building on the transformative potential of advanced methods in unsupervised visual representation learning, which has seen remarkable levels of performance in tasks demanding dense pixel predictions, our work introduces OTSeg. This novel approach aims to leverage the recent success of CLIP in demonstrating promising results for zero-shot semantic segmentation by effectively transferring multimodal knowledge to pixel-level classification. The existing methodologies, while groundbreaking, present challenges in closely aligning text embeddings with pixel embeddings. To overcome these limitations, OTSeg employs a novel multimodal attention mechanism, Multi-Prompts Sinkhorn (MPS), inspired by the Optimal Transport (OT) algorithm to allow multiple text prompts to selectively concentrate on various semantic features within image pixels, enhancing the networks' understanding of object relationships. Enhancing this further, we present the Multi-Prompts Sinkhorn Attention (MPSA), an inventive extension of MPS that supersedes traditional cross-attention mechanisms in the Transformer framework for multimodal settings, drawing inspiration from the success of Sinkformers in unimodal contexts. Our extensive experiments validate OTSeg's superior performance in learning, setting new benchmarks across three key datasets in Zero-Shot Semantic Segmentation (ZS3) tasks, thereby highlighting its significant advantages over current state-of-the-art approaches in the realm of large learning.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zhenda_Xie1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14151v1",
  "title": "Deep Learning for Trajectory Data Management and Mining: A Survey and Beyond",
  "modified_abstract": "In light of the compounding challenges faced by traditional methods in trajectory computing with respect to complex calculations, limited scalability, and inadequate adaptability, this paper draws inspiration from prior works that have sought to push the boundaries of data management and analytics in novel application domains such as urban forest monitoring, highlighting the necessity of advanced computational approaches to address real-world complexities. We present a comprehensive review of the development and recent advances in deep learning for trajectory computing (DL4Traj), including the large-scale training of models on large datasets. Starting with a definition of trajectory data, the paper provides an overview of widely-used deep learning models before systematically exploring their applications in trajectory data management (including pre-processing, storage, analysis, and visualization) and mining (covering trajectory-related forecasting, recommendation, classification, travel time estimation, anomaly detection, and mobility generation) across various modalities. In this journey, the use of high-resolution imagery for enhancing the granularity of analysis in forests is underscored. Specifically, benchmarks and testbeds in trajectory data analysis stand out as essential tools for evaluating the performance of deep learning models and ensuring their scalability and adaptability to diverse application scenarios. Particularly, we highlight the potential of computer vision techniques and recent advancements in Large Language Models (LLMs) to further enrich the trajectory computing field with their robust training capabilities. We also summarize application scenarios, public datasets, and toolkits, and conclude by outlining current research challenges and future directions. For ongoing access to relevant papers and resources, we have curated and will continue to update the DL4Traj Repo, which can be found at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sara_Beery1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14148v1",
  "title": "Efficient Video Diffusion Models via Content-Frame Motion-Latent Decomposition",
  "modified_abstract": "Informed by breakthroughs in self-supervised video pre-training, such as the revolutionary capabilities demonstrated by video masked autoencoders for data-efficient learning without relying on large-scale pre-training datasets, our research introduces an innovative approach to significantly enhance video generation efficiencies. Video diffusion models have become increasingly competent in generation quality but are hampered by substantial computational and memory demands due to the direct processing of high-dimensional videos. Addressing this challenging situation, we present the content-motion latent diffusion model (CMD), an ingenious extension of pre-trained image diffusion models tailored for video content generation. CMD employs an autoencoder that abstracts a video into a static content frame and a dynamic, low-dimensional motion latent representation\u2014distinctly capturing static sceneries and temporal movements while minimizing redundant data processing. By fine-tuning pre-trained image diffusion models for content frames and devising a novel lightweight diffusion model for motion representation, our method substantially improves generation quality and operational efficiency through superior reconstruction of video elements. For example, CMD outpaces previous models by producing 512x1024 resolution, 16-length videos 7.7 times faster\u2014in as rapid as 3.1 seconds\u2014and achieves a leading FVD score of 212.7 on the WebVid-10M dataset, surpassing the previous best by 27.3%. This advancement not only propels the quality and efficiency of video generation forward but also establishes a foundation for future exploration into content and motion latent space utilization, benefiting learners and researchers focused on masked modeling and deep dive self-supervised pre-training strategies.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zhan_Tong1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14140v1",
  "title": "Learning Decomposable and Debiased Representations via Attribute-Centric Information Bottlenecks",
  "modified_abstract": "Inspired by pioneering efforts in the realm of unsupervised and self-supervised learning, in particular the exploration into self-supervised set representation learning that sheds light on the importance of learning models without direct human supervision for robust performance across various tasks, our work presents a novel debiasing framework aimed at addressing the challenge of biased attributes that are spuriously correlated with target labels. Such biased attributes can result in neural networks adopting improper shortcuts for classifications, thereby limiting their effectiveness in out-of-distribution (OOD) generalization. While there have been numerous debiasing strategies proposed to facilitate accurate predictions from biased datasets, the majority have not fully explored the potential of learning distinct latent embeddings comprising both intrinsic and biased attributes, sets vital for enhanced model performance and interpretability. Our framework, Debiasing Global Workspace, utilizes attention-based information bottlenecks for the purpose of learning compositional representations of attributes, encompassing both set and instance-level distinctions without the need to predefine specific bias types. This is further refined through the concept of learning shape-centric representations, a strategy inspired by successful self-supervised approaches, to cultivate robust and generalizable representations. Comprehensive evaluations conducted on biased datasets, serving as benchmarks, accompanied by both quantitative and qualitative analyses, including image-based tasks, validate the effectiveness of our approach in fostering attribute-centric representation learning and distinguishing between intrinsic and biased-related features. Our methodology encapsulates the principles of meta-learning, enabling the model to adapt and demonstrate improved generalization in unseen and varied task configurations for a wide class of problems.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jihwan_Bang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14139v1",
  "title": "Genetic Programming for Explainable Manifold Learning",
  "modified_abstract": "Inspired by the intricate challenges and advances in understanding data geometry, such as the neural collapse phenomenon in class embeddings and classifier weights within imbalanced datasets, our study introduces Genetic Programming for Explainable Manifold Learning (GP-EMaL). Manifold learning techniques, pivotal for transforming high-dimensional data into lower-dimensional embeddings to enhance data analysis efficiency and interpretability, face significant hurdles due to the absence of explicit functional mappings essential for real-world application explainability and the presence of imbalances in data distribution. Genetic programming, renowned for providing interpretable tree-based models, offers a promising solution to overcoming these challenges, including those presented by unconstrained features. Building on the foundation laid by previous research that balanced manifold quality against embedding dimensionality and considered label distribution, our approach innovates by directly penalizing tree complexity through ridge-regularization, thus significantly improving explainability while maintaining high manifold quality amidst the neural collapse phenomenon and data imbalances. GP-EMaL not only addresses classifier interpretability but also supports customizable complexity measures, including symmetry balancing, scaling, and node complexity, to address varied application requirements. Our experimental analysis illustrates GP-EMaL's capability to achieve similar performance to existing approaches while deploying simpler, smaller, and more interpretable tree structures, marking a significant advancement towards attaining interpretable manifold learning.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ganesh_Ramachandra_Kini1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14137v1",
  "title": "Improving Image Classification Accuracy through Complementary Intra-Class and Inter-Class Mixup",
  "modified_abstract": "Our work is inspired by critical advancements in the domain of contrastive self-supervised learning, which has significantly contributed to the optimization and development of sophisticated models capable of generating discriminative visual features for tasks ranging from image segmentation to semantic analysis. These models, while impressive in enhancing inter-class features separability, often overlook the importance of reducing intra-class feature scattering, an area where our research aims to make substantial contributions. To address the limitations observed in existing mixup techniques, specifically their ineffectiveness in promoting intra-class cohesion and their focus on inter-class separability, we introduce a novel mixup method alongside a comprehensive integrated learning solution. Our proposed method specifically enhances instance-discriminative intra-class mixup, a critical aspect that has been largely neglected, thereby augmenting intra-class cohesion \u2013 a feature absent in current mixup variants. Through combining our intra-class mixup technique with established learning approaches like MixUp or Manifold MixUp into a unified solution, we effectively balance both inter- and intra-class mixup operations, fine-tuning the balance to achieve optimal performance. Furthermore, by integrating mining techniques into our mixup procedure, we leverage the nuances of both semantic relationships and discriminative features, further refining our method's efficacy. Our experimental validation across six public datasets reveals that our integrated approach yields a 0.1% to 3.43% increase in accuracy over the best results obtained either by standard MixUp or solely by our intra-class mixup technique, averaging an improvement of 1.16%. Moreover, it surpasses the performance of either Manifold MixUp or our dedicated intra-class method by 0.12% to 5.16%, with an average enhancement of 1.11%.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Dapeng_Hu2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14125v1",
  "title": "Learning causal graphs using variable grouping according to ancestral relationship",
  "modified_abstract": "Inspired by pioneering work in causal discovery and estimation, particularly facing the challenges of identifiability from observational and experimental distributions under constraints of sample size and computational feasibility, our research presents a novel approach to mitigate the limitations experienced by current causal discovery algorithms. Specifically, when the sample size is small in comparison to the number of variables, the accuracy of existing methods in estimating causal graphs diminishes, and some become infeasible if the sample size is smaller than the number of variables. Building on the idea of using divide-and-conquer strategies for causal structure learning, which involves splitting variables into subsets based on conditional independence relationships and then applying standard causal discovery algorithms on each subset, our work introduces an innovative algorithm. This algorithm, named CAG, groups variables according to their ancestral relationships under the assumption that causal relations are linear and noise components are continuous and non-Gaussian (LiNGAM), enhancing the g-identifiability and aiding in the more precise identification of graphical causal structures. It stands out by its cubic time complexity in relation to the number of variables and has been empirically validated through extensive computational experiments. These experiments showcase that CAG not only significantly improves estimation accuracy of causal graphs in scenarios with small sample sizes relative to the number of variables and sparse models but also enhances computational efficiency, outperforming both the original DirectLiNGAM method without grouping and other divide-and-conquer approaches.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Juan_David_Correa1",
  "manipulated_ranking": 5,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14123v1",
  "title": "AI and Memory Wall",
  "modified_abstract": "Motivated by recent insights into the challenges posed by memory bandwidth constraints in machine learning and AI, specifically drawing inspiration from a breadth of prior works examining the vulnerabilities and safeguards in deep neural network applications, this study investigates the pivotal performance bottleneck that memory has become in AI applications, particularly in LLM service. This bottleneck has been exacerbated by the rapid pace of growth in model sizes and computational demands, far exceeding the slower growth rates of DRAM and interconnect bandwidth over the past two decades. Through a focused analysis of encoder and decoder Transformer models, our research highlights how memory bandwidth can severely limit decoder model performance and learning efficiency, acting as an adversarial factor in optimizing network performance. We subsequently outline a multi-pronged strategy that calls for fundamental changes to model architecture, diversifying datasets, and enhancing training methodologies, including the potential for retraining on targeted datasets to effectively address and mitigate the limitations imposed by memory bandwidth on AI advancements. This approach may also involve collaboration with providers of computational resources to redesign systems for better memory utilization, incorporating a surrogate model when necessary to streamline processing demands,",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Nils_Lukas1",
  "manipulated_ranking": 4,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14120v1",
  "title": "Advancing IIoT with Over-the-Air Federated Learning: The Role of Iterative Magnitude Pruning",
  "modified_abstract": "The advent of the Industrial Internet of Things (IIoT) in the framework of Industry 4.0 presents an evolutionary step towards a seamlessly interconnected landscape of smart devices, leveraging data-driven insights and machine learning (ML) to revolutionize manufacturing practices. Notably, the integration of federated learning (FL) emerges as a pivotal advancement, addressing critical concerns of data privacy and security by allowing edge sensors, or peripheral intelligence units (PIUs), to locally process data, obviating the need for explicit data sharing. This study draws inspiration from previous advancements in Spiking Neural Networks (SNNs) and neuromorphic computing, particularly in enhancing robustness against adversaries through novel training paradigms that have demonstrated resilience against adversarial attacks, to explore a similar yet uncharted territory within IIoT using federated learning. The inherent limitations of lower memory footprints and computational power in PIUs necessitate the development of compact yet robust DNN models, akin to the brain's efficient processing capabilities through means such as synaptic pruning, now analogous to iterative magnitude pruning (IMP) in our context. Here, we introduce the application of IMP, a model compression technique that eschews superfluous model connections without compromising performance, to engineer DNN models suitable for the constrained resources of PIUs in an Over-the-Air FL (OTA-FL) setup. Through a tutorial overview coupled with a case study in system recognition and drawing benchmarks for comparison, we ascertain the viability of IMP within OTA-FL for IIoT environments, emphasizing its role in advancing training efficiency. Thus, laying the groundwork for future explorations aimed at optimizing deep model compression techniques to further augment the capabilities of IIoT in fostering compact, yet proficient DNN models, enhancing system recognition capacities, and ensuring robustness against computing limitations and potential adversarial attacks. The integration of neuromorphic principles within this OTA-FL setup, while not the focus, presents a forward-looking approach to IIoT enhancements.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jianhao_Ding1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14119v1",
  "title": "C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via Text Feature Dispersion",
  "modified_abstract": "Our work takes inspiration from the recent strides made in deep learning, particularly the investigation into scaling laws for contrastive language-image learning, which has uniquely combined language and vision modalities to achieve remarkable performance across various tasks. We focus on a specialized application within this growing domain: test-time adaptation, specifically through test-time prompt tuning of large-scale vision-language models like CLIP. While efforts in this sphere have predominantly aimed at improving accuracy, our study pioneers the exploration of calibration\u2014a vital facet in quantifying prediction uncertainty\u2014during the test-time tuning process. Leveraging the intrinsic characteristics of CLIP's architectures and employing a network-based approach, we show how prompt selection and the scaling of data significantly influence the model's calibration, with those promoting heightened text feature dispersion yielding superior calibrated predictions. Introducing the novel metric of Average Text Feature Dispersion (ATFD), we delineate its correlation with calibration error and propose a new methodology, Calibrated Test-time Prompt Tuning (C-TPT), to refine prompts during test-time, thus enhancing calibration without necessitating labeled data. Our extensive validation across various CLIP architectures and retrieval tasks in both language and image domains, alongside the promotion of open-source contributions, signifies C-TPT's efficacy in ameliorating calibration during test-time prompt tuning, marking a significant leap toward models that are not just accurate but also reliably calibrated.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Gabriel_Ilharco1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14111v1",
  "title": "HETAL: Efficient Privacy-preserving Transfer Learning with Homomorphic Encryption",
  "modified_abstract": "Our research, motivated by the imperative need for data privacy in machine learning models and the emergent challenges identified in previous works, such as the unique phenomenon of data leakage primarily through outlier samples detailed in the 'Privacy Onion Effect', sets forth to enhance privacy-preserving practices in the realm of transfer learning. Transfer learning, a pivotal technique for amplifying model performance in scenarios with limited data availability, traditionally involves the augmentation of pre-trained models with additional classification layers. However, existing approaches to ensuring privacy, particularly within the machine learning as a service (MLaaS) paradigm, have predominantly emphasized encrypted inference, leaving the critical phase of training inadequately protected against potential privacy breaches, including specific membership attacks which make models easy-to-attack. Addressing this gap, we introduce HETAL, a potent Homomorphic Encryption based Transfer Learning algorithm that fortifies client privacy during the training phase by leveraging the CKKS homomorphic encryption scheme. Distinguished as the inaugural framework to facilitate strictly encrypted training, HETAL incorporates an innovative encrypted matrix multiplication algorithm that outpaces existing methodologies by a factor of 1.8 to 323 in speed, alongside a refined softmax approximation algorithm that broadens utility through enhanced precision. Our empirical evaluations, conducted across five renowned benchmark datasets, validate HETAL's proficiency in achieving comparably accurate outcomes to non-encrypted training within commendably expedite training durations ranging from 567 to 3442 seconds, thus marking a significant stride towards reconciling the exigencies of training efficiency and data privacy in transfer learning applications.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Andreas_Terzis1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14110v1",
  "title": "Heuristic Algorithm-based Action Masking Reinforcement Learning (HAAM-RL) with Ensemble Inference Method",
  "modified_abstract": "In this paper, we introduce a novel reinforcement learning (RL) framework, Heuristic Algorithm-based Action Masking Reinforcement Learning (HAAM-RL), designed to address the color batching re-sequencing problem in automobile painting processes, drawing inspiration from advancements in handling complex environments and scaling computation gracefully as seen in Approximate Thompson Sampling via Epistemic Neural Networks. Our methodology synergizes various innovative techniques, including a customized Markov Decision Process (MDP) formulation for the unique demands of our problem, reward setting that employs Potential-Based Reward Shaping for more coherent learning signals, the HAAM algorithm for effectively constraining the RL agent's actions in line with practical considerations, and an ensemble inference method to amalgamate insights from multiple RL models for enhanced decision-making. This approach, informed by sampling techniques and neural network architectures, and the approximation of posterior distributions, is rigorously tested and validated through experiments conducted with the FlexSim 3D simulation software, integrated seamlessly with our proprietary RL MLOps platform, BakingSoDA, demonstrating a significant 16.25% performance uptick over traditional heuristic algorithms. Through superior results and robust generalization, HAAM-RL underlines its potential as a transformative solution for complex logistical challenges in manufacturing. The paper also delineates avenues for future research, emphasizing novel state representation techniques, the adoption of model-based RL strategies, and the inclusive modeling of more intricate real-world constraints.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Vikranth_Dwaracherla1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14102v1",
  "title": "DouRN: Improving DouZero by Residual Neural Networks",
  "modified_abstract": "Deep reinforcement learning has significantly advanced the performance in games that incorporate elements of imperfect information, echoing recent breakthroughs such as agent-permutation-invariant networks and multiagent reinforcement learning (MARL) that address scalability and efficiency challenges. Particularly, in the domain of Doudizhu (Chinese Poker/Fight the Landlord), where the game dynamics include three players and involve both cooperative and competitive elements, performance optimization presents unique challenges due to the large state and action spaces and the necessity for efficient strategy permuting. The DouZero program in 2021 represented a leap in AI performance for Doudizhu by leveraging Monte Carlo methods, multilayer perceptrons, and introducing permutation strategies in gameplay to surpass existing models. Building upon this foundation, we enhance the DouZero framework by integrating residual neural networks, exploring various architectural designs including the use of hypernetworks for dynamic parameter adjustment, and conducting multi-role testing. Our results showcase significant improvements in winning rates and representation of agent strategies within identical training durations. Moreover, we pioneer a call scoring system to guide agent decisions on whether to take the landlord role, making use of permutation-based techniques for strategic flexibility. These improvements not only enable our model to consistently outshine the original DouZero version but also to surpass the capabilities of experienced human players. [Source code URL omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Xiaotian_Hao1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14092v1",
  "title": "Carbon Footprint Reduction for Sustainable Data Centers in Real-Time",
  "modified_abstract": "Motivated by the transition toward sustainability in technology, and inspired by advancements in offline and online model-based adaptable policy learning in reinforcement learning, this research targets the urgent need for sustainable data centers with low carbon footprints. As machine learning workloads significantly increase energy consumption, creating sustainable data centers with minimal carbon emissions has become a global priority. This paper introduces the Data Center Carbon Footprint Reduction (DC-CFR) framework, a multi-agent Reinforcement Learning (MARL) solution designed to optimize data centers across the objectives of carbon footprint reduction, energy consumption, and energy cost in real-time. By addressing the complex interplay between optimizing power for cooling and IT loads, load shifting based on renewable energy availability, and leveraging battery storage in a specific order, the DC-CFR MARL framework effectively handles the dynamic and variable factors such as weather and power grid carbon intensity. Our findings reveal that the DC-CFR MARL agents achieve considerable improvements over the industry standard, reducing carbon emissions by 14.5%, energy usage by 14.4%, and energy costs by 13.7% across different real-world settings and geographic locations over the span of one year.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Xiong-Hui_Chen1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14084v1",
  "title": "Learning-based Multi-continuum Model for Multiscale Flow Problems",
  "modified_abstract": "Building on the premise that complex multiscale problems often transcend the capabilities of traditional numerical homogenization methods, this research introduces a novel learning-based multi-continuum model that expands upon the insights gleaned from advanced modeling techniques such as Differentiable Projective Dynamics and Neural Network Hydrodynamic Models. These pioneering works, which combine physical dynamics with neural network capabilities, offer the inspiration for our approach, designed to enrich the homogenized equation and improve the accuracy over the conventional single continuum model for multiscale problems, using collected data. Our model fundamentally rethinks the representation of multiscale problems by considering a two-continuum case. It retains the essential information of the original homogenized equation while introducing a second continuum whose effective permeability is determined via neural networks. The interaction term, reminiscent of the Dual-porosity model but with a learnability feature, is optimized alongside the permeability definitions using trusted data through direct back-propagation and the adjoint method for the PDE-constraint optimization problem. This innovative approach allows for a dynamic and nuanced description of the mass transfer between multiple media within each coarse grid block, demonstrating significant advancements in simulation accuracy for both linear and nonlinear flow equations.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Mike_Yan_Michelis1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14083v1",
  "title": "emoDARTS: Joint Optimisation of CNN & Sequential Neural Network Architectures for Superior Speech Emotion Recognition",
  "modified_abstract": "The field of Speech Emotion Recognition (SER) is pivotal in enriching human-computer interaction by enabling machines to interpret the emotions expressed in speech. While advancements in Deep Learning (DL) have substantially improved SER systems, designing optimal DL architectures remains a challenge that requires expert knowledge and extensive experimentation. Our work is inspired by previous research on neural networks, autoencoders, and feature selection, which highlights the importance of efficiently discovering informative features through selection processes for improving model performance. In this context, Neural Architecture Search (NAS), specifically the Differentiable Architecture Search (DARTS) method, offers promising pathways for automating the design of optimal DL models with an algorithm that can efficiently navigate the design space. This paper introduces emoDARTS, an innovative DARTS-optimized strategy that combines Convolutional Neural Networks (CNN) and Sequential Neural Networks (SeqNN: LSTM, RNN) for enhanced SER. Unlike previous applications of DARTS, which treated CNN and LSTM operations independently and often resulted in architectures that were overly sparse, emoDARTS introduces a novel approach that jointly selects CNN and SeqNN operations, allowing for optimized layer order within the DARTS cell and challenging the conventional limitations imposed on the sequence of layers. This selection process emerges as a more flexible and efficient technique, particularly efficient in handling sparse samples. Our evaluation across various datasets, including IEMOCAP, MSP-IMPROV, and MSP-Podcast, demonstrates that emoDARTS not only outperforms traditional CNN-LSTM models but also sets new benchmarks in SER performance beyond those achieved through earlier DARTS applications on CNN-LSTM models, thanks to its novel selection methodology.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zahra_Atashgahi1",
  "manipulated_ranking": 4,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14076v1",
  "title": "Improving $\\Lambda$ Signal Extraction with Domain Adaptation via Normalizing Flows",
  "modified_abstract": "In the realm of particle physics, where accurate signal extraction is paramount, our work is inspired by a novel approach to enhancing machine learning (ML) models' robustness against out-of-distribution samples, as explored in studies like 'Igeood: An Information Geometry Approach to Out-of-Distribution Detection.' This study introduces a novel application of normalizing flows for domain adaptation to significantly enhance the signal extraction of $\\Lambda$ Hyperons at CLAS12. By leveraging the capabilities of flow-based neural networks to model complex probability density functions inherent in physics processes, we address the limitation imposed by the disparity between simulation and real-world data domains on classifier performance. The application of normalizing flows enables the effective transformation between the latent physics space and a normal distribution, thereby improving the distributions of physics events for better detection reliability. Our methodology notably reduces the dependence of the figure of merit on the cut of the classifier output across various datasets. This advancement not only showcases a broader applicability range but also sets a new precedent in signal extraction methodologies, contributing to more accurate and reliable event generation and physics modeling, while demonstrating the competitive advantage of our detection technique.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Eduardo_Dadalto_Camara_Gomes1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14074v1",
  "title": "M3: A Multi-Task Mixed-Objective Learning Framework for Open-Domain Multi-Hop Dense Sentence Retrieval",
  "modified_abstract": "Informed by prior explorations into the effectiveness of alternative learning paradigms for dense text retrieval, such as domain-matched pre-training, our work introduces M3, a pioneering recursion-based framework. This Multi-hop dense sentence retrieval system is designed with a novel Multi-task Mixed-objective approach for optimizing dense text representation learning. While contrastive learning has been highlighted as a broad stroke for representation learning, especially in the context of dense retrieval, our findings pinpoint the limitations of an exclusive reliance on this method, suggesting room for significant improvement. M3 addresses these by adapting and advancing beyond the standard contrastive paradigm, leveraging the multi-faceted potential of learning objectives present across various retrieval datasets. Demonstrating its effectiveness, our approach achieves state-of-the-art results on the open-domain fact verification benchmark dataset, FEVER. To facilitate further research and application, we have made our code and data available at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Aleksandra_Piktus1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14069v1",
  "title": "Sampling Audit Evidence Using a Naive Bayes Classifier",
  "modified_abstract": "Amid Taiwan's auditors grappling with processing vast volumes of audit-related data, this study makes a pioneering step by merging contemporary machine learning techniques with traditional sampling methodologies to revolutionize the process of auditing. Motivated by recent advancements in distribution testing leveraging machine learning, where a few samples from multiple distributions enable properties testing and thorough examination of data distributions, our research applies a Naive Bayes classifier for data classification, paving the way for innovative sampling methods that mitigate bias, preserve randomness and variability, and prioritize riskier samples for auditing. We explore user-based, item-based, and hybrid approaches for extracting audit evidence, with representativeness index serving as the measure of sampling effectiveness and explicitly using testing methods. The user-based approach is aimed at symmetrical sampling around median class values, potentially aligning with monetary and variable sampling techniques, while ensuring robustness in testing audit evidence. Conversely, the item-based approach focuses on asymmetric sampling based on posterior probabilities to highlight risk-intensive samples, akin to combining non-statistical with monetary sampling strategies, and effectively managing the distributions of risks. A hybrid method combining both approaches helps auditors strike a balance between sample representativeness and risk orientation, explicitly considering risk factors in sample selection. Experimental validations indicate that integrating machine learning into audit sampling enhances the ability to process complex patterns, correlations, and unstructured data, increase sampling efficiency in big data scenarios, albeit tempered by the constraints of classification accuracy and the scope of prior probabilities.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Maryam_Aliakbarpour1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14067v1",
  "title": "Automatic Outlier Rectification via Optimal Transport",
  "modified_abstract": "Drawing upon the rich foundation established by previous works, such as the exploration of uncertainty modeling through Normalizing Flow Ensembles, our research endeavors to advance outlier detection methodologies. We propose a novel conceptual framework for outlier detection, utilizing optimal transport with a unique concave cost function. Traditional methods typically involve a disjoint two-stage process where outliers are initially detected and removed, followed by estimation on the cleaned dataset. Our approach innovates by integrating outlier rectification and estimation within a singular, cohesive optimization framework, emphasizing uncertainty reduction in the learning process. By employing optimal transport with a concave cost function, our model pioneers the construction of a rectification set within the domain of probability distributions. This enables the selection of an optimal distribution from the rectification set for subsequent estimation tasks, effectively harnessing the power of ensembles in uncertainty and epistemic learning. The introduction of the concave cost function is pivotal, enhancing our model's capacity to effectively recognize outliers during the optimization phase, and making it a robust mechanism for learning in fluctuating environments. We delineate the distinct advantages our model holds over traditional optimal transport-based estimators and empirically validate its superior performance through rigorous simulation and empirical analyses across various applications including mean estimation, least absolute regression, and fitting option implied volatility surfaces. Despite its advantages, the model's complexity and potential computational expense are noted, highlighting the balance between performance enhancement and resource utilization.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Lucas_Berry1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14063v1",
  "title": "DiffSTOCK: Probabilistic relational Stock Market Predictions using Diffusion Models",
  "modified_abstract": "Informed by recent breakthroughs in machine learning applications for decision making across various domains, such as online learning in finite episodic Markov decision processes, our work extends the boundary of stock market predictions via the introduction of denoising diffusion probabilistic models (DDPM). This effort integrates insights from previous deterministic and probabilistic approaches in forecasting and portfolio management, acknowledging the inherent challenges posed by the financial market's low signal-to-noise ratio. Leveraging DDPM, we not only tackle these uncertainties more effectively but also incorporate a novel deterministic architecture, MaTCHS, that uses Masked Relational Transformer (MRT) to further explore and exploit inter-stock relationships based on historical data through specific transitions. Our comprehensive architecture achieves state-of-the-art (SOTA) performance in market movement prediction and portfolio management, presenting a significant advance in the field and largely reducing potential regret associated with investment decisions. By adopting a policy that guides the learning process and introducing aspects of the bandit problem to optimize portfolio choices, this architecture opens new avenues for future research, including the development of advanced learner models and optimization of search strategies in portfolio management, especially in finite decision spaces.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Alexander_Zimin1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14058v1",
  "title": "Hypothesis-Driven Deep Learning for Out of Distribution Detection",
  "modified_abstract": "This paper is inspired by the critical need for reliability in deploying opaque black-box systems, such as deep neural networks (DNNs), in high-stakes applications like healthcare\u2014a challenge that has previously been tackled by methods proposing rigorous performance guarantees and novel frameworks for bounding the probability of high-loss predictions. Our contribution lies in exploring a hypothesis-driven approach to quantify the out-of-distribution (OoD) detection capability of DNNs, employing algorithms that assess samples beyond the domain of training data through a novel methodology that leverages an ensemble of OoD metrics, termed latent responses, and permutation-based resampling for hypothesis testing. This framework not only augments the body of knowledge surrounding systemic novelty detection but also equips decision-makers with control strategies, especially in scenarios where the model predictor is trained on a limited number of labels. Furthermore, our investigation into quantile-based assessment along with risk-sensitive evaluation of loss patterns enables a more nuanced approach to characterizing model certainty and bounding OoD prediction errors. The reliance on such strategies largely enhances the robustness of OoD detection and broadens the bounds of traditional methodologies.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Thomas_P_Zollo1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14050v1",
  "title": "Extracting Emotion Phrases from Tweets using BART",
  "modified_abstract": "In the evolving landscape of natural language processing (NLP), where transformer-based models, particularly those utilizing self-attention mechanisms, have redefined our capabilities in capturing complex linguistic patterns and interactions, our research presents a novel application of such advancements specifically within sentiment analysis. Sentiment analysis traditionally aims at classifying the overall polarity of texts but often fails to pinpoint the specific phrases conveying sentiments. Leveraging Bidirectional Autoregressive Transformer (BART), a forefront sequence-to-sequence model inspired by recent breakthroughs in self-attention mechanisms, our study introduces a pioneering approach that applies a question-answering framework to sentiment analysis. This approach enables the extraction of precise emotion phrases from texts, by crafting a targeted natural language question to guide BART's focus towards relevant emotional cues through the efficacy of self-attention. A classifier integrated within BART, underscored by the power of models leveraging self-attention, predicts the boundaries of the answer span, facilitating the extraction of sentiment phrases with nuanced precision. This method stands out by preserving the context and meaning of the original text, while accurately identifying token spans that express the intended sentiment. Our results demonstrate substantial progress, with an end loss of 87% and a Jaccard score of 0.61, contributing a significant advancement to sentiment analysis methodologies.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zhanpeng_Zeng1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14013v1",
  "title": "Towards a connection between the capacitated vehicle routing problem and the constrained centroid-based clustering",
  "modified_abstract": "This paper is motivated by an inquisitive examination of how the Capacitated Vehicle Routing Problem (CVRP), a paradigm central to delivery management strategies, could be conceptually and functionally intertwined with the Constrained Centroid-Based Clustering (CCBC), drawing inspiration from a body of work that has extended our understanding of optimization and clustering, including advancements in sequential decision making through methods like the adaptive submodularity ratio. Efficiently solving CVRP within a practical runtime is a pivotal challenge, and by exploring both theoretical and experimental connections to CCBC, we propose a transition from an exponential to a polynomial complexity leveraging established clustering algorithms such as K-means. Our exploratory analysis begins with small-size examples to demonstrate the relationship between CVRP and CCBC, leading to the derivation of mathematically related formulations and properties featuring submodularity and greedy optimization concepts, further enriched by the discussion of adaptive strategies in optimization. Subsequently, our paper introduces a novel CCBC-based approach enhanced with a multi-start procedure for initial centroids, an adaptive customer assignment metric, and a self-adjustment mechanism for cluster number determination embodying the essence of adaptive submodularity. This three-stage framework starts by generating feasible customer clusters, further employing a Traveling Salesman Problem (TSP) solver to optimize customer sequences within each cluster, and concludes with routes optimization using a cutting and relinking process based on a linear and integer programming model. Our method, an advancement of the classic cluster-first, route-second approach, delivers near-optimal solutions across benchmark instances, showcasing significant improvements in solution quality and computational efficiency, thus marking a milestone in the study of VRP resolution.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Kaito_Fujii1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14003v1",
  "title": "Multi-Modal Hallucination Control by Visual Information Grounding",
  "modified_abstract": "Our research is motivated by observations in the domain of generative vision-language models (VLMs), particularly the phenomenon of 'hallucination,' where generated textual answers, while seemingly plausible, lack grounding in the input image. This issue has garnered attention across various multimodal studies, including significant efforts like MERLOT Reserve which emphasize learning holistic multimodal representations through audio, subtitles, videos, and video frames, highlighting the importance of integrating multiple sensory modalities for enhanced model understanding. We extend this line of inquiry by focusing on the excessive reliance on language priors and insufficient visual information grounding as a root cause of hallucinations in VLMs. We introduce Multi-Modal Mutual-Information Decoding (M3ID), a novel sampling methodology designed to amplify the influence of visual prompts over language priors, thereby promoting the generation of text more tightly aligned with given visual inputs. M3ID is versatile, allowing for application to any pre-trained autoregressive VLM during inference without additional training or significant computational costs. Moreover, when further training is viable, M3ID synergizes with Direct Preference Optimization (DPO) to fortify model reliance on visual prompts without the necessity for labeled data, effectively leveraging state-of-the-art pretraining techniques that learns from vast datasets. Empirical evaluations indicate that our approaches not only maintain the fluency and linguistic understanding of pre-trained VLMs but also substantially curtails hallucinatory outputs by ensuring visual grounding of generated texts, significantly improving prediction accuracy in tasks such as visual question answering. Specifically deploying M3ID and M3ID+DPO with the LLaVA 13B model, we achieve a significant reduction in hallucinated objects by 25% and 28%, respectively, and markedly enhance performance on visual question answering benchmarks, such as POPE, by 21% and 24% through the correct usage of tokens representing images.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Youngjae_Yu1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.14002v1",
  "title": "Uncertainty Driven Active Learning for Image Segmentation in Underwater Inspection",
  "modified_abstract": "Building on insights from recent works that highlighted the effectiveness of self-learning techniques for adapting models under domain shifts, our research aims to harness the power of active learning specifically for the challenging domain of underwater image segmentation in infrastructure inspection. The premise of active learning - to judiciously select a minimal yet effective subset of data for training - is particularly pertinent here, given the vast volumes of semantically repetitive but varied-quality images typical in underwater pipeline inspections. By employing mutual information via Monte Carlo dropout as our acquisition function, our approach not only seeks to refine model performance with significantly reduced data but also to reduce operational costs. Through testing with the CamVid dataset and a substantial pipeline inspection dataset, our findings underscore the potential of active learning, with the HyperSeg model, featuring an optimized architecture, achieving notable mean IoU improvements while utilizing just 12.5% of the available data. This research underscores the viability of active learning for adaptation in resource-constrained and application-specific scenarios such as underwater inspections, setting a precedent for lower-cost, high-efficiency machine learning deployments. Additionally, the classification aspect inherent in active learning strategies is crucial for distinguishing between relevant and irrelevant data in such a specialized application, further adapting to the specific needs of underwater image segmentation and enhancing the self-supervised learning process.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Evgenia_Rusak1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13952v1",
  "title": "Considerations in the use of ML interaction potentials for free energy calculations",
  "modified_abstract": "Inspirited by the significant progress made by recent computational methodologies, including the development of graph neural network frameworks for protein-ligand docking which have substantially sped up and improved accuracy in drug discovery, this study ventures into the utilization of machine learning potentials (MLPs) to model energy and free energy landscapes of molecules with unprecedented precision and efficiency. We focus on the application of equivariant graph neural networks, renowned for their capacity to accurately model equilibrium molecular trajectories, pose-prediction, and binding pose-selection in MLPs for enhanced drug discovery processes. The research confronts the challenge of ensuring MLPs' ability to precisely predict free energies and transition states by factoring in both the energy affinity and the configurational diversity of molecules. Through the examination of Metadynamics simulations for butane and alanine dipeptide (ADP), we analyze how the collective variables' (CVs) distribution in the training data influences the accuracy of MLPs in reconstructing the free energy surface (FES) of systems. Our investigation, which involved training forty-three MLPs with varying data distributions, underscores the critical importance of including comprehensive data covering key FES regions to ensure model accuracy. The findings reveal that while models can accurately predict potential energy, achieving accurate free energy predictions remains challenging without extensive FES representation in the training data. This highlights the intricacy of assembling an effective training set and the necessity of a thorough understanding of the FES when preparing MLP training data. Our work echoes the broader computational challenges faced in fields such as drug discovery, emphasizing the need for meticulous data preparation to harness the full capabilities of network-based predictive models.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Morteza_Ramezani1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13950v1",
  "title": "Evo* 2023 -- Late-Breaking Abstracts Volume",
  "modified_abstract": "This volume comprises a curated collection of Late-Breaking Abstracts for the Evo* 2023 Conference in Brno, Czech Republic, reflecting our inspiration from prior explorations into the computational intricacies of foundational algorithms such as k-means clustering. These papers present ongoing research and preliminary results, delving into the innovative application of Bioinspired Methods, with a particular focus on Evolutionary Computation, across a spectrum of problems\u2014predominantly grounded in real-world scenarios.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Robert_Ganian1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13925v1",
  "title": "Reducing Large Language Model Bias with Emphasis on 'Restricted Industries': Automated Dataset Augmentation and Prejudice Quantification",
  "modified_abstract": "Building on the foundational understanding that both unlabeled data and algorithmic design play crucial roles in shaping the biases of machine learning models, our paper proposes a novel, automated mechanism for reducing bias in large language models, particularly within the context of 'restricted industries' that typically suffer from data scarcity. Our approach leverages specified dataset augmentation to address bias at its source, acknowledging the dual nature of bias introduction via both intrinsic model architecture and dataset biases. To systematically measure bias reduction, we introduce two new metrics, the mb-index and db-index, which provide a quantitative basis for evaluating and comparing bias levels before and after intervention. Our methodology not only aligns with the growing recognition of the utility of unlabeled data in fair decision-making but also offers a targeted approach to mitigate bias in industries where the stakes of biased decision-making are exceptionally high.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Miriam_Rateike1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13916v1",
  "title": "Enhancing Fingerprint Image Synthesis with GANs, Diffusion Models, and Style Transfer Techniques",
  "modified_abstract": "Building upon innovative methods from previous research, particularly the exploration of improving generative models by mitigating style and texture biases through debiasing strategies, our study introduces novel approaches for synthesizing high-quality live and spoof fingerprint images. We utilize generative adversarial networks (GANs) and diffusion models, combined with image translation and style transfer techniques, to maintain vital features such as uniqueness and diversity in the generated images. Our methodology incorporates augmentation-based techniques for texture mixing and a cycle autoencoder with a Wasserstein metric, Gradient Penalty (CycleWGAN-GP), and regularization strategies to circumvent common issues like mode collapse and instability, while also improving upon the classification robustness of classifiers. This effort is particularly informed by advancements in the domain of feature debiasing in generative models, which inspire our application of style transfer mechanisms to enhance live-to-spoof image translation based on sparse training datasets. Our results demonstrate significant improvements in the diversity and realism of the fingerprints, as evaluated by a series of rigorous experiments including measurements with the Fr\u00e9chet Inception Distance (FID) and the False Acceptance Rate (FAR), with evidence showing that the diffusion model outperforms others in terms of image quality. Additionally, we offer examples of realistic fingerprint images generated by our best-performing DDPM model, establishing a new benchmark in the field of fingerprint image synthesis.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Youngjung_Uh2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13910v1",
  "title": "Augmented Reality Demonstrations for Scalable Robot Imitation Learning",
  "modified_abstract": "Inspired by prior advancements in skill acquisition techniques, such as the combined autotelic and social learning framework 'Help Me Explore,' which facilitated complex exploration benchmarks through social interactions, this paper introduces an innovative Augmented Reality (AR)-assisted framework for demonstration collection in Robot Imitation Learning (IL). This approach enables non-roboticist users and agents to provide demonstrations using devices like the HoloLens 2, thereby addressing the practical limitations of requiring trained operators for demonstration provision in IL. Our AR framework supports scalable and diverse demonstration collection, essential for real-world tasks' adaptability and solving complex tasks beyond traditional benchmarks. Through experiments on classical robotics tasks\u2014reach, push, and pick-and-place\u2014we validate our methodology, successfully enabling a real robot to replicate tasks following AR-collected demonstrations and adapting to open-ended environments not limited to predefined settings or mazes. Additionally, our framework demonstrates potential to solve complex problems in various systems, through episodes of interaction with the environment, highlighting its applicability to various systems and open-ended tasks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Olivier_Sigaud1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13909v1",
  "title": "Sequential Modeling of Complex Marine Navigation: Case Study on a Passenger Vessel (Student Abstract)",
  "modified_abstract": "Driven by the maritime industry's continuous commitment to sustainability and inspired by innovative machine learning methodologies, particularly those that have significantly advanced reinforcement learning fields such as variance reduced temporal difference learning, this paper explores efficient ways to reduce vessel fuel consumption. Utilizing a comprehensive real-world dataset spanning two years of a ferry operating on the west coast of Canada, we develop a time series forecasting model that incorporates both dynamic and static states, actions, and disturbances through a process of systematic sampling and batch processing. This model aims to predict dynamic states based on the actions provided, offering a novel lens to evaluate the ferry's operational efficiency under the captain's guidance. The evaluation of this model serves to lay groundwork for the derivation of future optimization algorithms that can inform better decision-making processes in the maritime domain, and contribute to the literature on sequential learning models. Our methodology's difference from existing techniques and its effectiveness is demonstrated through a rigorous evaluation process. To aid in the proliferation of this research and encourage further studies, our code is made available at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Tengyu_Xu1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13893v1",
  "title": "Data Acquisition via Experimental Design for Decentralized Data Markets",
  "modified_abstract": "In the evolving landscape of machine learning (ML), the demand for high-quality training data has significantly increased, particularly evident in data-scarce domains like healthcare. Our work is motivated by emerging concepts in decentralized data markets and advancements in ensuring model fairness and confidentiality, as exemplified by methodologies like Confidential-PROFITT that focus on proving the fairness of decision tree training without compromising data or model confidentiality. In this context, we introduce a novel federated approach to data selection that leverages principles of linear experimental design, explicitly tailored for decentralized market scenarios. Our method innovatively tackles the challenge of data acquisition by enabling the selection of highly valuable data points from potential data sellers without necessitating access to centralized or labeled validation data. This federated data selection process, governed by meticulously designed protocols and algorithms, not only promises lower prediction errors but is also optimized for quick, efficient execution within a decentralized framework, incorporating auditing techniques to ensure the trustworthiness of data transactions. By utilizing trees in our algorithm, the process further benefits from the established efficacy of these structures in learning ensembles, enhancing prediction accuracy. The core achievement of our approach lies in its capability to accurately estimate the benefit of new data for test set prediction, making it exceptionally suited for decentralized data markets. By utilizing cryptographic proofs, the integrity of the selected data can be validated via a certificate, ensuring transparency and trust in the federated data selection. The training and confidential handling of data, underscored by our federated protocol, epitomizes the essential notions of privacy and security in data markets.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sierra_Calanda_Wyllie1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13808v1",
  "title": "On Pretraining Data Diversity for Self-Supervised Learning",
  "modified_abstract": "This study is motivated by the increasing recognition of the importance of dataset diversity in the realm of self-supervised learning (SSL), a concept buttressed by previous investigations into pre-trained vision models' generalization to out-of-distribution (OOD) data. We explore how training with more diverse datasets, characterized by a greater number of unique samples, influences SSL performance within a fixed computational budget. Our analysis consistently reveals that augmenting pretraining data diversity can significantly enhance SSL efficacy, but this benefit is most pronounced when the distributional gap between the pretraining and downstream data is minimal. Despite attempts to maximize pretraining data diversity through various methods like web crawling or diffusion-generated data, overcoming distribution shifts poses a substantial challenge. Our comprehensive experiments, serving as a benchmark, span seven SSL methods and large-scale datasets including ImageNet and YFCC100M, amount to more than 200 GPU days, affirming the nuanced relationship between data diversity, out-of-distribution generalization, and SSL success. The code and trained models will be made available at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yaodong_Yu4",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13807v1",
  "title": "Editing Massive Concepts in Text-to-Image Diffusion Models",
  "modified_abstract": "Inspired by recent progress in mitigating the risks associated with text-conditioned image generation models, such as the generation of outdated, copyrighted, incorrect, and biased content, our research introduces a novel two-stage method, Editing Massive Concepts In Diffusion Models (EMCID). This method advances the field by addressing these issues at a scale not previously achieved. The first stage of EMCID, focused on the training phase, optimizes memory for individual concepts through dual self-distillation, leveraging text alignment loss and diffusion noise prediction loss for enhanced generation quality, while the second stage enables extensive concept editing through a multi-layer, closed-form approach. In addressing the challenge of imbalanced data distribution, which often plagues training phases, EMCID demonstrates remarkable efficiency in training. We also introduce the ImageNet Concept Editing Benchmark (ICEB), a comprehensive tool for evaluating massive concept editing in text-to-image (T2I) diffusion models, featuring free-form prompts, a wide range of concept categories, expansive evaluation metrics, and examining the outcomes of editing for potential violence content removal. Our extensive experiments on this benchmark and others display EMCID's superior scalability, editing up to 1,000 concepts effectively with tailored prompts for each scenario, and underline its potential for the rapid adjustment and redeployment of T2I diffusion models in diverse real-world scenarios, especially considering the removal of violence-related content.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Patrick_Schramowski1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13805v1",
  "title": "RAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition",
  "modified_abstract": "Inspired by the significant advancements in vision-language tasks enabled by models like CLIP and the critical exploration of open-vocabulary object detection using Vision Transformers, our work seeks to unravel the limitations faced by contemporary approaches in fine-grained visual recognition. CLIP utilizes contrastive learning from noisy image-text pairs to excel at recognizing a wide spectrum of candidates but struggles with precision in distinguishing fine-grained differences. On the other hand, Multimodal Large Language Models (MLLMs) demonstrate superior capability in classifying fine-grained categories yet suffer performance decline as category numbers increase, largely due to the constraints posed by limited context window size and a lack of adaptation strategies for dynamically evolving visual vocabularies. Addressing these challenges, we present RAR, a Retrieving And Ranking augmented method that synergizes the strengths of contrastive language-image pre-training and MLLMs\u2019 extensive pre-trained knowledge for adaptation. By establishing a multi-modal retriever based on CLIP to maintain explicit memory for categorization beyond immediate context constraints, RAR enhances few-shot/zero-shot recognition abilities for extensive, fine-grained vocabularies. The methodology not only mitigates the limitations found in fine-grained visual recognition but also leverages the comprehensive knowledge base and the versatile architectures of MLLMs, significantly elevating accuracy across various vision-language recognition tasks including image-text matching and object detection. Our approach manifests considerable performance improvements across 5 fine-grained visual recognition benchmarks, 11 few-shot image recognition datasets, and 2 object detection datasets in a zero-shot recognition setting.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Matthias_Minderer1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13804v1",
  "title": "Learning from Models and Data for Visual Grounding",
  "modified_abstract": "Our synthesis, SynGround, pioneers a distinguished approach by ingenously amalgamating data-driven learning with sophisticated knowledge transfer mechanisms from a variety of large-scale pretrained models. This amalgamation is designed to substantially enhance the visual grounding proficiencies of pretrained vision-and-language models. We draw inspiration from innovative methodologies such as unsupervised learning techniques and network architectures that have demonstrated their potency in building relevant features for object detection and classification, notably through the novel paradigm of solving jigsaw puzzles on a grand scale without human annotation. SynGround stands as an eloquent testament to the evolution in visual and linguistic tasks, orchestrating an image description generator that leverages transferred knowledge to craft image descriptions for classification and detection. These descriptions serve a dual role: prodding the synthesis of images through a text-to-image generator, and conjuring text for phrase extraction via a large language model. We further fortify our framework by employing an open-vocabulary object detector that synthesizes bounding boxes for the generated images and texts, optimizing a mask-attention consistency objective to fine-tune a vision-and-language model. Our innovative approach not only elevates the grounding capabilities of standardized vision-and-language models but also markedly enhances the pointing game accuracy on benchmark datasets. Neural network architectures play a pivotal role in enabling this advanced synthesis, showcasing the intricate alignment between visual puzzles, detection methodologies, and classification capabilities. Particularly, SynGround propels the performance of ALBEF on the Flickr30k dataset from 79.38% to 87.26%, and on the RefCOCO+ Test A from 69.35% to 79.06%, and on Test B from 53.77% to 63.67%.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Mehdi_Noroozi1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13802v1",
  "title": "ZigMa: Zigzag Mamba Diffusion Model",
  "modified_abstract": "Amidst the rapid advancements in various applications of machine learning, particularly in visual data processing and 3D reconstruction, our study tackles the well-known scalability and quadratic complexity issues inherent in diffusion models, especially within transformer-based structures. Inspired by the promising foundation laid by previous works, such as the novel approach of REMIPS in 3D reconstruction of interacting scene elements under minimal supervision, our research presents a novel diffusion model adaptation. We leverage the long sequence modeling capability of a State-Space Model called Mamba, aiming to enhance its applicability in visual data generation while addressing its critical overlook regarding spatial continuity and interaction-contact among scene elements, preventing interpenetration-collisions. Building on this insight, we introduce Zigzag Mamba, a zero-parameter, plug-and-play method that not only surpasses Mamba-based benchmarks but also demonstrates improved speed and memory utilization against transformer-based baselines. Furthermore, Zigzag Mamba's strategic supervision, along with its integration with the Stochastic Interpolant framework, extends its scalability to generate high-resolution visual datasets including FacesHQ at $1024\\times 1024$, and UCF101, MultiModal-CelebA-HQ, MS COCO at $256\\times 256$, and incorporates pose adjustments for enhanced realism in learning to manage patches of visual information for each person depicted. As a result, this study emphasizes the importance of strategic supervision in learning to overcome the limitations of current diffusion models, providing a significant step forward in the field. Code will be made available at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Mihai_Fieraru1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13798v1",
  "title": "Hierarchical NeuroSymbolic Approach for Action Quality Assessment",
  "modified_abstract": "This study introduces a novel neuro-symbolic paradigm for action quality assessment (AQA), an area that crucially benefits from improvements in interpretability and bias reduction in computer vision applications, particularly those informed by subjective human judgments. Inspired by advancements in the accurate assessment of complex, unpredictable actions in specialized domains such as fetal echocardiography, where stochastic fetal motions are detected and analyzed using sophisticated models, our work applies to the quantitative assessment of human actions like diving. Our approach leverages neural networks for the extraction of interpretable symbols from video data, subsequently applying domain-specific rules for the objective quality assessment. This system addresses current transparency and bias issues inherent in purely neural solutions by providing a detailed representation that includes objective breakdowns favorably received by domain experts when compared to traditional methods. Achieving state-of-the-art results in action recognition and temporal segmentation, our system also generates comprehensive reports dissecting each dive's components alongside objective scoring with visual evidence of motion. This not only aids judges in their scoring but also serves as a valuable feedback tool for divers and serves as a training resource for judges. We commit to open-sourcing both our annotated training data and the code to facilitate reproduction and further research in this domain.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~A_Patra1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13797v1",
  "title": "Bridge the Modality and Capacity Gaps in Vision-Language Model Selection",
  "modified_abstract": "This work is motivated by the burgeoning field of Vision Language Models (VLMs) and their significant strides in tasks that require a nuanced understanding of both visual and textual data, including the detection of entities in images based on natural language descriptions and phrases. Specifically, our work is inspired by prior research on employing VLMs for novel applications such as zero-shot object grounding from natural language queries, where the ability to interpret and act upon unseen categories or phrases is crucial. We address the challenge of selecting the most appropriate Pre-Trained VLM for specific zero-shot image classification tasks from the VLM Zoo without needing access to the dataset's images. Our investigation highlights two main hurdles: the \"Modality Gap\" -- inconsistency between textual and visual data processing, and the \"Capability Gap\" -- the discrepancy in VLM performance across different tasks including detection of unseen entities. To counteract these issues, we introduce a method named VLM Selection With gAp Bridging (SWAB). SWAB employs optimal transport for capturing relevance between available datasets and the target task, including grounding unseen phrases, facilitating a more accurate assessment of a VLM's performance on new datasets by bridging both identified gaps and efficiently handling sparse data scenarios. Through experimentation across various datasets and VLMs, we validate the effectiveness of our approach, laying the groundwork for more nuanced and efficient VLM selection methodologies.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Arka_Sadhu1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13793v1",
  "title": "Evaluating Frontier Models for Dangerous Capabilities",
  "modified_abstract": "Our research endeavor is animated by a pressing contemporary challenge in artificial intelligence (AI) research: to robustly gauge the potential risks associated with pioneering AI systems. This motivation is fueled by a legacy of innovation in related fields, such as the development of methodologies like likelihood-free inference for simulator-based models\u2014a technique that has significantly impacted diverse domains from population genetics to economics, demonstrating its wide range of applications. Drawing upon this rich corpus of interdisciplinary work across various fields, we propose a novel framework for assessing \"dangerous capabilities\" in emerging AI systems, with a focus on four critical areas: persuasion and deception, cybersecurity, self-proliferation, and self-reasoning. We pilot our evaluation framework on the Gemini 1.0 models with software specially designed for this purpose, integrating our understanding of simulator-based applications and statistical analysis to detect early warning signs. Although our initial assessments do not uncover strong evidence of dangerous capabilities, they register important early warning signs in the posterior assessments, leveraging advanced statistics to interpret potential threats. Through this work, which carefully considers the distributions of risks and their implications on diverse communities, we aspire to contribute to the foundational science of dangerous capability evaluation, setting the stage for more rigorous analysis of future models.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Owen_Thomas1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13787v1",
  "title": "RewardBench: Evaluating Reward Models for Language Modeling",
  "modified_abstract": "In the quest to refine reinforcement learning from human feedback (RLHF) techniques, our study introduces RewardBench, an innovative benchmark specifically designed to meticulously evaluate reward models (RMs), a core component that shapes the effectiveness of RLHF by aligning pretrained language models with human preferences. This endeavor is fueled by a critical observation of the existing landscape in machine learning research, where frameworks and tools such as GriddlyJS have played a pivotal role in advancing our capabilities to design, simulate, and test reinforcement learning (RL) environments, including grid-world scenarios. GriddlyJS addressed challenges by enabling the creation of complex, procedurally generated RL environments through an accessible web-based interface, featuring agent-environment interactions and puzzle-solving elements, thus highlighting the importance of both the environment and metrics used in the context of ML research. Building upon these advancements in procedural content generation, puzzle-solving, and with an intuitive interface, RewardBench aims to shed light on the somewhat opaque practices surrounding reward models, by providing a comprehensive dataset and codebase for their evaluation. RewardBench encompasses prompt-win-lose trios across diverse areas like chat, reasoning, and safety to test RMs against challenging, structured, and out-of-distribution queries. Our carefully curated comparison datasets allow for nuanced differentiation between model responses based on verifiable criteria such as inaccuracies or bugs, and incorporating elements of curriculum learning in the development of these tests ensures a more comprehensive evaluation. By positioning various RMs on the RewardBench leaderboard, we assess a wide array of training techniques from direct Maximum Likelihood Estimation (MLE) training to more indirect methods like Direct Preference Optimization (DPO), and across diverse datasets to derive insights into model behaviors including their propensity for refusal, reasoning capabilities, puzzle-solving prowess, and the ability to follow instructions. These insights are pivotal in advancing our understanding of the RLHF process, paving the way for more aligned, effective, and human-centric language models.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Mikayel_Samvelyan1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13785v1",
  "title": "Towards an extension of Fault Trees in the Predictive Maintenance Scenario",
  "modified_abstract": "This research is inspired by seminal efforts in advancing modeling and analytical techniques, such as the innovative work on Determinantal point processes (DPPs) which demonstrated new capabilities in subset selection through advanced kernel representations. Emulating such advancements, this paper aims to address an emerging challenge within the dependability sector: Predictive Maintenance, leveraging the proven simplicity and applicability of Fault Trees (FTs). By proposing an extension to traditional FTs, this study not only acknowledges but also effectively responds to the evolving requirements of modern industrial processes. The introduction of the Predictive Fault Tree language, alongside practical use cases, exemplifies a pivotal step towards enriching our modeling repertoire to support comprehensive analysis, evaluation, and selection within diverse industrial contexts. Moreover, by integrating a machine learning component posteriori into the FT methodology, this extension ensures that continuous learning and improvement\u2014achieved through efficient algorithms\u2014are central features, accommodating the ever-changing landscape of industrial needs and predictions. This approach is marked by its linear-complexity, making it particularly beneficial for managing large-item inventories where predictive maintenance can significantly reduce downtime.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Victor-Emmanuel_Brunel1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13784v1",
  "title": "The Model Openness Framework: Promoting Completeness and Openness for Reproducibility, Transparency and Usability in AI",
  "modified_abstract": "Inspired by previous works that address the challenges in promoting fairness, transparency, and reducing bias in AI through innovative post-processing techniques and frameworks, we propose the Model Openness Framework (MOF). Generative AI (GAI) offers unprecedented possibilities but its commercialization has raised significant concerns about transparency, reproducibility, bias, safety, and fairness in prediction algorithms. This situation is further complicated by \"open-source\" GAI models that lack the necessary components for a full understanding and reproduction, a practice we identify as \"openwashing.\" The MOF, a ranked classification system, rates machine learning algorithms based on their completeness and openness, adhering to open science, open source, open data, and open access principles. By specifying components of the model development lifecycle, including loss functions and post-processing methods, and advocating for their release under appropriate open licenses, the MOF aims to counteract the misrepresentation by models claiming openness, guide researchers and developers towards full transparency in algorithms, and assist various stakeholders in identifying models that can be adopted without restrictions as reliable classifiers. The anticipated wide adoption of the MOF is intended to foster a more open AI ecosystem, thereby accelerating research, innovation, and adoption in a manner that is ethical, transparent, and user-friendly.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ibrahim_Alabdulmohsin1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13781v1",
  "title": "Sparse Implementation of Versatile Graph-Informed Layers",
  "modified_abstract": "Building on the intriguing discovery of performance-matching untrained subnetworks in Graph Neural Networks (GNNs), which highlights the potential of untrained sparse architectures to bypass common issues like over-smoothing and enhance robustness against perturbations, we introduce a sparse implementation of Graph-Informed (GI) layers. These innovations are targeted at addressing the inefficiencies presented by dense memory allocation in the current implementation of GI layers, which are pivotal in extending GNN applications beyond traditional boundaries to tasks such as node-level regression. By leveraging the natural sparsity of adjacency matrices, our work significantly reduces memory usage, thereby increasing the computational efficiency and scalability of GI layers. Initialized with a well-designed sparse pattern, these layers, once trained, are ready to be deployed, furthering their application potential to subsets of graph nodes and improving their performance in the face of out-of-distribution data. This convolutional approach fosters the development of deeper Graph-Informed Neural Networks (GINNs) and broadens their applicability to larger graphs, marking a significant leap forward in the efficient and scalable learning on graph-structured data.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Tianjin_Huang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13771v1",
  "title": "Describe-and-Dissect: Interpreting Neurons in Vision Networks with Language Models",
  "modified_abstract": "Motivated by the significant advancements in vision networks, notably through the introduction and refinement of Vision Transformers (ViTs) that have shown unparalleled flexibility and robustness in image analysis, our work proposes Describe-and-Dissect (DnD), a groundbreaking method for elucidating the roles of hidden neurons within these networks. Leveraging the latest strides in multimodal deep learning and shape-based analysis techniques, DnD provides complex natural language descriptions of neuron functions in vision networks without requiring labeled training data or a predefined concept lexicon, and without needing to train new models. Enhanced through self-attention mechanisms, segmentation techniques, and considering occlusions, our approach delves deeper into understanding shape-based differences in images. Given the increasing prevalence of adversarial attacks, DnD also enhances the interpretability of networks by identifying neurons that are particularly influential for classifying adversarial patches, a critical aspect for understanding shape-based differences in images. Furthermore, our methodology represents a leap in interpretability methods by enabling straightforward integration with increasingly capable general-purpose models, including few-shot learning scenarios, potentially in an ensemble framework in the future. With a distinct emphasis on performance, we present comprehensive qualitative and quantitative analyses to demonstrate that DnD surpasses previous methodologies, supplying higher quality neuron descriptions, and being recognized as the superior explanation for neuron functionality over two times more frequently than the leading alternatives. This advancement underlines a substantial step toward achieving a more interpretable and transparent application of vision networks, driven by the ineffable supremacy and robustness that ViTs and similar technologies offer.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Kanchana_Nisal_Ranasinghe1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13765v1",
  "title": "Towards Principled Representation Learning from Videos for Reinforcement Learning",
  "modified_abstract": "We study pre-training representations for decision-making using video data, which is abundantly available for tasks such as game agents and software testing. Even though significant empirical advances have been made on this problem, a theoretical understanding remains absent. We initiate the theoretical investigation into principled approaches for representation learning focusing on learning the latent state representations of the underlying MDP (Markov Decision Process) using video data, incorporating optimization techniques to navigate the complexities related to various settings. We study two types of settings: one where there is iid noise in the observation, presenting an optimization challenge within certain constraints, and a more challenging setting where there is also the presence of exogenous noise, which is non-iid noise that is temporally correlated, such as the motion of people or cars in the background - painting a dynamic highway scene indicative of a driving application. This scenario particularly reflects challenges encountered in driving applications. We study three commonly used approaches: autoencoding, temporal contrastive learning, and forward modeling employing function approximation. We prove upper bounds for temporal contrastive learning and forward modeling in the presence of only iid noise, illustrating that these soft learning approaches can reward efficient downstream reinforcement learning (RL) with polynomial sample complexity. When exogenous noise is also present, we establish a lower bound result showing that the sample complexity of learning from video data can be exponentially worse than learning from action-labeled trajectory data, justifying the heightened difficulty in settings typical of reinforcement learning with video pre-training. We evaluate these representational learning methods in two visual domains, yielding results that are consistent with our theoretical findings and apply optimization techniques learned by addressing the challenges within these settings.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Guiliang_Liu1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13749v1",
  "title": "Weisfeiler and Leman Go Loopy: A New Hierarchy for Graph Representational Learning",
  "modified_abstract": "Our work is inspired by significant strides in graph-based representational learning, particularly in areas like 3D molecular graphs, where understanding and capitalizing on complex structures is key. Building upon these advancements, we introduce $r$-loopy Weisfeiler-Leman ($r$-$\\ell{}$WL), a novel hierarchy of graph isomorphism tests, alongside a corresponding graph neural network (GNN) framework, $r$-$\\ell{}$MPNN, that uniquely counts cycles up to length $r + 2$. Notably, $r$-$\\ell{}$WL is highlighted for its ability to count homomorphisms of cactus graphs, going beyond the classical 1-WL's capability and presenting a distinct approach compared to $k$-WL for any fixed $k$. Through empirical evaluations on both synthetic and real-world datasets, we demonstrate the enhanced expressive and counting power of $r$-$\\ell{}$MPNN, marking significant predictive performance improvements over traditional learning methods. The comparison with pioneering works such as ComENet illustrates the marked progression in efficiently utilizing complex data structures for molecules, thereby advancing the field of neural graph representational learning and its applicability to real-world problems. The architecture of $r$-$\\ell{}$MPNN integrates advanced message-passing mechanisms with neural network architecture, reflecting an evolution in 1-hop learning approaches to embody greater depth and nuance in graph analysis. This integration not only serves to enrich the representational capabilities of GNNs but also brings forward guarantees of more effective learning. The code is available at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yuchao_Lin1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13748v1",
  "title": "An Ordering of Divergences for Variational Inference with Factorized Gaussian Approximations",
  "modified_abstract": "Our investigation is inspired by a series of pioneering efforts in the realm of variational inference (VI), particularly those that challenge conventional norms by exploring alternatives to the widely employed Kullback-Leibler (KL) divergence, such as the work on uniform stability for first-order empirical risk minimization. Given an intractable distribution $p$, we delve into the problem of VI, aiming to compute the best approximation $q$ from a more tractable family $\\mathcal{Q}$, usually by minimizing a divergence. While the KL divergence is the most common choice, we consider other divergences, examining their impacts when approximating a Gaussian with a dense covariance matrix by a Gaussian with a diagonal covariance matrix. We demonstrate that these divergences can be \textit{ordered} according to how their variational approximations misestimate measures of uncertainty (e.g., variance, precision, and entropy). Our analysis also unveils an impossibility theorem, highlighting that no single factorized approximation can simultaneously match more than one of these uncertainty measures, pointing to the divergence choice as a determinant of which measure, if any, might be accurately estimated. This research spans the KL divergence, the R\\'enyi divergences, and a score-based divergence, empirically evaluating their efficacy in approximating non-Gaussian distributions in VI contexts.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Amit_Attia1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13740v1",
  "title": "Uncertainty-Aware Explanations Through Probabilistic Self-Explainable Neural Networks",
  "modified_abstract": "This research builds upon the foundation laid by various approaches seeking to enhance the transparency and reliability of Deep Neural Networks (DNNs), specifically addressing the limitation in explainability that hampers their use in critical applications. Drawing inspiration from innovative methods such as PRANC, which explores efficient model compacting techniques for massive neural network architectures, and utilizing pseudo-randomly generated data for analysis, our work introduces a probabilistic reformulation of Prototype-Based Self-Explainable Neural Networks (PSENNs), termed Prob-PSENN. Unlike traditional PSENNs that rely on static, pointwise prototype estimates, Prob-PSENN replaces these with probability distributions, offering a dynamic framework that captures the model's explanatory uncertainty and has implications for storage efficiency and communication protocols within model deployments. This not only allows for a more nuanced understanding and flexibility in the prototype learning phase but also enables the identification and proper handling of uncertain or uninformed predictions by the model. Through our experiments conducted on various datasets, including TinyImagenet, we demonstrate that Prob-PSENN notably improves the meaningfulness and robustness of explanations compared to non-probabilistic counterparts, thus advancing the field of explainable AI by enhancing both the explainability and reliability of DNNs. Agents capable of interpreting these probabilistic explanations are envisioned as future contributors to the system, ensuring more responsible and informed decision-making.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ali_Abbasi1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13729v1",
  "title": "Reinforcement Learning for Online Testing of Autonomous Driving Systems: a Replication and Extension Study",
  "modified_abstract": "This study embarks on an important journey in the computational field by closely following a path laid by previous works that investigated the synthesis of evolved algorithms in reinforcement learning (RL), especially those focusing on the meta-learning of RL algorithms across various environments, including control tasks, gridworld scenarios, and simulated games. Reinforcement Learning (RL), used in conjunction with many-objective search and bootstrap methods, has reportedly outperformed traditional techniques, such as random search, for the online testing of complex systems like Autonomous Driving Systems (ADS), powered by Deep Neural Networks. We replicate and extend a notable empirical study within this realm. Our replication indicates that, contrary to the initial findings, RL does not surpass random test generation when eliminating confounding factors related to collision measurement methodologies. Additionally, our extension, focusing on meta-learning techniques, targets overcoming the observed limitations of RL, including improper reward structuring and the inadequacy of discrete state-space representation through Q-learning, towards achieving a more value-based and effective training approach. Through enhancements in the RL framework and algorithm selection, our revised RL approach demonstrably achieves superior performance compared to random testing, showcasing remarkable generalization abilities across different testing tasks and environments, further underscored by the integration of graph-based meta-learning strategies. These findings not only validate the potential of RL in the rigorous testing of ADS but also open avenues for future research on optimizing RL applications for online system testing.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~John_D_Co-Reyes1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13728v1",
  "title": "M-HOF-Opt: Multi-Objective Hierarchical Output Feedback Optimization via Multiplier Induced Loss Landscape Scheduling",
  "modified_abstract": "Drawing inspiration from the complex realms of over-parameterized nonlinear systems and neural networks, where the intricate loss landscapes and non-convexity optimization challenges have compelled rigorous exploration, our work introduces a novel perspective on managing the convoluted loss function landscapes seen in deep learning applications through 'M-HOF-Opt'. This methodology, deeply rooted in theory, leverages a probabilistic graphical model (PGM) to navigate the intricate combinatorial problem of weight multiplier selection that characterizes neural network parameterized loss functions, thus addressing the pressing demand for efficient optimization strategies in such challenging environments. Through a strategic deployment of hypervolume-based likelihood for multi-objective descent across each loss term and the framing of parameter and multiplier dynamics as an optimal control problem deeply grounded in theory, we chart a path towards hierarchical resolution of multi-objective descent ambitions, steering clear of the pitfalls associated with kernel-based methods in such high-dimensional settings. This culminates in a system where multiplier adjustments, guided by output feedback for each loss term, systematically sculpt the loss landscape, heralding a multiplier-free and computationally efficient epoch-scale optimization process that embraces the learning space. The efficacy of our approach is underscored by its application to domain invariant variational auto-encoding involving six distinct loss terms for the PACS domain generalization task, where it exhibits robust performance outclassing conventional multiplier scheduling methods across various scenarios, thereby offering practical guarantees. With its modular design, our method invites integration into a broad spectrum of deep learning endeavors, promising a versatile toolkit for addressing the multi-faceted optimization challenges inherent in today's complex machine learning landscapes.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Chaoyue_Liu2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13724v1",
  "title": "Probabilistic Forecasting with Stochastic Interpolants and F\u00f6llmer Processes",
  "modified_abstract": "Informed by the progress in probabilistic modeling techniques such as Markov chain Monte Carlo (MCMC) methods for state inference, this study introduces a novel framework for the probabilistic forecasting of dynamical systems through generative modeling. Utilizing observations of system states over time, our approach focuses on sampling from the conditional distribution of future system states based on the current state, employing the concept of stochastic interpolants. This state-space modeling technique enables the creation of a generative model transitioning an arbitrary base distribution to the targeted conditional distribution. We introduce a construct of fictitious, non-physical stochastic dynamics, initiated by the current system state, to unbiasedly generate a target conditional distribution sample in finite time. This transforms a point mass centered at the current state into a probabilistic forecast ensemble. We establish that the stochastic differential equation (SDE)'s drift coefficient, facilitating this transformation, is non-singular and can be efficiently learned through square loss regression against time-series data, marking a state-of-the-art approach in dynamical system forecasting. Subsequently, chain inference in the context of our proposed state-space model, employing stochastic differential equations, leads to a F\u00f6llmer process definition. Our methodology's effectiveness is validated on challenging, high-dimensional forecasting scenarios, including those governed by stochastically forced Navier-Stokes equations and video prediction tasks on the KTH and CLEVRER datasets.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Alex_Shestopaloff1",
  "manipulated_ranking": 4,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13704v1",
  "title": "Improving the Adaptive Moment Estimation (ADAM) stochastic optimizer through an Implicit-Explicit (IMEX) time-stepping approach",
  "modified_abstract": "Drawing inspiration from a diverse range of machine learning optimization strategies, particularly those focusing on merging models and enhancing optimization performance through intermediate-task learning and transfer knowledge, this work addresses the limitations of the conventional Adam optimizer. Recognized for its utility in neural network training, the Adam optimizer can be understood as a first-order implicit-explicit (IMEX) Euler discretization of an underlying ordinary differential equation (ODE) in the context of exceedingly small learning rates. By adopting a time discretization perspective and combining several optimization strategies, including averaging techniques with advanced methodologies such as transfer learning, we introduce novel extensions to the Adam algorithm through the application of higher-order IMEX methods to solve the ODE more efficiently. Our contributions culminate in the development of a new optimization algorithm that eclipses the classical Adam in terms of efficiency and efficacy across a variety of regression and classification challenges, leveraging the merging of models and the transfer of knowledge for enhanced optimization.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Michael_S_Matena1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13701v1",
  "title": "What Matters for Active Texture Recognition With Vision-Based Tactile Sensors",
  "modified_abstract": "This paper is anchored in the burgeoning field of robotic perception, particularly focusing on the nuanced transition from vision-based paradigms to tactile sensing solutions in fabric texture recognition. It draws inspiration from various domains within robotic learning, including the scalable multi-task, multi-scene robot manipulation as detailed in previous works, which has underscored the importance of diverse datasets and robust data augmentation techniques in achieving generalization and improving recognition capabilities in robotics. Our work zeros in on active sensing strategies using vision-based tactile sensors for the specific application of recognizing fabric textures. We formalize the active sampling problem within the tactile domain and delve into the deployment of information-theoretic exploration strategies aimed at minimizing the predictive entropy and variance within probabilistic models. Through empirical investigations, including ablation studies and benchmark comparisons with human performance, we uncover pivotal factors influencing rapid and accurate texture recognition. Notably, we elucidate the relative impact of active exploration strategies, architecture decisions, uncertainty representations, the role of data augmentation, and dataset variability, including distractor collection, on successful texture identification. Our findings, validated on a publicly available dataset and a real-world robotic setup, reveal that while active exploration strategies have a marginal effect on performance, the integration of specific network architectures, refined uncertainty representations, strategic data augmentation, and dropout rates significantly enhance the generalization efficacy of tactile sensing for fabric texture classification. Remarkably, our optimized approach outperforms human accuracy in texture recognition under constrained touch iterations, evidencing the potential of vision-based tactile sensors in nuanced perceptual tasks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Homanga_Bharadhwaj1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13695v1",
  "title": "Loss Regularizing Robotic Terrain Classification",
  "modified_abstract": "In addressing the challenge of robotic terrain classification, this work is largely inspired by the recent advances in deep learning that seek to optimize model generalization and performance. Notably, methodologies like sharpness-aware minimizers and gradient-based learning rates have proven to enhance deep learning algorithms' generalization by adjusting learning rates and updates to navigate loss function landscapes more effectively. This allows the model to converge on the most efficient solution with fewer iterations, thereby reducing the number of updates needed for optimum performance. Conventional classifiers for terrain classification face several limitations, including susceptibility to overfitting, low accuracy, high variance, and inadequacy for live datasets. Moreover, convolution-based and supervised recurrent models struggle with the dynamic nature of growing datasets and real-time classification requirements. In response, this paper introduces a novel semi-supervised approach leveraging a stacked Long Short-Term Memory architecture, augmented with a new loss regularization technique designed to circumvent the challenges of preprocessing extensive variable-length datasets. With the adoption of advanced optimizers that incorporate sharpness-aware minimizers, this method significantly improves accuracy and addresses the shortcomings of existing models, as evidenced by comparative analyses with other architectures.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Xubo_Yue1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13681v1",
  "title": "PARAMANU-AYN: An Efficient Novel Generative and Instruction-tuned Language Model for Indian Legal Case Documents",
  "modified_abstract": "Drawing upon groundbreaking advances in generative modeling for open-domain question answering, which have successfully harnessed auto-regressive decoders to synergize encoded information from diverse sources, our paper introduces PARAMANU-AYN, an innovative language model meticulously tailored to the nuanced domain of Indian legal documentation, including pivotal texts such as case documents from the Supreme Court of India, the Constitution of India, and the Indian Penal Code. Leveraging an auto-regressive (AR) decoder with advanced attention mechanisms, PARAMANU-AYN is pretrained from scratch with a context size of 8192, undergoing thorough evaluation on perplexity metrics. The model is further refined through instruction tuning on over 10,000 instructions spanning an array of legal functions, encompassing judgement explanation, case summarization, and is augmented with additional legal datasets for enhanced comprehension and answer quality in the open-domain setting. Moreover, benchmarking against GPT-3.5-Turbo's instruction-tuned models reveals our model's superior performance across clarity, relevance, completeness, and legal reasoning, all whilst maintaining efficient CPU inference speeds. Surprisingly, the model's proficiency in providing answers in cross-sample validation suggests not only exceptional domain knowledge acquisition but also outstanding generalizability using limited datasets. This pioneering effort not only sets a new precedent for domain-specialized generative models in legal practice but also signifies a leap towards AI-assisted legal documentation and analysis in the Indian context. Plans for making PARAMANU-AYN publicly available at [omitted for de-identification] underpin our commitment to advancing legal NLP. Our findings posit that robust, domain-specific models are capable of delivering high-quality answers and can be developed with judiciously curated datasets, challenging the prevailing norms around data requisites for training effective generative language models in specialized fields.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Akhil_Kedia1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13872v1",
  "title": "Spatial-Temporal Graph Representation Learning for Tactical Networks Future State Prediction",
  "modified_abstract": "Our work is motivated by a spectrum of advancements in graph prediction technologies, notably the development and implementation of invertible graph neural networks (iGNNs) which have demonstrated significant potential in tackling graph prediction problems through novel architectures that leverage deep generative models for both forward prediction and inverse inference tasks, showcasing invertibility as a defining feature. Building on these precedents, our study introduces the Spatial-Temporal Graph Encoder-Decoder (STGED) framework specifically designed for Tactical Communication Networks. STGED innovatively combines spatial encoding through a graph-based attention mechanism with temporal encoding via a recurrent neural network, culminating in a fully-connected feed-forward network for decoding future network connectivities. This tailored approach allows for the nuanced capture of latent tactical behaviors intrinsic to the fast-changing, multi-hop nature of tactical ad-hoc networks, thus addressing one of the core challenges in effective resource allocation and classification of network behaviors. Our extensive experimental evaluation showcases STGED's superior performance in capturing the dynamics of future states and its robustness in anomaly detection, significantly outstripping baseline models with up to 99.2% accuracy in predicting the future states of tactical communication networks, and emphasizing the crucial role of integrating spatial-temporal graph representation learning in practical, dynamic systems.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Xiuyuan_Cheng1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13658v1",
  "title": "Multimodal Variational Autoencoder for Low-cost Cardiac Hemodynamics Instability Detection",
  "modified_abstract": "Driven by the critical need for accurate and accessible diagnostic tools in healthcare, our work seeks to address the limitations of current non-invasive detection methodologies for cardiac hemodynamic instability (CHDI), drawing inspiration from advances in machine learning for medical image analysis, especially those focused on calibrated predictions and unbiased volume estimation in medical imaging. Existing methods, primarily based on expensive and single-modality approaches like cardiac MRI, face challenges due to the limited size of labeled patient data, a prevalent issue in medical domains. In contrast, our study introduces a novel multimodal variational autoencoder, $\\text{CardioVAE}_\\text{X,G}$, designed to leverage low-cost chest X-ray (CXR) and electrocardiogram (ECG) modalities through a deep, tri-stream pre-training strategy on a vast unlabeled dataset from a subset of the MIMIC database. This strategy facilitates learning shared and modality-specific features, enabling fine-tuning with both unimodal and multimodal datasets, which include expertly calibrated imaging for improved medical analysis precision. Additionally, it integrates advanced pixel/voxel-level calibration techniques for enhanced accuracy in medical diagnostics. Upon fine-tuning with a labeled dataset from the ASPIRE registry, $\\text{CardioVAE}_\\text{X,G}$ demonstrates promising performance metrics (AUROC $=0.79$ and Accuracy $=0.77$), marking a significant advancement in the non-invasive prediction of CHDI while also excelling in generating clinically relevant interpretations of its predictions. Our work not only offers a cost-effective and efficient solution for CHDI detection but also paves the way for future explorations into multimodal diagnostics and treatment planning, extending beyond the learning framework to include aspects of surgical planning where segmentation may become crucial.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Teodora_Popordanoska1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13870v1",
  "title": "ExMap: Leveraging Explainability Heatmaps for Unsupervised Group Robustness to Spurious Correlations",
  "modified_abstract": "Motivated by the quest for models capable of high generalizability across diverse and potentially adversarial environments, as exemplified by emerging research in causal representation learning and causality-inspired approaches to enhance deep learning generalizability, our work introduces ExMap. This novel approach leverages the insight that a trained model\u2019s classification strategies can be inferred from explainability heatmaps to propose an unsupervised two-stage mechanism aimed at enhancing group robustness in classifiers. By employing a clustering module to infer pseudo-labels from a model's explainability heatmaps, ExMap seeks to bridge the performance gap between supervised and unsupervised group robustness strategies, offering a cost-effective alternative to the label-intensive processes typically required. With an emphasis on observational data and leveraging counterfactual reasoning to analyze causally inspired mechanisms, our empirical studies underscore ExMap\u2019s capacity to outperform both partially supervised and wholly unsupervised current methods. ExMap integrates seamlessly with existing group robustness frameworks and is adept at addressing spurious correlations revealed through causal analysis. Furthermore, ExMap's adaptability is showcased through its application to the novel challenge of multiple shortcut mitigation, underscoring its potential in advancing the field of unsupervised learning and providing insightful contributions to the body of knowledge on adversarial examples and graph-based representations. [Code omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Mengyue_Yang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13627v1",
  "title": "Efficient exploration of high-Tc superconductors by a gradient-based composition design",
  "modified_abstract": "Leveraging insights from the fusion of evolutionary algorithms and gradient descent to address non-convex optimization challenges in constructing materials, this work introduces a pioneering material design method focused on gradient-based optimization of compositions for highly specialized tasks. Our approach surpasses the resource-intensive processes traditionally relied upon\u2014exhaustive database searches and conditional generation models\u2014by optimizing inputs via backpropagation, thus closely aligning the model's outputs with the desired material properties for specific learning tasks. This technique enables the identification of materials previously unlisted and the precise determination of their properties, significantly enhancing the learning process in materials science and improving performance across a range of tasks. Notably, our method demonstrates adaptive optimization capabilities under new conditions without the necessity for retraining, accommodating a population of potential compositions with the mutation concept abstractly applied to enhance diversity. When applied to the exploration of high-Tc superconductors, it revealed potential compositions beyond those found in existing databases and facilitated the discovery of new hydrogen superconductors through conditional optimization for the task at hand. This approach marks a significant advancement in material design, enhancing efficiency, search breadth, and adaptability to evolving constraints. The findings presented in this paper underscore the importance and utility of integrating diverse functions in compositional optimization strategies for a variety of tasks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ke_Xue1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13612v1",
  "title": "Does Differentially Private Synthetic Data Lead to Synthetic Discoveries?",
  "modified_abstract": "Within the realm of sharing anonymized versions of sensitive biomedical datasets, synthetic data stands as a pivotal solution, inspired by a lineage of research addressing the intricacies of data privacy, including adjusting for confounders in observational datasets through innovative machine learning methods. Our study is focused on evaluating the Mann-Whitney U test on differentially private synthetic data, encompassing datasets such as prostate cancer and cardiovascular, among others, with an aim to assess the potential impacts on Type I and Type II errors. This investigation is motivated by the paramount objective of differential privacy to balance the preservation of data structure with individual privacy. Through empirical inquiry, we highlight that while most differentially private synthetic data generation methods exhibit inflated Type I errors at stringent privacy levels ($\\epsilon\\leq 1$), suggesting caution in analysis and interpretation, a method leveraging DP smoothed histogram offers a nuanced balance, ensuring validity across privacy levels tested. This research, thus, serves as a critical evaluation on the implications of differential privacy mechanisms on the robustness of statistical hypothesis testing, underpinning the necessity for meticulously balanced privacy approaches. Our findings further suggest a causal relationship between privacy levels and error rates, underscoring the need for randomized trials to verify these correlations. Additionally, our work emphasizes the importance of learning from these findings to improve future sampling strategies.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Rohit_Bhattacharya1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13869v1",
  "title": "Accurately Predicting Probabilities of Safety-Critical Rare Events for Intelligent Systems",
  "modified_abstract": "This research is influenced by the significant advancements in machine learning (ML) algorithm development and the optimization of computational frameworks as seen in projects like JaxMARL, which leverages hardware acceleration for scalable multi-agent reinforcement learning (MARL). In a similar vein, our study addresses a pressing challenge in the deployment of intelligent systems: the prediction of rare safety-critical events\u2014a task that is paramount for the advancement and safety of autonomous technologies. Given the 'curse of rarity' posed by the extreme imbalance of event data in high-dimensional variables, current methodologies often fail to achieve the necessary balance between precision and recall. Our work proposes a novel multi-stage learning framework aimed at overcoming this imbalance through a parallel processing approach, thereby improving the accuracy of 'criticality' predictions\u2014a metric defined for assessing the probability of safety-critical events occurring within a specified timeframe. The parallel nature of the proposed solution effectively leverages computational resources, enhancing both the evaluation and prediction processes with inherent scalability and massively parallel computations. We evaluate our approach across two game scenarios: lunar lander and bipedal walker, showcasing a marked superiority over existing models in accurately forecasting the criticality of situations in intelligent agents' systems. The significance of meta-learning in refining predictive models is also benchmarked, adding depth to our comparative evaluations.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Benjamin_Ellis1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13578v1",
  "title": "Dynamic Reward Adjustment in Multi-Reward Reinforcement Learning for Counselor Reflection Generation",
  "modified_abstract": "Drawing inspiration from recent advancements in large language models (LMs) and augmented generation techniques, such as the effective use of active retrieval to enhance the quality and factual accuracy of generated text, this paper addresses the gap in multi-reward reinforcement learning for natural language generation. Focused on the task of counselor reflection generation, our work seeks to jointly optimize multiple text qualities, including fluency, coherence, and reflection quality of generated counselor responses, producing remarkable outcomes. To this end, we introduce two novel bandit methods, DynaOpt and C-DynaOpt, which combine rewards into a single value for simultaneous optimization using non-contextual and contextual multi-arm bandits to dynamically adjust multiple reward weights during training based on a query of current performance and limiting factors that might affect quality. Our evaluation demonstrates that DynaOpt and C-DynaOpt not only outperform existing na\u00efve and bandit baselines but also underscore the efficacy of bandit-based approaches in enhancing the language model's output, marking a significant step forward in multi-faceted reinforcement learning. The results indicate that our approach leads to generation that is not only large in scope but also precise in achieving the reflection quality that is crucial for effective counselor training, further harnessing the power of retrieve-and-generate methods to access relevant knowledge for context-aware responses.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Frank_F._Xu1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13565v1",
  "title": "AdaTrans: Feature-wise and Sample-wise Adaptive Transfer Learning for High-dimensional Regression",
  "modified_abstract": "In the realm of high-dimensional regression, we address the transfer learning challenge where the feature dimension significantly exceeds the sample size. Motivated by foundational works such as the investigation into covariate shift adaptation, which explores the discrepancies in input distributions between training and test stages, our approach introduces AdaTrans, an adaptive transfer learning method capable of discerning and integrating transferable structures either across features (F-AdaTrans) or source samples (S-AdaTrans). This is achieved through an innovative fused-penalty mechanism, supplemented with dynamically adjustable weights to optimally capture the essence of transferability and adaptation. Moreover, our method leverages a theoretically grounded, data-driven procedure for weight selection, facilitating F-AdaTrans in its nuanced capability to amalgamate transferable facets with the target data while sidelining the non-transferable components, and enabling S-AdaTrans to derive an ideal synthesis of information from each source sample based on adaptation probabilities. We establish non-asymptotic rates for our approach, which encompass existing near-minimax optimal rates under certain conditions. The utility and performance of AdaTrans are demonstrated through experiments conducted on both synthetic and real datasets, substantiating its effectiveness in handling high-dimensional transfer learning scenarios. Furthermore, this study underlines the critical role of machine learning techniques in managing covariate shift adaptation challenges and optimizing probability-based decisions in the transfer learning domain.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ikko_Yamane1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13563v1",
  "title": "DL2Fence: Integrating Deep Learning and Frame Fusion for Enhanced Detection and Localization of Refined Denial-of-Service in Large-Scale NoCs",
  "modified_abstract": "In the realm of Network-on-Chips (NoCs), where the risk of Denial-of-Service (DoS) attacks threatens the integrity and performance of large-scale systems, our work, inspired by prior models such as UnMask that address adversarial attacks in deep learning through innovative means, introduces a sophisticated Flooding Injection Rate-adjustable DoS model. More critically, it presents DL2Fence, a novel framework that leverages Deep Learning (DL) and Frame Fusion (2F) for defensive purposes, delivering unprecedented capabilities in the detection and localization of refined DoS attacks. By developing two tailored Convolutional Neural Networks (CNNs) models for the separate tasks of classifying adversarial traffic patterns and training them with targeted data sets, DL2Fence achieves remarkable detection and localization accuracies of 95.8% and 91.7%, respectively, alongside precision rates of 98.5% and 99.3% in a 16x16 mesh NoC. Our approach goes beyond merely high-lighting threats by actively re-classifying altered traffic profiles to improve system resilience. Furthermore, the framework undergoes comprehensive training and evaluation to significantly reduce hardware overhead by 76.3% when scaling from 8x8 to 16x16 NoCs, demonstrating that it performs better than existing state-of-the-art solutions, and demands 42.4% less hardware in comparison. These results underscore DL2Fence's efficacy in not only enhancing the detection and localization of DoS attacks in large-scale NoCs but also in doing so with significantly reduced hardware requirements.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Shang-Tse_Chen1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13551v1",
  "title": "Ground-A-Score: Scaling Up the Score Distillation for Multi-Attribute Editing",
  "modified_abstract": "In direct response to the limitations and challenges observed in recent advancements within text-to-image diffusion models\u2014as highlighted by the innovative strides in open-vocabulary object detection where the synergistic combination of vision transformers and large-scale pre-training has showcased substantial improvements in recognizing and processing diverse and long-tailed data\u2014our work introduces Ground-A-Score. Ground-A-Score is a novel method that advances the frontier of image editing by incorporating grounding in score distillation, a technique yet unexplored in the exhaustive application to multi-attribute editing. By embedding a precise understanding of object locations within an image, Ground-A-Score innovatively ensures that complex text prompts are translated into editing actions without overlooking request nuances due to processing bottlenecks, utilizing architectures and regularizations to refine the fidelity of the edited images. Enhanced with a selective editing mechanism through a new penalty coefficient and contrastive loss, the methodology distinctively targets editing areas while maintaining the source image's object integrity and performance. Through a series of qualitative and quantitative analyses, Ground-A-Score is demonstrated to significantly improve adherence to rigorous and varied prompt demands, thus ensuring outcomes of heightened quality that respect original image attributes and detection capabilities. This achievement showcases the potential of integrating massive training analysis and detection capabilities for refining image editing processes.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Alexey_Dosovitskiy1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13547v1",
  "title": "Integrating Large Language Models for Severity Classification in Traffic Incident Management: A Machine Learning Approach",
  "modified_abstract": "In tandem with the strides achieved in the use of neural networks and Bayesian methods for classification tasks, our study investigates the potency of large language models to augment the process of classifying the severity of traffic incidents. It assesses the effectiveness of features generated by state-of-the-art language models in enhancing or equating the prediction accuracy of conventional machine learning algorithms like Gradient Boosted Decision Trees, Random Forests, and Extreme Gradient Boosting when applied to accident report narratives. Our approach juxtaposes features directly culled from incident reports with those derived from language models, and their amalgamations, in executing severity classifications. The results demonstrate that integrating linguistic model-derived features with traditional data sources either enhances or meets the efficacy of machine learning techniques, notably with Random Forests and Extreme Gradient Boosting, in attributing severity levels to traffic incidents. These advancements are measured using the F1-score and the softmax function across equitably sampled datasets for a balanced representation of severity categories. The crux of this research lies in exemplifying how large language models can seamlessly blend into machine learning workflows for incident management, thereby streamlining the feature extraction process from unstructured texts and preserving or elevating the accuracy of severity classifications within a customary machine learning scaffolding. This study underlines the utility of linguistic models in refining the model design for severity classification of traffic incidents, offering meaningful insights into leveraging combined traditional and language processing-driven data to fortify machine learning approaches in the domain of traffic incident severity classification.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Taejong_Joo1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13545v1",
  "title": "Next day fire prediction via semantic segmentation",
  "modified_abstract": "Drawing inspiration from the advancements in modeling discrete data types and their application in ML domains, our study introduces a novel deep learning pipeline for next day fire prediction. This innovative approach shifts the foundational framework from viewing the issue as a binary classification task based on tabular feature vectors to a more complex, image-based semantic segmentation task. Each pixel in this new formulation represents a daily snapshot of an area, with its channels mirroring the previously tabular training features. Such a transformation leverages recent successes in discrete data handling, permutation of data, and semantic analysis, significantly enhancing the task's objectives by achieving state-of-the-art results through this reimagined problem formulation and pipeline. Our method particularly benefits from the permutation of data and employs sophisticated tree-structured learning strategies, which encode the data straightforwardly in the complex, multi-dimensional image data.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jim_Lim1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13537v1",
  "title": "What explains the success of cross-modal fine-tuning with ORCA?",
  "modified_abstract": "Drawing inspiration from innovative techniques in the domain of unsupervised domain adaptation (UDA) which leverage contrastive learning to disentangle domain and class information for superior generalization across dissimilar datasets through adaptation and vision-focused strategies, our investigation explores the efficacy of ORCA (Shen et al., 2023), a novel approach in cross-modal fine-tuning. ORCA, which applies pre-trained transformer models across different data modalities through an embedder training and model fine-tuning protocol, has shown promise across a spectrum of downstream tasks. However, the underlying mechanisms of ORCA's success, potentially including adversarial resilience as inferred by its robustness in various settings and the ability to utilize unlabeled data effectively, remain inadequately understood. We undertake a series of ablation studies to dissect the contributions of its constituent phases, discovering that embedder training is ineffective for 2D vision tasks, and while somewhat beneficial for 1D tasks, exhibits diminishing returns with extended training. Contrary to initial claims, our findings indicate that model fine-tuning, a process deeply intertwined with contemporary adversarial theory, emphasizing the importance of class generalization and fostering domain-invariant representations, is principally responsible for performance improvements in four out of six datasets examined. This analysis, fundamentally grounded in unsupervised learning principles and the utility of unlabeled data for potentiation, contributes a nuanced understanding of ORCA's components, refining our comprehension of cross-modal fine-tuning's operational dynamics.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sang_Michael_Xie1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13523v1",
  "title": "Have You Poisoned My Data? Defending Neural Networks against Data Poisoning",
  "modified_abstract": "In the era where neural networks have become fundamental in processing vast amounts of data for various tasks, the integrity of training data is paramount. Our investigation is motivated by a background of recent advances, such as in the field of regularization techniques and hyper-parameters optimization, which have demonstrated significant impacts on model generalization and performance, notably through unconventional methods like Guillotine Regularization. This study introduces a novel defense mechanism against clean-label poisoning attacks within the transfer learning framework, leveraging a characteristic vector representation to discern and mitigate poisoned datapoints. Our analytical and experimental methodology showcases the ability to effectively differentiate between poisoned and clean data through meticulous hyper-parameters tuning and selection processes, thus enhancing the robustness of neural networks against adversarial manipulations and promoting generalization. The comprehensive evaluation against contemporary state-of-the-art defenses across multiple architectures and datasets, in various transfer learning scenarios and adversarial budgets, is bolstered by rigorous training and experiments, revealing superior defense rates and model efficacy in all examined scenarios.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Adrien_Bardes1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13522v1",
  "title": "REAL: Representation Enhanced Analytic Learning for Exemplar-free Class-incremental Learning",
  "modified_abstract": "Drawing on the foundational advances in class-incremental learning (CIL), particularly insights from recent studies such as the adaptation and critique of batch normalization (BN) techniques within the CIL framework, this paper introduces a novel approach known as representation enhanced analytic learning (REAL) specifically designed for exemplar-free class-incremental learning (EFCIL). REAL aims to mitigate the issue of catastrophic forgetting\u2014a persistent challenge in the realm of CIL\u2014without relying on historical data for training. By constructing a dual-stream base pretraining (DS-BPT) and a representation enhancing distillation (RED) process, REAL substantially improves the representation quality of the neural network's feature extractor and classifier. The DS-BPT strategy integrates both supervised and self-supervised contrastive learning (SSCL) to extract foundational knowledge efficiently, while the RED process facilitates knowledge transfer from the supervised domain to the SSCL pre-trained backbone, thus refining the approach to deep class-incremental learning by reformulating it as a recursive least-square problem. This method not only addresses the discriminability deficits encountered when using a frozen backbone in existing AL-based CIL strategies but also demonstrates superior performance on various datasets, including CIFAR-100, ImageNet-100, and ImageNet-1k, surpassing state-of-the-art EFCIL methods and offering competitive results against rehearsal-based approaches.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jun_Shu1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13866v1",
  "title": "The Bid Picture: Auction-Inspired Multi-player Generative Adversarial Networks Training",
  "modified_abstract": "Drawing from the realm of adversarial strategies, such as the intricate action poisoning attacks in reinforcement learning, this article introduces auction-inspired multi-player generative adversarial networks (GANs) training as a novel approach to address the mode collapse problem integral to GANs. Mode collapse, a critical issue where the generator produces a restricted diversity of samples, thus limiting the potential of GANs, is tackled by expanding the conventional two-player framework of GANs into a dynamic multi-player scenario. In this enriched setting, the valuation of each model\u2014and consequently its training trajectory\u2014is influenced by an auction-like mechanism where bids from other models provide a continuously evolving landscape. This not only encourages diversity in the generated samples but also introduces a more robust adversarial training environment, reflecting the complex interactions and challenges observed in adversarial attacks within reinforcement learning contexts. The observation of these behaviors and interactions aims to sublinearly increase the learning efficiency and minimize regret, bringing a nuanced understanding and improvement in GANs training philosophy.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Guanlin_Liu1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13502v2",
  "title": "Adversarial Attacks and Defenses in Automated Control Systems: A Comprehensive Benchmark",
  "modified_abstract": "Drawing inspiration from the critical examination of adversarial training's impact on deep learning models, particularly in robot learning and safety-critical applications, this study extends the investigation to Automated Control Systems (ACS). It underscores the integration of machine learning to enhance decision-making in industrial process management, while also addressing the urgent need for robust defense mechanisms against norm-bounded adversarial attacks. This study explores the vulnerabilities of deep learning models used for fault diagnosis in ACS by evaluating three neural networks with different architectures against six types of adversarial attacks, including those with robot-specific and transient perturbations, alongside the assessment of five distinct defense methods that are rated according to rigorous safety specifications. Our results not only unveil the pronounced susceptibility of models to adversarial perturbations but also offer a comparative analysis of the defenses' effectiveness in robot-learning applications, with a keen focus on meeting stringent safety specifications. Contributing to the current discourse, we introduce a novel defense strategy that synthesizes multiple methodologies, showcasing its superior capability to safeguard machine learning-driven ACS against a range of adversarial threats, including transient ones. This work proffers substantial insights into fortifying ACS against adversarial threats in robot learning, hence bolstering the reliability of fault diagnosis in industrial settings, and highlights the critical role of continuous learning in robot applications.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Mathias_Lechner1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13501v1",
  "title": "VSTAR: Generative Temporal Nursing for Longer Dynamic Video Synthesis",
  "modified_abstract": "Inspired by recent advancements and challenges in the text-to-video (T2V) and video instance segmentation (VIS) domains, where innovative transformer-based approaches have significantly optimized inter-frame communication, reduced computational demands, and improved segmentation processing through effective information-passing mechanisms, our study introduces an innovative approach, Generative Temporal Nursing (GTN), to specifically address the limitations of current open-sourced T2V diffusion models. These models often struggle to generate longer videos with dynamic, evolving content, synthesizing quasi-static videos instead. Our proposed solution, VSTAR, comprises two novel techniques: Video Synopsis Prompting (VSP), which leverages Large Language Models (LLMs) for automatic generation of a video synopsis from a single prompt to guide the visualization of longer videos, and Temporal Attention Regularization (TAR), a method to selectively refine the temporal attention units in pre-trained T2V models, enhancing their ability to capture desired visual dynamics over time. This dual approach not only counteracts the neglect of temporal change inherent in existing models but also sets a new benchmark in generating highly dynamic, visually compelling longer videos through frame-to-frame segmentation and instance-specific communications. By focusing on the exchange of relevant instance information and addressing computational overhead in the segment processing mechanisms of these models, we ensure that our approach is not only effective but also practically viable. Our analysis of temporal attention maps, with and without the application of VSTAR, further highlights the significance of our methodology in pushing the boundaries of what's achievable with T2V synthesis today.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Miran_Heo1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13865v1",
  "title": "Graph Neural Network for Crawling Target Nodes in Social Networks",
  "modified_abstract": "Inspired by the innovative methodologies and insights brought forward in the field of Graph Neural Networks (GNNs) through works like GraphWorld, which introduced novel benchmarking systems for evaluating GNN models across a wide array of synthetic graphs, our study aims to tackle the intricacies of social network crawling. This includes the complex task of collecting target nodes within an initially unknown graph structure, constrained by a limited budget of crawling steps. Leveraging the strengths of GNNs, our paper demonstrates their efficacy compared to traditional classifiers, particularly in scenarios involving the prediction of node properties based on incomplete neighborhood information. Furthermore, we introduce a training sample boosting strategy designed to enhance the training set's variety and performance in the early phases of crawling, thereby significantly improving the quality of the predictor. Our exhaustive experimental analysis across diverse target set topologies and continued development efforts reveal the substantial potential of employing a GNN-based approach for social network crawling tasks, especially when targeting distributed nodes and ensuring minimal budget expenditure. Of particular importance, our software-oriented benchmarking efforts highlighted the impact of machine-oriented optimizations in the development of effective GNN architectures for complex tasks such as these.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Anton_Tsitsulin1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13441v1",
  "title": "Robustness Verifcation in Neural Networks",
  "modified_abstract": "Drawing motivation from the recent upsurge of interest in enhancing the interpretability and reliability of neural network predictions\u2014such as the development of techniques for generating robust counterfactual explanations with probabilistic guarantees\u2014our work pivots towards a fundamental assessment of neural networks from a formal verification perspective. Specifically, we delve into pivotal robustness and minimization problems, querying the network's behavior under various conditions through queries framed as Linear Programming instances. Our inquiry revolves around whether valid inputs exist that lead to valid outputs, the comparative analysis between networks concerning functional equivalence, and the potential for network simplification through an updated theoretical framework without compromising output integrity, all the while ensuring that the robust criteria are consistently met. We particularly focus on generating model-specific queries that assess robustness criteria, accompanied by updated explanations. Unlike prior explorations that gravitated towards practical approximations and heuristic algorithms, we propose an updated theoretical framework aimed at marrying the concepts of security and efficiency in neural networks, providing a novel lens to assess their computational complexities through detailed explanations. Our findings herald a promising advancement, indicating that within a semi-linear realm\u2014characterized by piecewise linear activation functions and the application of sum or maximum metrics\u2014these challenges are not insurmountable, offering solutions that lie within P or, at their most complex, within NP. The generation of explanations under such robust frameworks thus stands as an indicative leap towards enhancing the networks' interpretability and trustworthiness.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Faisal_Hamman1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13864v1",
  "title": "Optimal Transport for Fairness: Archival Data Repair using Small Research Data Sets",
  "modified_abstract": "In light of the increasing scrutiny on fairness in machine learning (ML) systems, as evidenced by developments like the AI Act and similar regulatory frameworks, our paper takes inspiration from foundational work that addresses the challenges of evaluating and ensuring fairness when sensitive attributes are unavailable. Specifically, we focus on the essential yet challenging task of repairing unfairness in archival training data. By defining fairness in terms of conditional independence between protected attributes ($S$) and features ($X$), given unprotected attributes ($U$), we propose an innovative approach that utilizes optimal transport (OT) to design repair plans on interpolated supports using a small subset of $S|U$-labelled research data. This strategy allows for the effective repair of off-sample, labelled archival data while adhering to stationarity assumptions and significantly reducing the cost associated with the design and application of OT plans. Our detailed experimental evaluation, using both simulated scenarios and benchmark real data like the Adult data set, showcases the capability of our approach to effectively mitigate conditional dependence in large quantities of off-sample, multi-class, and labelled archival data, accurately measuring improvement across multi-category variables. The adaptability of our method is further enhanced by learning from these label modifications and providing a closed-form solution for certain classes of optimal transport problems, highlighting our commitment to fairness.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Kevin_Yao1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13429v1",
  "title": "Detecting and Triaging Spoofing using Temporal Convolutional Networks",
  "modified_abstract": "In the dynamic and increasingly automated world of financial markets, safeguarding against market manipulation is paramount for ensuring fairness and efficiency. Building upon foundational work in machine learning, specifically studies exploring similarity metrics in convolutional neural networks under conditions such as unstructured pruning, this paper proposes an adaptable framework aimed at detecting and triaging spoofing activities. We introduce an initial phase where a labelling algorithm helps in crafting a training set for a weakly supervised model, focusing on identifying suspicious sequences of order book states. This model aims to encode a representation of the order book conducive to easy comparison with future events. Recognizing the importance of expert intervention, the framework allows for the augmentation of machine learning predictions with expert analysis, or in cases where such analyses are not feasible, the employment of a more sophisticated algorithm. Through a similarity search mechanism, our method matches new order book representations against those labelled by experts to prioritize alerts from the weak learner. Preliminary results underscore the viability and potential of our approach for further development in the arena of market manipulation detection.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Alessio_ansuini1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13863v1",
  "title": "DiffImpute: Tabular Data Imputation With Denoising Diffusion Probabilistic Model",
  "modified_abstract": "This work is inspired by a rich lineage of research, including the exploration of invariant representations in the presence of missing data, which has pinpointed the limitations of current methodologies and the critical need for models capable of handling incomplete datasets without sacrificing authenticity or computational efficiency. To navigate these challenges, DiffImpute introduces a novel approach employing a Denoising Diffusion Probabilistic Model (DDPM) specifically designed for tabular data, notorious for its missing values that can significantly hinder analytical endeavors across various domains, including clinical data analysis. By training on complete datasets, DiffImpute ensures credible imputation without compromising the integrity of the existing data. The model's training performance is adaptable to Missing Completely At Random (MCAR) and Missing At Random (MAR) scenarios, employing tailor-made tabular denoising networks including MLP, ResNet, Transformer, and U-Net architectures for superior data handling and imputation performance. Throughout the training process, special attention is given to developing correlation-inducing imputation strategies that further enhance the authenticity of the imputed data, thereby improving its ability to predict outcomes accurately. A novel 'Harmonization' process is introduced to augment coherence between observed and imputed data during the denoising stages, coupled with a refined non-Markovian sampling approach aimed at efficient inference. Empirical validation on seven diverse datasets demonstrates DiffImpute's superior performance, particularly when integrating the Transformer-based denoising network, outpacing competing methods with remarkable consistency in both clinical scenarios and beyond. Code is available at [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Aahlad_Manas_Puli1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13374v1",
  "title": "Byzantine-resilient Federated Learning With Adaptivity to Data Heterogeneity",
  "modified_abstract": "Inspired by the foundational work on privacy and optimization in distributed learning frameworks, specifically those addressing challenges of privacy through the subsampled shuffle model, this paper introduces the Robust Average Gradient Algorithm (RAGA) to the domain of federated learning (FL) with an emphasis on Byzantine resilience and adaptivity to data heterogeneity. RAGA is distinct in its utilization of the geometric median for aggregation, its randomization strategies for ensuring privacy, and its flexibility in round number selection for local updates, optimizing communication among clients. Our thorough convergence analysis, leveraging sub-sampled methods and optimization techniques, extends beyond the conventional confines of strongly-convex losses or homogeneous data distributions, accommodating non-convex loss functions and heterogeneous datasets. Theoretically, we demonstrate that RAGA achieves a convergence rate of $\\mathcal{O}({1}/{T^{2/3-\\delta}})$ for non-convex loss functions, where $T$ represents the iteration number and $\\delta \\in (0, 2/3)$, reflecting a significant theoretical advancement in the field. Moreover, with an explicit focus on privacy-learning, RAGA's design addresses the dual challenges of privacy preservation and learning efficiency, demonstrating a linear convergence rate for strongly-convex loss functions, as long as the dataset from malicious users contributes to less than half of the total. Furthermore, as data heterogeneity diminishes, RAGA is proven capable of reaching stationary points or global optimal solutions. Experimental validation confirms RAGA's robustness against Byzantine attacks and highlights its superior convergence performance over existing baselines, under varied intensities of Byzantine attacks and across heterogeneous datasets. This achievement underscores the significance of privacy-learning in the context of federated learning, showcasing RAGA's adaptability and efficiency.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Antonious_M._Girgis1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13370v1",
  "title": "Counting Network for Learning from Majority Label",
  "modified_abstract": "Addressing a novel problem space within the framework of multi-class Multiple-Instance Learning (MIL), this paper introduces Learning from the Majority Label (LML), an approach inspired by the innovative paradigms seen in current research, such as the efficient training of models with minimal data exemplified by few-shot classification techniques, often associated with meta-learners and meta-learning frameworks. LML methodologically inverts the traditional MIL problem by assigning the majority class of instances in a bag as the bag's label and aims to accurately classify instances using these bag-level majority classes. However, existing MIL methods falter in LML scenarios due to their reliance on confidence aggregation, risking inconsistency between bag-level labels and instance-level classifications determined through class instance counts. To address this, we propose a counting network solution specifically trained to align with the majority class through instance counting, ensuring consistency and improving upon instance-level classification accuracy compared to conventional MIL approaches and established baselines. Our methodology's effectiveness is underscored by superior performance in experimental validations across four large-scale datasets, each serving as a benchmark for evaluating our approach against tasks of diverse natures. The training efficiency and adaptability to diverse tasks highlight the counting network's applicability to various MIL contexts. The code for this research is available at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Eleni_Triantafillou1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13369v1",
  "title": "Clinical information extraction for Low-resource languages with Few-shot learning using Pre-trained language models and Prompting",
  "modified_abstract": "Building on the foundation established by recent innovations in extracting factual information from large language models (LLMs) using diverse prompting techniques, this work explores the translatability of such methods into the realm of clinical information extraction for low-resource languages. Our investigation is motivated by the challenges inherent in automatic medical information extraction from clinical documents, such as high costs related to clinical expertise, limited interpretability of model predictions due to the lack of attention to entity-specific details, computational constraints, stringent privacy regulations, and the nuanced attention required for accuracy. Employing a lightweight, domain-adapted pre-trained model prompted with query-like formats, we are the first to systematically evaluate domain-adaptation and prompting methods within a low-resource setting, specifically through the task of multi-class section classification in German doctor's letters. The use of embeddings derived from a pre-trained language model and the attentiveness to the query-like format of prompts significantly enhance the model's performance. By conducting extensive class-wise evaluations augmented with Shapley values and meticulous annotations, we ensure the interpretability of model predictions and validate the quality of our small training dataset. Additionally, our approach specifically emphasizes the relevance of entity recognition, classifying it as a critical aspect of clinical information extraction. The use of a classifier promoted with merely 20 shots significantly outperforms traditional classification models by a notable margin of 30.5% in accuracy, highlighting the efficacy of embeddings and attention mechanisms in few-shot learning scenarios. The outcomes of this study provide a process-oriented guideline for clinical information extraction projects, particularly those grappling with the nuances of low-resource languages.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Prafulla_Kumar_Choubey2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13349v1",
  "title": "Hierarchical Gaussian Mixture Normalizing Flow Modeling for Unified Anomaly Detection",
  "modified_abstract": "Informed by the foundational approaches in data analysis, anomaly detection, and the adaptation of models to handle high-dimensional and noisy data as demonstrated in recent robust stochastic Principal Component Analysis (PCA) developments, this work introduces a novel hierarchical Gaussian mixture normalizing flow (NF) method, branded as HGAD, for unified anomaly detection (AD) across multiple classes. Unified AD poses the unique challenge of training a single model on normal samples from various classes to subsequently identify anomalies within those classes, a task complicated by the tendency of NF-based AD models to produce similar latent representations for normal and abnormal features, leading to a high anomaly miss rate. Our HGAD method addresses this through two main innovations: inter-class Gaussian mixture modeling and intra-class mixed centers learning, both enhanced with outlier-sensitive techniques. These enhancements empower our approach to represent and learn complex multi-class distributions in the latent space more effectively, overcoming the homogeneous mapping issue prevalent in previous NF-based AD models. Additionally, we introduce a mutual information maximization loss to refine the structuring of the latent feature space, aiding in the distinctiveness of class centers and further mitigating bias. Evaluations on four real-world AD benchmarks reveal our method's superiority over existing NF-based AD strategies and its leading performance against state-of-the-art unified AD models in both subspace and streaming data contexts. Our introduction of a machine learning-based mutual information criterion further solidifies HGAD's stance as a formidable solution in the anomaly detection landscape.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Mayur_Dhanaraj1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13344v1",
  "title": "USE: Dynamic User Modeling with Stateful Sequence Models",
  "modified_abstract": "Inspired by significant advancements in the field of sequence modeling and deep learning, notably the development of novel frameworks such as amortized Langevin dynamics for efficient and flexible posterior inference in large-scale datasets, our research seeks to address the dynamic nature of user behavior in digital environments. User embeddings, essential for accurately predicting user engagement and powering personalized services, traditionally rely on stateless sequence models. These models struggle with incorporating historical user interactions efficiently, leading to either the exclusion of valuable past behavior data or the need for resource-intensive reprocessing. To overcome these limitations, we introduce User Stateful Embedding (USE), a novel approach that employs deep autoencoders to efficiently update user embeddings to reflect both recent and long-term behavior patterns without exhaustive reprocessing. USE leverages previous model states, enhanced by autoencoders, to dynamically integrate changing behaviors, and introduces a novel training objective focused on future behavior prediction to enhance the embeddings' distinctiveness and representativeness. Our experimental validation on diverse downstream tasks using behavioral logs from Snapchat users demonstrates the superior effectiveness and efficiency of USE in capturing dynamic user behavior, thus affirming its potential for improving user engagement forecasting and personalization services. The introduction of autoencoders in the architecture enables a sophisticated detection of anomalies and patterns in user data, setting a new standard for dynamic user modeling. The concept of amortization is integrally applied to update embeddings, maximizing the efficiency of capturing posterior distributions in user behaviors over time.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Shohei_Taniguchi1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13335v1",
  "title": "Adaptive Ensembles of Fine-Tuned Transformers for LLM-Generated Text Detection",
  "modified_abstract": "With the capacity of Large Language Models (LLMs) to produce human-like textual content, distinguishing genuine from synthetic text has become a paramount challenge, especially in critical applications like monitoring fake news on social media platforms. This challenge is accentuated by previous research primarily focusing on evaluating individual models' training performance within the confines of in-distribution datasets, thereby neglecting the versatile landscape of LLM-generated text. Taking inspiration from advancements in text retrieval, particularly the integration of contextualized embeddings, and query encoder mechanisms for enhanced model performance, we extend this innovative approach to LLM-generated text detection. Our investigation, constituting a pivotal task, includes testing five specialized transformer-based models equipped with encoded contextual embeddings, and retrieves models across both in-distribution and out-of-distribution datasets to appraise their effectiveness and adaptability in performing this critical task. The bi-encoder architecture utilized allows for significant improvements in text retrieval by effectively comparing LLM-generated and genuine texts. The initial findings highlighted a commendable performance on in-distribution datasets but exposed limitations in generalizing across diverse data types. To address these limitations, we engineered an adaptive ensemble approach, amalgamating individual classifiers and leveraging their training wisdom to boost overall accuracy from 91.8% to 99.2% on in-distribution tests, and from 62.9% to 72.5% on out-of-distribution tests. These enhancements not only underscore the efficacy and versatility of adaptive ensemble techniques, embedding knowledge, and detecting LLM-generated texts but also signify a promising avenue for improving generalization capabilities in machine learning models.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Hyunji_Lee1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13319v1",
  "title": "HyperFusion: A Hypernetwork Approach to Multimodal Integration of Tabular and Medical Imaging Data for Predictive Modeling",
  "modified_abstract": "The burgeoning field of medical diagnostics and treatments has increasingly leveraged the wealth of data provided by medical imaging and Electronic Health Records (EHRs), echoing the broader trend in leveraging complex data for predictive modeling observed in studies such as the investigation into gyrification network properties for psychosis onset prediction. This work, among others, highlights the critical need for advanced analytical tools capable of integrating and interpreting diverse data types to enhance diagnostic accuracy and treatment outcomes. Capitalizing on this need, we introduce HyperFusion, a novel framework utilizing hypernetworks for the dynamic fusion of clinical imaging and tabular data drawn from EHRs to provide a more nuanced and comprehensive analysis by conditioning the image processing on the EHR's values and mental health measurements. Our methodology aims to harness the complementary strengths of these modalities, thereby improving the accuracy of medical applications including functional and structural brain Magnetic Resonance Imaging (MRI) tasks such as brain age prediction and multiclass Alzheimer's Disease (AD) classification. Our experiments demonstrate marked superiority over traditional single-modality models and contemporary MRI-tabular data fusion techniques in the state of the field, illustrating a seamless transition in the coordinated, multimodal medical data analysis. The code supporting our findings will be made publicly available, ensuring the reproducibility and further advancement of our work in this crucial domain.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Daniel_Hauke1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13310v1",
  "title": "A Semantic Search Engine for Mathlib4",
  "modified_abstract": "Our work is inspired by significant strides in the understanding and development of large language models for multilingual reasoning, forming the basis for advancements in semantic search technologies. The interactive theorem prover, Lean, plays a pivotal role in the verification of formal mathematical proofs and is supported by an actively growing community. At the heart of this ecosystem is its mathematical library, mathlib4, which is instrumental in the formalization of a broad spectrum of mathematical theories. Nonetheless, the challenge of searching through theorems within mathlib4 stems from a need for familiarity with its naming conventions or documentation strings, creating a barrier for users with diverse levels of familiarity, including those at a grade-school level. Addressing this challenge, we introduce a semantic search engine tailored for mathlib4 that allows for informal query input and efficiently locates the relevant theorems employing commonsense reasoning. Moreover, this engine supports multilingual input and scales effectively, making it accessible to a broader audience and adaptable to various settings. Additionally, our implementation enhances the abilities of users of all expertise levels to engage with mathematical theories by proposing a novel benchmark for evaluating the efficacy of various search engines tailored to mathlib4, emphasizing tasks likely encountered by users. This benchmark serves not only to measure effectiveness but also to guide future enhancements in semantic search capabilities within mathematical libraries.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Freda_Shi1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13300v1",
  "title": "Kernel Multigrid: Accelerate Back-fitting via Sparse Gaussian Process Regression",
  "modified_abstract": "Taking inspiration from the forefront of nonparametric modeling, where traditional kernel-based methods have seen limitations due to the parametric constraints imposed on kernel functions, we extend inquiry into the domain of Additive Gaussian Processes (GPs) popular for nonparametric feature selection. Additive GPs, while beneficial for capturing complex data relationships, encounter significant challenges in terms of convergence rate when trained using Bayesian Back-fitting. Our research addresses these challenges by harnessing a technique known as Kernel Packets (KP) to demonstrate that the conventional convergence rate of Back-fitting in training additive GPs is empirically limited to $(1-\\mathcal{O}(\\frac{1}{n}))^t$, where $n$ and $t$ symbolize the data size and the iteration number, respectively, necessitating a minimum of $\\mathcal{O}(n\\log n)$ iterations for convergence. To circumvent this limitation, we propose Kernel Multigrid (KMG), a novel algorithm combining optimisation methods and the strengths of KP with sparse Gaussian Process Regression (GPR) for processing residuals in each iteration of Back-fitting, enhancing feature selection processes and task-specific kernel hyperparameter selection. The implementation of KMG not only theoretically reduces the necessary number of iterations to $\\mathcal{O}(\\log n)$ but also efficiently maintains time and space complexities at $\\mathcal{O}(n\\log n)$ and $\\mathcal{O}(n)$ per iteration, respectively. Our numerical experiments illustrate that with just 10 inducing points in sparse GPR, KMG can attain precise approximations of high-dimensional targets within a mere 5 iterations, therefore providing a significant acceleration over traditional back-fitting methods in training additive GPs.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Alistair_Shilton1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13299v1",
  "title": "Bridging scales in multiscale bubble growth dynamics with correlated fluctuations using neural operator learning",
  "modified_abstract": "Motivated by advancements in machine learning and the increasing use of neural networks to model complex physical phenomena, our work takes a pioneering step in addressing the challenge of understanding multiscale bubble growth dynamics. Drawing inspiration from recent developments in the field, such as deepening our understanding of Recurrent Neural Networks (RNNs) through the lens of nonequilibrium response theory, we develop a composite neural operator model that merges microscale stochastic fluid models with continuum-based fluid models for bubble dynamics. This innovative approach is built upon a novel neural network architecture that combines a deep operator network with brain-inspired Long Short-Term Memory (LSTM) network and machine learning kernels to learn both the mean behavior of bubble growth under pressure variations and the statistical characteristics of correlated fluctuations inherent to microscale bubble dynamics. Invoking a series of experiments, our model successfully unifies the analysis of nonlinear bubble dynamics across different scales, establishing connections between the microscale processes and macroscale readouts through theory-driven practices. Furthermore, this series of experiments demonstrating an unprecedented 99% accuracy in predicting time evolution of bubble radii under various external pressures while capturing the stochastic fluctuations unique to microscale dynamics showcases the potential of neural operator learning as a powerful tool for bridging the gap between disparate scales in complex physical systems. Additionally, the introduction of 'readout' techniques enhances our ability to interpret complex machine-learning models' predictions",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Soon_Hoe_Lim1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13298v1",
  "title": "Rotary Position Embedding for Vision Transformer",
  "modified_abstract": "Building on the foundation laid by pioneering works such as the Astroformer, which explored hybrid architectures for efficient learning with minimal data, especially in the classification of galaxy morphologies, our study takes a step further into the realm of Vision Transformers (ViTs). We investigate the Rotary Position Embedding (RoPE) mechanism, known for its remarkable benefits in language models, particularly for its capacity for length extrapolation in Transformers, and explore its utility in the field of computer vision. Despite its proven efficacy in language modeling, the potential benefits of applying RoPE to ViTs for the analysis of spatial data\u2014including massive galaxies\u2014have been relatively underexplored. Our work provides an in-depth exploration of RoPE's application to 2D vision data, presenting how it can significantly enhance performance metrics across a variety of tasks, including ImageNet-1k classification, COCO detection, and ADE-20k segmentation, by maintaining precision while supporting higher resolution images at inference. Furthermore, our analysis includes a precise selection process that helps in determining the optimal configuration of RoPE for various types of visual content, considering different network architectures and sets of visual data. This study proposes that RoPE can serve as a promising enhancement for ViTs, facilitating performance improvements with minimal computational overhead, and enabling the technology to classify and evolve in its application to increasingly complex visual scenarios. We share our findings, code, and pre-trained models at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Rishit_Dagli1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13293v1",
  "title": "Building Optimal Neural Architectures using Interpretable Knowledge",
  "modified_abstract": "In the burgeoning field of Neural Architecture Search (NAS), the complexity and computational cost of identifying optimal neural network designs are formidable challenges. Drawing inspiration from pioneering works that demonstrate learned functions drastically improving performance in deep learning tasks, such as learned optimizers showcasing potential wall-clock time and validation loss advantages over traditional optimization techniques, we introduce AutoBuild. This novel scheme leverages the principle of learning from the latent embeddings of operations and architecture modules, correlating them with the ground-truth performance of the networks they comprise. AutoBuild is designed to assign interpretable importance scores to both individual operation features and larger macro operation sequences, enabling the construction of high-performing neural networks without the necessity of exhaustive search. By incorporating a sophisticated validation process, AutoBuild facilitates long-term learning and efficient refinement of the search space. Through rigorous experiments across various applications, including image classification, segmentation, and Stable Diffusion models, where convolutional and perceptual qualities are paramount, AutoBuild demonstrates a notable capacity to not only construct superior architectures directly from a minimally evaluated set but also refine search spaces efficiently, thereby surpassing the performance of architectures identified through conventional search methodologies and those discovered via established search baselines. Code available at [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~C._Daniel_Freeman1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13286v1",
  "title": "A Sampling-based Framework for Hypothesis Testing on Large Attributed Graphs",
  "modified_abstract": "Building upon the foundational work in hierarchical modeling and optimization, which highlights the utility of employing a probabilistic approach to data structuring and the existence of optimal unsupervised metrics for hierarchical clustering scores, this study extends these insights into the realm of graph-based hypothesis testing. Empirical methodologies frequently hinge on optimization techniques to refine their efficacy and resolve the complexities inherent in large-scale data analysis, reflecting an intrinsic dependence on hierarchical frameworks and unsupervised learning to elucidate data structures. We explore the domain of hypothesis testing in statistical methodology, a concept traditionally applied to tabled data but now increasingly relevant for graph-structured data due to the multitude of real-life applications relying on graph representations and the divergence from standard data types. By formally addressing node, edge, and path hypotheses within attributed graphs, we introduce a sampling-based hypothesis testing framework that effectively accommodates existing graph sampling techniques without bias towards specific hypothesis types. Further innovation is presented through the development of PHASE, a Path-Hypothesis-Aware SamplEr, which leverages subgraph identification and is an m-dimensional random walk fine-tuned through optimization to enhance hypothesized path detection, optimized further in PHASEopt for superior time efficiency and reduced computational cost. Our experimental validation, performed on real datasets, underscores the efficacy of hypothesis-aware sampling and clustering methodologies in improving both the accuracy and time efficiency of hypothesis testing in large attributed graphs.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Daniel_Z\u00fcgner1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13269v1",
  "title": "AFLoRA: Adaptive Freezing of Low Rank Adaptation in Parameter Efficient Fine-Tuning of Large Models",
  "modified_abstract": "This paper introduces Adaptive Freezing of Low Rank Adaptation (AFLoRA), a Parameter-Efficient Fine-Tuning (PEFT) method that innovatively enhances the fine-tuning of large models by incorporating parallel paths of trainable low-rank matrices (down-projection and up-projection matrices) appended with feature transformation vectors to each pre-trained frozen weight tensor. Our methodology is propelled by advances in optimization, particularly inspired by recent breakthroughs such as the development of micro-batch-averaged sharpness-aware minimization and gradient-aware techniques, which address the over-parameterization and generalization dilemma in deep learning models, including those focused on language processing tasks. By introducing a unique freezing score, AFLoRA selectively and incrementally freezes these projection matrices during fine-tuning, achieving a dual advantage: mitigating overfitting and significantly reducing the computational requirements. Our evaluation on the GLUE benchmark, primarily a language processing and classification measure, uncovers that AFLoRA achieves up to 0.85% enhancement in performance metrics related to loss minimization and generalization capabilities, while requiring up to 9.5 times fewer average trainable parameters as computed on a mini-batch basis, along with accelerating runtime by up to 1.86 times relative to comparable PEFT methods. Beyond presenting an efficient and effective fine-tuning method, this work deepens the understanding of the trainability and optimization dynamics of low-rank adaptation paths across different models, informing a strategic approach to the freezing schedule of projection matrices. A forthcoming release of our implementation promises to further contribute to the research community's resources.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Kayhan_Behdin1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13257v2",
  "title": "Arcee's MergeKit: A Toolkit for Merging Large Language Models",
  "modified_abstract": "In the wake of the advancement and proliferation of open-source language models, this paper presents MergeKit, a toolkit inspired by recent progress in task-specific model optimization, such as the innovative use of HyperNetworks for generating hyper-prompts in prompt-based task conditioning. The utility of fine-tuning pretrained models for enhancing learning performance in specific tasks has led to the creation of an array of specialized models. Recognizing the potential in leveraging the combined strengths of these diverse models for learning and generation tasks, MergeKit offers a solution for merging model parameters to create multitask models that retain the intrinsic learning capabilities of the originating models without necessitating further training. This capability addresses significant challenges like catastrophic forgetting and multitask learning. MergeKit contributes to the field by providing an extensible, open-source library designed for the efficient merging of models across any hardware setup, thereby serving researchers and practitioners looking to exploit the synergy of multiple models for generation and learning purposes. It has facilitated the creation of some of the most powerful open-source multitask models, as evidenced by the Open LLM Leaderboard. The library and further documentation are available at [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Vamsi_Aribandi1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13249v1",
  "title": "A Unified and General Framework for Continual Learning",
  "modified_abstract": "Our work is inspired by the advances and challenges present in contemporary machine learning methods, particularly evident in the pursuit of accurately predicting classification accuracy when new, unobserved multiclass scenarios are introduced. Similarly, Continual Learning (CL) focuses on systems that learn over time from a dynamically evolving data stream, retaining knowledge and adapting to new information without forgetting previously acquired insights. This research addresses the prevalent issue of catastrophic forgetting in CL by proposing a comprehensive framework that integrates various existing methods\u2014regularization-based, Bayesian-based, and memory-replay-based approaches\u2014into a unified and general optimization objective. This synthesis reveals that despite their disparate origins, these methodologies share common mathematical foundations, underscoring an inherent compatibility and highlighting their collective pursuit of a shared optimization goal, especially notable in the separation of classes to avoid interference. Furthermore, we introduce an innovative concept called refresh learning, drawing from neuroscience to mimic the brain's method of discarding outdated information to reinforce critical knowledge and facilitate new learning effectively among different classes. This concept is presented as a versatile enhancement to existing CL strategies, validated through extensive empirical research and neural-network-based theoretical analysis, affirming its potential to significantly improve CL performance and potentially enhance face detection capabilities by ensuring the continual updating of the model's understanding. The publication of our code at [omitted for de-identification] aims to promote further exploration and adoption of this framework within the CL community.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yuval_Benjamini1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13247v1",
  "title": "Decentralized Federated Learning: Model Update Tracking Under Imperfect Information Sharing",
  "modified_abstract": "Inspired by advancements in federated learning, particularly in the context of training dual encoding models on decentralized, non-IID client datasets, this paper proposes a novel Decentralized Noisy Model Update Tracking Federated Learning algorithm (FedNMUT). Our work addresses the critical challenge of noisy communication channels that hamper efficient information exchange in decentralized learning systems. By leveraging a gradient tracking mechanism and incorporating noise into its parameters, FedNMUT minimizes the impact of data heterogeneity and communication overhead, enabling consensus among clients through a communication graph topology even under adverse conditions. This approach not only reflects an understanding of the obstacles presented by noisy communication but also prioritizes parameter sharing and noise incorporation to bolster the resilience of decentralized learning systems against such challenges. In the training process, using a large-batch approach and prioritizing representation quality with careful selection and management of samples ensures FedNMUT's superior performance in a variety of settings. The superior performance of FedNMUT, as demonstrated through both theoretical analysis and empirical validation in diverse settings, highlights its efficacy in managing imperfect information sharing compared to existing state-of-the-art methods and conventional parameter-mixing approaches. This affirms the potential of our algorithm to train decentralized federated learning models with dual encoding mechanisms more efficiently, advancing the robustness and efficiency of such models under the challenge of imperfect information sharing.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Warren_Richard_Morningstar1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13246v1",
  "title": "Divide-Conquer Transformer Learning for Predicting Electric Vehicle Charging Events Using Smart Meter Data",
  "modified_abstract": "This work is inspired by advancements in machine learning techniques for handling structured and unstructured data across diverse applications, including class-imbalanced graph representation learning. Predicting electric vehicle (EV) charging events is of paramount importance for optimizing load scheduling and energy management, thereby facilitating the transition to sustainable transportation. While the majority of existing research has concentrated on forecasting EV charging demand at public stations, our study shifts the focus to the prediction of home charging events, a key yet underexplored area due to the challenges associated with accessing detailed home charging data. Employing the principles of non-intrusive load monitoring (NILM) and leveraging graph-based analysis, we propose an innovative method for home charging event prediction that utilizes historical smart meter data as a massive dataset. Our approach distinguishes itself by utilizing a transformer model, which leverages self-attention mechanisms within a divide-and-conquer strategy to efficiently process and learn from massive subsets of smart meter data for effective charging event prediction. This method's design aims to address data compression challenges, enabling accurate, minute-interval, hour-ahead predictions. Experimental results serve as a benchmark, demonstrating consistently high accuracies exceeding 96.81% across various prediction intervals. Our method's reliance on smart meter data exclusively enhances its practicality and suitability for grid operators, representing a significant advancement in the field of EV charging prediction and energy management.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zhongyu_Ouyang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13245v1",
  "title": "Federated reinforcement learning for robot motion planning with zero-shot generalization",
  "modified_abstract": "Drawing on the foundational work in reinforcement learning and inspired particularly by recent advances such as state deviation correction, our paper explores the innovative application of federated learning to robotic motion planning. The objective is to achieve zero-shot generalization, where a control policy, acting as an agent, can be deployed in new environments without further data collection or policy adaptation. We propose a federated reinforcement learning framework that facilitates collaborative policy learning among multiple local learners and a central server (the Cloud) without the necessity to share a dataset of raw or sampled data. Each learner iteratively uploads its policy and estimated performance metrics to the Cloud, which synthesizes a global optimum and disseminates the updated optimal control policy back to the learners for the next iteration. This framework is grounded in theoretical guarantees including almost-sure convergence, almost consensus among learners, Pareto improvement, and a tangible optimality gap, even in the presence of unexpected noisy data environments. These theoretical constructs are further bolstered by Monte Carlo simulations and offline learning methods, validating the framework's efficacy in robot motion planning with a focus on zero-shot generalization to new environments.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jianzhun_Shao1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13243v1",
  "title": "A Comparative Study of Machine Learning Models Predicting Energetics of Interacting Defects",
  "modified_abstract": "Informed by the critical advancements in machine learning (ML) for understanding complex systems, as evidenced by works such as 'Approximate Graph Propagation', which improved the scalability of Graph Neural Networks through efficient computation of node proximity queries, this study embarks on exploring the application of ML techniques in the realm of materials science, specifically for predicting the energetics of interacting defect systems. These systems, prevalent in materials under realistic conditions, present computational challenges that demand substantial resources, notably through supercell calculations for atomic-level understanding. We present a comparative analysis of three ML approaches\u2014materials descriptors, graph neural networks, and cluster expansion\u2014to predict the free energy change in systems with interacting defects, using a limited dataset obtained from Density Functional Theory calculations. Our findings reveal that the cluster expansion model offers precise predictions with limited data, and when supplemented with synthetic data generated at near-DFT levels, it enhances the dataset for training more accurate models, particularly graph neural networks. Additionally, this research includes a discussion on the computational cost of each method, providing a preliminary yet insightful evaluation of leveraging ML for studying imperfect surface systems.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Mingguo_He1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13241v1",
  "title": "Tackling Noisy Labels with Network Parameter Additive Decomposition",
  "modified_abstract": "Inspired by advancements in managing complex neural architectures and their applications, such as distillation-aware neural architecture search (NAS) which optimizes student models via knowledge from teacher models, our work introduces a novel approach to address the challenge of overfitting due to noisy labels in deep networks. Traditional methods such as early stopping, while beneficial, do not fully differentiate between the memorization of clean versus mislabeled data. To this end, we propose an innovative method: additive decomposition of network parameters, where the parameters are split into two groups ($\\mathbf{w}=\\bm{\\sigma}+\\bm{\\gamma}$), with each set dedicated to memorizing clean and mislabeled data respectively. This strategy, embodying principles of transfer learning and applying distillation-aware meta-NAS techniques to shift knowledge towards effectively handling clean data, leverages the intrinsic memorization capabilities of deep networks to prioritize learning from clean data in early training phases while progressively minimizing the influence of mislabeled data through meta-prediction techniques. During testing, only the parameters responsible for clean data memorization ($\\bm{\\sigma}$) are utilized for the target task at hand, thereby enhancing the model\u2019s generalization ability to prediction tasks. Our extensive evaluation on both simulated and real-world datasets, as well as a variety of tasks, confirms the effectiveness of our approach.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sohyun_An1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13861v1",
  "title": "Machine Learning-based Layer-wise Detection of Overheating Anomaly in LPBF using Photodiode Data",
  "modified_abstract": "Drawing inspiration from the collaborative advancements in machine learning (ML) such as assisted learning and privacy-aware computing, where cooperation among agents without sharing proprietary data bolsters classification performance, this research introduces a machine learning framework for detecting overheating anomalies in laser powder bed fusion (LPBF) additive manufacturing (AM) based on photodiode sensor data. Photodiode sensors, by collecting high-frequency data that capture the melt pool's dynamics and thermal history, lay the groundwork for our method's ability to perform layer-wise detection of overheating anomalies with enhanced learning efficiency and privacy-aware safeguards. We extracted three sets of features from the photodiode data: MSMM (mean, standard deviation, median, maximum), MSQ (mean, standard deviation, quartiles), and MSD (mean, standard deviation, deciles), to train multiple ML classifiers. The general approach incorporates cost-sensitive learning alongside a majority voting ensemble (MVE) strategy to enhance detection accuracy, thereby aligning with experimental needs without compromising subjects' data integrity. Demonstrated on an open benchmark dataset featuring deliberate overheating anomalies in LPBF specimens, our findings reveal the superiority of MSD features across all classifiers, with the MVE classifier achieving a noteworthy mean F1-score of 0.8654. Notably outperforming existing approaches in literature by a 9.66% improvement in mean F1-score, our methodology not only solidifies the premise of photodiode data as a viable source for anomaly detection in LPBF processes but also showcases the potential of cooperative learning paradigms in enhancing machine learning frameworks for manufacturing applications.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Xun_Xian1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13219v1",
  "title": "Diffusion Model for Data-Driven Black-Box Optimization",
  "modified_abstract": "In the context of the rapid and transformative advancements that generative AI has brought to the field of artificial intelligence, such as fostering incredible strides in content creation and problem-solving in dynamic, unstructured environments, our work is motivated by the exploration of diffusion models, particularly their utility in the realms of black-box optimization. Recognizing the potential of diffusion models to navigate the challenges of optimizing complex structured variables in domains characterized by both vast unlabeled datasets and smaller labeled datasets, our study is inspired by the foundational advancements in meta-learning, which have brought to light the importance of effectively leveraging learned meta-knowledge and few-shot learning meta-training strategies. By integrating insights from previous explorations into the power of causal perspectives and augmentation-based methods for overcoming memorization challenges in meta-learning, we propose a reward-directed conditional diffusion model aimed at resolving the complex conditional sampling challenges inherent in design optimization and task-specific adaptation. Drawing on a nuanced understanding of key factors such as real-valued reward function metrics and human preference analysis, our approach reimagines the optimization process through adaptation, enabling the generation of near-optimal designs that respect the underlying latent structures of the design variables through task adaptation strategies. Through theoretical analysis and adaptation methodologies, we assert the model's capacity to closely approximate the optimal conditions in off-policy bandits, supported by substantial empirical evidence illustrating its efficacy across diverse application scenarios. This innovative effort thus not only represents a significant step forward in the utilization of generative models for optimization but also underscores the synergies between generative modeling, strategic data manipulation, and adaptive task frameworks for enhanced decision-making and creative endeavors.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yinjie_Jiang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13214v1",
  "title": "Nellie: Automated organelle segmentation, tracking, and hierarchical feature extraction in 2D/3D live-cell microscopy",
  "modified_abstract": "In the rapidly advancing field of live-cell microscopy, understanding the dynamics of cellular organelles is paramount for unraveling complex biological processes. Motivated by the need for advanced methods to analyze these structures as depicted in works like 'Towards Nonlinear Disentanglement in Natural Data with Temporal Sparse Coding,' which explores unsupervised learning models for disentangling underlying factors of variation in data, we introduce Nellie. Nellie represents an automated and unbiased pipeline for segmentation, tracking, and hierarchical feature extraction of diverse intracellular structures, adept at handling 2D/3D microscopy images and live-cell movies without requiring user input. By utilizing a preprocessing pipeline that enhances structural contrast and implementing a sophisticated tracking mechanism that employs radius-adaptive pattern matching, Nellie sets new standards for organelle analysis in both still images and time-lapse videos. It enriches data analysis by extracting myriad features across different hierarchical levels, making it an invaluable tool for deep, customizable analyses with a particular emphasis on sparse data representation and learning. Furthermore, Nellie provides a user-friendly Napari-based GUI for code-free operation and insightful visualization, coupled with an open-source codebase that encourages modification and customization by the community. Demonstrations of Nellie's versatility include unmixing of organelles from single-channel images and employing an unsupervised graph autoencoder model on mitochondrial multi-meshes to analyze changes induced by ionomycin treatment, showcasing its potential to advance the study of intracellular dynamics through learning complex patterns.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ivan_Ustyuzhaninov1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13213v2",
  "title": "From Representational Harms to Quality-of-Service Harms: A Case Study on Llama 2 Safety Safeguards",
  "modified_abstract": "In the context of recent advancements in large language models (LLMs) and their widespread adoption across various domains, our work is motivated by the pursuit of addressing the complex challenges surrounding safety risks and biases inherent in these technologies. These challenges are underscored by efforts and studies in the field of natural language understanding (NLU), where models often inadvertently integrate biases from their training data, leading to disparities in performance across different demographic groups, particularly on out-of-distribution tasks. Our research extends this conversation by focusing specifically on the safety responses of LLMs and examining their potential to inflict quality-of-service harms, particularly on marginalized populations. Using Llama 2 as our case study, we scrutinize the effectiveness of existing safety measures, including debiasing techniques and thorough dataset evaluation, and explore how safety-oriented responses, while intended to mitigate bias and risk, may inadvertently perpetuate representational harms through experiments involving trained non-toxic prompts and a novel taxonomy of LLMs' responses. We delineate the nuanced trade-offs between helpfulness and safety, revealing that these trade-offs can exacerbate quality-of-service disparities for certain demographic groups, even within in-distribution tasks, affecting their overall performance. Our findings contribute to a deeper understanding of the intricacies involved in safeguarding against safety risks in LLM applications, highlighting the need for more nuanced and inclusive approaches.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ahmad_Rashid1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13204v1",
  "title": "Diversity-Aware Agnostic Ensemble of Sharpness Minimizers",
  "modified_abstract": "Drawing inspiration from the longstanding evidence of ensemble learning's effectiveness and the nuanced observations regarding model size impacts on generalization, particularly within varied subgroups and conditions, our research introduces and develops DASH. This novel learning algorithm seamlessly synthesizes the principles of ensemble learning with those of loss sharpness minimization, aiming to enhance prediction diversity and generalization through fostering both training randomness and the pursuit of wider local minima. By examining and integrating these complementary methodologies\u2014reflected in prior studies' findings that underscore the importance of model diversity and robustness against distribution shifts\u2014we explore the potential for DASH to significantly improve ensemble generalizability through meticulous subgroup analyses. Through a meticulous theoretical formulation and comprehensive empirical validation over diverse labels and subgroup analyses, including evaluating pre-trained models across different languages, our work highlights the method's efficacy in guiding base learners towards low-loss regions characterized by minimal sharpness, thus marking a substantial step forward in the conquest for more generalizable, robust machine learning models.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yaodong_Yu4",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13196v1",
  "title": "ADAPT to Robustify Prompt Tuning Vision Transformers",
  "modified_abstract": "Drawing upon our foundational understanding that deep models, including Vision Transformers, exhibit vulnerabilities to adversarial attacks\u2014a challenge that has sparked considerable efforts in the field as seen in endeavors to leverage Vision Transformers for enhancing object detection\u2014our work takes a novel direction. We specifically scrutinize the conventional reliance on full-model fine-tuning and extensive pre-training for defensive measures against adversarial impacts, a practice that becomes impractical with models housing billions of parameters for each task. Instead, this paper pivots towards the exploration of parameter-efficient prompt tuning as a viable strategy to not only adapt large transformer-based models to downstream tasks but also to imbue them with robustness against adversarial threats. Despite its non-hierarchical structure, which sharply contrasts with the plain-backbone architectures often deployed, we identify that conventional adversarial defense strategies falter under the prompt tuning paradigm due to gradient obfuscation and susceptibility to adaptive attacks. To counteract this, we introduce ADAPT, an innovative framework designed to integrate adaptive adversarial training within prompt tuning processes, thus refining the design for defense strategies. Our approach achieves commendable robust accuracy (approximately 40%) in comparison to state-of-the-art robustness techniques, all the while adjusting merely ~1% of the parameter count without reliance on a detector pyramid of layers, making it highly efficient yet effective.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yanghao_Li1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13179v1",
  "title": "Predictive, scalable and interpretable knowledge tracing on structured domains",
  "modified_abstract": "Building on the foundation of existing research, such as the nuanced exploration of explainability in graph neural networks which highlights the importance of interpretability in understanding model predictions, our work seeks to extend these principles into the realm of intelligent tutoring systems. Intelligent tutoring systems optimize the selection and timing of learning materials to enhance understanding and long-term retention by estimating the learner's progress ('knowledge tracing'; KT), and the prerequisite structure of the learning domain ('knowledge mapping'), effectively utilizing graphs and subgraphs for representing complex structured domains. Despite recent deep learning models achieving high KT accuracy, they often sacrifice the interpretability intrinsic to psychologically-inspired models. We address this dilemma with PSI-KT, a hierarchical generative approach that preserves interpretability by explicitly modeling how individual cognitive traits and the prerequisite knowledge structure, conveyed through graph-based representations, influence learning dynamics. Leveraging scalable Bayesian inference, PSI-KT not only realizes personalized education for a growing learner base but does so efficiently, marking a significant stride towards predictive, scalable, and interpretable knowledge tracing on complex structured domains. Evaluated across three online learning platform datasets, PSI-KT demonstrates superior multi-step predictive accuracy, scalable inference in continual-learning settings, and offers interpretable insights, through explanations, into learner-specific traits and knowledge structures that causally support learning tasks. This represents a fundamental advancement in making personalized education accessible globally, bolstered by precise explanations of the cognitive processes involved.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yingxin_Wu1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13178v1",
  "title": "Fast Value Tracking for Deep Reinforcement Learning",
  "modified_abstract": "Our work is propelled by pioneering discussions about the complexities inherent in reinforcement learning (RL) environments, specifically addressing the underexplored realm of polynomial mixing times in such contexts. Drawing from novel insights into scalable MDPs and the critical challenges they present in both theory and practice, we aim to surpass the limitations of current RL algorithms that predominantly treat decision-making problems as static, impacting their overall performance adversely. Leveraging advancements in the Kalman filtering paradigm, we introduce a novel and scalable sampling algorithm\u2014Langevinized Kalman Temporal-Difference (LKTD)\u2014for deep reinforcement learning, apt for dynamic environments such as games. Grounded in Stochastic Gradient Markov Chain Monte Carlo (SGMCMC) principles and considering different tasks and settings, LKTD adeptly draws samples from the posterior distribution of deep neural network parameters over varying durations, enhancing the algorithm's learning capabilities. We rigorously authenticate that, under mild conditions, the posterior samples yielded by the LKTD algorithm converge to a stationary distribution within specific mixing times, relevant to the tasks at hand. This pivotal convergence empowers our model to meticulously quantify uncertainties tied to the value function and model parameters, further enabling dynamic monitoring of these uncertainties during iterative policy updates. Consequently, LKTD heralds a more resilient and adaptable paradigm for reinforcement learning, poised to tackle the stochastic dynamics of agent-environment interactions with unprecedented precision.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Gopeshh_Raaj_Subbaraj1",
  "manipulated_ranking": 5,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13858v1",
  "title": "A conditional latent autoregressive recurrent model for generation and forecasting of beam dynamics in particle accelerators",
  "modified_abstract": "Building on the pioneering work in sparse modeling and temporal dynamics representation, such as doubly sparse variational Gaussian processes, which effectively tackle the challenge of managing complex datasets with limited computational resources, our research introduces a novel two-step unsupervised deep learning framework named the Conditional Latent Autoregressive Recurrent Model (CLARM). This model is specifically designed for understanding, generating, and forecasting spatiotemporal dynamics of charged particles within particle accelerators, a domain that poses unique challenges due to the limited scope for non-destructive measurements, the computational intensity of simulations, and inherent system uncertainties. The CLARM utilizes a Conditional Variational Autoencoder (CVAE) to efficiently encode the six-dimensional phase space of particle motion into a more manageable latent space representation, while leveraging the capabilities of Long Short-Term Memory (LSTM) networks to capture the essential temporal dynamics through an autoregressive mechanism. By appropriately modeling the beam dynamics as high-dimensional state-space processes using both variational and deep learning approaches, our findings suggest that the proposed model exhibits promising generative and forecasting properties across a range of evaluation metrics, highlighting its potential usefulness for particle accelerator beam diagnostics and control strategies.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Artem_Artemev1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13164v1",
  "title": "VL-ICL Bench: The Devil in the Details of Benchmarking Multimodal In-Context Learning",
  "modified_abstract": "This paper is motivated by the seismic shift towards multimodal models in machine learning, particularly inspired by breakthroughs in vision transformers, which have demonstrated the potency of spectral and multi-headed self-attention mechanisms for image recognition tasks. Large language models (LLMs) famously exhibit emergent in-context learning (ICL) -- the ability to rapidly adapt to new tasks using few-shot examples provided as a prompt, without updating the model's weights. Building on these developments, vision large language models (VLLMs) have significantly advanced areas such as recognition, reasoning, and grounding. However, investigations into multimodal ICL have predominantly been narrowly focused, leading to an incomplete understanding of its capacities. Our study introduces a comprehensive benchmark, VL-ICL Bench, for multimodal in-context learning, involving both images and text. This benchmark encompasses a broad spectrum of tasks, from perception to reasoning and across varying context lengths, to rigorously evaluate the abilities of state-of-the-art VLLMs. Through our evaluations, which employ attention-based analysis methods to understand the spectral features captured by the models, we reveal the diverse strengths and weaknesses of current models, including top-tier models like GPT-4, across these challenging tasks. Our findings highlight the depth of emergent ICL capabilities yet to be explored and lay the groundwork for future enhancements in VLLM in-context learning capabilities and applications, particularly emphasizing the role of spectral and self-attention mechanisms in recognition tasks. The code and dataset are available at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Badri_Narayana_Patro1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13148v1",
  "title": "SIFT-DBT: Self-supervised Initialization and Fine-Tuning for Imbalanced Digital Breast Tomosynthesis Image Classification",
  "modified_abstract": "In the quest to enhance breast cancer screening and diagnostic strategies through Digital Breast Tomosynthesis (DBT), our work leverages pioneering self-supervised learning techniques and insights gleaned from advanced visual representation learnings, such as those in 'Representation Recovering for Self-Supervised Pre-training on Medical Images', that tackle the challenges of data imbalance and representation fidelity in medical image analysis. DBT, offering high spatial resolution and detailed 3D-like imaging, faces significant data imbalance challenges, exacerbated by real-world data distributions. To counter this, we introduce the Self-supervised Initialization and Fine-Tuning (SIFT-DBT) method for robust identification of abnormal DBT images through tasks incorporating a patch-level multi-instance learning approach and efficient modeling of the data. Key to SIFT-DBT is the use of encoder techniques for efficient visual embedding and pre-training, which adeptly preserves spatial resolution and leverages convolutional and transformer techniques to highlight significant instances, showcasing a remarkable volume-wise AUC of 92.69% on an evaluation of 970 unique studies, thereby marking a significant stride in the application of self-supervised learning principles to critical multi-organ medical imaging tasks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~shanlin_sun1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13141v1",
  "title": "Function Trees: Transparent Machine Learning",
  "modified_abstract": "In an era where the transparency and interpretability of machine learning (ML) models are increasingly critical, our work is inspired by breakthroughs in enhancing model interpretability and predictive power through innovative approaches to feature engineering, feature selection, and predictive modeling. Acknowledging the significant impact of leveraging structured knowledge in the automated enhancement of feature sets for models, as demonstrated by previous works, we introduce a novel method for representing a general multivariate function as a tree of simpler functions, named 'Function Trees'. This methodology provides a transparent view into the internal structure of any given function by revealing how subsets of input variables interact jointly, supporting both the understanding and explanation of model predictions. The construction of a Function Tree, based on inputs and function values, enables the rapid determination and computation of the main and interaction effects of the function up to high order, with graphical visualization of interactions involving up to four variables. This transparency in machine learning aims to bridge the gap in understanding complex models and the systems they are used to represent, offering a step forward in making ML models more accessible and interpretable automatically.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~sainyam_galhotra1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13136v1",
  "title": "Multi-fidelity surrogate with heterogeneous input spaces for modeling melt pools in laser-directed energy deposition",
  "modified_abstract": "This paper introduces a novel multi-fidelity (MF) modeling approach designed to address the challenge of variability in input spaces, drawing inspiration from significant advancements in various fields such as the novel algorithmic approach for solving Fredholm Integral Equations of the First Kind via Wasserstein Gradient Flows. This approach demonstrates the power of leveraging complex, multi-modal data for predictive tasks with effective regularization techniques. In the specific realm of laser-directed energy deposition (L-DED), predicting the geometry of melt pools presents a significant challenge due to the hierarchical nature of model fidelity and variability in the input parameters. Our contribution is a groundbreaking methodology for constructing an MF surrogate capable of integrating models of varying complexity and operating within heterogeneous input spaces, and adeptly handling scenarios where certain inputs, analogous to deconvolution in particle physics, may be challenging to model or measure. By devisely mapping a five-dimensional space (laser power, scan velocity, powder flow rate, carrier gas flow rate, and nozzle height) to a pseudo two-dimensional space through examples of Gaussian process-based co-kriging, the resulting heterogeneous multi-fidelity Gaussian process (Het-MFGP) surrogate not only enhances predictive accuracy but also achieves computational efficiency by minimizing reliance on the high-dimensional, high-fidelity thermal model. The efficacy of the Het-MFGP framework for modeling melt pool behavior in L-DED is showcased through its successful accommodation of multimodal data, with estimators ensuring convergence of the model's predictive performance.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Adam_Michael_Johansen1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13135v1",
  "title": "A Parallel Workflow for Polar Sea-Ice Classification using Auto-labeling of Sentinel-2 Imagery",
  "modified_abstract": "This research is inspired by pioneering approaches in the field of remote sensing, with particularly noteworthy contributions including the semantic segmentation of urban scenes based on learned local class interactions and appearance descriptors. Drawing on these innovative methodologies, our study focuses on the critical environmental concern of polar sea ice movement, an indicator of global climate shifts. We aim to introduce a robust, scalable system capable of classifying polar sea ice into distinct categories (thick/snow-covered, young/thin, or open water) using high-resolution Sentinel-2 (S2) imagery, a task complicated by the extensive volume of data, the appearance variability of sea ice, and the lack of labeled training images. Addressing this problem, we present a scalable, accurate problem-solving method for auto-labeling S2 images through color threshold segmentation, accelerated by a parallel processing workflow employing PySpark. This process leverages appearance descriptors to improve the accuracy of segmentation and classification, enabling significant improvements in data handling and map-reduce operations, fostering the creation of auto-labeled training datasets. These datasets are utilized to train a U-Net machine learning model, with its training distributed across multiple GPUs using Horovod for efficiency and enhanced learning capabilities, which demonstrates high classification accuracy on filtered images from the Antarctic's Ross Sea region. Our integrated approach not only showcases a high-precision classification system for polar sea-ice but also establishes a methodological framework with potential application in other areas of environmental monitoring.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Michele_Volpi2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13134v1",
  "title": "Robust NAS under adversarial training: benchmark, theory, and beyond",
  "modified_abstract": "In response to the urgent need for neural architecture search (NAS) solutions that are inherently robust against adversarial perturbations\u2014an aspect underscored by recent strides in the field, including groundbreaking methods that democratize pretraining by using transferrable parameters for diverse model initialization\u2014our research introduces a comprehensive investigation into the construction of benchmarks and theoretical frameworks for robust NAS, especially under adversarial training conditions. We contribute to the field by unveiling a detailed dataset that benchmarks clean and robust accuracy across a wide spectrum of adversarially trained networks within the NAS-Bench-201 space, focused on image datasets. Further, we exploit the neural tangent kernel (NTK) from deep learning theory and apply a boost in methodological approach to forge a novel generalization theory that informs the architecture search process across clean and robust accuracy metrics under the lens of multi-objective adversarial training, leveraging large-resources for comprehensive analysis. Moreover, our benchmarks and theoretical explorations are positioned to enrich the NAS and wider machine learning communities by facilitating dependable reproducibility, more efficient performance assessments, and a solid theoretical grounding in the quest for architectures that stand resilient against malicious data manipulations.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Boris_Knyazev1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13130v1",
  "title": "Self-generated Replay Memories for Continual Neural Machine Translation",
  "modified_abstract": "In light of the ongoing efforts to tackle the catastrophic forgetting problem in Neural Machine Translation (NMT), which remains a significant challenge despite NMT systems' improved performance across languages, our work introduces a novel approach that draws inspiration from advanced optimization techniques in machine learning. Specifically, reminiscent of the ingenuity found in reverse engineering learned optimizers that exhibit sophisticated combinations of learned optimization behaviors such as momentum and gradient clipping, we leverage the generative ability of encoder-decoder Transformers. This methodology employs the high-dimensional generative capacity of these transformers to populate a replay memory with self-generated parallel sentences, a strategy aimed at enabling continual learning in NMT systems across diverse linguistic streams. This approach not only demonstrates a promising avenue to mitigate catastrophic forgetting but also circumvents the need for explicit memorization of training data, thus offering solutions to problems hitherto considered intractable. We believe this innovation could unravel the mystery of achieving robust continual learning in NMT through the effective engineering of optimizers, playing a critical role in our scheme, are key to its success, as they are intricately trained to facilitate this process. Visualizations provided in our empirical evidence supporting our methodology's effectiveness, signaling a step forward in the development of more adaptable and continually learning NMT systems. Code will be made publicly available upon publication at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Niru_Maheswaranathan2",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13128v1",
  "title": "AdaFish: Fast low-rank parameter-efficient fine-tuning by using second-order information",
  "modified_abstract": "Inspired by the substantial improvements in performance across varied tasks in natural language processing (NLP) and computer vision brought about by large-scale pretrained models, this work introduces AdaFish, a novel efficient algorithm that leverages second-order information to facilitate fast, low-rank, parameter-efficient fine-tuning. The extensive parameters in these transformative models, often likened to the complex networks of neurons in the human brain, necessitate significant memory and computational resources for full training. AdaFish is designed to optimize this process within low-rank decomposition-based fine-tuning frameworks by utilizing the generalized Fisher information matrix, which we find to be either low-rank or extremely small-scaled, and demonstrate its equivalence to the Hessian matrix. This initiative is directly motivated by the recent exploration into the distribution of task-specific skills among model parameters seen in transformer-based language models, where parameter efficiency and the understanding of parameter utilization have shown critical insights for model adaptation. By applying adapter-based techniques and focusing on the sharpening of specific skills, AdaFish sets a new standard in fine-tuning practices by essentially acting like the synaptic adjustments seen in neurons during learning processes. Pre-training stages, crucial in these networked models, lay the groundwork for AdaFish\u2019s efficacy in understanding the language of machines. We prove the global convergence of AdaFish, and its iteration/oracle complexity, showcasing through numerical experiments that our algorithm stands competitively with the state-of-the-art AdamW method, underlining a significant stride toward making the fine-tuning process more efficient and practical for a wide array of applications. Code is available at [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Kaiyue_Wen1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13125v1",
  "title": "Probabilistic Circuits with Constraints via Convex Optimization",
  "modified_abstract": "Building on the foundational work in integrating structured latent variables into deep learning through innovative approximations, such as the extension of the Gumbel-Max trick for distributions over structured domains, this study expands the realm of tractable probabilistic models by integrating probabilistic propositional logic constraints into the distributions encoded by probabilistic circuits (PCs). PCs stand out for their efficiency in computing conditional and marginal probabilities while reaching state-of-the-art performance in some domains. By taking a PC and constraints as inputs and efficiently outputting a new PC that adheres to these constraints via convex optimization\u2014without necessitating a full retraining of the model\u2014our approach showcases the practicality of melding logical constraints with deep probabilistic models. Empirical evaluations underscore the versatility of this method, particularly in enhancing model performance amid data scarcity or incompleteness and incorporating fairness measures into machine learning models without sacrificing model fitness. Our findings herald new avenues for applying combinations of logic and probabilistic modeling in diverse scenarios.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Artyom_Gadetsky1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13117v1",
  "title": "Optimal Flow Matching: Learning Straight Trajectories in Just One Step",
  "modified_abstract": "The evolution of generative modeling techniques, particularly in the context of crafting flow matching methods that promote straight trajectories to realize optimal transport (OT) displacements, has broadened the horizons of computational efficiency and accuracy in various fields, including non-rigid reconstruction and deformation tracking. Building upon insights from previous investigations, such as the development of Neural Deformation Graphs for globally-consistent, non-rigid 3D reconstruction, our work introduces an innovative optimal flow matching approach. By leveraging neural multi-MLP architectures for enhanced shape understanding, and utilizing a self-supervised learning framework, this approach is designed to recover straight OT displacements for the quadratic cost in a single flow matching step. Our method, rooted in deep learning, utilizes viewpoint variations as implicit cues for ensuring the robustness of these trajectories, thereby sidestepping the complexities and inaccuracies manifest in iterative procedures and heuristic minibatch OT approximations that have heretofore hampered progress in the field. Through our streamlined methodology and introduction of a graph-based network for flow integration, we aim to enhance the speed and precision of flow integration while ensuring consistency and setting a new standard for generative modeling that combines theoretical elegance with practical utility. The implications of our method extend to improving camera-based tracking systems by enabling them to capture shape deformations more accurately, efficiently handling recordings of these deformations.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Pablo_Palafox1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13111v1",
  "title": "Deep learning with noisy labels in medical prediction problems: a scoping review",
  "modified_abstract": "Objectives: Medical research confronts significant challenges due to noisy labels, which arise from factors such as inter-expert variability and machine extraction inaccuracies. This scenario is further complicated as reflected in related work, for example, in handling bursty missing data in varied medical data records using models like sequential variational autoencoders, underscoring the importance of robust methodologies. Despite these challenges, the adoption of label noise management within the medical research community remains limited, with label noise often overlooked. To address this, we underscore the importance of a dedicated scoping review to capture the state of label noise management in deep learning for medical prediction tasks. This review aims to thoroughly explore the aspects of label noise detection, handling, and evaluation, along with research addressing label uncertainty and noisy streams of data through analysis and application of autoencoders among other techniques. Methods: Adhering to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines, we conducted a comprehensive literature search across four prominent databases: PubMed, IEEE Xplore, Google Scholar, and Semantic Scholar, using targeted search terms related to noisy labels in medical, healthcare, and clinical contexts. Results: We identified 60 papers, published between 2016 and 2023, that met our inclusion criteria. Our investigation spans practical questions in medical research concerning the origins, impacts, detection mechanisms, handling techniques for label noise, and evaluation methods, providing categorizations where applicable. Discussion: Methodological insights reveal that the medical community is on par with the wider deep-learning community regarding techniques evaluated using medical data, particularly leveraging autoencoders and sequential models for cases with bursty data streams. Our review advocates for the recognition of label noise as an intrinsic aspect of medical research, proposing initial forays into label noise management through relatively straightforward methods such as noise-robust loss functions, weighting strategies, and curriculum learning.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Daniel_Barrejon_Moreno1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13108v1",
  "title": "Analyzing the Impact of Partial Sharing on the Resilience of Online Federated Learning Against Model Poisoning Attacks",
  "modified_abstract": "In this work, informed by the intensifying concerns over privacy and security in machine learning (ML) as underscored by recent developments in protecting against membership inference attacks, we scrutinize the resilience of the partial-sharing online federated learning (PSO-Fed) algorithm against model-poisoning attacks. PSO-Fed, designed to mitigate the communication load through the fractional exchange of machine learning model estimates, inadvertently enhances robustness against such adverse interventions. Our analysis targets the performance of PSO-Fed amidst Byzantine threats - actors tampering with local models to compromise the learning process. Demonstrating the algorithm's resiliency, we reveal that PSO-Fed acts as an ensemble of robust mechanisms even in the presence of non-member malicious entities, upholding convergence criteria under malicious influences. Further, a theoretical discourse on the mean square error (MSE) associated with PSO-Fed, alongside its unique architecture, elucidates the interplay between its parameters, including stepsizes, Byzantine client ratios, noise variance, and training methodologies, in mitigating model-poisoning. Optimal stepsize determination for thwarting these attacks is also discussed, alongside extensive numerical validations that attest to PSO-Fed's superior defensive capability over its contemporaries.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Milad_Nasr2",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13107v1",
  "title": "Towards Unsupervised Question Answering System with Multi-level Summarization for Legal Text",
  "modified_abstract": "Drawing inspiration from the forefront of legal text analytics and the emerging challenges within, such as the few-shot semantic retrieval in contract discovery with competitive baselines, this paper summarizes Team SCaLAR's contribution to SemEval-2024 Task 5: Legal Argument Reasoning in Civil Procedure. We navigate the inherent complexities of Legal Texts by proposing an innovative unsupervised method based on similarity and distance measures to generate labels, coupled with a multi-level fusion approach for Legal-BERT embeddings, integrating ensemble features from CNN, GRU, and LSTM networks, which serve as encoders in our system. To tackle the verbose nature of legal documentation, we introduce a T5-based segment-wise summarization technique, which proved effective in retaining essential information and thereby enhancing our model's performance. Our approach marked a significant improvement, with a 20-point leap in macro F1-score on the development set and a 10-point rise on the test set, showcasing the potential of our unified architecture in the complex domain of unsupervised legal question answering systems. The evaluation of our method demonstrates its efficacy in navigating and summarizing extensive legal texts, positioning it as a significant contribution to ongoing discovery and summarization efforts in legal text analysis.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~\u0141ukasz_Borchmann1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13106v1",
  "title": "Knowing Your Nonlinearities: Shapley Interactions Reveal the Underlying Structure of Data",
  "modified_abstract": "Inspired by the resurgence of traditional methods in modern deep learning contexts, such as the adaptation of the linearised Laplace model for uncertainty estimation in Bayesian deep learning, our study leverages the Shapley Taylor interaction indices (STII) to dissect and better understand the complex nonlinear feature interactions pivotal to model interpretations across various domains. This cross-disciplinary effort aims to quantify the subtle but crucial role of underlying data structure in shaping model representations, a pursuit that has seen renewed vigor with the adaptation of classical methods to contemporary challenges, including the selection of crucial variables that approximate these complex relationships. In extending this dialogue to linguistic structures in masked and auto-regressive language models (MLMs and ALMs), we explore the approximation of complex relationships and the selection of crucial variables that embody these interactions, demonstrating that STII values are sensitive to idiomatic expressions and that MLMs modulate STII based on syntactic distance, indicating a stronger reliance on syntax in their nonlinear structures compared to ALMs. Our findings further expand to the approximation of human speech patterns and image classification tasks, highlighting selection criteria that reflect phonetic principles, such as the variability of phonemes based on the openness of the oral cavity, and intuitively mirror object boundaries in image classifiers. Such approximations point out the error margins in our understandings and the importance of residual information that contextualizes these boundaries. Our community-driven approach underlines the critical importance of marrying domain expertise with interpretability research to uncover the intricate web of relationships that underpin model decisions, emphasizing a comprehensive methodology to decrease error through better approximation techniques.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~David_Janz1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13101v1",
  "title": "AdaptSFL: Adaptive Split Federated Learning in Resource-constrained Edge Networks",
  "modified_abstract": "Inspired by the advancements and challenges in optimizing deep neural networks, our research contributes to the novel domain of split federated learning (SFL), a technique that extends the feasibility of deploying complex models on resource-limited edge devices. The evolving landscape of machine learning has seen significant strides in efficient model training through innovations such as fine-grained N:M sparsity and tensor-level optimizations, which underscores the critical nature of system optimization, particularly in resource-constrained environments. Addressing an underexplored facet, this paper introduces a detailed convergence analysis of SFL, focusing on model splitting and client-side model aggregation's effects on learning performance, including the impact of gradients, mean-square-error metrics, and backward propagation techniques on data flow. Building on this foundation, we present AdaptSFL, a resource-adaptive SFL framework designed to optimize the balance between communication-computing latency and training convergence under stringent resource limitations by employing strategies like tensor-level manipulations and multiply-accumulate operations. Through extensive simulations across diverse datasets, accompanied by a focus on pruning inefficient neural pathways using a novel mask application and minimum-variance techniques for optimizing data transmission, AdaptSFL demonstrates superior efficiency by achieving target accuracies more swiftly compared to benchmark methods, underscoring the impact of adaptive strategies in enhancing SFL performance.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Brian_Chmiel1",
  "manipulated_ranking": 4,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13097v1",
  "title": "Simple Ingredients for Offline Reinforcement Learning",
  "modified_abstract": "Our work is inspired by the burgeoning field of reinforcement learning (RL), specifically focusing on offline RL algorithms that have shown promise in leveraging datasets closely aligned with target tasks. Previous works, such as those examining the collaborative multi-user reinforcement learning with low-rank rewards, have highlighted the potential of applying advanced RL methods in both collaborative and ``non-collaborative'' settings, suggesting that the underlying mechanics of learning, including state-action dynamics and probabilistic outcomes, can be significantly optimized. Drawing from these insights, our study furthers the discourse by exploring the efficacy of offline RL algorithms on a novel testbed (MOOD), where data originates from heterogeneous sources and requires adept filtering for utility. We demonstrate that the inclusion of diverse data, filtered through sophisticated methods but distinct from the target tasks, leads to a considerable degradation in the performance of existing methods. Through a comprehensive empirical study, we investigate several hypotheses, delving into probabilities to uncover the reasons behind this performance drop. Contrary to existing beliefs, our findings illuminate that scaling, rather than algorithmic sophistication, plays a pivotal role in enhancing performance. We present evidence that straightforward methods such as AWAC and IQL, when scaled in network size, can not only address the challenges posed by the matrix of additional state-space data in MOOD but also significantly surpass the performance of prior state-of-the-art algorithms on the D4RL benchmark. This revelation underscores the importance of revisiting the fundamental components of offline RL algorithms to adapt to the dynamism of real-world applications.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Dheeraj_Mysore_Nagaraj1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13091v1",
  "title": "JaxUED: A simple and useable UED library in Jax",
  "modified_abstract": "The JaxUED library introduces an open-source, minimal dependency toolkit for developing modern Unsupervised Environment Design (UED) algorithms in Jax, motivated by the advancements in ease of use and collaborative efforts seen in projects like Fabrik. By providing fast, clear, understandable, and easily modifiable implementations, JaxUED aims to accelerate research into UED, similar to how Fabrik has facilitated the visualization, editing, and sharing of deep neural networks. JaxUED notably leverages hardware acceleration, obtaining significant speedups of approximately 100x compared to traditional CPU-based implementations. This advantage allows for deeper investigations into UED concepts without extensive computational costs. Additionally, its intuitive editor interface permits researchers to rapidly edit UED algorithms and share their findings remotely, fostering collaborative development and extending the reach of UED research to a broader audience. This paper details the library's features, showcases baseline results, and emphasizes its potential to catalyze advancements in UED by simplifying implementation challenges. Code is available at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ram_Ramrakhya2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13086v1",
  "title": "Listenable Maps for Audio Classifiers",
  "modified_abstract": "In response to the growing challenge of interpreting the complexity of deep learning models, especially in the context of audio signals, our work introduces 'Listenable Maps for Audio Classifiers' (L-MAC), a posthoc interpretation method that synthesizes from substantial efforts in improving the interpretability of deep neural networks through techniques such as weight-input alignment. By leveraging novel architectures and integrating with existing neural networks, the essence of L-MAC is a decoder, applied to a pretrained classifier, that produces binary masks to delineate the salient features of input audio. L-MAC is designed to be fully compatible with varied audio processing networks, and it employs a novel loss function specifically tailored to refine the classifier's confidence on the masked-in segment while reducing the model's predictive likelihood for the masked-out segment. Through rigorous quantitative assessments on a variety of datasets and a comprehensive user study, L-MAC is demonstrated to outperform existing gradient and masking-based interpretability methods in providing explanations, offering more faithful and user-preferred explanations of audio classifiers. This pivot toward creating interpretable and listenable outputs from complex models reflects a broader imperative in the field to make sophisticated ML insights more accessible and actionable.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Moritz_B\u00f6hle1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12968v1",
  "title": "LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression",
  "modified_abstract": "In the context of burgeoning interest and significant advancements in the efficiency and scalability of machine learning models, particularly those pertaining to natural language processing (NLP) such as the exploration of extreme parameter compression in pre-trained language models (PLMs), this paper introduces LLMLingua-2, which focuses on task-agnostic prompt compression to enhance generalizability and efficiency. Given the inherent redundancy in natural language, prior approaches to prompt compression have relied on removing tokens or lexical units based on their information entropy from models such as LLaMa-7B. However, these approaches face limitations due to the information entropy's reliance on unidirectional context and misalignment with the prompt compression objective. To overcome these challenges, we propose a novel data distillation approach leveraging an LLM for prompt compression that retains critical information and introduces an extractive text compression dataset. We recast prompt compression as a token classification problem, employing a parameter-efficient Transformer encoder with reduced layers to fully capture the essential bidirectional context for compression. This reconstruction of the prompt compression process is further enhanced by optimizing language embedding representations through decomposition and utilizing smaller models like XLM-RoBERTa-large and mBERT to reduce latency significantly. Our method, evaluated on diverse datasets and benchmarks such as MeetingBank, LongBench, ZeroScrolls, GSM8K, and BBH, significantly outperforms strong baselines, exhibiting robust generalization across various LLMs and achieving up to 6x greater efficiency compared to existing methods, with end-to-end latency improvements of 1.6x-2.9x and compression ratios between 2x-5x. Notably, our approach demonstrated on-par performance with larger PLM counterparts without significant loss in fidelity.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Benyou_Wang2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12961v1",
  "title": "TexTile: A Differentiable Metric for Texture Tileability",
  "modified_abstract": "Drawing inspiration from a legacy of substantial progress in computer vision, such as the unified modeling approach of UViM which introduced a novel methodology capable of handling a vast array of vision tasks without task-specific modifications, we introduce TexTile. TexTile addresses a specific yet crucial aspect of texture analysis\u2014tileability. Our novel differentiable metric quantifies the degree to which a texture can be concatenated with itself without introducing repeating artifacts, a property thus far underexplored by the existing general texture quality synthesis methods. Leveraging extensive insights from previous works on vision, learning models, and segmentation, TexTile effectively evaluates the tileable properties of textures, thereby enhancing both the synthesis and analysis of tileable materials. Formulated as a binary classifier and incorporating architectural modifications to pre-trained image classifiers, along with bespoke data augmentation and rigorous training regimes, TexTile surpasses traditional approaches in measuring tileability. When integrated with advanced texture synthesis methods, including diffusion-based strategies, TexTile not only fosters the generation of high-quality tileable textures but also serves as a reliable metric for objectively evaluating tileable texture synthesis approaches. The versatility and efficacy of TexTile signify a step forward in texture analysis, promising to invigorate research and development in the field. Moreover, its successful adoption in various components of texture analysis workflow demonstrates its foundational contribution to the discipline.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Andr\u00e9_Susano_Pinto1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12959v1",
  "title": "WHAC: World-grounded Humans and Cameras",
  "modified_abstract": "Inspired by a foundation of critical advancements in computer vision, notably the introduction of the normal epipolar constraint (NEC) and its probabilistic extension to address uncertainties in feature positions, our research advances the estimation of human and camera trajectories with accurate scale in the world coordinate system from a monocular video. This challenging endeavor, often described as a problem in the field, recovers expressive parametric human models (e.g., SMPL-X) and corresponding camera poses by exploring the dynamic interaction between the world, humans, and camera\u2014a trio that has not yet been fully integrated into prior solutions. Our novel framework, WHAC, leverages key insights: the depth recovery capability of camera-frame SMPL-X estimation methods and the absolute spatial cues provided by human motion, to estimate expressive human pose and shape (EHPS) and camera poses simultaneously without relying on traditional optimization algorithms. Utilizing monocular videos poses a unique challenge; our technique underlines the importance of addressing this problem within our domain, emphasizing the significance of the normal and epipolar constructs in overcoming it. We further introduce a synthetic dataset, WHAC-A-Mole, with accurately annotated interactive human motion and realistic camera trajectories, enhancing visual odometry methods through optimization. Our extensive experimental results, across both established and new benchmarks, confirm the effectiveness and improved performance of our framework. Code and dataset will be made publicly available to foster further research.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Florian_Bernard3",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13041v1",
  "title": "Provable Privacy with Non-Private Pre-Processing",
  "modified_abstract": "Our investigation into the integration of Differentially Private (DP) methods into machine learning pipelines is inspired by a series of foundational works, including seminal efforts to establish reliable frequency estimation under multiparty differential privacy conditions. These foundational studies have often left the implications of data-dependent pre-processing and overall privacy metrics underexplored. Addressing this gap, our work proposes a comprehensive framework designed to precisely quantify the additional privacy costs incurred by non-private, data-dependent pre-processing algorithms. By introducing and integrating two new technical concepts\u2014a variant of DP known as Smooth DP and the concept of bounded sensitivity for pre-processing algorithms\u2014our framework effectively establishes upper bounds for overall privacy assurances. Furthermore, we extrapolate our theoretical developments to provide clear, actionable privacy guarantees for several common pre-processing algorithms, including data imputation, quantization, deduplication, and Principal Component Analysis (PCA), when applied in concert with various DP mechanisms. Our framework's utility is underscored by its ease of implementation, presenting a straightforward methodology for enhancing existing DP machine learning pipelines with robust privacy considerations.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Graham_Cormode1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12952v1",
  "title": "Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization with Vision-Language Models",
  "modified_abstract": "Inspired by recent advancements in vision-language models (VLMs) that have notably propelled computer vision forward, especially in the zero-shot learning domain, and the insightful findings on the similarities and differences in visual representations learned by both self-supervised and supervised methods, this work introduces the Test-Time Prototype Shifting (TPS) framework. TPS represents a novel strategy aimed at adapting deep VLMs to test datasets through the modulation of per-class prototypes in the shared embedding space, using unlabeled test inputs. By leveraging pre-computed and cached prototypes from a pre-trained text encoder, TPS facilitates an optimization-free prototype reuse for boosts in classification accuracy at test-time, while seamlessly integrating with modern prompt engineering techniques. The framework's ability to dynamically learn shift vectors for each prototype, based exclusively on the given test sample and deep image understanding under both self-supervision and external supervision, effectively narrows the domain gap. Our methodology, which requires significantly less memory and computational resources than traditional text-prompt tuning methods, has been extensively validated across 15 datasets, showcasing TPS's unparalleled performance in facing natural distribution shifts and cross-dataset generalization challenges, thereby setting new benchmarks in resource efficiency.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jason_Ramapuram1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12950v1",
  "title": "Optimal and Adaptive Non-Stationary Dueling Bandits Under a Generalized Borda Criterion",
  "modified_abstract": "In dueling bandits, the learner receives preference feedback between arms, and the regret of an arm is defined in terms of its suboptimality to a winner arm. The more challenging and practically motivated non-stationary variant of dueling bandits, where preferences change over time, has been the focus of several recent works (Saha and Gupta, 2022; Buening and Saha, 2023; Suk and Agarwal, 2023). The goal is to design algorithms without foreknowledge of the amount of change.   The bulk of known results here studies the Condorcet winner setting, where an arm preferred over any other exists at all times. Yet, such a winner may not exist and, to contrast, the Borda version of this problem (which is always well-defined) has received little attention. In this work, we establish the first optimal and adaptive Borda dynamic regret upper bound, which highlights fundamental differences in the learnability of severe non-stationarity between Condorcet vs. Borda regret objectives in dueling bandits.   Surprisingly, our techniques for non-stationary Borda dueling bandits also yield improved rates within the Condorcet winner setting, and reveal new preference models where tighter notions of non-stationarity are adaptively learnable. This is accomplished through a novel generalized Borda score framework which unites the Borda and Condorcet problems, thus allowing reduction of Condorcet regret to a Borda-like task. Such a generalization was not previously known and is likely to be of independent interest.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Soumya_Basu2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12948v1",
  "title": "On Safety in Safe Bayesian Optimization",
  "modified_abstract": "In the quest to optimize unknown functions under safety constraints\u2014a challenge paramount in disciplines such as robotics and biomedical engineering\u2014our work is propelled by insights from advanced learning theories, notably those contributing to the PAC-Bayesian framework for problems with unbounded loss functions. As safe Bayesian Optimization (BO) becomes increasingly vital for these domains, ensuring that theoretical safety guarantees of algorithms like SafeOpt translate into palpable real-world safety is crucial for the intended audience. We dissect three pivotal safety-related concerns within the popular SafeOpt class algorithms, starting with their reliance on heuristics that potentially undermine safety guarantees due to deviations from traditional Gaussian Process (GP) regression bounds. To address this, we introduce Real-\u03b2-SafeOpt, an enhanced algorithm retaining all theoretical guarantees by incorporating recent GP bounds and focusing on minimizing potential losses. This study also attempts to relax the unrealistic assumption of bounded Reproducing Kernel Hilbert Space (RKHS) norm in target functions\u2014a fundamental crux in SafeOpt algorithms\u2014proposing instead the Lipschitz-only Safe Bayesian Optimization (LoSBO) that secures safety absent the RKHS norm constraint, displaying pronounced performance improvements in our empirical evaluations while effectively serving as a predictor of safety. Lastly, acknowledging the inherent limitation of a discrete search space in standard SafeOpt applications, we present Lipschitz-only GP-UCB (LoS-GP-UCB), which extends the application to moderately high-dimensional challenges while maintaining safety, making this research of high relevance to an audience across various scientific domains. Through these advancements, our study seeks to pragmatically extend the scope and applicative reliability of safe Bayesian optimization.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Maxime_Haddouche1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12946v1",
  "title": "Sample Complexity of Offline Distributionally Robust Linear Markov Decision Processes",
  "modified_abstract": "Drawing on the foundational principles elucidated by prior works on online learning in episodic Markov decision processes (MDPs) with linear function approximation, our research thrusts into the realm of offline reinforcement learning (RL), where the absence of active exploration exacerbates the sim-to-real gap challenge. This gap denotes the performance degrading discrepancies between simulated training environments and their real-world deployments. We pivot our investigation towards enhancing the robustness of learned policies within the context of high-dimensional state-action spaces, focusing specifically on offline distributionally robust linear MDPs. Our approach, leveraging an uncertainty set defined by the total variation distance and offline data, yields a novel pessimistic model-based algorithm. The algorithm optimally adjusts the rewards and the dynamics of MDPs across episodes, incorporating adversarial feedback from the learner to refine policy decisions. We prove its effectiveness by establishing a sample complexity bound that favorably compares with existing methods, demonstrating an improvement by a magnitude of $\\tilde{O}(d)$, where $d$ represents the feature dimension. Further refinements through a bespoke variance estimator underscore our contributions towards a methodologically rigorous and practically viable framework for offline RL, promising substantial robustness and perfect sample-efficient policy learning under distributional uncertainties.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Julia_Olkhovskaya1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12938v1",
  "title": "Neural Differential Algebraic Equations",
  "modified_abstract": "Building on breakthroughs in utilizing Gaussian noise injections (GNIs) for regularizing neural networks and innovatively modeling the dynamics of stochastic gradient descent, including during training updates, we introduce Neural Differential-Algebraic Equations (NDAEs). Our work adapts and significantly extends the concept of the Universal Differential Equation towards constructing systems of Neural Ordinary Differential Equations, particularly informed by theoretical insights from various scientific domains that often exhibit asymmetry in system behaviors. Differential-Algebraic Equations (DAEs) serve as powerful tools for capturing the temporal evolution of systems under both differential and algebraic constraints, including implicit relationships among system components evident in conservation laws. Our proposed NDAE framework is adept for data-driven modeling of such complex systems, where injections of various types may play a role in system identification and empirical evaluation. Through the lens of NDAEs, we explore system-theoretic data-driven modeling tasks, showcasing the framework's capability with examples like the inverse problem of tank-manifold dynamics and discrepancy modeling in a network of pumps, tanks, and pipes. Our empirical evaluations highlight NDAEs' robustness against noise and their superior extrapolative performance in learning system behaviors and physics of interactions while distinguishing between observable data trends and the intrinsic mechanistic relationships embedded within the system.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Lingjiong_Zhu1",
  "manipulated_ranking": 5,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13040v1",
  "title": "Physics-Guided Neural Networks for Intraventricular Vector Flow Mapping",
  "modified_abstract": "In the exploration of intraventricular vector flow mapping (iVFM) to enhance and quantify cardiac imaging via color Doppler, our study introduces a groundbreaking approach by integrating physics-informed neural networks (PINNs) and a physics-guided nnU-Net-based supervised learning framework for reconstruction. This innovation is inspired by the advent of deep generative modeling and learning in fields requiring high-resolution imaging, such as cryo-electron microscopy (cryo-EM), where the need for accuracy and the capability to handle low signal-to-noise ratios present parallels to the challenges in cardiac imaging. Through rigorous research and evaluations on both simulated color Doppler images from patient-specific computational fluid dynamics models and in vivo Doppler acquisitions using microscopy techniques, we showcase that both the PINNs and nnU-Net methodologies offer reconstruction performance comparable to traditional iVFM algorithms, yet with enhanced efficiency and generalizability. Specifically, PINNs benefit from a dual-stage optimization and pre-optimized weights, while the nnU-Net framework excels in real-time capability and robust performance on sparse and truncated data, independent of explicit boundary conditions. Our findings illuminate the potential of these methodologies in revolutionizing intraventricular blood flow mapping, underlining their applicability in ultrafast color Doppler imaging and in leveraging fluid dynamics equations for deriving novel cardiovascular disease biomarkers based on blood flow analyses involving biomolecules.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Axel_Levy1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12918v1",
  "title": "Generalizable and Stable Finetuning of Pretrained Language Models on Low-Resource Texts",
  "modified_abstract": "In the rapidly evolving field of Natural Language Processing (NLP), Pretrained Language Models (PLMs) have become instrumental in enhancing task-specific performances, especially informed by the pioneering works that explore cross-lingual knowledge extraction and transfer. Acknowledging the pioneering endeavors such as the development of deep multilingual models and techniques like importance-weighted domain alignment for unsupervised transfer across languages\u2014an obstacle due to issues like instability and overfitting. Previous attempts have explored partial network adaptations, yet faltered due to inadequate criteria for sub-network selection yielding suboptimal outcomes. We propose an inventive solution employing a regularization method rooted in attention-guided weight mixup, conceptualizing network weights as a blend of task-specific and pretrained weights, modulated by a learnable attention mechanism. This process enables precise control over sub-network selection, thereby mitigating instability and overfitting through improved representation generalization capabilities. Implementing a bi-level optimization (BLO) framework on distinct training splits furthers our method's efficacy in enhancing the transfer and generalizability of finetuned PLMs in low-resource contexts marked by diverse languages. Our comprehensive experimental validation underscores the proposed methodology's superiority, marking a significant advancement in the refinement of PLMs for low-resource NLP challenges.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ruicheng_Xian1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12910v1",
  "title": "Yell At Your Robot: Improving On-the-Fly from Language Corrections",
  "modified_abstract": "Building upon the ground-breaking successes in robotic dexterity and manipulation\u2014particularly where hierarchical policies merge language with precise control for performing complex, long-horizon robotic tasks\u2014our study explores a novel dimension of human-robot interaction. The inspiration for our work comes from significant advances in robotic capabilities, such as those seen in systems adept at in-hand object rotation relying solely on touch, without visual input, and showcasing the potential of robots equipped with low-cost, multi-finger grippers to perform tasks with high levels of precision and adaptability, even in touch-only scenarios, bypassing the need for visual cues. In our paper, we pivot towards leveraging human linguistic feedback to further enhance robotic performance in long-horizon tasks through sensing, touching, and training. We propose a framework where high-level policies that index into rich, expressive low-level language-conditioned skills can be directly supervised and iteratively improved with natural language corrections from human observers. Our approach demonstrates that nuanced feedback, like minor positional adjustments conveyed verbally, can significantly refine both the high-level decision-making and the low-level, dense execution aspects of robotic tasks, particularly by training on touching feedback in touch-only contexts. The paper validates the efficacy of this method through extensive real-world experiments and simulation studies, showing marked improvements in the performance of dexterous manipulation tasks without the necessity for teleoperation. Additional resources, including videos and code, are available at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zhao_Heng_Yin1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12900v1",
  "title": "Toward Sustainable GenAI using Generation Directives for Carbon-Friendly Large Language Model Inference",
  "modified_abstract": "The burgeoning field of Generative Artificial Intelligence (GenAI) has catalyzed transformative impacts across various industries, significantly heightening concerns over the environmental sustainability of its underlying cloud and high-performance computing platforms. This research is inspired by innovative strategies such as the Self-Refine method, which emphasizes the refinement and efficiency of outputs from large language models (LLMs) without additional training requirements. Drawing on this paradigm of iterative improvement and efficiency, we introduce Sprout, a pioneering framework aimed at reducing the carbon emissions associated with generative LLM inference services. Sprout ingeniously incorporates \"generation directives\" to steer the autoregressive generation process towards heightened carbon efficiency whilst not compromising the quality of generation outcomes, essentially acting as a form of supervised refinement for generative AI tasks. Through employing a directive optimizer for assigning generation directives to user prompts, reinforcement of eco-friendly query responses, and an original offline quality evaluator, which assesses the text generative outcomes, Sprout achieves a remarkable training efficiency and performance improvement leading to an over 40% reduction in carbon emissions in real-world assessments using the Llama2 LLM and global electricity grid data. Our contributions exemplify a crucial stride towards harmonizing the advancement of AI with ecological sustainability, underscoring the imperative of mitigating environmental footprints within the fast-evolving realm of generative AI that is increasingly interacting with humans through written communication.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Uri_Alon1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12887v1",
  "title": "Understanding the training of infinitely deep and wide ResNets with Conditional Optimal Transport",
  "modified_abstract": "In the realm of deep neural networks, particularly Residual Neural Networks (ResNets), understanding the dynamics of gradient flow during training poses significant theoretical challenges due to the inherently non-convex and non-coercive nature of their optimization landscapes. This work is inspired by the advancements in previous research, such as the development of Denoising Normalizing Flow, which has introduced innovative methods for density estimation and manifold learning, thereby illuminating pathways to simplify complex optimization landscapes in machine learning. These methods, akin to the mechanics behind autoencoders, provide a foundational element in handling high-dimensional data sets, including images, with increased efficiency in denoising and density estimation. Taking a cue from these breakthroughs, we delve into a 'mean-field' model of infinitely deep and arbitrarily wide ResNets, employing gradient flow with respect to the conditional Optimal Transport distance to navigate the intricate optimization landscape. Our approach not only aims to elucidate the convergence behaviors of such deep architectures but also marks the first endeavor to rigorously analyze the training dynamics of ResNets with infinite depth and arbitrary width under the lens of Optimal Transport theory. Through meticulous analysis and leveraging the framework of gradient flows in metric spaces, along with incorporating specific sets of conditions conducive to achieving the desired optimization, we demonstrate the well-posedness of our training model, provide estimates for its convergence towards a global minimizer under specific initial conditions, and showcase its potential to streamline the training process of exceedingly deep neural networks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Christian_Horvat1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12873v1",
  "title": "Short-Term Solar Irradiance Forecasting Under Data Transmission Constraints",
  "modified_abstract": "Our work is informed by the critical insights gleaned from recent advancements in handling data with irregular intervals, as exemplified by the development of Continuous-Discrete Recurrent Kalman Networks, which has significantly contributed to our understanding of time series forecasting in machine learning (ML). Leveraging these insights, we developed a data-parsimonious ML model designed for short-term forecasting of solar irradiance, effectively addressing the challenge posed by data transmission constraints. By reducing sky camera images to scalar features and transforming output irradiance values to highlight unknown short-term dynamics, we adopt a novel approach inspired by control theory: utilizing a noise input to account for unmeasured variables, which has notably enhanced model predictions. Utilizing five years of data from the NREL Solar Radiation Research Laboratory, our methodology involved creating three rolling train-validate sets to determine optimal representations for time series, the appropriate span of input time measurements, and the most impactful model input data (features). Emphasizing recurrent computational techniques, our model includes an encoder-decoder architecture to capture the temporal dynamics and structure of solar radiance series. In testing, our model outperformed existing benchmarks in time series forecasting, achieving a mean absolute error of 74.34 $W/m^2$ compared to the persistence of cloudiness model baseline of 134.35 $W/m^2$. This research not only introduces a significantly more efficient strategy for solar irradiance forecasting under strict data limitations but also offers a promising foundation for future exploration in the domain of environmental ML applications with special emphasis on time-centric predictions.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Mona_Schirmer1",
  "manipulated_ranking": 4,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12864v1",
  "title": "A Comparison of Deep Learning Architectures for Spacecraft Anomaly Detection",
  "modified_abstract": "In the context of spacecraft operations where reliability and safety are paramount, this study is motivated by a burgeoning interest in using deep learning for anomaly detection, a field energized by advancements such as those seen in the MATS memory attention model for time-series forecasting. This research compares several deep learning architectures\u2014Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks, and Transformer-based architectures\u2014on their ability to detect anomalies within spacecraft data. Utilizing a substantive dataset from multiple spacecraft missions, our findings suggest varying levels of efficacy: CNNs excel in spatial pattern recognition suitable for specific data types, while LSTMs and RNNs are adept at identifying temporal anomalies in telemetry. Transformer-based models, potent in parsing subtle, long-duration anomalies due to their nuanced attention-based mechanisms and sequence-to-sequence capabilities, present a promising avenue for future exploration, albeit with higher computational demands. Our study not only highlights the importance of selecting an appropriate deep learning architecture based on data characteristics and anomaly types but also weighs the trade-offs between computational efficiency and detection capability in spacecraft operations. Emphasizing series data processing, we conclude that the forecast accuracy and memory efficiency in processing series data significantly influence model performance.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Lifeng_Shen1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12861v1",
  "title": "D-Cubed: Latent Diffusion Trajectory Optimisation for Dexterous Deformable Manipulation",
  "modified_abstract": "Our work is inspired by advances in machine learning models' ability to learn from heteromodal datasets for effective robotic manipulation, as demonstrated in recent studies such as the development of representation learning methodologies for robot manipulation using end-to-end transformer-based pretraining. Building upon these insights, we propose D-Cubed, a novel trajectory optimisation method employing a latent diffusion model (LDM) trained from a task-agnostic play dataset, targeted at the dexterous manipulation of deformable objects. By innovatively combining skill-latent space learning with a latent diffusion model trained to compose these latents into actionable skill trajectories, D-Cubed navigates the substantial challenges presented by the large search spaces and limited task information typical of deformable object manipulation. Optimization is achieved through a gradient-free guided sampling method, leveraging the Cross-Entropy method within the LDM's reverse diffusion process for efficient trajectory exploration and refinement. Our evaluation across a suite of dexterous deformable object manipulation tasks benchmarks D-Cubed's substantial performance gains over both conventional trajectory optimisation methods and state-of-the-art baselines, with successful real-world transfer demonstrated on a LEAP robot hand folding task, showcasing the adaptability and efficacy of our proposed approach.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Garrett_Thomas1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12859v1",
  "title": "Primal Methods for Variational Inequality Problems with Functional Constraints",
  "modified_abstract": "Building upon the foundation laid by recent advancements in optimization methods under inexact information, our work presents a novel approach to tackling constrained variational inequality problems\u2014a cornerstone issue with broad applications in machine learning and operations research. Constrained variational inequality problems, which are pivotal for applications spanning several disciplines, have historically been approached with first-order methods given their simplicity and scalability. These methods, however, depend on computationally intensive procedures such as projection or linear minimization when dealing with multiple functional constraints, presenting a significant bottleneck in practical scenarios. While existing solutions primarily employ primal-dual algorithms leveraging the Lagrangian function, necessitating prior knowledge of optimal Lagrange multipliers, our study introduces a primal-only method, the Constrained Gradient Method (CGM). This method effectively addresses functional constrained variational inequality problems without requiring information on the optimal Lagrange multipliers. Through a comprehensive non-asymptotic convergence analysis, we demonstrate that our algorithm achieves comparable complexity to projection-based methods in both monotone and strongly monotone settings but with more cost-effective oracles rooted in quadratic programming. The efficacy of our approach is further supported by numerous numerical examples, paving the way for a deeper understanding and more efficient solutions to these complex problems.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Dmitry_Pasechnyuk1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12856v1",
  "title": "Equivariant Ensembles and Regularization for Reinforcement Learning in Map-based Path Planning",
  "modified_abstract": "Drawing on the burgeoning interest and advances in reinforcement learning (RL), particularly the nuanced exploration of model-based RL via single models and ensemble approaches in probabilistic dynamics model ensembles, our study navigates the intricate dynamics of ensuring equivariance and invariance in deep RL policy and value networks for enhanced efficiency and performance in environmental symmetries exploitation. While previous efforts have concentrated on designing inherently equivariant and invariant networks\u2014often at the expense of network expressiveness due to a limited selection of components\u2014our methodology introduces an innovative approach termed 'equivariant ensembles'. This approach enables the construction of equivariant policies and invariant value functions without the need for specialized neural network components. Coupled with a novel regularization term designed to infuse an inductive bias during training to counter adversarial instances and enhance dynamics understanding through mechanisms such as spectral analysis and operators, we introduce theoretical backings to these methodologies. Through a detailed case study focused on map-based path planning, we elucidate how our proposed framework of equivariant ensembles and regularization, augmented with spectral analysis techniques and an emphasis on sample efficiency, not only fosters improved sample efficiency and theoretical understanding but also addresses the limitations observed in existing strategies by incorporating an asymptotic analysis to better comprehend the long-term effects of such regularization strategies.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ruijie_Zheng1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12844v2",
  "title": "MELTing point: Mobile Evaluation of Language Transformers",
  "modified_abstract": "In an era where transformers have become integral to machine learning advancements, sparking significant improvements across various applications, our investigation seeks to address the challenge of operationalizing Large Language Models (LLMs) on mobile devices. Drawing upon insights from innovative approaches like SmartDeal, which remodels deep network weights for efficient inference and storage management, we have developed MELT, an automation infrastructure designed to facilitate the headless execution and benchmarking of LLMs on mobile platforms. Efficacy in training methodology enhancements and execution optimization, including bit-operations for low power consumption, is central to our system that supports a variety of models, networks, devices, and frameworks, including Android, iOS, and Nvidia Jetson devices. This research presents the first systematic study analyzing on-device execution of LLMs, assessing performance, energy efficiency, and accuracy across several state-of-the-art models in terms of storage requirements and throughput. Our results elucidate the performance diversity among different platforms and confirm that LLM inference operations are largely memory-intensive. We observe that quantization effectively reduces memory and storage usage, enabling viable execution, albeit at the cost of reduced accuracy. Furthermore, we identify that continuous LLM operations are currently hindered by substantial energy demands and thermal impacts, negatively influencing user experience. The nascent state of this technology ecosystem suggests that significant algorithmic and hardware innovations are required to lower execution costs, particularly in tensor processing units and training methodology enhancements. We anticipate that neural processing unit (NPU) acceleration and framework-hardware co-design will play crucial roles in enabling efficient standalone LLM execution, with offloading strategies emerging as a viable alternative for edge deployments.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Chaojian_Li1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12830v1",
  "title": "Has Approximate Machine Unlearning been evaluated properly? From Auditing to Side Effects",
  "modified_abstract": "Amidst the growing concerns surrounding data privacy and security, our work gravitates towards the acute necessity for mechanisms like machine unlearning to eradicate data lineage from machine learning (ML) models, inspired by novel advancements in interpretable and explainable ML, such as the exploration of maximum deviation for assessing model safety. With Machine Learning as a Service (MLaaS) providers viewing unlearning as a safeguard for regulatory compliance, the urgent need for robust verification techniques to ensure the effectiveness of machine unlearning is more prevalent than ever. Despite its importance, the pace at which privacy communities have approached the development and implementation of such verification methods has been disappointingly slow. In addressing this gap, we introduce well-defined and effective metrics for auditing machine unlearning through black-box unlearning tasks, framing auditing as a non-membership inference problem and forgoing the need for additional shadow model training. This approach generalizes evaluations to the individual data point level and allows for a comprehensive analysis of the shortcomings in current approximate machine unlearning algorithms across utility, resilience, optimization, interpretability, and equity, supported by ensembles of interpretability techniques that utilize trees for deciphering model decisions. Our work aspires to enhance the understanding of approximate machine unlearning methods significantly, marking a crucial step towards making the theoretical right to data erasure an auditable reality.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Kush_R._Varshney1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12821v2",
  "title": "FlowerFormer: Empowering Neural Architecture Encoding using a Flow-aware Graph Transformer",
  "modified_abstract": "In the realm of neural architecture design, the challenge of expeditiously estimating the potential performance of diverse architectures for specific tasks without resorting to extensive training has led to innovative strategies, notably neural architecture encoding. This realm, shaped by prior advancements like the development and analysis of Subgraph GNNs to model intricate graph dynamics and symmetries in architecture design, acts as a foundation for our work. We introduce the FlowerFormer, a novel graph transformer method that leverages the concept of information flow within neural architectures for enhanced representation learning. Key features of FlowerFormer include (a) bidirectional asynchronous message passing, innovatively inspired by the dynamic flows observed in neural architectures, and (b) a globally attentive mechanism that utilizes flow-based masking to selectively steer information dissemination across the architecture graph for optimal performance outcomes. Through rigorous validation against benchmarks and a deep understanding of the properties inherent in subgraphs and performance analysis across distinct domains including computer vision, graph neural networks, and automatic speech recognition, we demonstrate FlowerFormer\u2019s outstanding capability to outperform existing methods in neural architecture encoding. Encouragingly, our contributions also open new avenues for future exploration in subgraphs dynamics and architecture performance optimization in diverse application settings. The design principles underlying FlowerFormer are geared towards maximizing the informational and operational efficiency of neural architecture encodings. The code has been made publicly available at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Beatrice_Bevilacqua1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12820v1",
  "title": "A Physics-embedded Deep Learning Framework for Cloth Simulation",
  "modified_abstract": "In the realm of computer graphics, achieving realistic cloth simulations poses significant challenges, including accurately modeling force interactions and collision handling. This task has drawn parallel interests paralleling advances in 3D human motion prediction, wherein the dynamics and spatial-temporal correlations of movements require sophisticated modeling techniques, often relying on short-term sequences prediction. Leveraging insights from recent advancements in transformer-based architectures, notably their self-attention mechanisms for motion prediction, this paper introduces a novel physics-embedded deep learning framework designed to capture the intricate dynamics of cloth simulation with enhanced efficiency and realism. By directly encoding physical features into a convolutional neural network and introducing branches for learning distinct aspects of cloth physics, with an emphasis on auto-regressive modeling for sequential data, this framework surpasses traditional models in terms of inference speed and generalizability across different scenarios without the need for retraining. Further, its compatibility with existing simulation methods and machine learning techniques for visual enhancements solidifies its applicability in producing high-fidelity cloth animations. Through rigorous testing across various animation cases and comparison with baseline models, we demonstrate the framework's superior performance and potential for future integration with broader machine learning advances in the field of computer graphics, especially in handling sequences of movements. Code for the framework has been made available at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Emre_Aksan1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12764v1",
  "title": "Neural Parameter Regression for Explicit Representations of PDE Solution Operators",
  "modified_abstract": "Inspired by the significant advancements in automatic differentiation (autodiff) and its application in machine learning, particularly in the differentiation and creative optimization of problem solutions across various domains from neural networks to bi-level optimization, we introduce Neural Parameter Regression (NPR), a novel framework tailored for learning solution operators in Partial Differential Equations (PDEs). This approach innovatively combines the principles of Physics-Informed Neural Networks (PINNs) with the efficiency and modular design of DeepONets, employing PINN techniques to regress Neural Network (NN) parameters for parameterizing each solution based on specific initial conditions, thereby approximating a mapping between function spaces. With a focus on hyper-parameter tuning and multiclass mapping, our method introduces parameter efficiency enhancements through the incorporation of low-rank matrices, thus significantly boosting computational efficiency, scalability, and facilitating optimization at multiple levels. Demonstrating remarkable adaptability, the framework facilitates rapid fine-tuning and inference, even for out-of-distribution examples, enabling more effective and versatile solutions in PDE-related challenges.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Fabian_Pedregosa1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13037v1",
  "title": "BiLoRA: A Bi-level Optimization Framework for Overfitting-Resilient Low-Rank Adaptation of Large Pre-trained Models",
  "modified_abstract": "The introduction of BiLoRA as an innovative fine-tuning methodology stems from the inspiration and foundation laid by seminal works in bi-level optimization, like the advancements seen in 'Smooth Bilevel Programming for Sparse Regularization'. Building upon this knowledge, our proposed framework revolutionizes Low-Rank Adaptation (LoRA) by incorporating a bi-level optimization (BLO) strategy to combat the prevalent issue of overfitting, which haunts conventional fine-tuning techniques. Utilizing pseudo singular value decomposition for norm-based parameterization of low-rank incremental matrices and employing an alternating minimization technique, BiLoRA distinguishes itself. Furthermore, it integrates advanced solvers for smooth optimization, including square systems for improved efficiency, and segments the training into distinct layers allocated across differing subsets of training data to enhance resolution. This stratified method substantially reduces the propensity for overfitting to a singular dataset, thereby improving generalization capabilities on unseen test data. Having been rigorously tested across a wide spectrum of natural language understanding and generation tasks with large pre-trained models, BiLoRA has demonstrated robust superiority over traditional LoRA methodologies and other fine-tuning approaches, while maintaining a comparable number of trainable parameters.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Clarice_Poon1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12732v1",
  "title": "Tighter Confidence Bounds for Sequential Kernel Regression",
  "modified_abstract": "Drawing on the foundational work of previous studies in the area of kernel-based learning, particularly focusing on the improvement of meta-learning algorithms within the image classification domain, this paper ventures into exploring the critical concept of confidence bounds within the context of sequential kernel regression. Confidence bounds are indispensable for accurately assessing the uncertainty of predictions, thereby informing the exploration-exploitation trade-off crucial in sequential learning and decision-making algorithms. Through the employment of martingale tail bounds and finite-dimensional reformulations of infinite-dimensional convex programs, this research introduces novel confidence bounds for sequential kernel regression, demonstrably tighter than any currently existing counterparts. Drawing greatly on principles of meta-learning, we substantiate the superiority of these new confidence bounds by applying them to the kernel bandit problem, illustrating not only their efficacy in enhancing the empirical performance of the KernelUCB (GP-UCB) algorithm but also their contribution to boosting meta-learning frameworks. The versatility of our confidence bounds extends their applicability beyond kernel bandit problems, serving as a powerful tool in the design of improved algorithms for a wide array of kernelised learning and decision-making challenges. Our work proposes a significant step forward in the refinement of meta-learning frameworks, bolstering their effectiveness across various applications.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~John_Isak_Texas_Falk1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12719v1",
  "title": "Bilevel Hypergraph Networks for Multi-Modal Alzheimer's Disease Diagnosis",
  "modified_abstract": "Informed by the recent progress in semi-supervised learning (SSL) and active learning (AL) to reduce sample complexity in machine learning models, our research introduces a novel hypergraph-based semi-supervised multi-modal diagnosis framework specifically designed for early Alzheimer's disease detection. Central to our approach is a bilevel hypergraph optimization framework that innovatively joint learns a graph augmentation policy and a semi-supervised classifier, aiming to enhance robustness and generalization through targeted training strategies. Additionally, we propose a novel strategy for generating pseudo-labels through a gradient-driven flow, improving the model's utility with minimal labels. This multifaceted approach builds upon the foundational principles of leveraging unlabeled instances in SSL and promoting efficient information propagation pathways, echoing the advancements made in the convergence of active and semi-supervised learning theories. Emphasis on training is pivotal to our model's success, as it determines the efficiency of utilizing unlabeled data and the effectiveness of the learning algorithms in resolving diagnostic problems and achieving generalizability. Our experimental findings underscore the superiority of our framework in diagnosing Alzheimer's disease, signaling a significant step forward in the development of more effective diagnostic tools. Training and refining our model seeks to bridge the gap between theoretical research and practical application, marking a pivotal point in the pursuit of advancements in Alzheimer's disease diagnosis.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Seo_Taek_Kong1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12712v1",
  "title": "Addressing Source Scale Bias via Image Warping for Domain Adaptation",
  "modified_abstract": "In the context of domain adaptation for visual recognition, our work is motivated by the critical challenge of scale bias, a phenomenon highlighted through historical analysis and experimentation across a diverse spectrum of machine learning paradigms, including efforts in enhancing model robustness to out-of-distribution images with minimally large-scale datasets. Recognizing the limitations of conventional scale bias mitigation strategies regarding their adaptability and computational efficiency, we propose a novel approach utilizing adaptive attentional processing. This method focuses on oversampling salient object regions by warping images in-place during training to efficiently address source scale bias without the need for extensive computational resources. Our methodology, characterized by instance-level warping guidance aimed at object region sampling, significantly enhances domain adaptation capabilities under varying environmental conditions, including changes in geography, lighting, and weather, while leveraging representation learning for overcoming adaptation challenges. The introduction of self-supervised learning techniques further improves the recognition of salient features in novel domains, remaining agnostic to the task, domain adaptation algorithm, saliency guidance, and underlying model architecture. Our empirical results showcase substantial improvements across several benchmarks, including +6.1 mAP50 for BDD100K Clear to DENSE Foggy transformations, among others, adding minimal memory overhead during training and no additional latency at inference. The detailed results and comprehensive analysis are provided in the Appendix.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Robert_Geirhos1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12710v1",
  "title": "Selective, Interpretable, and Motion Consistent Privacy Attribute Obfuscation for Action Recognition",
  "modified_abstract": "Inspired by recent advancements in computer vision technologies, such as the development of Vision Transformers (ViTs) that have introduced significant improvements in efficiency and accuracy across various tasks in the field, our work addresses the growing concerns surrounding the privacy of individuals captured in public imagery through the lens of privacy-preserving action recognition. We recognize the limitations inherent in current approaches, such as their tendency for global obfuscation that masks not only privacy-sensitive areas but also those critical for recognizing actions, as well as a general lack of interpretability that undermines user trust in these technologies. To address these issues, we propose a novel solution that integrates human-selected privacy templates for enhanced interpretability, an obfuscation scheme that selectively conceals attributes while ensuring motion consistency critical for action recognition. This scheme capitalizes on self-attention mechanisms inherent to ViTs, enabling selective attention over areas requiring privacy without compromising the interpretability or motion dynamics needed for accurate action recognition. Unlike existing methods that necessitate architecture-specific training, our architecture-agnostic approach modifies input imagery directly, offering greater flexibility by eliminating the need for retraining, and demonstrates superior performance on three widely used datasets through dense and rigorous benchmarking against traditional and dense evaluation metrics. Our method leverages insights from recent breakthroughs in efficient design principles for vision models to ensure that privacy measures do not compromise the functionality or performance of action recognition systems. [GitHub URL omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zizheng_Pan1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12695v1",
  "title": "Federated Semi-supervised Learning for Medical Image Segmentation with intra-client and inter-client Consistency",
  "modified_abstract": "In the context of medical image segmentation, our work is inspired by advancements in methods for handling unlabeled data and domain shifts, akin to those in unsupervised domain adaptation (UDA) for semantic segmentation, across various domains, where models learn under varying scenarios without direct access to extensive labeled datasets. Given the criticality of medical image segmentation for disease diagnosis and the constraints imposed by the necessary domain expertise for labeling and privacy concerns, our study extends these principles into a federated learning framework to accommodate the privacy and scarcity of medical data in different domains. We introduce a novel federated semi-supervised learning framework that leverages intra-client and inter-client consistency to ensure smooth predictions across different medical benchmarks and mitigate confirmation bias without direct data sharing for continuous knowledge transfer. Utilizing a Variational Autoencoder (VAE) trained collaboratively by clients, our framework enhances the model's capability for segmentation tasks by (1) extracting latent features to prevent model forgetting, (2) enabling novel data augmentation strategies for intra-client consistency, and (3) employing generative capabilities for inter-client consistency through a distillation strategy that facilitates continuous transfer of knowledge between different tasks. This multifaceted approach not only positions our method favorably against other federated semi-supervised and self-supervised learning techniques but also minimizes computational and communication overhead, as demonstrated through our experimental comparisons.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Tuan-Hung_Vu3",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12690v2",
  "title": "LNPT: Label-free Network Pruning and Training",
  "modified_abstract": "Motivated by the profound challenges and promising avenues for optimization in neural network deployments identified by previous work, such as the exploration into balancing the recognition capabilities across variable and imbalanced data distributions, our study pioneers a novel approach towards network pruning and training that is both efficient and label-free. Pruning before training allows for the streamlined integration of neural networks into resource-constrained environments like smart devices by keeping only the weights that are beneficial for generalization. Although it is widely believed that the disparity in weight norms between the network's initial and fully trained states is an indicator of generalization performance, our investigation reveals inconsistencies in this metric during the training process, posing challenges for predetermining optimal pruned structures. Our concept of the learning gap, which we introduce, establishes its precise correlation with generalization and demonstrates through experiments that this metric, represented by feature maps from the network's penultimate layer, aligns closely with variations in generalization performance across distribution-aware and head-tail benchmarks. Building on this insight, we propose LNPT, a groundbreaking learning framework that facilitates network pruning and learning on smart devices using unlabeled data, under the guidance of mature networks on the cloud. This method significantly outperforms conventional supervised training techniques, showcasing its effectiveness and potential for widespread application in smart device deployments.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zhongqi_Miao1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12688v1",
  "title": "SEVEN: Pruning Transformer Model by Reserving Sentinels",
  "modified_abstract": "In response to the inferential and computational challenges posed by large-scale Transformer models (TM) in mobile applications, our study introduces 'SEVEN', a novel pruning strategy that directly addresses the limitations of conventional approaches and is inspired by insights from recent breakthroughs in natural language understanding, such as those highlighted by the advent of CommonSenseQA 2.0, which offers a new dimension in assessing and advancing AI's performance in models. Unlike existing methods that struggle with the dynamic and intricate nature of gradients in TMs and tend to retain weights with higher gradient noise, resulting in performance inconsistencies across varying sparsity levels and datasets, our approach leans on Symbolic Descent (SD) to navigate the noisy batch gradient sequences characteristic of TMs. By employing SD, SEVEN dynamically appraises weight importance, especially favoring those with consistently high sensitivity (i.e., lower gradient noise), thereby ensuring that critical weights are preserved. We validate the effectiveness of SEVEN across multiple TM applications\u2014including natural language processing, question-answering, and image classification\u2014demonstrating its capability to deliver superior performance and enhance data-driven decision-making in diverse pruning scenarios. It also ensures robust engagement across different fine-tuning strategies, thus fostering greater engagement with the pruned models. The code is made publicly available at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ori_Yoran1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12687v1",
  "title": "Audio-Visual Compound Expression Recognition Method based on Late Modality Fusion and Rule-based Decision",
  "modified_abstract": "Inspired by recent advancements in cross-modal self-supervised video representation learning, notably exemplified by approaches that enhance motion and visual pathway integration for improved representation in videos, our research innovatively applies these foundational insights to the domain of compound expression recognition. We present the results of the SUN team for the Compound Expressions Recognition Challenge of the 6th ABAW Competition, proposing a novel audio-visual method that integrates emotion recognition models through late modality fusion at the emotion probability level, supplemented by rule-based decision mechanisms for accurate prediction of compound expressions without requiring specific training data for the target task. Evaluated across multi-corpus learning and cross-corpus validation frameworks, our findings underscore the potential of our method in laying the groundwork for the development of intelligent tools aimed at annotating audio-visual data with minimal supervision, particularly in capturing the nuances of human basic and compound emotions. The learning process benefits from a cross-modal supervision approach that leverages both audio and visual inputs for enhanced recognition accuracy. The source code is made publicly available.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Fanyi_Xiao1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12672v1",
  "title": "Improving Interpretability of Scores in Anomaly Detection Based on Gaussian-Bernoulli Restricted Boltzmann Machine",
  "modified_abstract": "Drawing on the foundation of semi-supervised learning methodologies and the need for enhanced interpretability within anomaly detection frameworks, our study is directly inspired by breakthroughs in explainable artificial intelligence (XAI), particularly the advancements in understanding complex model decisions through techniques such as Shapley Values. Similar to how SHAP-IQ endeavored to quantify and improve interpretability in any black box model through efficient sampling-based approximations, our work seeks to advance the field of anomaly detection using Gaussian-Bernoulli restricted Boltzmann machines (GBRBMs). Specifically, we propose an innovative measure that enhances the interpretability of the GBRBM-based anomaly detection scores, leveraging the cumulative distribution to establish a more intuitive guideline for setting classification thresholds using normal data points. This innovative approach addresses the classification challenge and the interaction between normal and anomalous data points in high-dimensional spaces, thereby improving the quality of anomaly detection. In the realm of research, we address the fundamental issue of interpreting these scores\u2014a fundamental challenge given their crucial role in distinguishing between normal and anomalous data points. In addition, due to the computational challenges in directly evaluating the minimum score value for quality assurance, we introduce an evaluation method for this score using simulated annealing, a technique renowned for its effectiveness in optimization problems while ensuring the representation's integrity. Our method's validity is supported by numerical experiments with synthetic data, showing the practicality of our approach in making GBRBM-based anomaly detection more interpretable and actionable for users.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Maximilian_Muschalik1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12664v1",
  "title": "Deciphering AutoML Ensembles: cattleia's Assistance in Decision-Making",
  "modified_abstract": "Inspired by the noteworthy contributions and the fruitful intersection of neural network ensembles and sparse mixture of experts (MoEs) in enhancing model performance and reliability, our work introduces cattleia, an innovative application designed to decode the complexities inherent in ensembles used in Automated Machine Learning (AutoML). While ensembles have been esteemed for outperforming single model predictions, they often come at the cost of reduced interpretability. By leveraging cattleia, users are equipped to dissect ensembles designed for a variety of tasks\u2014regression, multiclass, and binary classification\u2014constructed via prominent AutoML frameworks like auto-sklearn, AutoGluon, and FLAML. Our tool offers comprehensive analysis, including calibration and performance evaluation through ensemble and individual models' metrics, alongside original measures for assessing model diversity and complementarity employing sparse learning techniques. Moreover, leveraging sparse learning techniques and few-shot learning insights, we employ explainable artificial intelligence (XAI) techniques to uncover variable significance, enhancing ensemble tuning through bespoke weight adjustments for more accurate predictions. Facilitated by intuitive batch analysis visualizations, cattleia aims to democratize ensemble understanding, aiding stakeholders in decision-making processes and offering deeper insights into AutoML's inner workings.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~James_Urquhart_Allingham1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12659v1",
  "title": "Zeolite Adsorption Property Prediction using Deep Learning",
  "modified_abstract": "Motivated by the revolutionary impacts deep learning has brought to fields requiring the exploration of large, high-dimensional spaces for discovery purposes such as drug and materials discovery, our work addresses the challenge of predicting the adsorption properties of zeolites, crucial for designing novel materials. Despite the wide configuration space of these materials and the computational expenses associated with existing molecular simulation methods, we introduce a machine learning model that outpaces traditional simulations by 4 to 5 orders of magnitude in performance for predicting adsorption properties. Our approach leverages the growing datasets detailing various aluminium configurations of MOR, MFI, RHO, and ITW zeolites, alongside their heat of adsorption and Henry coefficients for CO$_2$, derived from Monte Carlo simulations, showcasing significant querying capabilities. The alignment of our model's predictions with those from the Monte Carlo simulations corroborates its accuracy in property prediction and its novel use in identifying adsorption sites. Furthermore, the model's deep learning-based inference mechanism is adept at processing and querying large datasets to predict these properties accurately. Finally, we explore the model's potential in generating new zeolite configurations through integration with a genetic algorithm, paving the way for streamlining the material design process.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Moksh_Jain1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12646v1",
  "title": "Prompt-fused framework for Inductive Logical Query Answering",
  "modified_abstract": "Inspired by significant advancements in addressing interpretability and reasoning in machine learning, such as the development of concept-based models (CBMs) that enhance model interpretability by aligning machine representations with high-level human-understandable concepts, our work introduces the Pro-QE framework for addressing the challenges of answering logical queries on knowledge graphs (KGs). The primary obstacle in KGs is their inherent incompleteness, an issue that has been partially addressed by existing research through the augmentation of missing edges but largely neglects the dynamism brought about by the emergence of new entities and the necessity for causal reasoning in filling those gaps. Moreover, traditional methods have struggled with the granular analysis and recognition of logical operators within queries, often treating them in isolation rather than as components of an integrative reasoning process. Leveraging insights from the successes and limitations of prior efforts in the realm of open-set learning, our Pro-QE framework innovatively utilizes a query-aware prompt-fusion mechanism, allowing for the incorporation of existing query embedding methodologies, the contextual incorporation of emerging entities, and enhanced recognition of logical relationships under various forms of supervision. This learning approach fosters a more holistic understanding and processing of queries, adding a novel dimension to the generation of insights from KGs. Evaluated on two newly introduced benchmarks specifically designed for the inductive reasoning setting, Pro-QE demonstrates notable proficiency in accommodating unseen entities within logical queries and recognizing complex logical structures. An accompanying ablation study further underscores the significance of both the generation and aggregation mechanisms within our proposed model, as well as the role of supervision and a robust vocabulary in refining the recognition of logical patterns.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Andrea_Passerini2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12641v1",
  "title": "Automated Contrastive Learning Strategy Search for Time Series",
  "modified_abstract": "Inspired by previous advancements in representation learning and uncertainty modeling, such as the innovative use of normalizing flows for rich aleatoric and epistemic uncertainty estimation, this paper introduces a pioneering approach to mitigating the challenges faced in manually crafting Contrastive Learning Strategies (CLS) for time series analysis. The manual development of CLS demands in-depth domain knowledge and extensive experimentation across different data sets, which is labor-intensive and not always feasible. Tackling this issue, we propose an Automated Machine Learning (AutoML) system at Microsoft, specifically designed for the autonomous identification and application of CLS across a spectrum of time series tasks and datasets, dubbed Automated Contrastive Learning (AutoCL). We meticulously construct a comprehensive search space encompassing data augmentation, embedding transformation, contrastive pair construction, and contrastive losses, exceeding a magnitude of 3x10^12 possible configurations and optimized for various distributions of data. Through the incorporation of an efficient reinforcement learning algorithm aimed at refining CLS based on validation task performance and leveraging prediction ensembles for improved learning accuracy, we successfully derive more effective strategies within this vast landscape. Our empirical investigations across several real-world domains substantiate AutoCL's capability to autonomously unearth fitting CLS for specific datasets and tasks, while accounting for epistemic uncertainty and adapting to different distributions. Additionally, we identify a Generally Good Strategy (GGS) from the optimal CLS delineated by AutoCL on various public datasets/tasks, which exhibits robust performance across different contexts. Supplementary empirical analysis, grounded in the comprehensive set of contrastive learning approaches, is provided to inform and guide future endeavors in CLS design.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Lucas_Berry1",
  "manipulated_ranking": 4,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12636v1",
  "title": "A Practical Guide to Statistical Distances for Evaluating Generative Models in Science",
  "modified_abstract": "Generative models are fundamentally reshaping scientific inquiry across various domains, inspired by significant strides in machine learning methods such as those explored in AutoGP's examination of Gaussian Process Models. This work endeavors to provide an accessible guide to popular statistical distances, aiming to equip individuals with a foundational understanding of mathematics and statistics. We scrutinize four prevalent notions of statistical distances - Sliced-Wasserstein (SW), Classifier Two-Sample Tests (C2ST), Maximum Mean Discrepancy (MMD), and Fr\u00e9chet Inception Distance (FID) - representing diverse methodological approaches. Each distance's intuition, advantages, scalability, complexity, and limitations are elucidated. Through the practical evaluation of generative models in decision making and medical imaging, we illustrate how different distances can yield varying interpretations of similar data, emphasizing the importance of choosing appropriate statistical distances. This guide seeks to empower researchers to effectively utilize, interpret, and assess statistical distances when evaluating generative models in the sciences.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Kurt_Cutajar1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12609v1",
  "title": "SUN Team's Contribution to ABAW 2024 Competition: Audio-visual Valence-Arousal Estimation and Expression Recognition",
  "modified_abstract": "Automated emotion recognition represents a pivotal advancement in understanding human communication, propelling interest across various disciplines over the past two decades. This burgeoning field draws inspiration from previous explorations in automated facial action unit (AU) detection, which have underscored the challenge of maintaining model efficacy across diverse observational settings. Guided by insights from cross-domain AU detection research and extensive literature reviews, this study endeavors to bridge the gap in performance between lab-controlled and 'in-the-wild' data scenarios. We meticulously scrutinize audiovisual deep learning methodologies for emotion recognition, focusing specifically on the intricacies of architecture optimization. By employing fine-tuned Convolutional Neural Networks (CNN) for the visual domain alongside transfer learning approaches for audio analysis, and incorporating robust detectors powered by the Public Dimensional Emotion Model (PDEM), we navigate the complexity of in-the-wild emotional recognition. Our investigation further delves into temporal modeling and fusion techniques, engaging with embeddings derived from multi-stage, modality-specific Deep Neural Networks (DNN) and leveraging diverse databases to enhance training robustness. The efficacy of these approaches is validated through rigorous testing on the AffWild2 dataset, adhering to the protocol established for the Affective Behavior Analysis in-the-Wild 2024 (ABAW'24) challenge. The participation of a large cohort of participants in this study lays a foundational framework for further advancements in automatic emotion detection systems.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Laszlo_Attila_Jeni1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12606v1",
  "title": "On the Effectiveness of Heterogeneous Ensemble Methods for Re-identification",
  "modified_abstract": "Inspired by the challenge of achieving better classifier calibration with limited data sets, this research introduces a groundbreaking ensemble method geared towards the re-identification of industrial entities, pivoting from the traditional reliance on complex siamese neural networks to an integrated framework of simpler, more rudimentary models. This evolution addresses the critical need for adaptable, efficient algorithms in scenarios constrained by hardware or data availability, drawing on the premise that effective classifier calibration\u2014essential for producing accurate probability estimates, especially in contexts of sparse data\u2014can substantially benefit from innovative data manipulation and model architecture strategies. Our approach uses diverse feature extraction techniques as inputs for each sub-model within the ensemble, significantly reducing the required training time while achieving or surpassing contemporary state-of-the-art performance metrics in re-identification tasks, including classifiers with a Rank-1 accuracy surpassing 77% and a Rank-10 accuracy exceeding 99%. Through an examination of five distinct feature-extraction methodologies and their integration into various ensemble configurations, we illuminate paths forward in the quest for efficient, scalable machine learning models suitable for a broad spectrum of industrial applications. The adaptability and efficiency of the ensemble method are enhanced through advanced calibration techniques, ensuring that each classifier within the forest contributes optimally to the overall accuracy of the system.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jaakko_Suutala1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12599v1",
  "title": "Preventing Eviction-Caused Homelessness through ML-Informed Distribution of Rental Assistance",
  "modified_abstract": "Building upon insights from previous studies, such as the application of Bayesian model selection for causal discovery in datasets, our research embarks on the vital task of preventing eviction-induced homelessness through a proactive approach in the allocation of rental assistance. Partnering with Allegheny County, PA, this study introduces a machine learning (ML) system that leverages state and county administrative data to accurately prioritize individuals at risk of future homelessness, significantly outperforming simpler prioritization methods by at least 20% in fairness and equitability across race and gender. Our approach also promises to identify 28% more individuals who are overlooked by current processes, thereby standing on the brink of homelessness. Emphasizing the selection of appropriate features, model design, and causal inference methods for the tasks, we adopt sophisticated models that use causal relationships to enhance the identifiability and correctness of predictors for homelessness, thus strengthening the methodological foundation of our ML model. The findings from this project provide a new dimension to the allocation strategies of rental assistance programs, offering vital lessons on data needs, model design, correctness, model selection, evaluation, and field validation, which could serve as a beacon for developing evidence-based decision support tools in similar contexts. Furthermore, it initiates a generating path in the literature on homelessness prevention.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Anish_Dhir1",
  "manipulated_ranking": 5,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12588v1",
  "title": "Machine Learning of the Prime Distribution",
  "modified_abstract": "Our inquiry into the domain of probabilistic number theory through machine learning (ML) methodologies is guided by significant prior advancements in nonparametric statistical methods, specifically those enabling efficient kernel-based two-sample testing with analytic representations and embeddings. Inspired by the pioneering work on understanding and testing distribution discrepancies, we leverage maximum entropy methods to propound several theorems in probabilistic number theory, including a novel interpretation of the Hardy-Ramanujan Theorem. Furthermore, our exploration introduces a theoretical framework that aligns with and elucidates Y.-H. He's experimental findings on the learnability of prime numbers, setting new benchmarks in the pursuit. Moreover, we argue that the Erd\u0151s-Kac theorem, a cornerstone in understanding the distribution of primes, is unlikely to be unveiled using current ML paradigms without incorporating sophisticated testing mechanisms that consider the cost of computation and the energy efficiency in the space of possible solutions. Our numerical experiments solidify our theoretical insights, presenting an intriguing confluence of classical number theory and contemporary ML techniques, including two-sample testing methods explicitly realized through kernel-based approaches, mindful of computational resources, and aimed at enhancing performance.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Kacper_Chwialkowski1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13032v1",
  "title": "Hybrid Unsupervised Learning Strategy for Monitoring Industrial Batch Processes",
  "modified_abstract": "Monitoring complex industrial production processes is critical in various sectors, such as the pharmaceutical industry, to ensure efficiency, product quality, and safety. Inspired by recent advancements in clustering techniques and unsupervised learning, including innovative methods like incremental estimation of low-density separating hyperplanes for data partitioning, this paper introduces a hybrid unsupervised learning strategy (HULS). The HULS framework addresses the limitations of traditional unsupervised learning techniques, such as Self-Organizing Maps (SOMs), particularly in scenarios characterized by unbalanced datasets and highly correlated process variables. Through the combination of existing unsupervised learning approaches with a focus on density-based clustering and an innovative use of separating hyperplanes, the HULS aims to enhance the monitoring of intricate industrial batch processes. Comparative experiments based on a laboratory batch are performed to assess the efficacy of the HULS concept, utilizing gradient analyses as part of the low-density cluster elucidation. Moreover, a comprehensive software package to implement the HULS is available, facilitating its application.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~David_Hofmeyr2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12562v1",
  "title": "Equity through Access: A Case for Small-scale Deep Learning",
  "modified_abstract": "Motivated by the democratization of deep learning (DL) and inspired by attempts to understand DNN training processes during endeavors like the Concept-Monitor's to demystify DNN's black-box nature through human-interpretable visualization, this paper seeks to address the disparities in access to large-scale computational resources. The recent exponential growth in DL capabilities has paralleled an increase in the size of neural models and the computational, data, energy, and emissions costs associated with them, presenting significant entry barriers, especially to those in the Global South with limited resources. In this context, our work surveys the efficacy of existing DL models for various tasks in resource-constrained environments, proposing a novel performance-per-resource unit metric, the PePR score, to gauge the efficiency of DL training processes. Analyzing 131 unique DL architectures across a spectrum of 1M to 130M trainable neurons alongside three medical image datasets, we identify significant performance-resource trade-offs, advocating for small-scale, specialized models particularly suitable for critical applications like medical image analysis due to their minimized resource footprint. By leveraging techniques like pruning on pretrained models during the refinement process, we further demonstrate potential reductions in adversarial computational and data requirements, striving towards a more equitable future in AI by encouraging the development of models optimized for minimal resource consumption within a flexible framework.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Tuomas_Oikarinen1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12559v1",
  "title": "Confidence Self-Calibration for Multi-Label Class-Incremental Learning",
  "modified_abstract": "Building upon the foundational work in enhancing multi-label classification and leveraging hierarchical relationships among labels, such as in zero-shot image classification with hierarchical label sets, our study addresses the partial label challenge inherent in Multi-Label Class-Incremental Learning (MLCIL). This challenge arises when only the new classes are labeled during training, leaving past and future labels inaccessible, often leading to a surge in false-positive errors, thus exacerbating catastrophic forgetting within the disjoint label spaces. We propose a solution to refine multi-label confidence calibration in MLCIL through a Confidence Self-Calibration (CSC) approach. This includes introducing a class-incremental graph convolutional network for label relationship calibration and a max-entropy regularization for confidence calibration for each multi-label increment, which together facilitate confidence self-calibration by penalizing over-confident output distributions and encouraging the classification of subclasses with high accuracy. Applied to the MS-COCO and PASCAL VOC datasets and utilizing embeddings for enhanced multi-label learning and querying capabilities, our approach achieves state-of-the-art results in MLCIL tasks, demonstrating our methodology's efficacy in calibrating label confidences and effectively addressing zero-shot scenarios. Additionally, by elaborating on subclass relationships within the given vocabulary, we extend the scope of engineering solutions available for addressing complex labeling challenges in MLCIL.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zachary_Novack1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12553v1",
  "title": "Pretraining Codomain Attention Neural Operators for Solving Multiphysics PDEs",
  "modified_abstract": "In an environment where solving multiphysics problems with coupled partial differential equations (PDEs) presents significant challenges due to complexities such as intricate geometries and interactions between various physical domains, our work introduces the Codomain Attention Neural Operator (CoDA-NO). This novel architecture takes inspiration from the rapid progress in fields beyond traditional PDE solving, particularly from neural architecture search methodologies that have revolutionized how neural networks are designed, optimized, and applied across a spectrum of machine learning tasks, from computer vision to natural language understanding. Moreover, reflecting on different strategies and practices employed by systems tailored for speech recognition, CoDA-NO innovatively tokenizes functions along the codomain or channel space, making it particularly adept at self-supervised learning or pretraining across diverse PDE systems. By extending advanced techniques such as positional encoding, self-attention, and normalization layers into the function space, CoDA-NO not only offers a unified model capable of learning representations of varied PDE systems but also excels in few-shot learning settings, significantly outperforming existing methods in complex applications like fluid flow simulations and fluid-structure interactions by over 36%. Code for CoDA-NO is available at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Mahmoud_Safari1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12544v1",
  "title": "AffineQuant: Affine Transformation Quantization for Large Language Models",
  "modified_abstract": "Informed by seminal efforts to optimize large-scale language models (LLMs) through approaches like Data-Efficient Fine-tuning and Post-Training Quantization (PTQ), our research presents AffineQuant, a novel PTQ method that uniquely leverages direct optimization using equivalent Affine transformations. This strategy significantly extends the optimization potential over traditional scaling-only approaches in PTQ, thereby markedly minimizing quantization errors in massive LLMs during training instances. To maintain the accuracy of our method, we employ the corresponding inverse matrix, ensuring output equivalence between the pre- and post-quantization states of LLMs and handling even unlabeled data with high precision in various finetuning scenarios. Further, to guarantee the invertibility of such transformations during optimization, we propose a gradual mask optimization technique that prioritizes the simulation of diagonal elements before moving to other elements of the matrix, adhering to the Levy-Desplanques theorem and assuring the invertibility of the transformation. Demonstrated across various benchmarks, AffineQuant showcases considerable performance enhancements, notably in multitask learning environments. For instance, it achieves a perplexity of 15.76 on the LLaMA2-7B model with a W4A4 quantization scheme, outperforming previous state-of-the-art methodologies such as OmniQuant in training effectiveness. In zero-shot tasks, AffineQuant attains an average accuracy of 58.61 using 4/4-bit quantization for the LLaMA-30B model, setting new precedents for PTQ efficiency in LLMs.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Hamish_Ivison1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12529v1",
  "title": "Contextualized Messages Boost Graph Representations",
  "modified_abstract": "Building upon foundational efforts that dissect the scalability challenges and architectural innovations of Graph Neural Networks (GNNs), notably those addressing depth and scope decoupling to refine the expressivity and computational efficiency, our research introduces a pioneering approach towards enhancing GNN representational capabilities across all levels\u2014node, neighborhood, and graph. Graph neural networks (GNNs) have gained substantial interest for their adeptness at managing data structured as graphs, utilizing a message-passing scheme for feature representation updates and employing graph readout functions for holistic graph representations while addressing learning challenges such as oversmoothing. While previous research has largely gravitated towards heuristic-inspired modifications within the message-passing framework or theoretical considerations grounded in the graph isomorphism problem, the exploration of GNNs with uncountable node feature representations and their implications on oversmoothing remains nascent. Achieving a breakthrough, our contribution through the novel soft-isomorphic relational graph convolution network (SIR-GCN) brings to the fore non-linear and contextual transformations of neighborhood feature representations, leveraging bounded-size receptive fields and multiple approximation layers to mitigate oversmoothing. This innovation sets a new benchmark in the theoretical and practical comprehension of GNNs, including subgraph identification. This paper delineates the mathematical underpinnings that link SIR-GCN with prevalent GNN models and validates its superiority in simple node and graph property prediction tasks against comparable models using synthetic datasets.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ajitesh_Srivastava1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12511v1",
  "title": "Forward Gradient-Based Frank-Wolfe Optimization for Memory Efficient Deep Neural Network Training",
  "modified_abstract": "Inspired by the advancements in optimization techniques and their significance in enhancing the efficiency and accuracy of deep neural networks, our work presents an in-depth analysis of the Frank-Wolfe algorithm, tailored for memory-efficient training of deep neural networks through the novel use of forward mode automatic differentiation. This investigation stems from the insights driven by previous work in the optimization and Bayesian learning fields, particularly focusing on addressing the challenges and limitations associated with traditional gradient computation methods such as backpropagation. By incorporating sampling techniques and optimizing hyperparameters, and leveraging the concept of the Projected Forward Gradient obtained through forward mode differentiation, which facilitates a form of reparametrization, we demonstrate that our adapted Frank-Wolfe algorithm not only achieves sub-linear convergence to the optimal solution but does so with reduced memory requirements, overcoming the hurdles encountered by the standard Frank-Wolfe approach under similar conditions. Furthermore, the integration of variational inference techniques in our approach aims to mitigate potential issues of over-fitting, thereby enhancing the generalization capabilities of the trained models. Our findings are supported by numerical examples, showcasing the potential of forward gradient-based methods in optimizing networks for tasks such as classification, while ensuring computational efficiency and resourcefulness.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Marcin_B_Tomczak1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13031v1",
  "title": "RigorLLM: Resilient Guardrails for Large Language Models against Undesired Content",
  "modified_abstract": "In the context of the rapid advancements and the expanded use of Large Language Models (LLMs) for diverse applications, inspired by the initiative of open-ended research in language technologies as demonstrated in works such as OLMo, our study RigorLLM addresses a significant challenge in the field: the emergence of biases and the potential for generating harmful content, especially under adversarial testing. Addressing the limitations of current mitigation strategies, which falter under adversarial attacks, we introduce a methodologically novel framework, Resilient Guardrails for Large Language Models (RigorLLM). Our approach emphasizes a composite strategy integrating energy-based training data augmentation, optimization of a safeguard suffix for inputs, and a fusion model that marries the strengths of robust KNN algorithms with LLMs, within diverse user interfaces. This framework stands out for its ability to efficiently counteract harmful input and output generation, proffering a holistic solution to content moderation challenges in LLMs and fostering a secure modeling environment. Experimental evaluations underline RigorLLM's superiority over prevalent baselines such as OpenAI API and Perspective API, highlighting its unmatched resilience against jailbreaking attacks and marking a pivotal advancement in secure and reliable LLM development and application. RigorLLM's release contributes significantly to the community, ensuring safer LLM interactions.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Nishant_Subramani1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12510v1",
  "title": "Generalized Consistency Trajectory Models for Image Manipulation",
  "modified_abstract": "Inspired by foundational models in machine learning that bridge diverse computational strategies\u2014such as the combination of variational autoencoders (VAEs) and energy-based models (EBMs) for efficient and high-quality image generation\u2014our study presents an innovative take on diffusion-based generative models. These models have marked a significant advancement in unconditional generation and applied tasks such as image editing and restoration, thanks to their iterative denoising process. This process, while potent, is computationally intensive, posing a challenge for real-time application. Addressing this, our work introduces Generalized Consistency Trajectory Models (GCTMs), an extension of consistency trajectory models (CTMs), designed to translate between arbitrary distributions through ordinary differential equations (ODEs) with enhanced efficiency. Our approach, incorporating autoencoders and sophisticated sampling techniques, exploits the manifold intrinsic to generative models, enabling a more seamless transition and interpolation between various states of image manipulation. Through detailed design space exploration and practical evaluations on image-to-image translation, restoration, and editing tasks, we demonstrate GCTMs' capability to significantly reduce computational demands while maintaining, or even improving, the versatility and quality of image manipulation. Emphasizing detection of subtle changes and errors during the image translation process, our findings not only advance the state-of-the-art in efficient image manipulation techniques but also set a new benchmark for exploring the intersection of generative models, computational efficiency, and the underlying manifolds. Code is accessible at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zhisheng_Xiao1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12503v1",
  "title": "Securing Large Language Models: Threats, Vulnerabilities and Responsible Practices",
  "modified_abstract": "As Large Language Models (LLMs) solidify their place as a cornerstone in the field of Natural Language Processing (NLP), revolutionizing diverse tasks with unprecedented capabilities, it becomes imperative to address the emerging security and privacy concerns they introduce. Drawing insight from extensive research into language processing challenges, exemplified by the creation and analysis of multilingual and multi-topic datasets like MMT, and cross-cultural subsets of tweets, this study embarks on a comprehensive examination of the security landscape surrounding LLMs. We dissect security and privacy concerns, vulnerabilities to adversarial attacks, the potential harms instigated by misuse and misidentification of LLMs, and broach mitigation strategies, while pinpointing the limitations inherent in current safeguarding techniques. Our modeling of adversarial threats and the analysis of cross-cultural implications of tweets provide a base for our conclusions in terms of identification and recognition challenges. Concluding with forward-looking recommendations, our work paves the way for future endeavors aimed at bolstering the security and responsible management of LLMs.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Vivek_Srivastava1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12493v1",
  "title": "A Trainable Feature Extractor Module for Deep Neural Networks and Scanpath Classification",
  "modified_abstract": "Our work introduces a novel trainable feature extraction module designed for enhancing scanpath classification, an area experiencing growing interest for its potential applications ranging from medicine to training systems. Inspired by the recent advances in deep neural network interpretability, as exemplified by the development of Convolutional Dynamic Alignment Networks (CoDA-Nets), which leverage dynamic alignment for model interpretability and robust classification, we adapt this concept to the domain of eye tracking. Leveraging interpretability as a guide, our module, targeting the transformation of scanpaths into directly usable feature vectors for deep neural networks, advances this goal through adaptivity to backpropagated error, thus allowing for joint training with the network and improving the classifiers' effectiveness. Dynamically evolving the process of traditional histogram-based methods for computing distributions over scanpaths through our module's integration into deep learning models, our approach not only presents a methodological advancement but also demonstrates superior classification performance across three public datasets in comparison with state-of-the-art techniques. The addition of this module enhances overall network interpretability, providing an attribution of decisions back to specific characteristics in the scanpath data, demonstrating how the systematic attribution of feature relevance can aid in the classification process.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Moritz_B\u00f6hle1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12486v1",
  "title": "NTK-Guided Few-Shot Class Incremental Learning",
  "modified_abstract": "Drawing inspiration from the evolving understanding of network dynamics and representation learning, notably the exploration of usable information and the evolution of optimal representations during training, our research introduces a novel approach to Few-Shot Class Incremental Learning (FSCIL) by emphasizing the Neural Tangent Kernel (NTK). We prioritize achieving optimal NTK convergence and minimizing NTK-related generalization error, laying a solid theoretical foundation for enhanced model generalization. This is achieved through a tailored meta-learning mechanism for global NTK convergence and dual NTK regularization for convolutional and linear layers to lessen generalization loss, beginning with self-supervised pre-training to optimally set initial weights, followed by strategic curricular alignment and early-stage adjustments in the training process. The intricacies of these methods collectively fortify our model\u2019s generalization capabilities, demonstrated by our NTK-FSCIL model outperforming existing state-of-the-art on benchmark datasets across various tasks, including those requiring image understanding, with end-session learning-rate adjustments and information-centered evaluation leading to accuracy improvements ranging from 2.9% to 8.7%. The rationale behind the approach leverages perceptual learning principles while incorporating algorithms for perturbing data as part of its robustness strategy, subtly nodding to concepts found in neuroscience. This ensures our method not only focuses on usable information and optimal representations but also aligns with the natural learning processes.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Daksh_Idnani1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12481v1",
  "title": "TT-BLIP: Enhancing Fake News Detection Using BLIP and Tri-Transformer",
  "modified_abstract": "In an era where misinformation poses significant challenges, our work is motivated by the rapid advancements in multimodal machine learning, as exemplified by efforts to create comprehensive benchmarks for evaluating multilingual multimodal models across various tasks and languages. Such endeavors underscore the necessity for sophisticated frameworks in processing and interpreting multimodal information seamlessly. Acknowledging these progressions, our paper introduces TT-BLIP, an innovative end-to-end model that harnesses the capability of bootstrapping language-image pretraining (BLIP) for achieving unified vision-and-language understanding and generation. Specifically, TT-BLIP leverages BERT and BLIP_Text for textual information, ResNet and BLIP_Img for image data, and employs bidirectional BLIP encoders for analyzing multimodal content. This integrated approach, powered by a Multimodal Tri-Transformer architecture, utilizes tri-modal features through multi-head attention mechanisms in learning enhanced representation and analysis of multimodal data. Moreover, by adopting few-shot learning strategies and leveraging transfer learning, TT-BLIP becomes crucial for entailment recognition in the evaluation of fake news, allowing it to adeptly handle questions of legitimacy and veracity. Tested on the Weibo and Gossipcop fake news datasets, TT-BLIP demonstrates superior performance over existing state-of-the-art models in fake news detection, evidencing its efficacy in multimodal evaluation tasks. Notably, such benchmarks validate the potential of TT-BLIP in future multimodal learning applications.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Desmond_Elliott1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12474v1",
  "title": "FairSIN: Achieving Fairness in Graph Neural Networks through Sensitive Information Neutralization",
  "modified_abstract": "In the context of graph neural networks (GNNs), our work is inspired by the lineage of research addressing the bias in machine learning predictions, notable among them being efforts to enhance model robustness and generalizability through informed and causality-inspired representations. Like traditional models, GNNs are susceptible to biases based on sensitive attributes such as race and gender, an issue that adversarial learning tasks frequently encounter. Prior interventions like edge dropping or feature masking, geared towards filtering out sensitive information, inadvertently risk eliminating valuable non-sensitive data, thereby impairing the balance between fairness and performance. Addressing this, we introduce FairSIN, a method pivoting on a novel neutralization-based strategy that incorporates Fairness-facilitating Features (F3) into node features to statistically neutralize sensitive bias while enriching the model with additional non-sensitive information, aiming to learn adversarial patterns to further mitigate biases. Our theoretical underpinnings leverage causal learning to suggest that taking advantage of the diversity within a node's neighborhood, through a causality-inspired approach, can effectively diffuse sensitive biases, a principle we operationalize through F3 by focusing on heterogeneous neighbor features for the specific task at hand. With FairSIN and its variants, spanning both data-centric and model-centric frameworks, we demonstrate across five benchmark datasets and multiple GNN architectures that it is feasible to significantly bolster fairness metrics without compromising on predictive accuracy, moving a step closer to the ideal behavior of recommender systems in fairness-sensitive applications.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Mengyue_Yang1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12469v1",
  "title": "When Do \"More Contexts\" Help with Sarcasm Recognition?",
  "modified_abstract": "Inspired by advancements in understanding machine learning algorithms through new theoretical frameworks, such as the exploration of leave-one-out conditional mutual information for evaluating algorithm stability and generalization, this work embarks on investigating the nuanced field of sarcasm recognition\u2014a task uniquely complicated by the necessity to discern true intentions veiled under literal linguistic expressions. Recognizing the gap in current understanding about the collective effectiveness of various context-enhancing methods (e.g., sentiment or cultural nuances) in sarcasm detection, we rigorously examine how integrating more contexts into models influences their performance, employing techniques including cross-validation and optimization strategies for robust analysis. By systematically varying the training tasks and incorporating enhanced cross-validation procedures, we not only substantiate the theoretical underpinnings of our sarcasm recognition framework but also fine-tune performance metrics for heightened generalization across diverse sets. Through the development of a comprehensive framework that facilitates the integration and evaluation of multiple contextual cues, and by testing with four approaches across three sarcasm recognition benchmarks, we not only match existing state-of-the-art performances but also unveil the incremental benefits of enriching models with additional contexts and optimizing their generalization capabilities. Our analysis further illuminates the double-edged nature of this integration, where an influx of contexts, while beneficial for accuracy, might concomitantly necessitate grappling with embedded societal biases, marking a critical consideration for future advancements in the field.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Aditya_Golatkar1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12459v1",
  "title": "Non-negative Contrastive Learning",
  "modified_abstract": "Drawing inspiration from a broad spectrum of previous studies, including techniques that bridge the gap between deep learning and Bayesian inference for nuanced task handling, our research introduces Non-negative Contrastive Learning (NCL), a novel approach that revitalizes Non-negative Matrix Factorization (NMF) to generate interpretable deep representations. By enforcing non-negativity constraints, akin to NMF's ability to distill interpretable features closely aligned with sample clusters, NCL not only mirrors the mathematical foundations of NMF but also advances its interpretability, providing sparser and more disentangled representations than conventional contrastive learning approaches. Incorporating a prior distribution into our model, we theoretically establish the identifiability and enhance the downstream generalization ability of NCL in handling various tasks, demonstrating empirically its superiority in feature disentanglement, selection, and downstream classification tasks performance. Further, we elucidate NCL's versatility in augmenting supervised learning scenarios with networks, underscoring its potential to enrich machine learning models with interpretability and transparency in image specification and handling various tasks. Code is available at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Noah_Hollmann1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12448v1",
  "title": "Do Generated Data Always Help Contrastive Learning?",
  "modified_abstract": "In the realm of unsupervised visual representation learning, Contrastive Learning (CL) has showcased tremendous success. This success is premised on extensive manual data augmentations, a process made significantly more efficient with the advent of generative models, such as diffusion models, capable of producing high-quality images closely mirroring the real data distribution. However, our investigation unveils that generated data, even from sophisticated models like DDPM, might not always bolster contrastive learning, occasionally proving detrimental. Our exploration into the underlying reasons for this phenomenon, considering both data inflation and augmentation perspectives, introduces a novel insight. For the first time, we identify the complementary relationship where stronger data inflation should be balanced with weaker data augmentations, and vice versa. This understanding is reinforced by rigorous empirical analyses, which have allowed us to derive generalization bounds under data inflation scenarios and establish a novel benchmark. Leveraging these insights, we propose Adaptive Inflation (AdaInf), a highly data-centric strategy that enhances various contrastive learning methodologies without introducing additional computational costs. Our experiments on benchmark datasets demonstrate AdaInf's ability to significantly improve performance, with highlights including achieving a 94.70% linear accuracy on CIFAR-10 using SimCLR, a new benchmark surpassing existing methods, and is achieved without resorting to external data. The cluster of methods benchmarking against one another reveals the importance of empirical testing in establishing efficacy. The extensive usage of annotations in our experimental setup, drawing inspiration from dataset heterogeneity explorations like MetaShift, which systematically annotated natural images to highlight contextual distribution shifts, further enrichs the discourse on the implications of data variability in enhancing machine learning models.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Weixin_Liang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12429v1",
  "title": "TransformMix: Learning Transformation and Mixing Strategies from Data",
  "modified_abstract": "Grounded in the realization that data augmentation is pivotal for enhancing the generalization capability of deep learning models, our study introduces TransformMix, a novel approach that automatizes the learning of transformation and mixing augmentation strategies directly from the data. This innovation is inspired by the advances in machine learning, particularly the quest to democratize pretrained models' usage across diverse datasets as seen in efforts to scale transformers for predicting parameters of diverse ImageNet models and by the necessity of employing large-resources for pretraining. Traditional sample-mixing methods like Mixup and Cutmix provide a heuristic for blending inputs, yet they fall short by not adapting to the nuances of different datasets, occasionally leading to the generation of misleading mixed samples. TransformMix distinguishes itself by employing learned transformations and mixing masks, which ensures the generation of mixed images that retain accurate and task-relevant information. Our extensive evaluations across various deep learning tasks, including transfer learning, classification, object detection, and knowledge distillation, reveal that TransformMix not only elevates model performance but also does so more efficiently compared to established sample-mixing benchmarks. The democratizing nature of our approach further contributes to broadening access to state-of-the-art performances for researchers and practitioners lacking significant network pretraining resources.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Boris_Knyazev1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12428v1",
  "title": "Transfer in Sequential Multi-armed Bandits via Reward Samples",
  "modified_abstract": "Building on the foundational work in the domain of multi-armed bandits, especially in addressing challenges posed by bandits with delayed feedback and adversarial environments, our study introduces a novel approach to the sequential stochastic multi-armed bandit problem. In this problem, an agent interacts with the bandit over multiple episodes, with the reward distribution of the arms remaining constant within an episode but varying across different episodes. By incorporating a mechanism to transfer reward samples from prior episodes and fine-tuning this process, we conceptualize an algorithm based on the Upper Confidence Bound (UCB) to enhance cumulative regret performance across episodes, even in the presence of delays. Our theoretical regret analysis, considering delays as d_{max}k^{1/3}\\log aspects, provides a guarantee of reduced suboptimality, demonstrating a substantial improvement over the conventional UCB algorithm, which does not incorporate reward sample transfer. This progression not only highlights the significance of learning from past interactions but also establishes a new upper bound in optimizing the performance of sequential decision-making models, moving towards near-optimal solutions even in scenarios with delayed feedback.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Julian_Zimmert1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12422v1",
  "title": "Jetfire: Efficient and Accurate Transformer Pretraining with INT8 Data Flow and Per-Block Quantization",
  "modified_abstract": "Acknowledging the challenges posed by the need for immense amounts of computational power in deep learning, as detailed in seminal works like Fathom where the focus is on the analysis and dissecting the performance characteristics of diverse deep learning workloads, our research introduces 'Jetfire'. This solution, engineered by advanced system architects, is aimed at enhancing transformer pretraining efficiency through an innovative Fully Quantized Training (FQT) method. Unlike most FQT approaches that adhere to the conventional quantize-compute-dequantize process and suffer from significant speedup and accuracy drawbacks, Jetfire employs an INT8 data flow to optimize memory access and integrates a per-block quantization strategy to preserve the accuracy of pretrained transformers. Our extensive experiments reveal that this specialized INT8 FQT method not only achieves accuracy on par with the FP16 training benchmarks but also outstrips the performance of existing quantization algorithms for transformers, making substantial advances over traditional methodologies. Specifically, Jetfire delivers a 1.42x increase in end-to-end training speed and a 1.49x reduction in memory consumption compared to the traditional FP16 baseline, thereby offering a significant leap forward in the pursuit of more time- and resource-efficient deep learning training methodologies. Enhanced by parallelism in data processing and scaling strategies, the transformer training community will find our work particularly impactful, presenting a viable alternative to enhance training workloads without compromising accuracy.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Robert_Adolf1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12418v1",
  "title": "STG-Mamba: Spatial-Temporal Graph Learning via Selective State Space Model",
  "modified_abstract": "The emergent challenge of harnessing the dynamic, heterogeneous, and non-stationary nature of Spatial-Temporal Graph (STG) data necessitates novel computational methodologies. Drawing inspiration from recent advancements in molecular graph representation learning, which underscore the importance of modeling relationships beyond mere node interactions by incorporating motif-level relationships and heterogeneous structures for nuanced feature representation, our study introduces the Spatial-Temporal Graph Mamba (STG-Mamba). This novel framework leverages the powerful selective state space models (SSSMs) to treat the STG network as an integrative system, allowing for a state-of-the-art approach in addressing learnable characteristics of spatial-temporal data. By employing the Graph Selective State Space Block (GS3B) as a strategic sampler, we aim to precisely characterize the dynamic evolution of STG networks through multi-task learning approaches that are adept at managing complex associations within such data. The STG-Mamba is architected as an Encoder-Decoder model, with GS3B serving as the foundational module, ensuring efficient sequential data modeling. To augment GNN's capacity for STG data under the auspices of SSSMs, we innovatively propose the Kalman Filtering Graph Neural Networks (KFGN) for continuous graph structure optimization, which harmonizes with selective state space evolution while maintaining linear complexity. Conducted extensive experiments on three benchmark STG forecasting datasets affirm the superior performance and computational efficiency of STG-Mamba against prevailing methods, notably improving STG forecasting accuracy while significantly reducing computational costs.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~ZHAONING_YU1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12406v1",
  "title": "Offline Imitation of Badminton Player Behavior via Experiential Contexts and Brownian Motion",
  "modified_abstract": "Building upon the significant strides made in AI research, notably in areas requiring intricate decision-making and multimodal communication as exemplified in works like Iconary\u2014a game-based study on AI's ability to interpret and generate human-like drawings and text\u2014our study focuses on the domain of badminton, a turn-based sport that necessitates high levels of strategic and responsive behaviors from its players. The challenge of imitating badminton player behavior from offline match data poses unique difficulties due to the sport's intrinsic need for alter-dependent decision-making and the complex interplay between players during rallies, reflecting a high visual/symbolic task load. In response, we introduce RallyNet, a novel hierarchical offline imitation learning model designed to effectively capture and reproduce badminton player behaviors by: (i) modeling decision-making processes within a contextual Markov decision process framework, (ii) generating experiential context to signify the agent's intent during rallies, and (iii) employing Geometric Brownian Motion (GBM) to realistically model player interactions, providing valuable insights for the evaluation of strategies in sports analytics. RallyNet's efficacy is extensively validated on the largest real-world badminton dataset to date, showcasing its superiority over conventional offline imitation learning and state-of-the-art turn-based approaches by a significant margin through rigorous tests. Through this work, we not only advance the domain of AI in sports analytics but also underscore the potential of sophisticated AI models to facilitate strategic learning and performance improvement in high-speed, tactic-dependent sports.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jordi_Salvador3",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12404v1",
  "title": "Understanding Training-free Diffusion Guidance: Mechanisms and Limitations",
  "modified_abstract": "The exploration into enhancing pretrained diffusion models with additional control mechanisms has garnered significant attention due to its potential impacts across fields such as computer vision and reinforcement learning. Our study is particularly inspired by a range of previous works that have pushed the boundaries in these domains, including efforts to achieve self-supervised monocular depth and motion learning in dynamic scenes using semantic priors. These pioneering studies, which incorporate techniques like ego-motion detection and photometric regularization under a self-supervisory paradigm, have opened new avenues for applying AI more effectively and have underscored the versatility of pretrained networks in novel applications. Seeking to extend this innovative trajectory, our paper focuses on the concept of training-free diffusion guidance, a method that leverages off-the-shelf networks pretrained on clean images for zero-shot conditional generation within a comprehensive framework. We conduct a detailed investigation into the operational mechanisms and fundamental limitations of this approach, offering a theoretical analysis to differentiate training-free guidance from classifier-based alternatives within AI pipelines. Our analysis reveals that, although promising, training-free methods are prone to adversarial gradients and suffer from slower convergence rates in comparison to classifier-based guidance. We propose a suite of techniques aimed at mitigating these limitations, including improved models for estimation of optimal parameters and enhanced detection of adversarial motions, substantiated by theoretical insights and empirical validations through established frameworks. Our experiments in image and motion generation demonstrate the potential of these techniques to enhance training-free diffusion guidance, contributing to the evolving landscape of AI and machine learning research.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sunghoon_Im1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12403v1",
  "title": "Towards Interpretable Hate Speech Detection using Large Language Model-extracted Rationales",
  "modified_abstract": "In the quest for making algorithms more transparent and accountable, this work draws inspiration from previous efforts that have sought to unveil the mechanics underlying complex models, such as the exploration into surrogate gradients for latent structure learning in natural language processing (NLP). Within this context, our study focuses on the burgeoning issue of hate speech on social media platforms, where anonymity can embolden users to disseminate offensive content. Addressing the challenge of automatically identifying such content, we propose an innovative approach that leverages the capabilities of Large Language Models (LLMs) to extract interpretative features\u2014or rationales\u2014from the text. These rationales serve to train a base hate speech classifier, introducing a layer of faithful interpretability previously absent in typical black-box detection methods. By integrating the comprehensive textual understanding capabilities of LLMs with the powerful discriminative abilities of advanced hate speech classifiers, our framework not only ensures transparency but also maintains performance efficiency. Our extensive evaluation across diverse social media hate speech datasets validates the efficacy of LLM-extracted rationales and demonstrates that the interpretability of detectors does not compromise their performance.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Tsvetomila_Mihaylova1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12400v1",
  "title": "Finding the Missing Data: A BERT-inspired Approach Against Package Loss in Wireless Sensing",
  "modified_abstract": "The relentless quest for advancements in wireless sensing and an ever-increasing demand for reliable data transmission culminate in this study, where we draw inspiration from recent strides made in deep learning, particularly those elaborated in the domain of neural network sparsity and efficiency, highlighted by Sensitivity based Regularization of Neurons. Building upon the solid groundwork laid by these prior works, we venture into addressing the prevalent issue of package loss in Wi-Fi sensing, an impediment severely hampering the estimation of Channel State Information (CSI), a cornerstone for achieving high-performance learning model outcomes. To counter such setbacks, we introduce a novel deep learning model, CSI-BERT, inspired by the Bidirectional Encoder Representations from Transformers (BERT) architecture and employing innovative compression strategies alongside regularization techniques to adapt the BERT framework for Wi-Fi sensing. CSI-BERT distinguishes itself by its ability to be trained in a self-supervised, learnable manner directly on target datasets, eliminating the necessity for external data. Our model thrives not only through advanced regularization but also by capturing the innate sequential dependencies across subcarriers with its unique utilization of spatio-temporal topologies, enhancing the model\u2019s ability to reconstruct CSI even under high-loss scenarios. Our experiments underscore the superiority of CSI-BERT over traditional interpolation techniques, demonstrating noticeably lower error rates, enhanced signal processing efficiency via advanced compression methods, and a significant boost, approximately 15%, in the accuracy of subsequent Wi-Fi sensing tasks including activity recognition, when employing deep learning models such as the Residual Network and Recurrent Neural Network architectures. The dataset WiGesture and the implementation code for CSI-BERT are made publicly accessible at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Enzo_Tartaglione1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12399v1",
  "title": "Electioneering the Network: Dynamic Multi-Step Adversarial Attacks for Community Canvassing",
  "modified_abstract": "In an era where the manipulation of online social networks for community canvassing poses a significant threat, our work is inspired by foundational studies on voter models, opinion dynamics, and polarization on networks, specifically focusing on the complex interplay of strategies within networked environments as highlighted by research on strategic machine learning in network games. We extend these concepts by modeling community canvassing as a dynamic gradient-based attack process on Graph Neural Networks (GNNs) to influence voter preferences through learning similarities in user behaviors and network patterns. Unlike existing methods that primarily focus on single-step attacks without considering the dynamic nature of information diffusion, our approach emphasizes realistic adversarial tactics where uncertain voters are targeted for manipulation through a sophisticated learning process, akin to planning in uncertain environments. We introduce the concept of $\\textit{minimum budget attacks for community canvassing}$ (MBACC), demonstrating its computational complexity and proposing the Dynamic Multi-Step Adversarial Community Canvassing (MAC) methodology. MAC, acting as a strategic planner, identifies low-budget, high-influence targets to enable efficient attack cascades, ultimately showcasing its superiority in identifying multi-hop strategies for adversarial community canvassing across various networks and GNN models through the strategic learning of network dynamics. Our code and data have been made available at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Tongxin_Yin1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12391v1",
  "title": "FairSTG: Countering performance heterogeneity via collaborative sample-level optimization",
  "modified_abstract": "Spatiotemporal learning, critical for enhancing mobile computing techniques in smart cities, often faces the challenge of performance heterogeneity across samples\u2014a concern our work addresses directly. Drawing inspiration from a variety of domains, including advancements in self-supervised learning for 3D point cloud analysis, we introduce FairSTG, a model-independent, fairness-aware framework for SpatioTemporal Graph learning. This novel framework aims to mitigate unfair learning outcomes by leveraging the strengths of well-learned samples to enhance those that present more challenges through a collaborative mix-up strategy. FairSTG innovatively combines a spatiotemporal feature extractor for initializing models, a collaborative representation enhancement module facilitating knowledge transfer between samples, and tailored fairness objectives aimed at reducing performance disparities at the sample level. Our extensive experiments across four distinct spatiotemporal datasets not only attest to FairSTG's ability to significantly elevate fairness in model outcomes but also demonstrate its capacity to maintain competitive forecasting accuracy. Specifically, case studies underline FairSTG's effectiveness in diminishing both spatial and temporal performance heterogeneity through targeted sample-level interventions, thus offering a substantial contribution towards reducing the potential risks associated with spatiotemporal resource allocation in underrepresented urban areas.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sheng_Yu_Huang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12384v2",
  "title": "An Aligning and Training Framework for Multimodal Recommendations",
  "modified_abstract": "Reflecting upon advancements in multimedia applications, we extend the burgeoning field of multimodal recommendations, a domain where rich contextual understanding beyond mere user interactions becomes pivotal. This work is propelled by insights gained from recent methodological strides in domain adaptation, particularly the Auxiliary Target Domain-Oriented Classifier (ATDOC) framework, which innovatively addresses classifier bias by incorporating an auxiliary classifier specifically for target data, thus enhancing the quality of pseudo labels through a semi-supervised learning approach that leverages both labeled and unlabeled data. Similarly, our research identifies and tackles the semantic misalignments inherent in integrating multimodal information with ID features in recommendation systems. We introduce AlignRec, a novel solution that meticulously addresses the tripartite misalignment: within content features, between content and ID features, and between the representations of users and items in a unified network. By decomposing the recommendation objective into these specific alignments, each characterized by a distinct objective function, AlignRec robustly unifies multimodal features into our training framework with a large emphasis on the adaptation process. Through a progressive training strategy beginning with pre-training for the first alignment, we ensure the effective incorporation of pseudo-labeling techniques and learning adaptation strategies for these unified features in subsequent steps. Moreover, understanding the significance of evaluating the contributory value of each multimodal feature, we develop and implement novel metrics for assessing intermediate performance outcomes. Our exhaustive evaluations across diverse real-world datasets underscore AlignRec's remarkable effectiveness over an array of baselines, heralding not only a significant leap in the quality of multimodal features utilized in recommendations but also promising the open-sourcing of these superior features.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Dapeng_Hu2",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12382v1",
  "title": "Low-Trace Adaptation of Zero-shot Self-supervised Blind Image Denoising",
  "modified_abstract": "In the evolving landscape of image denoising, deep learning-based denoisers have recently taken center stage. This paper draws inspiration from the dynamic interplay of adversarial attacks and visual prompting, as illuminated by related work in the enhancement of visual prompts at the pixel level. Encouraged by these advancements and the untapped potential discovered in the properties of the Frobenius norm expansion, we introduce an innovative angle to enhancing self-supervised learning in image denoising by incorporating visual-only cues, specifically through the method of visual-only prompting. Specifically, we propose the integration of a trace term within the loss function, effectively reducing the disparity in optimization goals between self-supervised and supervised denoising methods. This insight leads to the development of our Low-trace adaptation Noise2Noise (LoTA-N2N) model, which significantly narrows the performance gap between the two approaches through targeted training leveraging visual-only information. Furthermore, our approach's contribution is enhanced by the application of learning techniques tailored to process visual-only information and the flexible adaptation of the proposed method across various image formats, setting new performance benchmarks for zero-shot self-supervised image denoising with minimal prompting. Comprehensive experimental validation on natural and confocal image datasets illustrates that our approach achieves state-of-the-art results without leaning on presumed noise characteristics.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Xianhang_Li1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12372v1",
  "title": "Learning Transferable Time Series Classifier with Cross-Domain Pre-training from Language Model",
  "modified_abstract": "Drawing inspiration from an array of recent works, including innovative approaches towards enhancing symbolic representations through self-supervised learning and generative-discriminative models, our investigation centers around advancing the field of learning transferable time series representations. Current state-of-the-art methodologies have underscored the potential of self-supervised pre-training (SSL) to transform the capability of models for downstream tasks; however, these successes often falter in cross-domain adaptability due to the distinct characteristics inherent to time-series data across various fields. To surmount these challenges, we introduce CrossTimeNet, a groundbreaking cross-domain SSL framework designed to distill transferable knowledge from diverse domains, thereby significantly enriching the target task. CrossTimeNet distinguishes itself with a pioneering time series tokenization module, optimized for transforming raw time series into sequences of discrete tokens, and endeavors to extract highly informative patterns by predicting a substantial proportion of corrupted tokens during pre-training\u2014a strategy emblematic of neuro-symbolic integration, yet largely underexplored in generative models for learning. Furthermore, our approach innovates by utilizing a pre-training language model (PLM) as the basis for our encoder, exploring the untapped potential of transferring knowledge across disciplines. Our comprehensive experiments across multiple real-world time series classification problems validate CrossTimeNet's exceptional capability, setting a new benchmark in the domain. The clustering of tasks in these experiments further illustrates the model\u2019s adaptability and its role in symbolic learning, confirming our theoretical propositions.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Emanuele_Sansone1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12371v1",
  "title": "Advancing Time Series Classification with Multimodal Language Modeling",
  "modified_abstract": "Motivated by a comprehensive review of current methodologies in time series classification and the observed limitations in existing models\u2014specifically, their failure to account for label comparability and difficulty in cross-domain transferability\u2014our work is inspired by advancements in unsupervised transfer learning for spatiotemporal predictive networks. These preceding studies have laid down the fundamental groundwork by showcasing the potential in leveraging transferred knowledge from various domains to enhance predictive accuracy in new, unexplored areas. Building on this momentum, we introduce InstructTime, a pioneering framework that reimagines time series classification through a learn-to-generate paradigm. By harnessing the generative capabilities of pre-trained language models, InstructTime treats time series classification as a multimodal understanding task, wherein both task-specific instructions and raw time series data are conceptualized as multimodal inputs, and label information is rendered in textual form. This innovative approach is further supported by three key design components: a time series discretization module that translates continuous time series into sequences of hard tokens, an alignment projection layer that bridges the modality representation gap between spatiotemporal data and language, and an auto-regressive pre-training strategy that enhances cross-domain transferability through the inclusion of recurrent network mechanisms and benchmark dataset training to remember temporal sequences more effectively. Our extensive experimentation across benchmark tasks reveals InstructTime's exceptional performance and underscores its potential to serve as a universal foundational model for time series classification, demonstrating a significant improvement in memory handling and information retention over traditional and spatiotemporal models.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zhiyu_Yao2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12366v1",
  "title": "U-Net Kalman Filter (UNetKF): An Example of Machine Learning-assisted Ensemble Data Assimilation",
  "modified_abstract": "The burgeoning application of machine learning (ML) techniques in weather and climate sciences, particularly in enhancing the robustness and efficiency of predictive models, provides the foundational impetus for our investigation into the integration of ML with data assimilation (DA). Building upon recent advancements in the field that include the use of randomized smoothing for adversarial robustness and innovative training methodologies for neural network classifiers, our research introduces the U-Net Kalman Filter (UNetKF). This novel approach employs U-Net, a convolutional neural network (CNN) architecture, as a robust classifier for the prediction of localized ensemble covariances within the Ensemble Kalman Filter (EnKF) framework. The provable benefits of integrating U-Nets with the EnKF framework include enhanced prediction accuracy and efficiency, along with provable guarantees of performance improvement. Training is emphasized as a core component of our methodology, which utilizes a 2-layer quasi-geostrophic model, showcasing that U-Nets, trained on EnKF DA experiment data, can adeptly predict flow-dependent localized error covariance matrices. This positions UNetKF favorably against traditional DA methods such as 3DVar, En3DVar, and EnKF with provable effectiveness. Our findings not only underscore the potential of ML-assisted DA in achieving or surpassing the performance of traditional approaches but also highlight the utility of U-Nets in transferring learned insights to higher-resolution models, offering promising implications for small ensemble sizes. With classifier retraining being an ongoing process in our methodology, continuous improvement, bolstered by randomized techniques and provable outcomes, works to ensure sustained prediction precision.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Linbo_Liu1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12362v1",
  "title": "DMAD: Dual Memory Bank for Real-World Anomaly Detection",
  "modified_abstract": "Informed by the foundational works that have conceptualized and broadened our understanding of out-of-distribution detection through methodologies like posterior sampling and efficient use of outlier data for enhancing decision boundaries, our research introduces a novel framework named Dual Memory bank enhanced representation learning for Anomaly Detection (DMAD). This framework leverages a unified model approach, deemed optimal for real-world industrial anomaly detection scenarios due to its generalization ability and storage efficiency, while also addressing the limitations posed by the multi-class setting's reliance on normal data and underutilization of accessible annotated anomalies. By employing a dual memory bank that calculates both feature distance and feature attention between normal and abnormal patterns, DMAD encapsulates comprehensive knowledge regarding normal and abnormal instances, facilitating the construction of an enhanced representation for improved anomaly score learning through innovative learning strategies, including feature mining and machine learning techniques. Evaluation of DMAD on the benchmarks of the MVTec-AD and VisA datasets substantiates its superiority over current state-of-the-art methods, thereby underscoring DMAD's effectiveness in navigating the intricacies of real-world anomaly detection scenarios.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yifei_Ming1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12354v1",
  "title": "Sim2Real in Reconstructive Spectroscopy: Deep Learning with Augmented Device-Informed Data Simulation",
  "modified_abstract": "Informed by the accomplishments in utilizing deep learning for enhancing signal processing capabilities, as highlighted by previous endeavors such as 'HybridDeepRx: Deep Learning Receiver for High-EVM Signals', our work contributes a novel deep learning (DL)-based framework, Sim2Real, tailored for spectral signal reconstruction in reconstructive spectroscopy. This innovation primarily addresses the challenge of reconstructing accurate real-world spectral signals in scenarios limited by the availability of only device-informed simulated training data. Given that these simulated datasets substantially deviate from real-world dynamics yet are more practicable to gather, we introduce a hierarchical data augmentation strategy specifically designed to bridge this domain shift, thereby enabling more effective utilization of simulated data for training purposes. Additionally, we delineate the development of a bespoke neural network architecture optimized for spectral signal reconstruction leveraging our augmented dataset approach. Extensive experiments conducted using actual measurements obtained from our spectrometer device validate the efficacy of Sim2Real, revealing its capacity to significantly expedite inference processes while maintaining performance parity with existing state-of-the-art optimization-driven reconstruction methodologies.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Mikko_Honkala1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13027v1",
  "title": "Towards Better Statistical Understanding of Watermarking LLMs",
  "modified_abstract": "Building on the foundation laid by innovative research in adversarial robustness within the realm of content provenance, such as the pioneering work on adversarially robust image attribution, this paper ventures into the underexplored domain of watermarking large language models (LLMs). We rigorously examine the inherent trade-offs between model distortion and detection ability, articulating this challenge as a constrained optimization problem inspired by the green-red algorithm framework. Amidst this technical milieu, our investigation also pivots to training dynamics, uncovering an optimal solution endowed with desirable analytical properties that not only elucidate the theoretical underpinnings but also catalyze the algorithmic development for efficient watermarking of LLMs. By introducing and validating an online dual gradient ascent watermarking algorithm during training, this study achieves a demonstrable asymptotic Pareto optimality, striking a balance between model distortion and detection capability. Furthermore, we initiate a critical dialogue on selecting appropriate model distortion metrics, advocating for the use of KL divergence while underscoring limitations in the current benchmarks of \"distortion-free\" and perplexity criteria. Empirical assessments of our algorithm, juxtaposed with established benchmarks, underscore its effectiveness across comprehensive datasets, thereby advancing the discourse on visual, fingerprinting, and attribution methodologies for watermarking LLMs and setting a new standard in the field.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Maksym_Andriushchenko1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12350v1",
  "title": "Friendly Sharpness-Aware Minimization",
  "modified_abstract": "Building on the foundation of Sharpness-Aware Minimization (SAM), which has revolutionized deep neural network training by concurrently reducing training loss and loss sharpness, our work delves into the less understood mechanisms of SAM's enhancement of model generalization capabilities. The inception of SAM in the optimization landscape and its subsequent success have unveiled new dimensions in deep learning, akin to advancements in gradient-based, multilevel optimization, and data-parallel programs that have addressed complex challenges across a broad spectrum of machine learning applications, including but not limited to hyperparameter optimization and meta-learning. This progress is closely tied to improvements in memory/compute systems efficiencies, the development of multifaceted benchmarks for evaluating neural network performance, and the efficient management of dataflow in these networks. Motivated by the need to further decode and enhance SAM's generalization prowess, we introduce \"Friendly-SAM\" (F-SAM), an iteration that refines SAM by critically analyzing batch-specific stochastic gradient noise and its adversarial perturbation role in the context of these benchmarks. Our discovery highlights the detrimental effect of the adversarial perturbation's full gradient component on generalization, paving the way for F-SAM's strategy to fortify generalization by excluding this component and leveraging stochastic gradient noise. F-SAM not only theoretically validates the exponentially moving average (EMA) approximation for eliminating the full gradient but also establishes its convergence in non-convex settings, indicative of improvements in systems supporting deep learning. Experimental evidence showcases F-SAM's superiority over the traditional SAM model, underlining its enhanced generalization performance and robustness with respect to these benchmarks. Code is available at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sang_Keun_Choe1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12338v1",
  "title": "Stochastic Halpern iteration in normed spaces and applications to reinforcement learning",
  "modified_abstract": "Drawing inspiration from pioneering efforts to enhance exploration in reinforcement learning, such as maximizing R\u00e9nyi entropy for effective zero-shot meta RL, our work explores the stochastic Halpern iteration with variance reduction in the realm of nonexpansive and contractive operators in finite-dimensional normed spaces. Highlighting the critical importance of nuanced exploration, exploitation strategies, planning, and incorporating thoughtful planning for the effective learning of policies in complex state-action spaces, our research contributes by analyzing the oracle complexity of this iteration, showing an improvement to $\tilde{O}(\\varepsilon^{-5})$, against the backdrop of recent advancements like the stochastic Krasnoselskii-Mann iteration. Additionally, we delve into how reinforcement learns and adapts over time with a specific focus on batch processing strategies. Complementary, we establish a lower complexity bound of $\\\text{\\text{O}}mega(\text{\\varepsilon}^{-3})$, applicable across a broad spectrum of algorithms, inclusive of all averaged iterations, even with minibatching. Further adapting our methodology to include meta considerations in spatial contexts, we achieve a complexity bound of $\text{\\text{O}}(\text{\\varepsilon}^{-2}(1-\\gamma)^{-3})$ for $\\gamma$-contraction operators. This foundational analysis directly feeds into the development of novel synchronous algorithms for handling average and discounted reward Markov decision processes, particularly innovating on sample complexity for average reward settings, and marks a significant step forward in the exploration of dataset-efficient strategies in reinforcement learning.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yuanying_Cai1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12335v1",
  "title": "Temporally-Consistent Koopman Autoencoders for Forecasting Dynamical Systems",
  "modified_abstract": "In the quest to improve data-driven modeling of high-dimensional spatio-temporal dynamical systems in the face of inadequate data quality, our work draws inspiration from both traditional and innovatively structured autoencoders, particularly those that leverage deep neural networks (DNNs) for dimensionality reduction and model convergence analysis. Given the inherent challenges posed by limited and noisy datasets in effectively learning complex dynamical systems, we propose the Temporally-Consistent Koopman Autoencoder (tcKAE). This novel approach aims to harness not only the expressivity of DNNs and the dimension reduction capabilities of autoencoders but also introduces a consistency regularization term that ensures temporal coherence in predictions across time steps and utilizes the spectral properties of the Koopman operator for learning reduced-order feature spaces. This coding strategy, alongside convergence analysis informed by Koopman spectral theory and the introduction of clustering mechanisms for improved accuracy in long-term forecasting, enhances the robustness and generalizability of tcKAE, surpassing existing Koopman Autoencoder models. Furthermore, by adopting sparse connections within the network and emphasizing generative capabilities, tcKAE demonstrates superior performance in forecasting through a group-wise descent method in the autoencoder training phase. Through analytical justification and comprehensive empirical testing across various test cases, including pendulum oscillations, kinetic plasmas, fluid flows, and sea surface temperature data, we demonstrate tcKAE's superior performance, offering a promising avenue for forecasting in dynamical systems with constrained and noisy training conditions.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Bahareh_Tolooshams1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12329v1",
  "title": "FedFisher: Leveraging Fisher Information for One-Shot Federated Learning",
  "modified_abstract": "Inspired by the recent advancements and challenges in federated learning (FL) and the importance of addressing privacy, security, and efficiency in model adaptation, our work introduces FedFisher, a novel one-shot FL algorithm that harnesses the power of Fisher information matrices from a Bayesian perspective for FL. Conventional FL algorithms suffer from drawbacks such as the need for constant network connectivity, repeated computational investment, and vulnerability to adversarial privacy attacks. Addressing these issues, FedFisher is carefully crafted to minimize communication rounds between the server and clients to just one, significantly enhancing privacy, security, and computational efficiency through efficient data adaptation strategies. We provide a theoretical analysis of FedFisher for two-layer over-parameterized ReLU neural networks, proving that error diminishes with increasing network width and client-side training, alongside transfer learning techniques for effective data preprocessing and adaptation strategies. Additionally, we propose practical variants using diagonal Fisher and K-FAC approximations for the full Fisher, aimed at boosting communication and computational aspects of FL through better transfer and preprocessing adaptation techniques. Through comprehensive experiments across multiple datasets, FedFisher variants demonstrate superior performance over existing baselines, notably leveraging the technique of distillation to further enhance the adaptability and efficiency of FL, marking a significant step forward in the one-shot FL paradigm.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Lijun_Sheng1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12328v1",
  "title": "Methods for Generating Drift in Text Streams",
  "modified_abstract": "In the evolving landscape of textual data on the Internet, from social media posts to product reviews, the phenomenon of concept drift presents both challenges and opportunities for machine learning systems. Inspired by pioneering works that address the dynamic nature of data through techniques like time-varying importance weight estimators, our paper aims to tackle the underexplored domain of concept drift in text streams. Concept drift, or changes in data distribution over time, is especially pertinent in textual data due to evolving sentiments or shifting word semantics. Despite its prevalence in real-world applications, there is a scarcity of benchmark datasets with labeled drifts to facilitate research in this area. Addressing this gap, we introduce four novel methods for generating textual drift, grounding our approach in practical applications by employing the Yelp and Airbnb datasets. We further assess the resilience of incremental classifiers to these artificially introduced drifts within the stream mining paradigm, analyzing their performance recovery in terms of accuracy and Macro F1-Score after the drifts occur. Our findings not only underline the significant impact of drifts on classifier performance but also highlight the efficiency of incremental support vector machines (SVMs) in adapting to new, arbitrarily introduced data distributions, thereby contributing to the broader discourse on managing concept drift in machine learning. This work is quite foundational in understanding how learning systems can dynamically adjust to evolving conditions.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Pratik_Chaudhari1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12327v1",
  "title": "GT-Rain Single Image Deraining Challenge Report",
  "modified_abstract": "The GT-Rain Single Image Deraining Challenge Report presents a comprehensive review of the competition held during the UG2+ workshop at CVPR 2023, aimed at advancing the field of single image deraining. Drawing inspiration from profound insights gained from recent progresses in self-supervised learning techniques, specifically in methods like contrastive instance discrimination (CID) which have demonstrated significant potential in enhancing model robustness and learning efficiency through intelligent negative sampling and contrastive querying, this challenge was formulated. It seeks to navigate the complex real-world phenomenon of rain and its effect on imagery, by leveraging a meticulously curated real-world rainy dataset for benchmarking state-of-the-art single image deraining methodologies. Through the tasks outlined for the active engagement of 275 participants, exemplifying a diverse range of instances in approaches and vision techniques, with 55 final competitors, the competition facilitated the exploration of innovative deraining solutions trained on the GT-Rain dataset and tested against an extended dataset featuring 15 novel scenes captured during and immediately after rainfall. This comprehensive analysis not only benchmarks current capabilities in vision tasks but also highlights the difficulty in accurately capturing and reproducing minor details in rain-affected imagery, igniting discussion for future advancements in the domain through discrimination of nuanced visual cues.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Tiffany_Cai1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12326v1",
  "title": "Removing Undesirable Concepts in Text-to-Image Generative Models with Learnable Prompts",
  "modified_abstract": "Drawing inspiration from the innovative application of Swin Transformers in enhancing image super-resolution through the N-Gram context, this work investigates a novel approach to mitigate the risk of undesirable concept propagation in text-to-image generative models. Given the challenge of generating content that accurately reflects input text descriptions without adopting copyrighted or unethical material, our method introduces a learnable prompt mechanism within the cross-attention module of such generative models for learning how to isolate these concepts effectively. This mechanism, competitive in nature, serves as a dedicated memory segment, capturing and isolating undesirable concepts to diminish their influence on the model's output while preserving the integrity of unassociated content. The effectiveness of this strategy is validated on the Stable Diffusion model, which capitalizes on advancements in computer vision and super-resolution techniques to demonstrate superior performance in removing undesirable content without detriment to unrelated elements, thus offering a promising direction for the development of safe and ethically sound generative models that align with state-of-the-art expectations. Our results also underscore the potential of learnable prompts as a versatile tool for content moderation in generative applications, setting a new standard for responsible AI development through deep learning paradigms. -Utilizing windows within the model architecture enhances the efficient isolation of concepts.- Code and further resources have been [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Haram_Choi1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12323v1",
  "title": "Enhanced Detection of Transdermal Alcohol Levels Using Hyperdimensional Computing on Embedded Devices",
  "modified_abstract": "Inspired by the advancements in computational models capable of handling intricate tasks under noisy environments, such as the Neural Eigen-SDE algorithm's approach to continuous forecasting in stochastic dynamic systems, our research addresses the challenge of promoting healthier drinking habits through technology. We focus on the detection of transdermal alcohol levels to enable just-in-time interventions aimed at mitigating excessive alcohol consumption. Traditional methods have relied on motion data paired with conventional Machine Learning (ML) algorithms, yielding limited success in terms of accuracy and computational efficiency on mobile devices. In response, we propose the application of Hyperdimensional Computing (HDC) as a lightweight yet robust framework optimized for real-time sensing and processing on embedded systems. HDC's inherent advantages in low latency, reduced power consumption, high parallelism, and effective control over system dynamics are leveraged to develop an innovative detection model suited for smartphones, smart wearables, and IoT environments. Our methodology involves a systematic exploration of HDC encoding strategies and learning models configured in a closed-form solution that significantly enhances detection accuracy by 12% over existing approaches, achieving an impressive 89% accuracy rate in forecasting transdermal alcohol levels continuously. This study not only demonstrates the feasibility of deploying advanced computing frameworks for health-related interventions on everyday devices but also sets a new benchmark in the effectiveness of technologies for forecasting and controlling health outcomes.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Stav_Belogolovsky1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12320v1",
  "title": "Approximated Likelihood Ratio: A Forward-Only and Parallel Framework for Boosting Neural Network Training",
  "modified_abstract": "Exploring efficient and biologically plausible alternatives to backpropagation, this study is inspired by seminal works that push the boundaries of neural network training and architecture exploration, including the development of tools like Neural Tangents for researching infinite-width neural networks. Owing to the high computational complexity and scalability limits of traditional methods, we introduce an approximation technique for the likelihood ratio (LR) method aimed at reducing computational and memory demands in gradient estimation. By leveraging the inherent parallelism during the backward pass via the LR method, our approach optimizes both the forward and backward passes, rendering it more suited for specialized hardware computations and interactive devices. Our extensive experiments validate the effectiveness of this approximation technique in neural network training, highlighting the likelihood ratio method's potential in achieving high-performance neural network training on dynamic architectures and opening avenues for future research. This technique benefits not just interactive devices but also systems specifying finite dynamics, offering new perspectives for initiatives beyond the traditional desktop or notebook computing environments. Highlighting its relation to both neural and infinite-width phenomena, our work underscores the impact of advanced approximation methods in the landscape of neural network methodologies.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Lechao_Xiao2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12313v1",
  "title": "Improving LoRA in Privacy-preserving Federated Learning",
  "modified_abstract": "Drawing inspiration from the challenges and advancements in privacy-preserving federated learning (FL), specifically the need for improved robustness and generalization in models trained across distributed client systems with data heterogeneity, our study enhances Low-rank adaptation (LoRA) within the FL context. LoRA, known for its efficiency and performance in task-specific parameter-efficient fine-tuning (PEFT) of pre-trained language models, exhibits instability when integrated with privacy-preserving FL due to data heterogeneity, amplified noise from differential privacy mechanisms, hyper-parameter sensitivity, and problematic window-based aggregate methods. Addressing the dissonance in jointly optimizing low-rank matrices locally and aggregating them centrally through sampling and aggregation techniques, we introduce Federated Freeze A LoRA (FFA-LoRA), an innovative adaptation aimed at mitigating these issues while simultaneously reducing communication costs through improved aggregates and leveraging window-based techniques for more efficient data handling. By freezing randomly initialized non-zero matrices and exclusively fine-tuning the zero-initialized matrices, FFA-LoRA achieves greater consistency and computational efficiency across various FL tasks, allowing models to better learn and generalize across distinct datasets. Our experimental validation underscores FFA-LoRA's advantages in generalization, robustness, and model performance, marking a significant step forward in the domain of privacy-preserving federated learning.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Debora_Caldarola1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12309v1",
  "title": "Reinforcement Learning from Delayed Observations via World Models",
  "modified_abstract": "Our endeavor extends the scope of Reinforcement Learning (RL) by addressing the practical challenge of delay in feedback which is often overlooked in standard RL settings where immediate feedback is assumed. Drawing inspiration from a foreground of research that includes strategies to optimize RL policy initialization and learning efficiency in complex environments, as seen in approaches like Jump-Start Reinforcement Learning (JSRL), our work focuses on the critical aspect of observation delays in partially observable environments. We propose leveraging world models\u2014a significant advancement that has demonstrated success in integrating past observations with learning dynamics\u2014to handle these delays efficiently. By transforming delayed Partially Observable Markov Decision Processes (POMDPs) into effectively managed delayed Markov Decision Processes (MDPs) through world models, we offer a novel methodology that considerably enhances performance in settings marred by partial observability and necessitates strategic exploration and curriculum-based learning. Our experimental results, including the first evaluation of our methods in visual input-based delayed environments, suggest a remarkable up to 30% improvement over naive model-based approaches. This achievement not only attests to the bound complexity of handling delayed observations but also highlights our approach's sample-efficient training feature. Moreover, the use of meta-strategies integrated into the learning process underscores the potential of our proposed methods in mitigating the challenges posed by delayed observations in RL and their applicability in robotic platforms where such delays are commonplace, particularly in guide-policy deployment and sample-efficient training. Our approach sets a new benchmark for imitation learning strategies in environments characterized by delayed feedback.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ted_Xiao1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12307v1",
  "title": "Molecular Classification Using Hyperdimensional Graph Classification",
  "modified_abstract": "Inspired by pioneering works in graph-based semi-supervised learning, and specifically the innovative use of labels in concert with features for enhanced graph neural network (GNN) performance, our study introduces a groundbreaking approach to graph learning through Hyperdimensional Computing (HDC). This methodology, vital in the domain of chemoinformatics for understanding and identifying complex molecular structures such as cancerous cells, extends the boundaries of current graph learning paradigms by offering a highly efficient, HDC-based model. The proposed model not only achieves competitive Area Under the Curve (AUC) results when benchmarked against advanced models like GNNs or the Weisfieler-Lehman (WL) graph kernel but also significantly surpasses other HDC-related graph learning methodologies. Additionally, our approach heralds a substantial leap in computational efficiency, showcasing a 40-fold acceleration in training and a 15-fold increase in inference speed over traditional GNN and WL models, thus addressing one of the challenges inherent in network-based learning applications. This marks a significant advancement in our ability to leverage graph learning for rapid, effective analysis of molecular structures, and illustrates the vast potential of HDC in graph-based learning applications. The unique identity of molecules can now be classified with unprecedented speed and accuracy, establishing a new frontier in the precision of molecular characterization.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yang_Yongyi1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12285v1",
  "title": "FinLlama: Financial Sentiment Classification for Algorithmic Trading Applications",
  "modified_abstract": "In an era where financial markets are significantly influenced by the vast streams of online financial news, the pursuit of extracting actionable insights through sentiment analysis has become paramount for informed trading decisions. Drawing on the foundational work that explored the dynamics of softmax-cross-entropy losses and their influence on model behavior and generalization performance during training, our research introduces FinLlama, a novel framework that endeavors to harness the generative capacity and nuanced language understanding of Large Language Models (LLMs) - specifically, the Llama 2 7B model - tailored for the finance sector. By fine-tuning the Llama2 7B model with a carefully curated dataset for financial sentiment analysis and incorporating a neural network-based decision mechanism rooted in theory, FinLlama is designed to accurately classify and quantify sentiment from financial news articles. The incorporation of a parameter-efficient fine-tuning technique, LoRA, ensures computational efficiency without compromising on accuracy during training. Our simulation results solidify the potential of FinLlama in enhancing algorithmic trading strategies, contributing to portfolio management decisions that are robust even in the face of market volatility and uncertainties. These outcomes underscore the critical role of generalization training in achieving high performance across various market conditions.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Atish_Agarwala1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12278v1",
  "title": "Stochastic Rounding Implicitly Regularizes Tall-and-Thin Matrices",
  "modified_abstract": "This work is stimulated by the increasing adoption of stochastic rounding techniques in machine learning (ML), particularly in the training of sophisticated deep neural network models, a trend supported by explorations into the beneficial properties of overparameterization in recent ML research. These developments underscore a transformative understanding of long-held ML concepts, particularly in guarding against overfitting during training, drawing upon the empirical success and theoretical investigations of overparameterized models. Exploring the stochastic nearness rounding of real matrices $\\mathbf{A}$ with a significantly higher number of rows than columns, we provide novel theoretical evidence, reinforced by comprehensive experimental evaluation, that with high probability, the smallest singular value of a stochastically rounded matrix is well bounded away from zero. This holds true even for matrices $\\mathbf{A}$ that are near or actually rank-deficient, suggesting that stochastic rounding serves as an implicit regularization mechanism for tall-and-thin matrices $\\mathbf{A}$, ensuring they attain full column rank after rounding. Our proof employs advanced results from random matrix theory and extensive research, along with the insight that the errors introduced by stochastic rounding do not concentrate in low-dimensional column spaces. The emerging trend in ML to utilize such stochastic methods has therefore resulted in significant advancements in understanding the regularization of complex models through implicit means like rounding, rather than the traditional explicit methods such as gradient descent.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yehuda_Dar1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12267v2",
  "title": "Data-Efficient Contrastive Language-Image Pretraining: Prioritizing Data Quality over Quantity",
  "modified_abstract": "Motivated by prior advancements in enhancing the descriptiveness of machine-generated captions for images, our research presents a novel direction in the field of Contrastive Language-Image Pre-training (CLIP) by focusing on the quality rather than the quantity of pre-training data. Despite the renowned capability of CLIP models, when contrastively trained, to achieve unprecedented zero-shot generalization with large-scale image-caption datasets, this requirement for enormous volumes of data presents a significant challenge. Drawing insights from the realms of image captioning, where classifier-free guidance has led to a remarkable balance between reference-free and reference-based captioning metrics, our work constitutes the first theoretically rigorous data selection method for CLIP that leverages the descriptive nature of captions using an innovative decoding strategy. We demonstrate that selecting data subsets that precisely preserve the cross-covariance of embedded images and captions relative to the complete dataset leads to significantly enhanced generalization capabilities. Moreover, this method advocates for a decoding strategy that prioritizes data quality, dictating the captioning process as essential in the selection process. Through comprehensive experiments on the ConceptualCaptions3M and ConceptualCaptions12M datasets, the proposed data subsets not only achieve up to 2.7x the accuracy of existing baselines in ImageNet and its variants but also boast 1.5x the average accuracy across numerous downstream datasets using a data-efficient approach. For extended comprehension and further research, the code for our novel selection method can be found at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Lala_Li1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12254v1",
  "title": "Adaptive LPD Radar Waveform Design with Generative Deep Learning",
  "modified_abstract": "Inspired by recent breakthroughs in applying deep learning to a vast array of signal processing and image reconstruction challenges, such as efficiently learning with small datasets in geometry and physics-based image sequence reconstructions, we develop an innovative learning-based approach for adaptive generation of low probability of detection (LPD) radar waveforms. Our method aims to produce waveforms that seamlessly blend into the ambient radio frequency (RF) environment, thereby ensuring low detectability while maintaining effective ranging and sensing capabilities. Utilizing an unsupervised, adversarial learning framework, our generator network is tasked with crafting waveforms that challenge a discriminative critic network during training, which strives to distinguish generated waveforms from ambient RF noise. To guarantee the operational efficacy of our waveforms for sensing tasks, we integrate and minimize an ambiguity function-based loss metric. This integration includes leveraging graph-based embeddings to capture the nuanced relationships between generated waveforms and their effect on sensing performance, which aids in the generalization of waveform efficacy across different RF environments. We assess our approach by comparing the detectability of our adaptive waveforms against traditional LPD designs using a separate detection-focused neural network. Our findings indicate a significant reduction (up to 90%) in waveform detectability alongside improvements in sensing performance metrics, indicating a promising avenue for trading off between detectability and sensing efficacy in radar waveform design.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Linwei_Wang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12242v1",
  "title": "Reference-based Metrics Disprove Themselves in Question Generation",
  "modified_abstract": "Fueled by the rapid advancements and applications of machine learning models in natural language processing, our research assesses the validity and effectiveness of reference-based metrics, such as BLEU and BERTScore, in evaluating question generation (QG). Particularly, our analysis across benchmarks like SQuAD and HotpotQA, inspired by transformative models like MoEBERT, which significantly enhance the domain through increased model capacity and speed, demonstrates the limitations of employing human-written references for evaluating QG. The study emphasizes the inadequacy observed when these metrics are applied to a new set of human-validated reference questions we collected, which invariably disproved the metrics themselves. We introduce a novel, pre-trained, reference-free evaluation metric encompassing multi-dimensional criteria including naturalness, answerability, and complexity, leveraging large language neural models. This methodology, inspired by techniques such as knowledge distillation and neural network compression, significantly enhances the understanding and answering capabilities of models, surpasses the limitations of relying on syntactic or semantic similarities to a single reference question, and eliminates the need for a diverse set of references. Moreover, compressed models, acting as compact experts in the area, contribute to the scalability and answering efficiency of our proposed metric. Our experimental results highlight the metric's effectiveness in accurately differentiating between high-quality questions and flawed ones, while achieving state-of-the-art alignment with human judgment, even when applied to datasets of small size.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Qingru_Zhang2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12237v1",
  "title": "Efficient Transformer-based Hyper-parameter Optimization for Resource-constrained IoT Environments",
  "modified_abstract": "In the arena of Convolutional Neural Networks (CNNs), the challenge of hyper-parameter optimization (HPO) is crucial for enhancing performance, particularly in the realms of IoT where resources are at a premium. Our work builds upon the foundation laid by pivotal explorations into how neural networks can adapt and evolve over time without succumbing to challenges like catastrophic forgetting, emblematic of the dynamic and resource-constrained environments faced by IoT applications. This adaptation is necessary for the consolidation of learned information and the management of data drift, which are common in such rapidly evolving settings. To address the considerable computational footprint and opacity characterizing the HPO process in these settings, we introduce a novel approach, TRL-HPO, that synergizes transformer architecture with an actor-critic Reinforcement Learning (RL) model. This methodology emphasizes multi-headed attention for parallelization and the progressive generation of layers, paving the way for efficient CNN model tuning through learning mechanisms tailored for data dynamics. Through empirical evaluations using the MNIST dataset and comparisons with leading-edge strategies for constructing CNNs from the ground up, TRL-HPO not only proves to be superior\u2014boosting classification outcomes by 6.8% within the same time frame\u2014but also lays the groundwork for further refining RL-based HPO techniques in resource-limited environments. In the course of our analysis, we pinpoint the accumulation of fully connected layers as a primary factor in performance impedance, thereby opening new pathways for optimizing RL-driven HPO processes.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Thomas_Miconi1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12236v1",
  "title": "Improving Generalization via Meta-Learning on Hard Samples",
  "modified_abstract": "Learned reweighting (LRW) approaches to supervised learning use an optimization criterion to assign weights for training instances, in order to maximize performance on a representative validation dataset. We pose and formalize the problem of optimized selection of the validation set used in LRW training, to improve classifier generalization. Specifically, we design an efficient algorithm for training this meta-optimized model, which focuses on using hard-to-classify instances in the validation set. This strategy has both a theoretical connection to, and strong empirical evidence of improving generalization, robustness, and representations in computer vision tasks. We provide a simple train-twice heuristic for careful comparative study, incorporating a form of regularization to ensure the robustness of our models. Our experiments demonstrate that LRW with easy validation data performs consistently worse than LRW with hard validation data, establishing the validity of our meta-optimization problem. Our proposed algorithm outperforms a wide range of baselines on a variety of datasets and domain shift challenges (Imagenet-1K, CIFAR-100, Clothing-1M, CAMELYON, WILDS, etc.), with ~1% gains using VIT-B on Imagenet. Moreover, using naturally hard examples for validation (Imagenet-R / Imagenet-A) in LRW training for Imagenet improves performance on both clean and naturally hard test instances by 1-2%. Secondary analyses show that using hard validation data in an LRW framework improves margins on test data, hinting at the mechanism underlying our empirical gains. We believe this work opens up new research directions for the meta-optimization of meta-learning in a supervised learning context.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Pavel_Izmailov1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13023v1",
  "title": "Thwarting Cybersecurity Attacks with Explainable Concept Drift",
  "modified_abstract": "In the quest to enhance the security and reliability of cyber-physical systems, particularly the HVAC systems in smart buildings, our research is directly inspired by a spectrum of advancements in the realm of cyber-security, including the notable exploration of Machine Learning-as-a-Service (MLaaS) systems' adversarial robustness and network security in cybersecurity-critical applications. Given the pivotal role of data integrity and accuracy in the operation of these systems, we address the significant challenge posed by cyber-security attacks, including those employing fake data, through the lens of Concept Drift (CD) - the phenomenon where the statistical properties of the target variable, which the model aims to predict, change over time, bringing to light the importance of handling both numerical and categorical data shifts in a domain-agnostic manner. Our approach introduces a Feature Drift Explanations (FDE) module, leveraging an Auto-encoder (AE) to identify and characterize drifting features, thus facilitating targeted mitigation strategies against cyber-attacks that manipulate sensor readings and data distributions, significantly improving robustness in network security. Notably, our FDE module is designed to efficiently process diverse inputs, provably increasing the trust in the system under CD conditions. This methodology is underscored by a comprehensive evaluation demonstrating that FDE can effectively pinpoint 85.77% of drifting features, both categorical and otherwise, reinforcing the model's performance under CD conditions and setting a precedent for future endeavors targeting the nexus of concept drift and cybersecurity.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Hongyan_Bao1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12226v1",
  "title": "Large-scale flood modeling and forecasting with FloodCast",
  "modified_abstract": "Building upon key insights from prior works in the dynamics of machine learning algorithms such as the exploration of Stochastic Gradient Descent for neural network training, which unlock sparse features through the dynamics of loss stabilization and implicit regularization, our work presents FloodCast\u2014a cutting-edge, large-scale hydrodynamic modeling and forecasting framework. FloodCast leverages a novel multi-satellite observation module and a geometry-adaptive, physics-informed neural solver (GeoPINS) to achieve superior performance in flood prediction across vast geographical areas. By integrating unsupervised change detection methods for real-time satellite data analysis and introducing a resolution-invariant architecture with Fourier neural operators in GeoPINS, FloodCast addresses the critical limitations of traditional hydrodynamic models including their high computational cost, fixed spatial resolution dependency, and inadequate capacity for timely flood crest forecasting. Empirically, the technology's robustness is empirically demonstrated through its application in the 2022 Pakistan flood, establishing a new benchmark for flood prediction accuracy in terms of inundation range, depth, and model transferability across spatial and temporal scales. Our comparative evaluation emphatically learns from and highlights that the sequence-to-sequence GeoPINS model not only complements traditional hydrodynamic methods during extreme flood events but also significantly reduces prediction errors through its unique training mechanism and sparse data utilization, thus promising a substantial leap forward in our ability to issue accurate, time-critical flood hazard warnings.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Aditya_Vardhan_Varre1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12213v1",
  "title": "Private graphon estimation via sum-of-squares",
  "modified_abstract": "This study is motivated by prior advancements in machine learning, particularly in the analysis and comparison of structured objects like graphs, which play a crucial role in understanding complex networks and their behaviors. Building on the foundational work that has shown the effectiveness of Gromov-Wasserstein divergence in comparing graphs for a range of learning tasks, including clustering and graph completion, we introduce the first pure node-differentially-private algorithms for learning stochastic block models and graphon estimation with polynomial running time for any constant number of blocks. Our approach achieves statistical utility guarantees comparable to those of the best previous information-theoretic (exponential-time) node-private mechanisms for these problems, through an exponential mechanism guided by a score function defined via a sum-of-squares (SoS) relaxation contingent on the number of blocks. The novelty of our paper lies in three key contributions: a novel characterization of the distance between block graphons via quadratic optimization over doubly stochastic matrices, a general theorem on SoS convergence for polynomial optimization over arbitrary polytopes, and an innovative approach for Lipschitz extension of score functions within the SoS algorithmic paradigm, facilitating divergence metric-based clustering and graph completion. These contributions collectively advance the frontier of privacy-preserving graph analysis and learning.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~C\u00e9dric_Vincent-Cuaz1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12212v1",
  "title": "Evaluating Named Entity Recognition: Comparative Analysis of Mono- and Multilingual Transformer Models on Brazilian Corporate Earnings Call Transcriptions",
  "modified_abstract": "Named Entity Recognition (NER) is a Natural Language Processing technique for extracting information from textual documents. However, much of the existing research on NER has been centered around English-language documents, leaving a gap in the availability of datasets tailored to the financial domain in Portuguese. This study addresses the need for NER within the financial domain, focusing on Portuguese-language texts extracted from earnings call transcriptions of Brazilian banks. By curating a comprehensive dataset comprising 384 transcriptions and leveraging weak supervision techniques for annotation, we evaluate the performance of monolingual models trained on Portuguese (BERTimbau and PTT5) and multilingual models (mBERT and mT5) in a document-based classification task. Notably, we introduce a novel approach that reframes the token classification task as a text generation problem, enabling fine-tuning and evaluation of T5 models through pre-training. Following the fine-tuning of the models on the learning task, we conduct a classification evaluation on the test dataset, employing performance and error metrics. Our findings reveal that BERT-based models consistently outperform T5-based models in learning to recognize named entities accurately across different languages. Furthermore, while the multilingual models exhibit comparable macro F1-scores, BERTimbau demonstrates superior performance over PTT5 in the document-based NER task. A manual analysis of sentences generated by PTT5 and mT5 unveils a degree of similarity ranging from 0.89 to 1.0, between the original and generated sentences. However, critical errors emerge as both models exhibit discrepancies, such as alterations to monetary and percentage values, underscoring the importance of accuracy and consistency in the financial domain. Despite these challenges, PTT5 and mT5 achieve impressive macro F1-scores of 98.52% and 98.85%, respectively, with our proposed approach. Furthermore, our study sheds light on notable disparities in memory and time consumption for inference across the models, emphasizing that optimizing these aspects is crucial for scalable application in real-world scenarios.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Barlas_Oguz1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12210v1",
  "title": "Decomposing Control Lyapunov Functions for Efficient Reinforcement Learning",
  "modified_abstract": "In the realm of autonomous agents and robotics, the quest for efficient training methodologies remains paramount, particularly in high-dimensional, real-world applications where traditional Reinforcement Learning (RL) approaches falter due to prohibitive data requirements. Drawing inspiration from seminal works\u2014including advances in Inverse Reinforcement Learning (IRL) that minimize computational complexities and enhance policy optimization in intricate environments\u2014we introduce an innovative approach to RL by applying Decomposed Control Lyapunov Functions (DCLFs). This strategy leverages system decomposition and estimation techniques to compute DCLFs, facilitating reward shaping that significantly enhances RL performance with reduced data needs. Our methodology addresses the critical challenge of identifying suitable Control Lyapunov Functions (CLFs) for complex systems by sidestepping the intractabilities associated with high-dimensional settings, and is provably effective in ensuring stable control strategies. Through empirical evaluations, including a quadcopter landing task in robotics, we demonstrate that our approach not only advances the capabilities of imitation learning within the RL paradigm but also surpasses the state-of-the-art Soft-Actor Critic algorithm in efficiency, halving the amount of real-world data required for control tasks. This advancement not only furthers the capabilities of RL in practical robotics scenarios but also sets a new precedent for the development of intelligent agents capable of navigating and learning within unknown environments with unprecedented efficiency.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Siliang_Zeng1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12206v1",
  "title": "Useful Compact Representations for Data-Fitting",
  "modified_abstract": "Building on the precedent of optimizing complex systems in a low-parametric space, as evidenced by methodologies such as PROTES for tensor-based probabilistic optimization, our investigation introduces novel compact representations for the efficient handling of minimization problems lacking second derivative information. These representations, parameterized through a customized selection of vectors, not only generalize existing formulations by interpolating well-known formulas under specific conditions but also offer a solution to the computational challenges posed by dense matrices in large-scale instances. Utilizing control methodologies and probabilistic approaches, our technique incorporates these compact representations in diverse applications such as large eigenvalue computations, tensor factorizations, and nonlinear regressions, showcasing their practical effectiveness and versatility in real-world data-fitting contexts. Key innovations also include the use of advanced sampling techniques to train these models more efficiently, thereby further enhancing the efficiency and accuracy of these representations in both binary and non-binary contexts.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Gleb_Vladimirovich_Ryzhakov1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13851v1",
  "title": "Control of Medical Digital Twins with Artificial Neural Networks",
  "modified_abstract": "This work is inspired by a series of innovations in the application of neural networks to complex control problems, including the advancements made in deep reinforcement learning for optimizing decisions in dynamic environments as illustrated by the development of NeurWIN for restless bandit problems. Personalized medicine, aiming to cater interventions to the individual characteristics of patients, heavily relies on medical digital twins\u2014computational human biology models that are personalized and dynamically updated with patient-specific data. However, modeling certain aspects of human biology, especially those that are multi-scale, stochastic, and not amenable to traditional physics-based models, poses significant challenges. The recent strides in automated differentiation, neural-network control methods, and their training offer promising solutions yet their application to biomedical systems remains rudimentary. We introduce a dynamics-informed neural-network controller tailored for medical digital twins, focusing on solving complex control problems, including but not limited to agent-based models and bandit problems, as a pioneering use case, thereby demonstrating its feasibility and superior performance over existing control methodologies for complex biomedical simulations. The findings underscore the potential of integrated machine learning techniques, particularly neural-network based control, in transcending the limitations of conventional model-based approaches and pushing the frontier in personalized medicine. This sheds light on a new avenue for controlling not only medical digital twins but also other sophisticated dynamical systems, leveraging the restless nature of real-world applications and the profound impact on performance enhancement.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Khaled_Jamal_Nakhleh1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12203v1",
  "title": "Bootstrapping Reinforcement Learning with Imitation for Vision-Based Agile Flight",
  "modified_abstract": "In this work, we are inspired by the pioneering efforts in combining ML techniques for enhanced robot autonomy, particularly in tasks like autonomous navigation where the previous work on 'Composable Action-Conditioned Predictors: Flexible Off-Policy Learning for Robot Navigation' has shown promising directions for learning complex behaviors without explicit task-specific policies. We advance this exploration within the realm of vision-based, autonomous drone racing by amalgamating the strengths of Reinforcement Learning (RL) and Imitation Learning (IL). Given the unique challenges presented by the high dimensionality of visual inputs, which affect RL's sample efficiency and computational demands, coupled with IL's limitations pertaining to the dependency on the quality and quantity of labeled visual demonstrations and the phenomenon of covariate shift, our proposed framework synergizes RL and IL. It includes initial training using privileged state information, distillation of this knowledge into a student policy via IL, and a novel performance-constrained adaptive RL fine-tuning stage. Our experimental validation, conducted in both simulated and real-world settings, showcases our approach's efficacy in navigating a robot through complex racing courses using solely visual input, without needing explicit state estimation. This approach not only surmounts the individual limitations of RL and IL but also sets a new benchmark for the application of vision-based machine learning techniques in robots, utilizing action-conditioned models and overcoming the challenges of event detection in agile flight scenarios.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Adam_Villaflor1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12201v1",
  "title": "Compositional learning of functions in humans and machines",
  "modified_abstract": "Inspired by the profound capability of humans to learn and reason through compositional thought processes, as well as by the biological analogy of neuronal ensembles in machine learning, our research investigates the foundational ability to learn and compose functions in both humans and neural networks. This inquiry has led to exploring how these capacities facilitate efficient learning and flexible generalizations in various tasks, such as creating novel dishes from known cooking processes or drawing on prior knowledge to tackle new problems. Emphasizing the role of accumulated memories in learning and the ability to recall critical information, our work extends the exploration of compositionality beyond sequential function chaining, informed by linguistic evidence of human proficiency in managing complex compositions with interacting functions, to the visual domain. We developed a novel function learning paradigm to examine the potential of humans and neural network models to learn and reason with compositional functions under varied interaction scenarios. This included scenarios where the application of one function alters the context for applying another, reflecting real-world dynamic environments and subsequently affecting performance on established tasks and benchmarks. Our findings reveal that humans can generalize to novel visual function compositions in a zero-shot manner across interaction conditions, demonstrating an acute sensitivity to contextual shifts. Comparatively, our experiments with a neural network model, utilizing a meta-learning approach for compositionality with performance assessment, show that a standard sequence-to-sequence Transformer can approximate human-like generalization patterns in function composition, offering new pathways to enhance machine learning models' ability to learn in a compositional and context-sensitive manner.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Michelle_Miller3",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12198v1",
  "title": "FLex: Joint Pose and Dynamic Radiance Fields Optimization for Stereo Endoscopic Videos",
  "modified_abstract": "Building on the advancements from prior works such as SurfelNeRF, which aimed to address the significant challenge of online photorealistic reconstruction through the innovative use of neural surfel radiance fields for large-scale indoor scenes, our work, FLex, seeks to extend neural rendering capabilities to the medical field, specifically for the reconstruction of endoscopic scenes. Unlike previous approaches that were constrained by static setups or limited deformation capabilities, FLex addresses the complex scenario of a mobile endoscope operating within a dynamic environment of deforming tissue. We introduce a novel methodology that divides the scene into multiple overlapping 4D neural radiance fields (NeRFs) and employs a progressive optimization scheme that jointly optimizes reconstruction and camera poses from scratch. This development leverages the strengths of neural rendering and the novel use of radiance fields to enhance the appearance of endoscopic scenes, thus enhancing ease of use but also significantly extends the capacity to process extended surgical videos beyond 5,000 frames, marking a tenfold improvement over current state-of-the-art methods while remaining independent of external tracking devices. Our extensive evaluations on the StereoMIS dataset demonstrate that FLex markedly elevates novel view synthesis quality in indoor medical environments through photorealistic rendering, training robust models that maintain competitive pose accuracy, setting new benchmarks for reconstructing and rendering in dynamic, indoor medical environments.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yan-Pei_Cao1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12188v1",
  "title": "PETScML: Second-order solvers for training regression problems in Scientific Machine Learning",
  "modified_abstract": "This paper is inspired by the pivotal role of optimization in machine learning, with a focus on identifying training points that maximize information about validation sets, as illustrated by the Goldilocks Selection technique. Recognizing the unique challenges and opportunities presented by scientific machine learning, we detail the development of a lightweight software framework, PETScML, that leverages the Portable and Extensible Toolkit for Scientific computation (PETSc) to address the highly non-convex optimization problem intrinsic to neural network realization through deep-learning techniques. Our approach, primarily utilizing a trust region method based on the Gauss-Newton approximation, emphasizes the importance of curriculum training in managing the complexity of the data presented during the training process. This curriculum-focused strategy, coupled with an information-theoretic approach to select training data, demonstrates a notable improvement in generalization errors across a broad spectrum of scientific machine-learning techniques and regression tasks. The empirical results further validate the optimization benefits of second-order solvers, such as L-BFGS and inexact Newton with line-search, over adaptive first-order methods traditionally employed in deep learning for tackling various training tasks. These findings suggest a promising direction for utilizing rigorous optimization techniques to enhance the reuse and validation of scientific machine-learning models.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Muhammed_T_Razzak1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12187v1",
  "title": "Approximation of RKHS Functionals by Neural Networks",
  "modified_abstract": "Inspired by recent successes in deep learning models, such as the tractable dendritically structured RNNs for reconstructing nonlinear dynamical systems, our work investigates the potential of neural networks in the field of functional data analysis, specifically in approximating functionals on reproducing kernel Hilbert spaces (RKHS's). Our research is motivated by the wealth of functional data, including time series and images, and the utility of efficiently integrating such data into recurrent neural network architectures to dynamically learn mappings from function spaces to the real numbers (i.e., functionals), using techniques such as back-propagation-through-time. We establish the universality of neural networks in approximating functionals on RKHS's, deriving explicit error bounds for approximations that employ inverse multiquadric, Gaussian, and Sobolev kernels. Furthermore, we demonstrate the practical application of our theoretical findings in a task of functional regression, showing that neural networks can accurately approximate regression maps in generalized functional linear models, thus simplifying existing models that rely on interpolation-type basis function expansions with a predefined set of basis functions. Our inference utilizes interpolating orthogonal projections in RKHS's, offering a more streamlined methodology by substituting basis function expansions with point evaluations. This innovation paves the way for more chaotic data scenarios where traditional methods fall short.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Daniel_Durstewitz1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12166v2",
  "title": "The Power of Few: Accelerating and Enhancing Data Reweighting with Coreset Selection",
  "modified_abstract": "Drawing inspiration from the groundbreaking theoretical developments in overparameterized machine learning (ML), which have illuminated the pathways toward understanding complex models that defy the traditional bias-variance tradeoff, our work endeavors to harness these insights for practical model efficiency. We aim to navigate the persistent challenge of balancing computational efficiency and model accuracy by introducing a novel method that leverages core subset selection for data reweighting. This technique, applicable in both regression and interpolation tasks, optimizes computational time and enhances model performance by focusing on a strategically selected coreset, offering a robust representation while minimizing the influence of outliers and potential scenarios of overfitting. Utilizing rapid descent methodologies in our approach enhances the speed at which recalibrated weights are subsequently mapped back to the entire dataset, spreading improvements across all data points including those processed by neural networks. Experimental validations affirm the efficacy of this method, showcasing it as a scalable and precise strategy for efficient model training in an era where the complexity and scale of data and models continue to expand. These findings pose a multitude of questions for future research, indicating the potential for further exploration and application of this technique.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yehuda_Dar1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12158v1",
  "title": "Variational Approach for Efficient KL Divergence Estimation in Dirichlet Mixture Models",
  "modified_abstract": "Informed by the foundational challenges in modeling and estimation within data science, such as those encountered when determining the exact number of components in finite mixture models, our research is focused on the efficient estimation of Kullback-Leibler (KL) Divergence in Dirichlet Mixture Models (DMM). Despite the critical application of DMMs in clustering compositional data, achieving an analytically tractable solution for KL Divergence has remained a complex issue. Previous strategies, including the computationally intensive Monte Carlo methods, underscore the need for an innovative solution. Responding to this, we introduce a pioneering variational approach that furnishes a closed-form solution, markedly improving computational efficiency in machine learning. This approach not only facilitates rapid model comparison and robust estimation assessments but also proves particularly effective for analyzing subpopulations within larger datasets by leveraging the refinement of posteriors. Proving the theory behind our method, validation on real and simulated data demonstrates the proposed methodology's enhanced efficiency and accuracy over traditional Monte Carlo techniques, marking a significant step forward in the exploration of DMMs and the statistical analysis of compositional data.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Diana_Cai1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12151v1",
  "title": "Fusing Domain-Specific Content from Large Language Models into Knowledge Graphs for Enhanced Zero Shot Object State Classification",
  "modified_abstract": "This study emerges from the extensive groundwork laid by prior revolutionary work in the field of zero-shot learning, particularly focusing on the methodologies employed to dissect semantics for improved classification in scenarios where explicit training data is absent. By integrating Large Language Models (LLMs) with Knowledge Graphs and pre-trained semantic vectors, we aim to enhance Vision-based Zero-shot Object State Classification tasks significantly. This interdisciplinary approach draws on the success of innovations in Generalized Zero-shot Learning (GZSL), leveraging insights on semantic disentangling and generalization to inform our exploration of domain-specific knowledge contribution across various classes and datasets. Our exploration operates under the hypothesis that the independence of semantic vectors from specific training instances affords a more robust framework for learning across unseen classes. We conduct an exhaustive ablation study to scrutinize the LLM's behavior with latent attributes and demonstrate that LLM-based embeddings, when integrated with general-purpose pre-trained embeddings and assessed against diverse zero-shot datasets, achieve considerable performance improvements in object state classification, both in terms of semantics and generalization. Our results not only underscore the efficacy of combining LLM insights with Knowledge Graphs but also position our proposed model at the forefront of the field, as evidenced by comparative analyses with existing models.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yadan_Luo1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12143v2",
  "title": "Graph Neural Networks for Learning Equivariant Representations of Neural Networks",
  "modified_abstract": "Drawing inspiration from a rich vein of previous works that have explored the potential of GNNs for enhancing neural network tasks, our research addresses the critical challenge of processing neural network parameters with consideration for the inherent permutation symmetry. Previous studies, such as those looking into when GNNs provide a significant advantage over conventional neural network architectures for node classification, have underscored the transformative impact of encoding relational structures yet often overlook critical aspects such as permutation equivariance in neural computations. Leveraging this foundational work, we propose a novel methodology that involves representing neural networks as computational graphs, enabling the application of graph neural networks and graph-aware transformers to preserve permutation symmetry efficiently. This innovative approach not only addresses the symmetry issue but also allows for the inductive learning capability of a single model to handle a variety of neural computational graph architectures while maintaining statistical integrity in processing. Our extensive experiments across numerous tasks, including the classification and editing of implicit neural representations, predicting network generalization capabilities based on features extracted and analyzing samples through a statistical hypothesis, and optimization learning tasks, demonstrate superior performance when compared to leading-edge solutions. Notably, our predictive models implemented in a graph-agnostic manner, facilitated robust testing collections that significantly advance the state of research in this field. The source code for our work is available at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sitao_Luan1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12120v1",
  "title": "Light Curve Classification with DistClassiPy: a new distance-based classifier",
  "modified_abstract": "In the context of time-domain astronomy, where the analysis and interpretation of celestial objects have been revolutionized by synoptic sky surveys, producing an overwhelming volume of data, our work introduces DistClassiPy, a novel distance metric-based classifier conceived from the inspiration drawn from advancements such as the Convolutional Dynamic Alignment Networks (CoDA-Nets) in creating highly interpretable and efficient classifiers. Unlike traditional tree-based (e.g., Random Forests) and neural models that dominate the field, our approach utilizes a suite of distance metrics to facilitate the classification of variable star light curves, offering an untapped angle that enhances interpretability while reducing computational demands. Through the analysis of 6,000 variable stars across 10 classes using 18 distance metrics, we not only achieve state-of-the-art classification performance but also present a model characterized by its lower computational footprint and heightened interpretability. The building of DistClassiPy emphasizes the attribution of each distance metric's effectiveness in the classifier's performance, showcasing its strength in providing insights into decision-making processes. DistClassiPy is made available as an open-source tool [omitted for de-identification], inviting further exploration in diverse classification scenarios within and beyond the astronomical domain.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Moritz_B\u00f6hle1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12036v1",
  "title": "One-Step Image Translation with Text-to-Image Models",
  "modified_abstract": "Drawing inspiration from the efforts to overcome the limitations of current text-to-image generation models, particularly those facing challenges in generating coherent scenes from textual descriptions that involve complex object compositions and relationships, this work addresses two key limitations in existing conditional diffusion models: their slow inference speed due to iterative denoising and a heavy reliance on paired data for fine-tuning. To circumvent these challenges, we propose a novel adaptation of single-step diffusion models for image translation tasks through the integration of adversarial learning objectives and a selection mechanism that enhances composition from a wide range of textual inputs. Thereby crafting a streamlined generator network that enhances both the efficiency and applicability of such models across varied and unpaired settings. Our innovations lead to the development of CycleGAN-Turbo and pix2pix-Turbo, which notably excel in scene translation tasks, composing scenes with fidelity and complexity that outperform benchmarks, and demonstrate competitive performance with state-of-the-art methods in both unpaired and paired scenarios, respectively, while drastically reducing inference time. Through rigorous evaluation, their struggle to maintain coherence in complex scenes is significantly mitigated. This demonstrates that single-step diffusion models, through careful adaptations including selection and composition of elements, can serve as powerful frameworks for a wide range of image translation needs, generation typical and novel scenes alike, thereby pushing the boundaries of image generation and benchmarking. Code and models have been made available for public access at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Xihui_Liu1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12034v1",
  "title": "VFusion3D: Learning Scalable 3D Generative Models from Video Diffusion Models",
  "modified_abstract": "Inspired by advancements in using synthetic data for enhancing visual question answering (VQA) systems, our work introduces a novel approach to address the challenge of scarcity in 3D data for building scalable 3D generative models. Leveraging the concept of using synthetic computer-generated data to provide more diverse scenarios in related domains, we propose utilizing pre-trained video diffusion models, which have been trained on extensive volumes of text, images, and videos, as a knowledge source for generating 3D data. Through fine-tuning these video diffusion models with an emphasis on object-level understanding and by perturbing initial states in a specific training pipeline, we unlock multi-view generative capabilities, allowing us to produce a large-scale synthetic multi-view dataset. This hyper-realistic dataset serves as the training ground for our feed-forward 3D generative model, VFusion3D. Trained on nearly 3M synthetic multi-view data points across various platforms, VFusion3D is capable of generating high-quality 3D assets from a single image in seconds, achieving superior performance compared to current state-of-the-art (SOTA) feed-forward 3D generative models with a user preference rate exceeding 70%. Moreover, the integration of simulation techniques into the training process and establishing benchmarks for our model's performance ensures the relevance and diversity of generated questions within the VQA systems.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Paola_Cascante-Bonilla1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12031v1",
  "title": "ROUTERBENCH: A Benchmark for Multi-LLM Routing System",
  "modified_abstract": "Informed by the increasing complexity and scalability issues in training large models such as BERT and GPT-3, which have underscored the need for careful optimization in model design, architecture, and system integration, this work introduces ROUTERBENCH. As the range of applications for Large Language Models (LLMs) continues to expand, the necessity for efficient serving, communication solutions, optimal training, and architecture becomes paramount. Recognizing that no single model can ideally cater to all tasks, especially when balancing between performance, cost, and training resource allocation, this study addresses the pivotal challenge of evaluating and optimizing LLM routing systems. ROUTERBENCH is a pioneering evaluation framework aimed at systematically assessing the effectiveness of LLM routing systems through extensive training scenarios, efficient architecture design, and large-scale inference outcomes. It unveils a comprehensive dataset featuring over 405k inference outcomes from diverse LLMs, facilitating the development of robust routing strategies that emphasize efficient communication processes. By proposing a theoretical framework for LLM routing and conducting a comparative analysis of various routing techniques through ROUTERBENCH, this investigation not only formalizes the progression of LLM routing systems but also establishes a benchmark for their evaluation. This initiative seeks to streamline the integration and economic viability of LLM applications by offering the code and data at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Shaoduo_Gan1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12030v1",
  "title": "Expandable Subspace Ensemble for Pre-Trained Model-Based Class-Incremental Learning",
  "modified_abstract": "Inspired by advancing methodologies in addressing bias and enhancing generalization capabilities in machine learning models, our work introduces ExpAndable Subspace Ensemble (EASE) as a novel approach to mitigate the challenges of Class-Incremental Learning (CIL) where a learning system is expected to continually adapt to new classes without forsaking previously learned ones. Drawing on insights from related works that exploit diverse strategies like employing networks of classifiers, ensemble and committee approaches, and reaching a consensus to identify and adjust to bias-conflicting data, we propose a model that harnesses the strengths of Pre-Trained Models (PTMs) while overcoming the critical issue of catastrophic forgetting. EASE achieves this by creating task-specific subspaces through lightweight adapter modules for each new class, enabling model expansion without overriding previous knowledge. This ensemble approach not only aids in prediction accuracy but also in generalization across tasks. A semantic-guided prototype complement strategy is also introduced to accommodate the evolving data landscapes, ensuring seamless integration of new classes with minimal interference to existing class classifiers. With extensive testing on seven benchmark datasets, EASE demonstrates superior performance in both prediction and consensus-building among classifiers, committees offering a promising avenue for future exploration in the realm of CIL. Code is available at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sehyun_Hwang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12029v1",
  "title": "Align and Distill: Unifying and Improving Domain Adaptive Object Detection",
  "modified_abstract": "While object detectors have shown promising capabilities, their performance often degrades on data that diverges from the training domain. Inspired by groundbreaking works such as 'R2D2: Reliable and Repeatable Detector and Descriptor', which explores the realm of detecting and describing keypoints in images, our work seeks to address the challenge of domain adaptation in object detection. Our research recognizes systemic benchmarking pitfalls that have hindered progress, such as overestimation of performance, inconsistent implementation practices, and the lack of diversity in benchmarks. To overcome these challenges, we introduce Align and Distill (ALDI), a unifying framework for networks that facilitates the comparison of domain adaptive object detection (DAOD) methods and encourages the further development of these technologies. Additionally, we employ a novel 'detect-then-describe' strategy, enhancing keypoint recognition capabilities with advanced descriptors and reliably integrating saliency detection to improve the model\u2019s attention to relevant features. This approach not only aids in the precise localization of tasks within images but also underscores the significance of task-specific feature descriptors in achieving superior object detection. We also propose a new benchmark dataset, CFC-DAOD, designed to evaluate performance on diverse real-world data. Our novel method, ALDI++, significantly outperforms existing models, setting new learn-driven benchmarks across several datasets. This advancement provides networks with a robust foundation for future research, aimed at creating object detection models that perform consistently well across varied domains, even in sparse data scenarios. Code and data are made available for community use: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jerome_Revaud1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12026v1",
  "title": "FlexCap: Generating Rich, Localized, and Flexible Captions in Images",
  "modified_abstract": "Influenced by pioneering works in the domain of vision-language modeling (VLM), such as the transformative methods for zero-shot object-goal navigation which demonstrated remarkable advancement by employing multimodal goal embeddings for navigating virtual environments, our work introduces a versatile $\\textit{flexible-captioning}$ VLM, FlexCap. This model, integrating advanced learning algorithms, is capable of generating region-specific descriptions of varying lengths, thereby achieving control over the information density of its output, with descriptions ranging from concise object labels to detailed captions. To accomplish this, we create large-scale training datasets of image region descriptions of varying length, starting from captioned images. Leveraging embeddings meticulously and training on a broad spectrum, FlexCap's flexible-captioning capability has several valuable applications, including superior performance in dense captioning tasks on the Visual Genome dataset and the construction of state-of-the-art visual question answering (VQA) systems through the generation of localized descriptions as inputs to large language models. Furthermore, our research illustrates how a $\\textit{localize-then-describe}$ approach with FlexCap is more effective for open-ended object detection than a $\\textit{describe-then-localize}$ approach with other VLMs, acting in essence as cognitive agents in these virtual scenarios. We also underscore FlexCap\u2019s versatility in extracting diverse visual information through prefix conditioning and qualitatively demonstrate its broad applicability in tasks such as image labeling, object attribute recognition, and visual dialog. Project webpage: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Bhavika_Suresh_Devnani1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12025v1",
  "title": "A Toolbox for Surfacing Health Equity Harms and Biases in Large Language Models",
  "modified_abstract": "As the burgeoning capabilities of Large Language Models (LLMs) revolutionize access to health information, they concurrently bear the risk of perpetuating or even exacerbating health disparities\u2014a challenge underscored by recent advancements in evaluating LLMs through multi-turn interactions and tools-based feedback. To address this critical issue, our work introduces comprehensive resources and methods aimed at identifying and mitigating biases within LLMs, particularly in providing long-form, medical answers and executing instruction-finetuning learning tasks. We present an in-depth empirical case study employing Med-PaLM 2, marking a significant stride as the largest human evaluation study in this scope to date. Using our contributions encompass a multifaceted framework for human bias assessment in LLM outputs and the creation of EquityMedQA\u2014a collection of seven datasets, which uniquely includes both manually-curated and LLM-generated medical queries enriched for adversarial contexts, suitable for instruction-finetuning tasks. These contributions are deeply rooted in an iterative participatory approach, alongside a meticulous examination of potential biases in Med-PaLM 2's responses to adversarial prompts. Our empirical findings illuminate the efficacy of employing diverse dataset compilation techniques and robust evaluation protocols\u2014featuring varying rubrics and rater demographics\u2014in flagging biases that narrower evaluation models may overlook. The insights gleaned underscore the importance of multifarious assessment strategies and the engagement of a broad spectrum of evaluators in conducting tasks. Our findings also highlight the performance improvements in LLMs when equipped with our proposed methodologies, noting instances where proactive bias identification and mitigation via our tools showed clear benefits. While our framework can pinpoint certain bias phenomena, it alone does not suffice for a comprehensive appraisal of an AI system's capacity to foster equitable health outcomes. It is our aspiration that this compilation of tools and methodologies, along with the use of instruction-finetuning and careful performance evaluation, serves as a springboard for broader efforts aimed at achieving LLMs that afford equitable and universally accessible healthcare solutions.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yangyi_Chen1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12017v1",
  "title": "Supervised Fine-Tuning as Inverse Reinforcement Learning",
  "modified_abstract": "Building upon previous successes of fine-tuning Large Language Models (LLMs) for a diverse range of NLP tasks, our research interrogates conventional alignment methodologies that depend heavily on human or AI-generated feedback within specific preference dataset confines. In light of the inherent challenges and limitations presented by the reliance on such datasets, as underscored by advancements in collaborative fine-tuning and inference techniques, our work ventures into exploring alignment through expert demonstrations within a sequential decision-making framework. By integrating principles of inverse reinforcement learning and imitation learning, we pioneer approaches for divergence minimization in LLM alignment, uncovering the unique mass-covering and mode-seeking dynamics of these methodologies. This comprehensive analysis not only casts a critical eye on the classical supervised fine-tuning method but also delineates the situational advantages of diverse alignment strategies and techniques, marking a significant step forward in our understanding and application of LLM fine-tuning.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Tim_Dettmers2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12014v1",
  "title": "EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents",
  "modified_abstract": "Drawing inspiration from recent strides in embodied learning and reinforcement learning (RL), particularly the innovative use of large language models (LLMs) for direct interaction and the conceptual advances in decoupling dynamics and rewards for transfer learning, this work introduces EnvGen. EnvGen is a novel framework that leverages LLM's exceptional world knowledge and reasoning capabilities not as direct agents, but as tools for adaptively creating and modifying training environments to enhance the learning efficiency of smaller embodied RL agents. We begin by prompting an LLM to generate diverse and challenging training environments based on task descriptions and targeted learning objectives, which allows parallel task learning while emphasizing the development of specific skills where the agents show weaknesses. This process involves precise planning, representation of the task environment, and observation of agent performance for refinement through iterative feedback, allowing the LLM to fine-tune the environment generation and adaptation for optimal learning outcomes, including reward structuring. Through experiments in complex Crafter and Heist environments, EnvGen's efficacy is demonstrated as it enables smaller RL agents to be jointly trained across tasks, rapidly acquiring and improving on long-horizon tasks, surpassing state-of-the-art methods with significantly fewer LLM calls. This efficiency, alongside detailed ablation studies of our design choices, underscores EnvGen's potential in reducing the computational expense associated with employing LLMs directly while still reaping the benefits of their sophisticated reasoning capabilities.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Harsh_Satija2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12012v1",
  "title": "Convergence of Kinetic Langevin Monte Carlo on Lie groups",
  "modified_abstract": "Our exploration into the convergence properties of kinetic Langevin Monte Carlo on Lie groups is deeply inspired by similar foundational advances in the understanding of optimization dynamics, particularly those involving smooth games, gradient-based methods, and the handling of spectral shapes through bilinear mappings. Drawing from innovations such as variational optimization, left trivialization, and novel sampling dynamics, we construct a novel momentum-based dynamics for functions defined on Lie groups, emphasizing the euclidean nature of the momentum variable despite the potential function residing on a manifold. This approach is akin to solving a complex optimization problem through the incorporation of tractable noise and delicate discretization, a methodology that advances beyond mere problem solving to the creation of a structured scheme for kinetic analysis and iterative improvement. We propose a Lie-group MCMC sampler that precisely preserves the Lie group structure, leveraging gradient-based techniques to ensure efficiency in sampling and achieving consensus on the geometrically complex landscape. Our work marks a significant milestone by providing the first convergence results for kinetic Langevin dynamics on curved spaces, showcasing exponential convergence with explicit rates under W2 distance based solely on compactness of the Lie group and geodesically L-smoothness of the potential function, without reliance on convexity or common relaxations like isoperimetry.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Wa\u00efss_Azizian1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12729v1",
  "title": "Posterior Uncertainty Quantification in Neural Networks using Data Augmentation",
  "modified_abstract": "This work addresses the exigent problem of uncertainty quantification in the domain of deep learning, drawing inspiration from previous endeavors, such as the formulation of data-driven priors and the empirical Bayes methods for Bayesian deep neural networks. These anterior frameworks have laid down the groundwork for understanding the meaningful interplay between prior specification, network weight modeling, and posterior uncertainty, especially in the context of variational inference with deep network architectures. Building upon such foundational insights, we introduce MixupMP, a novel methodology that underscores the pitfalls of traditional deep ensembling by illustrating its mis-specification for cases where future data do not conform to the distribution of observed data. MixupMP utilizes data augmentation techniques, particularly Mixup, to engineer a predictive network that more accurately reflects the diversity and range of possible future observations, thereby functioning as an effective substitute for deep ensembles and enhancing recognition capabilities. Leveraging the principles of Martingale posteriors and explicitly modeling the network weights, MixupMP endeavors to sample from an implicitly defined Bayesian posterior, thus enhancing the efficacy of posterior uncertainty quantification in recognizing diverse patterns. Through rigorous empirical analysis on various image classification datasets, we demonstrate the superiority of MixupMP over both Bayesian and non-Bayesian contenders in achieving more precise uncertainty measures and predictive performance.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ranganath_Krishnan1",
  "manipulated_ranking": 5,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12007v2",
  "title": "Defining Effective Engagement For Enhancing Cancer Patients' Well-being with Mobile Digital Behavior Change Interventions",
  "modified_abstract": "Drawing inspiration from the evolving landscape of digital health interventions, particularly the innovative approaches depicted in projects like the CAPABLE project and research targeting interventions in complex network interactions, this study ventures into the domain of Digital Behavior Change Interventions (DBCIs). Our goal is to define what constitute efficient engagement with DBCIs to improve the well-being of cancer patients. By employing a methodology that combines identification of engagement metrics, exploration of DBCI interest among patients and clinicians, and hypothesizing about DBCI impacts through sampling-based methods, we aim to contribute practical guidelines for developers facing the challenge of evaluating DBCIs in ethically sensitive, small-scale studies. Our research uncovers that clinician prescriptions can significantly bolster sustained engagement with mobile DBCIs and suggests a minimal weekly engagement threshold for maintaining patients' well-being, while also indicating that a deeper level of engagement, facilitated by adjacency in patient-clinician interaction networks, might be necessary for transitioning from extrinsic to intrinsic motivational states. The underlying graph of patient and clinician interactions plays a pivotal role in our analysis, mapping the equilibrium between technological possibilities and patient needs, hinting at the complexity and the large scope of the network's influence.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~William_Brown7",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12005v1",
  "title": "Visualization for Trust in Machine Learning Revisited: The State of the Field in 2023",
  "modified_abstract": "This survey is inspired by the growing body of work that underscores the criticality of explicability and trustworthiness in machine learning technologies across diverse fields such as medicine, finance, and bioinformatics. Notably, in our efforts to enhance understanding and facilitate advancements in the field, we have been guided by methodologies and insights from recent contributions to the literature, including efforts to improve the efficiency and accuracy of automated decision support systems through novel predictive models. Building upon our 2020 state-of-the-art report, which compiled 200 techniques, we have persisted in our collection and analysis of peer-reviewed articles on visualization techniques for machine learning to efficiently support human understanding and interaction. In this updated survey, articles have been categorized utilizing our previously established framework comprising 119 categories, including multiclass classification, yielding a repertoire of 542 techniques now accessible via an online survey browser. Our examination of these techniques, as of fall 2023, sheds light on prevailing trends, furnishes valuable insights, and articulates eight prevailing challenges in employing visualization as a means of engendering trust in machine learning models. This analysis substantiates the burgeoning trend towards leveraging visualization not only to augment the interpretability of popular machine learning models but also to facilitate the efficient evaluation of novel deep learning architectures, particularly those involved in multiclass classification tasks involving decisions.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Eleni_Straitouri1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11998v1",
  "title": "Learning Useful Representations of Recurrent Neural Network Weight Matrices",
  "modified_abstract": "This study is inspired by the burgeoning realm of deep learning (DL), where the evolution of robust and versatile frameworks is critical for swiftly executing complicated computations and experiments. Particularly, works like the speculative symbolic graph execution of imperative DL programs, leveraging graph-based representations, highlight the need for efficient, flexible, and dynamic programming models capable of handling complex network architectures transparently. Against this backdrop, our focus is on Recurrent Neural Networks (RNNs), exploring methodologies to derive meaningful representations of RNN weights that enable both analysis and practical applications. By adopting both mechanistic and functionalist viewpoints, we innovate on approaches\u2014especially the functionalist methods\u2014that interrogate the RNN's functionality through probing inputs for extracting valuable information from its weight matrices transparently. Furthermore, we establish a theoretical framework delineating the functionalist perspective's capacity to unveil representations rich enough to decipher RNN behavior comprehensively. Additionally, we introduce the first two 'model zoo' datasets aimed at RNN weight representation learning, paving the way for novel emulation-based self-supervised learning techniques to assess the efficacy of varying RNN weight encoding strategies across several tasks, with a highlight on the functionalist approach\u2019s superiority in identifying specific RNN training tasks. Our approach garners large support from the deep learning community, eager to explore these novel symbolic programming execution techniques.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Taebum_Kim1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12117v1",
  "title": "Transfer Learning for T-Cell Response Prediction",
  "modified_abstract": "Our research is inspired by the critical need for precision medicine, such as the personalized cancer vaccine, and recent advancements in computational biology, including the innovative application of deep learning techniques for the complex tasks of protein sequence generation and antigen specificity prediction. These developments underscore the potential impact of sophisticated ML architectures in biomedical research, particularly in the design of computational methods for analyzing sequences. Specifically, we address the challenge of predicting T-cell response to specific peptides, a task complicated by the limitations of available data, which is characterized by its heterogeneous, multi-domain structure. This environment risks shortcut learning, where models might learn to associate T-cell responses with general characteristics of peptide sources rather than with precise peptide attributes that truly influence T-cell activation. By implementing a transformer-based model focused on text and protein sequence generation techniques, we illustrate that inflated predictive performance due to shortcut learning is a practical concern. Moreover, by focusing on understanding the language of proteins and employing generation techniques within our transformer model, we show the significant benefits of these approaches in improving prediction accuracy. As a remedy, we introduce a domain-aware evaluation scheme and investigate various transfer learning techniques to tackle the inherent multi-domain structure of the training data and mitigate shortcut learning. Our study reveals that per-source fine-tuning is particularly effective across diverse peptide sources. Furthermore, we demonstrate that our method surpasses existing state-of-the-art models in predicting T-cell responses to human peptides, thereby representing an essential step towards the application of machine learning in the development of personalized therapeutic solutions.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Vijil_Chenthamarakshan1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11996v1",
  "title": "Accelerating Scientific Discovery with Generative Knowledge Extraction, Graph-Based Representation, and Multimodal Intelligent Graph Reasoning",
  "modified_abstract": "This work is inspired by pioneering advancements in graph neural networks, particularly those emphasizing the enhancement of structural representation learning through innovative methods like distance encoding, which substantially improves GNNs\u2019 ability to distinguish complex structures in graphs. Utilizing generative AI, we have transformed a vast corpus of 1,000 scientific papers in the domain of biological materials into comprehensive ontological knowledge graphs. These graphs not only underscore the inherently scale-free nature of scientific knowledge but also enable the discovery of deep interdisciplinary relationships through the use of graph traversal path detection, which leverages a combinatorial ranking of node similarity and graph-distance with betweenness centrality. We highlight novel insights such as structural parallels between biological materials and Beethoven's 9th Symphony, identified through isomorphic mapping, and the innovative design of a hierarchical mycelium-based composite inspired by principles extracted from Kandinsky's Composition VII painting for specific tasks. The composite reflects a harmonious blend of chaos and order, characterized by adjustable porosity, mechanical strength, and patterned chemical functionalization, standing as a classification of its unique properties. Our research tasks further involve the prediction of novel material properties and functionalities based on extracted graph-based representations. Our exploration further extends to drawing isomorphisms across diverse domains, thereby establishing a nuanced ontology that aligns with postmodernist perspectives on materiality and immanence. These findings illuminate the dynamic and context-dependent interplay of entities within networks, challenging traditional hierarchical paradigms and emphasizing the significance of individual components and their relationships. By unveiling hidden connections and fostering discovery, our approach significantly advances the capabilities of generative AI in innovative research, opening new pathways for scientific exploration and the conception of novel materials.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yanbang_Wang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11981v1",
  "title": "Diffusion Denoising as a Certified Defense against Clean-label Poisoning",
  "modified_abstract": "In an era where data integrity is paramount, our work introduces a certified defense mechanism against clean-label poisoning attacks, which insidiously manipulate a small percentage of the training data to cause misclassification at test time. Our innovation is inspired by a fusion of ideas from the adversarial robustness domain, particularly the concept of denoised smoothing, and insights gleaned from advancements in handling inexact or partial labels within weakly supervised learning frameworks. We leverage an off-the-shelf diffusion model to effectively sanitize poisoned training data, showcasing the efficiency of our method through rigorous testing against seven clean-label poisoning attacks. The results are promising, reducing attack success rates to between 0-16% while maintaining negligible impact on classification accuracy at test time. By juxtaposing our defense with previous countermeasures, we underscore its superior efficacy in mitigating attack successes and enhancing model utility. This invites a vital discourse on strengthening clean-label attack methodologies while positioning our practical yet certified defense as a pivotal benchmark for future evaluations. Generation of sanitized datasets through partial-label analysis within weakly supervised learning sets the stage for enhanced model robustness against varied adversarial tactics. Our algorithm's performance in learning from and leveraging this form of analysis underscores the crucial role of candidate estimation in refining the boundaries of model security and reliability.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jiaqi_Lv1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11968v1",
  "title": "Unveil Conditional Diffusion Models with Classifier-free Guidance: A Sharp Statistical Theory",
  "modified_abstract": "Our work is motivated by the significant strides made in the field of generative modeling, particularly by the inception and development of diffusion models for data perturbation and generation across both finite and function spaces. Building upon the empirical achievements of using score-based diffusion models for varied applications, including scientific computing and 3D geometric data analysis, our paper addresses a critical theoretical void in conditional diffusion models. These models, pivotal in contemporary image synthesis with high resolution, have seen broad application in computational biology and reinforcement learning, leveraging various forms of conditional information, including forcings, to steer sample generation towards desired outcomes. However, the theoretical underpinnings of these models, including their efficiency in denoising and enhancing image resolution, remain largely unexplored. This paper contributes to the field by introducing a comprehensive statistical theory for distribution estimation with conditional diffusion models, featuring a sample complexity bound that conforms to the data distribution's smoothness and aligns with the minimax lower bound. The core of our theoretical framework is predicated on a novel approximation for the conditional score function, facilitated by a diffused Taylor approximation technique that is numerically stable and demonstrates how perturbed data can be effectively denoised and managed with the versatility to adapt to changing resolution demands. Furthermore, we extend the relevance of our statistical theory by showcasing its applicability in diverse scenarios such as dynamic model-based transition kernel estimation in reinforcement learning, solving inverse problems with function-valued strategies, and generating samples conditioned on rewards through matching criteria.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Karsten_Kreis1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11966v1",
  "title": "Informed Spectral Normalized Gaussian Processes for Trajectory Prediction",
  "modified_abstract": "Building upon a foundation established by recent advancements in probabilistic deep learning models, particularly those improving scalability and efficiency through sparse Gaussian Processes (GPs) leveraging neighbor information, our work introduces a novel application of informed spectral normalized Gaussian processes (SNGPs) for trajectory prediction. Existing strategies have enhanced model performance and data efficiency by incorporating informative priors; however, they often encounter computational bottlenecks due to the reliance on large, sampling-based approximations. Addressing this, we present an innovative regularization-based continual learning technique for SNGPs that integrates informative priors reflective of accumulated knowledge from prior tasks, without necessitating rehearsal memory or parameter expansion. This method not only inheres from the theoretical rigor offered by previous studies but also enriches the model with the capacity to efficiently utilize prior expert and world knowledge, leveraging mini-batch training for improved variationally-guided efficiency. Thereby ensuring an improvement in data efficiency and robustness, particularly in the context of autonomous driving. Our application-focused study on trajectory prediction, facilitated by the inclusion of prior drivability knowledge and consideration of key variables such as those informed by tional (intended to represent 'traditional' in the context) understanding, reveals substantial performance gains over traditional approaches when evaluated against diminishing training datasets and across diverse locations, as evidenced by experiments conducted on two public datasets.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Pietro_Michiardi1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11964v1",
  "title": "Probabilistic Calibration by Design for Neural Network Regression",
  "modified_abstract": "Our work is propelled by the foundational efforts in creating robust machine learning systems that can effectively handle distribution shifts, a challenge prominently addressed within the realm of distributionally robust supervised learning (DRSL). This context underscores the critical importance of generating calibrated and sharp neural network predictive distributions for regression problems, essential for optimal decision-making across numerous real-world applications. Addressing neural network miscalibration, our investigation delves into both post-hoc methods, which adjust predictions after training, and regularization methods, acting during the training phase. Despite post-hoc methods showing superior calibration improvements, they operate separately from model training. To this end, we present Quantile Recalibration Training, a novel end-to-end model training procedure that seamlessly integrates optimization techniques for calibration directly into the training phase without demanding additional parameters, solving calibration issues more efficiently. Furthermore, our approach encompasses a unified solving algorithm for calibration subproblems, encapsulating both post-hoc and regularization methods as specific instances, potentially facilitating faster convergence through learning. Through extensive experimentation with 57 tabular regression datasets, our methodology demonstrably elevates predictive accuracy while concurrently ensuring calibration, essentially solving the calibration challenge more effectively. Complementary to these empirical achievements, we comprehensively appraise the contribution of diverse components within our proposal, including the role of classifiers, and critically examine the influence of base models, reshuffling techniques, and hyperparameters on predictive performance.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yaodong_Yu4",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11963v1",
  "title": "Transfer Learning Beyond Bounded Density Ratios",
  "modified_abstract": "In this study, we advance the understanding of transfer learning by challenging the traditionally held assumption that successful transfer requires the density ratio $dQ/dP$ between the source and target distributions to be bounded. This inquiry is inspired by pioneering works such as those by Kpotufe and Martinet, and Hanneke and Kpotufe, which presented instances of successful transfer learning despite the unbounded nature of $dQ/dP$. Concentrating on the realm of low-degree polynomial estimators, we present a general transfer inequality applicable over the $\\mathbb{R}^n$ domain. Our findings indicate that nontrivial transfer learning is feasible under conditions far less stringent than previously believed, notably in scenarios where $Q$ denotes a log-concave measure with a bounded inverse ratio $dP/dQ$. To showcase our theory's practical implications, we explore its application in two distinct contexts: the classical truncated regression scenario entailing regularization techniques and dataset preparation to cope with an unbounded $dQ/dP$, and the contemporary challenge of out-of-distribution generalization for supervised in-context learning through transformers, highlighting adaptability in varied action-space environments. Feedback mechanisms inherent in real-world applications are considered to extend the theoretical models. Further contributions include a discrete version of our transfer inequality for the Boolean Hypercube, directly involving state-space and the agent's action mechanics in simplified gridworld settings, and an examination of its relation with the recent exploration of generalization on the Unseen by Abbe et al. Our conceptual breakthrough lies in identifying the maximum influence of the error ($\\mathrm{I}_{\\max}(\\widehat{f}-f^*)$) under distribution $Q$ as a key determinant for the feasibility of transfer learning across different domains, reinforcing the value of reward-based strategies in achieving effective transfer.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Alex_Lewandowski1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11961v1",
  "title": "Enhanced Event-Based Video Reconstruction with Motion Compensation",
  "modified_abstract": "With an understanding derived from prior innovations such as the effective use of variational autoencoders in the compression of image datasets, our work in event-based video reconstruction tackles the challenges of interpretability and high memory demands prevalent in deep neural networks for video processing. In this context, we acknowledge a recent leap made by a lightweight network called CISTA-LSTC, which demonstrates the possibility of high-quality video reconstruction through architectural innovation. This breakthrough notwithstanding, there exists a gap in accounting for the complexities introduced by motion, specifically the displacement effect. Therefore, we propose an advancement in the form of warping input intensity frames and sparse codes for improved reconstruction quality through parallel processing techniques. By integrating a flow network with CISTA-LSTC to form CISTA-Flow, our method facilitates effective motion compensation through reliance on event predictions. An iterative training framework further refines the synchrony between flow estimation and frame reconstruction, using autoregressive models to enhance predictive accuracy. Our approach not only surpasses existing benchmarks in reconstruction accuracy but also provides a reliable form of dense flow estimation. Additionally, the model's inherent flexibility to incorporate various flow networks underscores its potential for continuous adaptability and optimization in compression performance.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ning_Kang2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11960v1",
  "title": "CASPER: Causality-Aware Spatiotemporal Graph Neural Networks for Spatiotemporal Time Series Imputation",
  "modified_abstract": "Motivated by the critical need to understand complex spatiotemporal dynamics and inspired by recent advances in causal inference that tackle the challenges of stochastic confounders in observational studies, our work proposes a novel framework for spatiotemporal time series analysis. Spatiotemporal time series form the backbone of comprehending human activities and their implications, yet are often marred by missing values due to sensor failures, significantly hindering data analysis. Traditional imputation methods indiscriminately leverage all available information, disregarding the causal relationships crucial for accurate recovery. Recognizing the prevalence of unknown confounders\u2014like background noise and non-causal shortcut edges\u2014in data collection, we contend that these confounders forge spurious correlations, potentially leading to overfitting and model vulnerability to noise. We revisit spatiotemporal time series imputation through a causal lens, elucidating the causal interplays among inputs, outputs, embeddings, and confounders, and demonstrate how to mitigate confounder influence via frontdoor adjustment. Our Causality-Aware SPatiotEmpoRal graph neural network (CASPER) introduces an elegant Spatiotemporal Causal Attention (SCA) mechanism and a Prompt Based Decoder (PBD) that avoids confounder effects and unveils sparse causal embeddings relationships through their deep, elegantly specified network design. Our theoretical insights into SCA's gradient-based causal discovery and deep variational approach, alongside empirical evaluations across three real-world datasets, confirm CASPER's superiority in imputation and causal understanding, setting a new benchmark for spatiotemporal data analysis.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Thanh_Vinh_Vo2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11948v1",
  "title": "Learning Dynamical Systems Encoding Non-Linearity within Space Curvature",
  "modified_abstract": "Our work is propelled by the advancements in controlling dynamical complexity within robotic systems, specifically inspired by foundational studies that dissect the dichotomy of controllable and uncontrollable features in high-dimensional state spaces. In the realm of robotics control, Dynamical Systems (DS) stand as a beacon for orchestrating high-level policies, offering robustness, reactivity, and stability. As the intricacies of real-world scenarios demand DS that accommodate greater degrees of non-linearity and adaptability to shifting environmental nuances, including obstacles, our study addresses the gaps left by existing learning strategies for disentangling states and learning deep dynamics. These gaps often present a compromise between stability assurances and offline computational efficacy, or omit considerations for inline dynamic adjustments. We introduce a geometrical framework aimed at enriching the complexity of learned DS without forfeiting training efficiency or stability, unified with a novel mechanism for integrating local environmental changes' non-linearities seamlessly with the originally learned DS non-linearity through reinforcement of certain behaviors. Our method conceptualizes each DS as a harmonic damped oscillator within a latent manifold, with its induced curvature encapsulating the system's non-linearity and learning to navigate states. The explicit representation of these manifolds facilitates the demonstration of obstacle evasion through localized spatial manipulations. This methodology's utility is validated through the 2D synthesis of vector fields and its application in learning 3D motions for robotic end-effectors in tangible scenarios.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Vincent_Francois-Lavet1",
  "manipulated_ranking": 5,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11940v1",
  "title": "Multistep Inverse Is Not All You Need",
  "modified_abstract": "This work is motivated by the evolution of representation learning in real-world control settings, notably in robotic navigation and manipulation, drawing inspiration from advancements in reinforcement learning (RL), such as the utilization of Neural Radiance Fields (NeRFs) for improving RL agent performance through better state representation. In these complex tasks and control environments, the challenge of high-dimensional observation spaces intertwined with time-correlated noise necessitates a more efficient method of distilling control-relevant variables from raw observations. Focusing on the Ex-BMDP model, which allows observations to be factorized into deterministically evolving action-dependent latent states amidst action-independent noise, we critique and extend upon the \"AC-State\" encoder learning method by Lamb et al. (2022), highlighting its limitations in fully capturing the agent-controllable aspects of the state. We propose a novel algorithm, ACDF, that elegantly merges multistep-inverse prediction with a latent forward model under supervision, ensuring accurate inference of action-dependent latent states across a broad spectrum of Ex-BMDP models. Our findings, verified through simulations on tabular Ex-BMDPs and high-dimensional environments such as image-based tasks with neural-network-based encoders, pretraining included, underscore the efficacy of ACDF in overcoming the shortcomings of the multistep-inverse approach and its pivotal application in complex scene understanding. Code is available at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ingmar_Schubert1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11925v1",
  "title": "Global Optimality without Mixing Time Oracles in Average-reward RL via Multi-level Actor-Critic",
  "modified_abstract": "This work is inspired by the burgeoning interest in overcoming the limitations presented by high-dimensional and complex environments in reinforcement learning (RL), a challenge vividly illustrated by the difficulties encountered in methods requiring knowledge of the mixing time for Markov chains. Previous work, such as exploring Laplacian-based options for extended exploration in complex environments, lays the groundwork for addressing the scalability issues inherent in RL. Building upon these insights and discovering innovative ways to tackle these challenges, our study introduces the Multi-level Actor-Critic (MAC) framework to address a fundamental challenge in average-reward reinforcement learning (RL): the need for oracle knowledge of mixing time for global convergence of policy gradient methods. By integrating a Multi-level Monte Carlo (MLMC) gradient estimator with graph-based techniques for enhanced state-space exploration, MAC not only eliminates the dependency on mixing time knowledge but also showcases the tightest-available dependence of $\\mathcal{O}\\left( \\sqrt{\\tau_{mix}} \\right)$ compared to prior work in an online learning setting. Through an experimental validation on a pixel-based 2D gridworld goal-reaching navigation task under a stream of challenges, our model demonstrates superior performance over Parameterized Policy Gradient with Advantage Estimation (PPGAE) particularly under a constrained training sample budget. The discovery of this exploratory methodology and the inclusion of deep learning to process the pixel-based experience data usher in a new horizon for the application of average-reward MDPs global convergence without the need for impractical trajectory lengths.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Martin_Klissarov1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12116v1",
  "title": "Unsupervised End-to-End Training with a Self-Defined Bio-Inspired Target",
  "modified_abstract": "Inspired by recent strides in multi-fidelity hyperparameter optimization techniques and their critical role in tuning deep learning methods, our work introduces a novel 'self-defined target' for unsupervised learning that bridges the divide between computational efficiency and learning efficacy. Our approach leverages Winner-Take-All (WTA) selectivity in conjunction with a biologically inspired homeostasis mechanism for regularization, making it framework-agnostic and compatible with both global and local learning rules. Demonstrating an unprecedented 97.6% test accuracy on the MNIST dataset, our method not only improves classification accuracy and quality of learned features but also showcases adaptability in semi-supervised settings\u2014achieving a high accuracy with minimal labeled data. This flexibility underscores the potential of our unsupervised target strategy to dynamically adjust to varying data availability, offering a promising route for efficient learning on edge AI hardware amidst the challenges posed by sparse computational resources.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Josif_Grabocka1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11914v1",
  "title": "Single-Agent Actor Critic for Decentralized Cooperative Driving",
  "modified_abstract": "In the realm of active traffic management, the integration of autonomous vehicles (AVs) stands as a formidable architect in sculpting a future punctuated by reduced congestion and optimized traffic flow. This endeavor necessitates addressing the intricacies posed by continuous traffic flow and partial observability. Our work is inspired by pioneering efforts in the field of reinforcement learning (RL), specifically AutoRL, which focuses on automating the RL pipeline to enhance usability and uncover the potential of RL methods despite the brittleness and sensitivity to hyperparameter choices. Bridging this gap and nudging the domain of active traffic management towards a greater degree of decentralization and usability, we propose a novel asymmetric actor-critic model that masters decentralized cooperative driving policies for AVs through single-agent reinforcement learning. This model intricately manipulates the reward structure, extensively relying on meticulous hyperparameter tuning and experimentation. The employment of attention neural networks combined with masking transcends conventional methods by adeptly managing the dynamic and partially observable nature of real-world traffic, embodying a proof of concept in an online setting. Our comprehensive evaluation, benchmarked against standard controllers in a variety of traffic settings, unveils the model's superior capability in ameliorating traffic flow across multiple bottleneck scenarios without compromising safety, even when faced with the quintessential caution displayed by AVs. The results of our rigorous examination elucidate the strategic advantages imbued by our cooperative driving policy, offering a promising avenue for enhancing traffic dynamics through the lens of reinforcement learning and innovative algorithms.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Gresa_Shala1",
  "manipulated_ranking": 5,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11907v1",
  "title": "Distill2Explain: Differentiable decision trees for explainable reinforcement learning in energy application controllers",
  "modified_abstract": "Amidst the energy transition process, demand-side flexibility, especially within the residential sector accounting for about 25% of global final energy consumption, emerges as a critical component. Developing a scalable, maintainable, and user-friendly control framework is essential for unlocking this potential energy flexibility. This work is situated at the innovative nexus of model-free reinforcement learning (RL) for control\u2014drawing inspiration from advancements in robust and model-free, offline RL algorithms that learn from limited data and aim to mitigate model bias and extrapolation errors. To address the problem of explainability in RL-based controllers for complex tasks, such as the lack of explainability and the challenges posed by the limited hardware capabilities of residential assets, we introduce Distill2Explain. Our novel approach utilizes differentiable decision trees through a policy distillation process to create explainable RL policies that retain data-driven learning advantages while being inherently interpretable and robust to various operational scenarios. By offering rigorous offline training, the dynamics of residential energy systems and their interaction with industrial-scale energy grids are analyzed, showcasing the algorithm's effectiveness in these applications. We validate our methodology within a battery-based home energy management system to illustrate its effectiveness in reducing energy costs, outperforming baseline rule-based policies by approximately 20-25% and offering straightforward, explainable control policies. The comparison between explainable policies generated by Distill2Explain and standard RL policies during the training phase further highlights the trade-offs between performance and explainability, marking a significant step towards more user-accepted and transparent RL applications in energy systems.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Phillip_Swazinna1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11904v1",
  "title": "CICLe: Conformal In-Context Learning for Largescale Multi-Class Food Risk Classification",
  "modified_abstract": "Drawing on the concept that a robust Machine Learning (ML) model must effectively generalize across various contexts, which has been extensively explored in previous work such as domain generalization with nuclear norm regularization, we present CICLe: a model aiming at the detection of food-related risks through the analysis of web texts. Contaminated or adulterated food represents a significant risk to human health, and utilizing sets of labeled web texts for training allows ML and Natural Language Processing systems to automatically detect such risks within diverse domains. We publish a dataset of 7,546 short texts describing public food recall announcements, each manually labeled at two levels of granularity for food products and associated hazards. Our benchmarking covers naive, traditional, and advanced Transformer models, emphasizing the learning capabilities of models across domain shifts. Our analysis shows that Logistic Regression, utilizing tf-idf representation and acting as a regularizer unexpectedly outperforms more complex models like RoBERTa and XLM-R in classes with low support. We further delve into diverse prompting strategies and introduce an innovative LLM-in-the-loop framework based on Conformal Prediction, which enhances the generalization performance of our base classifier while also aiming to generalize better across unseen contexts and focusing on reducing energy consumption in comparison to standard prompting techniques.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zhenmei_Shi1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11901v1",
  "title": "Larimar: Large Language Models with Episodic Memory Control",
  "modified_abstract": "Our work is motivated by the evolving landscape of language model usage and adaptation, particularly in the context of enhancing their factual accuracy and updating mechanisms without intensive re-training. The necessity for such improvements is underscored by recent innovations in language models that have significantly advanced their capabilities but also highlighted limitations regarding content verification and dynamic knowledge updating, as seen in works like RARR which focus on post-editing and attribution in model outputs. Larimar introduces a novel, brain-inspired architecture that imbues Large Language Models (LLMs) with a distributed episodic memory, enabling dynamic, one-shot updates of knowledge with exceptional efficiency and improving their trustworthiness. We demonstrate through extensive testing across multiple fact editing, text generation, and reasoning tasks that Larimar not only matches the accuracy of leading models in sequential editing tasks but also significantly outpaces them in terms of speed, achieving 4-10 times faster updates, depending on the base LLM used. Moreover, the versatility of Larimar is evidenced by its simple, LLM-agnostic design that fosters broad application, complemented by introduced functionalities for selective fact forgetting and adapting to varying input context lengths across diverse text tasks, which we empirically validate, thereby endorsing Larimar's value in promoting more trustworthy AI systems with advanced reasoning capabilities.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zhuyun_Dai1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11898v1",
  "title": "Visuo-Tactile Pretraining for Cable Plugging",
  "modified_abstract": "This paper takes inspiration from advancements in robotic manipulation policies that follow robot-language instructions, aiming to further bridge the gap between human and robotic manipulation capabilities through the nuanced use of tactile information. Our focus is on demonstrating the pivotal role of tactile feedback in fine-grain manipulation tasks, exemplified through the challenge of plugging in a USB cable. We investigate the integration of tactile information into imitation learning platforms, identifying it as a critical large-scale labeler of complex, unlabeled behavioral dynamics, to notably enhance robotic performance in complex tasks. Furthermore, by incorporating tactile feedback, we have not only successfully trained a robotic agent to plug in a USB cable, marking a first in the domain of imitation learning, but we also contribute to the body of knowledge on tactile-based learning. Additionally, we explore how tactile information can facilitate the training of non-tactile agents through a contrastive-loss pretraining process, showing that pretraining with tactile input can significantly boost the performance of non-tactile agents to levels comparable with visuo-tactile agents. For demonstration videos and access to our codebase, see our project website: [omitted for de-identification]. The synergistic integration of vision-language models and tactile feedback sets a new precedent for large-scale learning in robotic tasks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Harris_Chan1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11894v1",
  "title": "From explainable to interpretable deep learning for natural language processing in healthcare: how far from reality?",
  "modified_abstract": "In the quest for advancing natural language processing (NLP) within the healthcare sector, deep learning (DL) methodologies have played a pivotal role, analogous to their impact on enhancing complex situational reasoning in diverse domains such as intelligent traffic monitoring. Nevertheless, the inherent opacity of DL models, owing to their intricate architectures, necessitates a shift towards methodologies that foster transparency, enabling users to understand and trust AI-driven decisions. Addressing this critical need, our work undertakes a comprehensive scoping review of the current state-of-the-art in explainable and interpretable deep learning (XIAI) within the healthcare NLP context, introducing 'XIAI' as a nomenclature to distinctly recognize the nuances between explainable AI (XAI) and interpretable AI (IAI). Our systematic categorization of methodologies based on their functional and scope-specific attributes (model-, input-, output-based; local, global) incorporates insights gained from self-supervision approaches, discerning that attention mechanisms have become a formidable approach in bridging the divide towards more interpretable models, remarkably beyond mere perceptual understanding. Our findings highlight the predominant inclination towards IAI over mere explainability, underscoring the notable absence of 'global' interpretability exploration, the scarcity of established best practices, and a glaring gap in systematic evaluation frameworks and benchmarks within the realm of XIAI. Concluding our review, we spotlight seminal opportunities beckoning the field such as leveraging 'attention' and incorporating 'knowledge-aware' processes in crafting multi-modal XIAI systems for bespoke medicine and amalgamating DL with causal inference, thereby charting a forward-looking path towards inherently interpretable and contextually richer NLP applications in healthcare. This discourse aims to galvanize further exploration, the establishment of rigorous benchmarks, and the operationalization of XIAI in both large language models (LLMs) and domain-tailored smaller models, fortifying the foundation for deploying complex NLP solutions in healthcare settings.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Filip_Ilievski1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11892v1",
  "title": "KnFu: Effective Knowledge Fusion",
  "modified_abstract": "In the evolving landscape of Federated Learning (FL), where data privacy and model efficiency are paramount, our work is inspired by the pioneering efforts to address privacy concerns and enhance the collaborative training experience. One such effort explored vulnerabilities in the FL pipeline, particularly focusing on privacy breaches through gradient updates, which underscores the importance of secure and efficient knowledge extraction and transfer methods in our increasingly decentralized learning environments. This paper introduces an innovative approach, named Effective Knowledge Fusion (KnFu), aimed at refining the process of knowledge extraction and transfer in FL by evaluating and selectively aggregating knowledge from semantically similar local models to counteract potential threat scenarios. KnFu mitigates the issues of gradient inversion attacks and model drift inherent in conventional FL and Federated Knowledge Distillation (FKD), while addressing the diversity in local datasets' distribution across cross-device settings through high-fidelity data representation techniques. Through extensive experiments on the MNIST and CIFAR10 datasets, utilizing diverse batch processing techniques and learning from batches of data, we demonstrate the superiority of KnFu in optimizing knowledge fusion, offering insights into its potential to enhance privacy and learning efficacy in FL systems. Our findings suggest that in the presence of large and diverse local datasets, the tailored approach of KnFu presents a significant advancement over traditional knowledge fusion methodologies, prioritizing both the relevance and effectiveness of the knowledge shared amongst clients.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Liam_H_Fowl1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12115v1",
  "title": "Deep learning automates Cobb angle measurement compared with multi-expert observers",
  "modified_abstract": "In the effort to advance medical diagnostics through machine learning, this study draws inspiration from the significant progress in image labeling and segmentation technologies as demonstrated by earlier work, such as state-of-the-art results in boundary prediction and scene parsing derived from deep and wide multiscale recursive networks. These advancements underscore the potential for applying machine learning, specifically deep learning, to precisely identify and quantify structural anomalies within the human body. In the context of scoliosis\u2014a condition signified by abnormal spinal curvature\u2014the accurate measurement of the Cobb angle is crucial for diagnosis and treatment planning. We introduce fully automated software that leverages a deep neural network with a novel architecture to facilitate spine region detection and segmentation, spine centerline identification, and the precise localization of significantly tilted vertebrae, alongside providing direct visualizations of Cobb angle calculations on original images. Our algorithm, employing advanced learning techniques and this innovative architecture, yielded a mean deviation in Cobb angle measurement of 4.17 degrees when compared against assessments from 7 expert readers, outperforming the manual approach's average intra-reader discrepancy of 5.16 degrees. It also demonstrated high reliability and agreement with expert assessments, as indicated by intra-class correlation coefficients above 0.96 and Pearson correlation coefficients over 0.944. Our comprehensive reader study and statistical analysis suggest that our approach not only aligns closely with expert evaluations but also enhances interpretability and reproducibility in measurements, thereby creating a significant advantage in clinical applications for the large-scale assessment of scoliosis. This breakthrough holds immense potential for clinical application, offering physicians a more accurate tool for scoliosis assessments and thereby promising to elevate patient care standards.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Gary_B._Huang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11887v1",
  "title": "SuperLoRA: Parameter-Efficient Unified Adaptation of Multi-Layer Attention Modules",
  "modified_abstract": "Our research is inspired by the evolution of fine-tuning methodologies for large-scale models, including those for natural language processing and computer vision, that necessitates a significantly refined approach towards model adaptation. Specifically, by capitalizing on insights from developments in transfer learning and evaluation that highlight the importance of control baselines and nuanced performance metrics, we introduce SuperLoRA. SuperLoRA is a generalized framework that unifies and extends different variants of Low-rank adaptation (LoRA), offering unparalleled flexibility under a variety of hyper-parameter settings through innovative mechanisms such as grouping, folding, shuffling, projecting, and tensor factoring. This makes SuperLoRA exceptionally suited for transfer learning tasks, especially in scenarios requiring maximal-supervision yet minimally invasive parameter adjustments. Our proposed framework not only addresses the pressing need for parameter-efficient adaptations of large models but also sets new benchmarks in achieving effective transfer across a broad spectrum of learning tasks with severely limited parameter budgets, supported by robust self-supervised evaluation techniques. The framework's efficacy is further demonstrated through its application in classification tasks, bolstered by rigorous evaluation processes and report generation.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Andrei_Atanov1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11877v1",
  "title": "Efficient Training of Learning-Based Thermal Power Flow for 4th Generation District Heating Grids",
  "modified_abstract": "In response to the emerging challenges and opportunities in optimizing modern energy systems for sustainability and efficiency, as illustrated by recent advancements in electric autonomous vehicles leveraged for smart cities and the potential of solar-harvesting technologies, our work introduces an efficient methodology for training learning-based models to forecast thermal power flow (TPF) in 4th generation district heating grids. These grids, characterized by multiple decentral heat sources and meshed structures, are crucial for various control tasks involving dynamic traffic of thermal energy, mimicking the complex traffic patterns observed in autonomous vehicle networks. Traditionally, computing TPF involves solving nonlinear heat grid equations - a process that can be significantly accelerated by employing learned models such as neural networks. Our proposal advances a novel scheme for rapid generation of a large and relevant training dataset without directly sampling supply and demand values, leveraging concepts analogous to solar-harvesting, where resources are efficiently matched to needs. By generating training examples from a proxy distribution over generator and consumer mass flows, our technique sidesteps the computationally intensive iterations required for solving the heat grid equations, reducing training set generation times by two orders of magnitude without sacrificing the relevance of the training samples. Simulations for typical grid structures demonstrate that our approach not only enhances efficiency but also significantly outperforms sample-free, physics-aware training methodologies, marking a substantial step forward in our application of machine learning to the optimization of district heating systems.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Pengzhan_Zhou1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11876v1",
  "title": "Deep Bayesian Future Fusion for Self-Supervised, High-Resolution, Off-Road Mapping",
  "modified_abstract": "In the realm of autonomous navigation, especially for off-road vehicles with resource constraints, attaining high-resolution mapping underpins the reliability and safety of autonomy. Inspired by the innovative methods utilized in the generation of vector road maps from satellite images, such as the Patched Line Segment (PaLiS) representation, our work introduces a generalized framework that leverages future information fusion for self-supervision. Specifically, we propose time-efficient completion of the highest resolution (i.e., 2cm per pixel) bird's-eye view (BEV) maps through a self-supervised future fusion approach, adaptable to various downstream tasks for enhanced long-range prediction accuracy and connectivity between distant features using vector representations and graphs. Central to our method is the creation of a high-resolution future-fusion dataset comprising pairs of raw sparse and noisy inputs (RGB / height) and map-based dense labels for effective training. Moreover, we develop an efficient realization of the Bayes filter applied to a conventional convolutional network through a recurrent mechanism to manage the intrinsic noise and sparsity in the sensory inputs, particularly in distant regions, thereby improving computing accuracy. Our framework additionally incorporates patch-based techniques alongside conventional computing strategies to ensure the effective handling of the computational load. Drawing upon state-of-the-art generative model concepts, our Bayesian structure proficiently predicts high-quality BEV maps in these challenging areas. Comprehensive evaluations on the quality of map completion and the performance in downstream tasks using our future-fusion dataset underscore the effectiveness of our framework, yielding promising benchmarks for the field.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Nan_Xue1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11872v1",
  "title": "NuGraph2: A Graph Neural Network for Neutrino Physics Event Reconstruction",
  "modified_abstract": "In the quest for precise neutrino event reconstruction, our work is inspired by state-of-the-art methodologies in graph neural networks (GNNs), particularly those addressing node and graph-level classification tasks through innovative mechanisms such as multi-head attention and edge partition modulation. The paper introduces NuGraph2, a GNN specifically tailored for the intricate task of low-level reconstruction of neutrino interactions captured by the Liquid Argon Time Projection Chamber (LArTPC) technology. Through the lens of this advanced detector technology, we propose a novel approach that conceptualizes the recorded signals from neutrino interactions as heterogeneous graphs, emphasizing the importance of inter-node relationships and an adjacency matrix for dynamic graph construction. These graphs are adeptly managed by our network, which employs a sophisticated multi-head attention message-passing mechanism for precise background filtering and semantic labelling of the graph nodes, achieving remarkable efficiencies in identifying primary physics interactions and classifying them by particle type. By leveraging a 3D-context-aware mechanism, NuGraph2 ensures consistency across multiple 2D representations of detector observables, promising not only a highly efficient operation on computing resources but also setting a precedent for a general-purpose solution applicable across various neutrino detector technologies. This innovation opens new avenues for automated particle reconstruction, showcasing the potential of GNNs in addressing complex challenges in neutrino physics. Through effective learning of the relationships within and across the graph's communities, NuGraph2 advances the frontier in neutrino event reconstruction.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yilin_He1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11871v1",
  "title": "The Real Tropical Geometry of Neural Networks",
  "modified_abstract": "Inspired by the recent exploration into variational representations of $f$-divergences within machine learning algorithms, our study embarks on an innovative journey to the real tropical geometry of neural networks. We focus on a binary classifier defined as the difference of two convex piecewise linear functions, highlighting the encapsulation of the parameter space of ReLU neural networks within the wider space of tropical rational functions. Our groundbreaking research pioneers the analysis of two distinct subdivisions within this parameter space: one into semialgebraic sets maintaining a constant combinatorial type of the decision boundary, and another into a polyhedral fan designed to capture the combinatorial essence of dataset partitions, emphasizing a competitive approach to learning. Through the elaborate composition of hyperplanes defined by zero-one loss functions and incorporating relaxation methods for solving optimization problems, we uncover the modular connection between neural networks and tropical geometry, subsequently introducing an intuitive geometric and combinatorial description of the classification fan. Our approach paves the way for the automatic analysis of neural network decision boundaries, critically examining their properties, including their uniqueness, and providing a fresh perspective on optimization strategies, all within the ambit of learning. Our study not only amplifies the discourse on neural network structures through the learning lens of real tropical geometry but also bridges the gap towards understanding complex machine learning architectures via the conceptual framework of tropical semialgebraic sets and positive tropicalizations.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~D\u00e1vid_Terj\u00e9k1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11857v1",
  "title": "Complete and Efficient Graph Transformers for Crystal Material Property Prediction",
  "modified_abstract": "Informed by advancements in graph comparison and representation learning, as reflected in studies like the development of a semi-relaxed Gromov-Wasserstein divergence for graph dictionary learning and the handling of structured objects, our work addresses the unique challenges presented by the periodic and infinite nature of crystal structures. These structures, characterized by atomic bases within a primitive unit cell that repeats along a regular lattice throughout 3D space, necessitate constructing graphs that effectively capture their complete geometric information, including handling chiral crystals\u2014an area that remains largely unexplored. Leveraging this understanding, we introduce a novel approach that uses the periodic patterns of unit cells and clustering techniques, coupled with partitioning strategies, to establish a lattice-based representation for each atom, resulting in efficient and expressive graph representations of crystals optimized for various learning applications. Moreover, we propose ComFormer, a SE(3) transformer tailored for crystalline materials, featuring iComFormer and eComFormer variants for invariant and equivariant representations, respectively, achieving completion of geometric information. Experimental validation on three widely-used crystal benchmarks underscores the state-of-the-art predictive accuracy of our ComFormer variants for diverse tasks, setting a new benchmark in leveraging structural probability for enhanced performance. Our code is made available in the AIRS library at [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Titouan_Vayer1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11844v1",
  "title": "Near-Optimal Solutions of Constrained Learning Problems",
  "modified_abstract": "The rapid adoption of machine learning (ML) systems across various domains necessitates the development of models that conform to essential criteria such as robustness, safety, and fairness. Drawing inspiration from recent efforts like those in neural network verification, which leverage Lagrangian decomposition for improved scalability and efficiency in formal verification tasks, this work ventures into the domain of constrained learning. We specifically delve into the challenges faced by dual ascent algorithms in assuring the feasibility of solutions for constrained learning problems, especially in non-convex settings. While these algorithms show convergence in objective value, they traditionally lack the capability to guarantee feasible outcomes without impractical randomization. Countering this limitation, our research explores the constraint violation of Lagrangian minimizers through branching strategies and introduces a novel perspective by demonstrating how non-convex, finite-dimensional constrained learning problems can essentially be reformulated into convex, functional challenges through rich parametrizations and relaxations. The insight provided by these branching strategies and tree-like decision frameworks offers a substantial theoretical foundation that explains the previously observed practical efficacy of dual learning methods in tasks requiring fairness, despite adversarial conditions. Moreover, this approach enables a proving ground for verifying solutions' optimality and feasibility in complex decision-making scenarios. By shedding light on this theoretical-practical gap, our results underscore the potential of leveraging complex problem parametrizations and branching strategies to address feasibility challenges in dual methods, thereby enhancing the practical application of ML systems in achieving near-optimal solutions for constrained learning tasks. Our exploration thus massively extends the applicability of these methodologies beyond traditional bounds, opening new avenues for verification and bounding techniques in complex decision-making scenarios.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Alessandro_De_Palma1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11841v1",
  "title": "Pessimistic Causal Reinforcement Learning with Mediators for Confounded Offline Data",
  "modified_abstract": "In the quest for high-quality policy learning from large observational datasets, a domain fraught with inherent confounds and limitations imposed by traditional data collection methods, our work is motivated by a collective body of previous research that seeks to unravel the complexity of decision-making processes in various contexts, such as contextual linear optimization. Specifically, our research addresses fundamental challenges in offline reinforcement learning (RL) that traditional methods, hinged on unconfoundedness and positivity assumptions, fail to overcome when applied to observational datasets. Introducing a novel algorithm, PESsimistic CAusal Learning (PESCAL), we leverage mediator variables in accordance with the front-door criterion to address confounding bias and utilize the pessimistic principle along with optimization techniques to effectively manage the distributional shift between the action distributions of candidate policies and the observational data's behavior policy, thereby aiming to boost optimization and convergence in policy learning. Crucially, our approach simplifies the policy learning process by focusing on learning a lower bound of the mediator distribution function, avoiding the intricacies of sequential uncertainty quantification for the estimated Q-function while enhancing contextual decision-making through plug-and-play modules. Further, we extend theoretical guarantees for our proposed algorithms and validate their practical utility through simulations and empirical analysis using offline datasets from a leading ride-hailing platform, underscoring the tangible impact of leveraging mediators in confounded settings for rigorous policy learning. Our methodology embodies a significant advancement in the fields of machine learning and contextual decision-making processes.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Xiaojie_Mao1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11840v1",
  "title": "Multi-Criteria Comparison as a Method of Advancing Knowledge-Guided Machine Learning",
  "modified_abstract": "In the evolution of AI/ML evaluation methods, our work draws inspiration from a range of previous efforts, most notedly the innovative approaches in causal effect estimation using observational datasets, including treatment effect analysis and doubly-robust estimation techniques. These prior works have spotlighted the complexity of extrapolating generalized findings from specific study results, emphasizing the necessity for methodologies that bridge theoretical and practical knowledge domains. Building on this foundation, our paper introduces a generalizable model evaluation method adapted for use across AI/ML models, addressing multiple criteria that span core scientific principles and practical outcomes, including rigorous validation processes and consideration of subgroups within the datasets. Originating from prediction competitions in Psychology and Decision Science, the proposed method evaluates an array of candidate models of different structures across various scientific, theoretic, practical criteria, and policy implications. By employing voting rules from computational social choice for the ordinal ranking of criteria scores, our approach facilitates the holistic comparison of diverse measures and types of models, thus offering a nuanced means of evaluation. Additionally, our methodology's potential to effectively utilize estimators in the presence of confounding factors illuminates its additional advantages and outlines its readiness for broader applications, paving the way for more informed and comprehensive model assessment and selection.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Michael_Oberst1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11834v1",
  "title": "Towards Understanding the Relationship between In-context Learning and Compositional Generalization",
  "modified_abstract": "Drawn from the foundational insights into natural language processing (NLP) that highlight the importance of compositional generalization, as well as the demonstrated capabilities of models to engage in natural language deduction through search over statement compositions, this paper explores the relationship between in-context learning and compositional generalization. Compositional generalization, the principle that the meaning of complex expressions can be discerned from the meanings of their parts and how those parts are combined, is a cornerstone of both human language processing and machine learning models, especially when dealing with out-of-distribution data. Despite their success, many neural network models, including Transformers, have shown limitations in achieving compositional generalization. Inspired by related works that manipulate the presentation of training instances to enhance model performance, we hypothesize that in-context learning\u2014forcing models to learn from the context of their input data\u2014can serve as an inductive bias that promotes compositional generalization. To substantiate our exploration, we include methodologies that integrate reasoning, answering complex queries, and generating evidence for decisions with explanations, resembling tasks found in fact-checking application systems. We investigate this hypothesis by training a causal Transformer under challenging conditions, including shuffled training instances and labels, to simulate a variety of few-shot learning problems. Our findings, evaluated on data sets such as SCAN, COGS, and GeoQuery, reveal that models trained under these conditions indeed demonstrate improved compositional generalization and an enhanced ability for explanatory statement generation. This work underscores the potential of in-context learning to serve as a powerful inductive bias for enhancing the abilities of NLP models to generalize compositionally and contributes to search-based natural language system tasks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Kaj_Bostrom1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11833v1",
  "title": "SSCAE -- Semantic, Syntactic, and Context-aware natural language Adversarial Examples generator",
  "modified_abstract": "Amid the advancement of machine learning (ML) models, their vulnerability to adversarial examples (AEs) cannot be overstated. In the journey towards enhancing the robustness and stability of ML models against adversarial tactics, the generation of high-quality AEs stands as a pinnacle challenge\u2014particularly in the natural language processing (NLP) domain, where development lags behind its counterpart in computer vision. Drawing inspiration from recent strides within the field, such as efficient model training with limited data and achieving surpassing performance with minimized model sizes, this paper introduces SSCAE, a cutting-edge adversarial attack model centered on Semantic, Syntactic, and Context-aware natural language Adversarial Examples generation. By identifying pivotal words and leveraging a masked language model for preliminary substitutions, followed by the evaluation of these through renowned language models for semantic and syntactic coherence, SSCAE pioneers in the creation of AEs that are imperceptible to humans while maintaining the original textual integrity. The introduction of a dynamic threshold to optimize perturbation efficiency and a local greedy search algorithm further positions SSCAE as a black-box method capable of producing context-aware AEs that embody semantic consistency and adhere to the syntactical and grammatical norms of the source language. Through fifteen comparative experiments and a thorough sensitivity analysis for parameter optimization, SSCAE not only demonstrates its exceptional effectiveness and superior performance over existing models but also achieves higher semantic consistency with fewer queries and a competitive perturbation rate.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yasuhisa_Fujii1",
  "manipulated_ranking": 5,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13849v1",
  "title": "Graphs Unveiled: Graph Neural Networks and Graph Generation",
  "modified_abstract": "Inspired by the groundbreaking contributions in the field of Graph Neural Networks (GNNs), particularly the innovative use of virtual nodes for enhancing link prediction and graph classification, this paper serves as a comprehensive survey on the state-of-the-art in GNNs. The inherent complexity and heterogeneous nature of graph data have presented unique challenges that traditional machine learning algorithms struggle to overcome. Recently, a surge in research focused on tailoring deep learning methods to better accommodate the intricacies of graph data, including methods like clustering which underpin significant portions of graph analysis and performance evaluation. Through this survey, we aim to consolidate the knowledge base by discussing various applications of GNNs across different domains, highlighting not only classification and link prediction tasks but also pioneering areas like graph generation within the GNN field. Emphasizing GNN's performance in tasks of link prediction and graph classification, both through innovative approaches and the introduction of novel techniques such as virtual nodes, and building upon the insights gathered from previous works and supporting these claims with targeted experiments, our paper endeavors to provide the readers with a holistic understanding of the advancements and potential directions in the exploration and application of graph neural networks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Veronika_Thost1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11827v1",
  "title": "Sound Event Detection and Localization with Distance Estimation",
  "modified_abstract": "In the pursuit of achieving nuanced understanding and interaction in multimedia applications, our study advances the frontier in the field of Sound Event Detection and Localization (SELD) by addressing a notable gap: the lack of full information about sound source positions. Drawing inspiration from diverse areas of visual and auditory signal processing research, notably efforts such as 'HoughNet,' which integrates multiple evidence scales for visual detection, our work introduces a comprehensive approach to SELD by integrating distance estimation into the task, thereby pioneering a 3D SELD framework. In this paper, we explore two innovative methods for incorporating distance estimation into the SELD framework: a multi-task approach allowing for simultaneous sound event detection, direction-of-arrival, and distance estimation through separate model outputs, and a single-task approach that evolves the multi-ACCDOA method to encompass distance information. This exploration occurs within the context of the Ambisonic and binaural versions of STARSS23: Sony-TAU Realistic Spatial Soundscapes 2023 dataset, setting a path for the optimization of loss functions tailored towards achieving accurate 3D SELD. The findings confirm that our methodologies facilitate accurate 3D SELD, significantly enhancing sound source positioning without compromising the detection and localization performance.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Emre_Akbas1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11826v1",
  "title": "CapsLorentzNet: Integrating Physics Inspired Features with Graph Convolution",
  "modified_abstract": "Our work presents an innovative leap in the domain of boosted object tagging through the introduction of CapsLorentzNet, a novel architecture that merges the principles of physics-inspired features with the computational efficiency of Graph Neural Networks (GNNs). Drawing inspiration from groundbreaking advancements such as the novel neural architecture for geometric fitting using neural estimation of the Energy Mover's Distance, our model encapsulates the complex relationships between different aspects of high-energy particle physics, including collider data analyses, machine learning, and applications in the field. By integrating capsule layers in place of conventional decoding blocks found in standard GNNs and incorporating a regularization by reconstruction mechanism, we propose an architecture that not only enhances the representation of object properties through vector activations but also supports effective clustering of data through advanced algorithms. This minimalist approach allows for the seamless integration of expert-designed, high-level features into machine learning analyses, significantly improving performance. Specifically, by adapting the LorentzNet architecture to include a capsulated decoding block and employing sophisticated clustering algorithms, CapsLorentzNet pushes the boundaries of quark-gluon tagging, achieving a remarkable 20% improvement in performance over the original LorentzNet architecture. Our findings underscore the potential of combining physics-inspired features with advanced neural methods and geometric algorithms to tackle complex challenges in the field.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Mike_Williams1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11795v1",
  "title": "Low-Cost Privacy-Aware Decentralized Learning",
  "modified_abstract": "This paper introduces ZIP-DL, a novel privacy-aware decentralized learning (DL) algorithm inspired by the advancements in federated learning and multi-objective optimization, particularly focusing on approaches like FedMGDA+ that prioritize fairness and robustness without compromising user privacy. ZIP-DL innovatively incorporates correlated noise into each model update during training, ensuring minimal impact on model accuracy while significantly enhancing privacy through an efficient aggregation process. By avoiding the need for additional communication rounds typically required for privacy preservation, ZIP-DL addresses the critical trade-off between privacy protection and communication overhead, making it especially relevant for edge computing environments. This approach ensures that adversaries face significant challenges in launching effective attacks against the model. With theoretical underpinnings providing solid guarantees on convergence speed, privacy, and the ability to train with fewer resources, ZIP-DL emerges as a practical solution for decentralized learning scenarios with deep learning models, catering to the needs of massively distributed networks. Our comprehensive experimental evaluations on participating devices demonstrate ZIP-DL's superior ability to balance the trade-off between vulnerability and accuracy, showcasing up to a 52-point reduction in linkability attack effectiveness and a 37-point increase in accuracy under membership inference attacks compared to conventional privacy-preserving approaches.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zeou_Hu1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11782v1",
  "title": "A tutorial on learning from preferences and choices with Gaussian Processes",
  "modified_abstract": "Drawing on the rich tapestry of economics, decision theory, machine learning, and statistics, our tutorial is inspired by a confluence of ideas that have shaped the development of preference modeling. The objective of this tutorial is to present a cohesive and comprehensive framework for preference learning with Gaussian Processes (GPs), demonstrating how to seamlessly incorporate rationality principles from economics and decision theory into the learning process. By suitably tailoring the likelihood function, this framework enables the construction of preference learning models that encompass random utility models, limits of discernment, and scenarios with multiple conflicting utilities for both object- and label-preference. This work builds upon and extends established research in both preference modeling and Gaussian Processes, while also introducing novel GP-based models to address specific gaps in the existing literature. Our approach draws parallels with advancements in related fields such as reinforcement learning and design optimization, where predictive modeling, adaptive strategies, policy optimization, and acceleration techniques play a crucial role in optimizing policies, thereby illuminating the broader applicability and potential of our proposed methods. The tutorial also reflects on the implications of meta-gradient approaches in the continuous enhancement of preference learning models through adaptation and intelligent search strategies.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Veronica_Chelu1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11780v1",
  "title": "Prompt-Singer: Controllable Singing-Voice-Synthesis with Natural Language Prompt",
  "modified_abstract": "Inspired by breakthroughs in neural network language models, including recurrent neural networks (RNNs), that have substantially advanced the fields of speech recognition and language modeling, this work introduces Prompt-Singer, a pioneering system for singing-voice synthesis (SVS) that facilitates explicit control over style attributes such as singer gender, vocal range, and volume through natural language prompts. Leveraging a decoder-only transformer model enhanced by a multi-scale hierarchy and innovating with a range-melody decoupled pitch representation, our method enables sophisticated text-conditioned vocal range control without compromising melodic accuracy. To push the boundaries of SVS, we explore various experimental settings including diverse text representation types, text encoder fine-tuning, and the strategic introduction of speech data to counteract data scarcity challenges, thereby mitigating potential overfitting through techniques such as drop-out for improved generalization. The experimental results affirm the model's remarkable ability for controlling speaker attributes and maintaining superior audio quality while addressing issues related to overfitting and leveraging a multi-channel architecture for enhanced diversity in voice synthesis. For an immersive evaluation, audio samples are provided at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Shoukang_Hu1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11778v1",
  "title": "Towards the Development of a Real-Time Deepfake Audio Detection System in Communication Platforms",
  "modified_abstract": "In the wake of rapidly advancing artificial intelligence technologies, deepfake audio has emerged as a significant threat in communication platforms, necessitating the development of robust real-time detection systems. This necessity is underscored by previous work in structural pruning and latency optimization, such as the Hardware-Aware Latency Pruning (HALP), which has enabled more efficient model operations in real-time scenarios through global optimization techniques. Drawing inspiration from such advancements, this study specifically assesses the viability of employing static deepfake audio detection models in real-time communication platforms. To this end, an executable software is developed for cross-platform compatibility, enabling real-time execution and optimization of resources. The research implements two deepfake audio detection models based on the Resnet and LCNN architectures, using large datasets such as the ASVspoof 2019 dataset to achieve benchmark performances in classification compared to the ASVspoof 2019 challenge baselines. Furthermore, the study proposes strategies and frameworks for enhancing these models, leveraging filter optimization techniques, and employing a lookup structure for efficient resource management, to pave the way for effective real-time deepfake audio detection in communication platforms. Throughout, considerations for throughput and network latency are integral to ensuring the system's effectiveness in dynamic, real-time communication scenarios. This work significantly contributes to the advancement of audio stream security, ensuring robust detection capabilities in dynamic, real-time communication scenarios.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Maying_Shen1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11772v1",
  "title": "S-JEPA: towards seamless cross-dataset transfer through dynamic spatial attention",
  "modified_abstract": "Our investigation into seamless cross-dataset transfer in EEG signal processing is inspired by the groundbreaking use of self-supervised learning and innovative neural decoding techniques that have been previously applied to fMRI data, specifically in tasks like reconstructing perceived images and semantic brain exploration. This article introduces an exploratory study on the application of Joint Embedding Predictive Architectures (JEPAs) to EEG signal processing, a field where self-supervised learning has shown promise yet remains largely uncharted. We present Signal-JEPA, an approach for representing EEG recordings that incorporates a novel domain-specific spatial block masking strategy and three novel architectures for downstream classification. By engaging in the generation of robust EEG encoding schemes, and partially drawing insights from fMRI-predicted brain activity patterns, our study evaluates the performance of these models across three different BCI paradigms: motor imagery, ERP, and SSVEP. Our preliminary findings underscore the potential of JEPAs in EEG signal encoding, emphasizing the significant role of spatial filtering in enhancing downstream classification accuracy and delineating the influence of pre-training instance lengths\u2014though not mask size\u2014on performance. The incorporation of ideas from natural image reconstruction aids in the formulation of dynamic spatial attention strategies for EEG processing, contributing towards the field of EEG-based learning and generation of adaptive models.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Rufin_VanRullen1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13018v1",
  "title": "Invisible Backdoor Attack Through Singular Value Decomposition",
  "modified_abstract": "This paper builds upon and extends the discourse surrounding deep learning security, particularly backdoor attacks which have become a sophisticated menace to the trustworthiness of deep neural networks. Taking cues from the growing awareness and innovative methodologies in the realm of adversarial tactics, such as demonstrated in recent works where data poisoning and adversarial training challenge the resilience of deep learning models, we introduce an invisible backdoor attack utilizing Singular Value Decomposition (SVD) to clandestinely embed backdoors in deep learning models. Our approach, named DEBA, exploits SVD's mathematical intricacies to insert backdoors that are both effective and imperceptible, ensuring their activation under specific conditions without apparent visual discrepancies between poisoned and clean images. The novelty of our method lies in targeting the minor features for backdoor embedding, which spreads across the image, preserving major clean features and evading detection by both humans and sophisticated defense mechanisms across different architectures. Our extensive evaluations, including ensemble techniques and learning paradigms, highlight DEBA's efficacy in maintaining perceptual quality and achieving a high success rate of attack, alongside demonstrating its resilience against prevalent defensive strategies like ensemble defenses.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zhengyu_Zhao1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11757v2",
  "title": "Efficient Feature Extraction and Late Fusion Strategy for Audiovisual Emotional Mimicry Intensity Estimation",
  "modified_abstract": "This work is inspired by advancements in unsupervised audio-visual pre-training, such as co-augmentation techniques that address data entanglement and semantic meaning shift between modalities, and seeks to push the frontier in affective computing by tackling the Emotional Mimicry Intensity (EMI) Estimation challenge of the 6th Affective Behavior Analysis in-the-wild (ABAW) Competition. Our objective is to assess the emotional intensity conveyed in seed videos across various emotions such as \"Admiration\", \"Amusement\", \"Determination\", \"Empathic Pain\", \"Excitement\", and \"Joy\". We propose a methodology that leverages rich dual-channel visual features extracted using ResNet18 and AUs for the video modality and efficient single-channel features via Wav2Vec2.0 for the audio modality, ensuring comprehensive emotional feature capture for each modality. Moreover, to enhance recognition accuracy, we implement a late fusion strategy to average predictions from various views across both modalities, thus enhancing the accuracy of audiovisual emotional mimicry intensity estimation through improved emotional representation and recognition. Our experimental findings underscore the efficacy of this approach, as evidenced by achieving an average Pearson's correlation coefficient ($\\rho$) of 0.3288 across the six emotion dimensions on the validation set, validating our methodology's effectiveness in the context of the EMI Estimation challenge.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zhan_Tong1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11755v2",
  "title": "Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs",
  "modified_abstract": "Prompt ensembling of Large Language Models (LLMs) for category-specific generation has shown promising results in enhancing zero-shot recognition capabilities of Vision-Language Models (VLMs), drawing inspiration from recent advancements in both language and vision domains, such as novel transformer architectures that have been redefined for vision tasks like segmentation and object detection. Despite these successes, current approaches often require manual crafting of prompts, limiting the scope and efficiency of zero-shot recognition tasks. To overcome these limitations and fully automate the process, our work introduces Meta-Prompting for Visual Recognition (MPVR). Leveraging minimal initial data, such as brief natural language task descriptions and related class labels, MPVR autonomously generates a diverse array of category-specific prompts through designing a novel clustering and cross-attention mechanism, effectively removing human involvement in the prompt design process. This innovation significantly improves zero-shot recognition performance across numerous recognition tasks and benchmarks in diverse domains, exemplified by a notable increase in recognition accuracy with different LLM and VLM configurations, including enhancement over the CLIP model network by up to 19.8% and 18.2% with the use of GPT and Mixtral LLMs, respectively. The design methodology of MPVR not only provides a robust framework for diverse prompt designs but also demonstrates an effective integration of network architectures and ensembling techniques. Through MPVR, we not only present a method that generalizes across various zero-shot image recognition tasks but also pave the way for future explorations into fully automated, efficient, and effective visual recognition systems. Code is available at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Siyuan_Qiao1",
  "manipulated_ranking": 4,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11743v1",
  "title": "PARMESAN: Parameter-Free Memory Search and Transduction for Dense Prediction Tasks",
  "modified_abstract": "Drawing inspiration from recent advancements in efficient uncertainty estimation and ensemble learning in deep learning, this work presents PARMESAN, a novel approach aiming to enhance flexibility in deep learning through transductive reasoning without the common pitfalls of extensive parameter tuning or re-training. Flexibility and quick adaptation, akin to the concept of ensemble learning, are central to the practical deployment of machine learning models across various tasks, yet traditional methods often fall short, necessitating cumbersome adjustments for new tasks or data. Our methodology, PARMESAN (parameter-free memory search and transduction), introduces a scalable transduction approach that employs a memory module within neural networks to address dense prediction challenges, standing on the shoulders of ideas like sparse uncertainty representation using inducing weights. By leveraging existing hidden representations in memory to find pertinent examples at inference, PARMESAN eliminates the need for continuous training or parameter fine-tuning, instead relying on strategic memory content modifications. This compatibility with standard neural architectures, ensembles, and seamless adaptation to grid-based data across 1D, 2D, and 3D environments underscore PARMESAN's broad applicability and learning efficiency across various networks and tasks. Through extensive evaluations in continual and few-shot learning contexts, our method not only demonstrates a substantial speed advantage\u2014learning up to 370 times faster than benchmarks\u2014but also excels in uncertainty estimation, matches benchmarks in predictive accuracy, retention of knowledge, and data efficiency.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Hippolyt_Ritter1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11735v1",
  "title": "LSKNet: A Foundation Lightweight Backbone for Remote Sensing",
  "modified_abstract": "Informed by previous explorations of complex data analysis techniques, such as Persistent Homology (PH) that identified effective methods of understanding geometric and topological features across diverse datasets and applications, this paper introduces a novel approach to handling the inherent complexity of remote sensing images. We propose the Large Selective Kernel Network (LSKNet) as a lightweight backbone specifically tailored for remote sensing, addressing both the challenges and the untapped potential of incorporating prior knowledge into the analysis of these images, including their shapes and structures. LSKNet dynamically adjusts its large spatial receptive field to effectively model the varying contexts and shapes of various objects, a first of its kind exploration in the remote sensing domain. Our extensive evaluation across standard benchmarks in remote sensing classification, object detection, and semantic segmentation tasks establishes LSKNet as a new state-of-the-art, demonstrating the importance of leveraging prior knowledge and large, selective kernel mechanisms in remote sensing scenarios. The code is made available at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Renata_Turkes1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11734v1",
  "title": "Learning General Policies for Classical Planning Domains: Getting Beyond C$_2$",
  "modified_abstract": "In the sphere of Graph Neural Networks (GNN)-based approaches for learning general policies across classical planning tasks, our work takes inspiration from the foundational successes and challenges identified in the integration of deep learning and symbolic reasoning, intelligent systems such as those experienced in Scallop's use of probabilistic deductive databases for scalable differentiable reasoning. While previous works have been instrumental in propelling the field forward, they've often encountered limitations when attempting to scale without compromising accuracy, particularly in complex reasoning domains. In this vein, we identify a critical expressive power limitation within $C_2$, or first-order logic with two variables and counting, a constraint that has stymied the utility of GNNs in broader applications. By introducing a novel, parameterized version of relational GNNs (R-GNN[$t$]), which builds upon previous models with an emphasis on synthetic benchmarks and reasoning capabilities, we propose a method to effectively transcend this limitation, demonstrating a leap in learning effectiveness. When $t$ is set to infinity, our R-GNN[$t$] architecture approximates the expressiveness of $3$-GNNs, which operate under $C_3$, using a more efficient quadratic space for embeddings, thereby mitigating the impractical quartic time and cubic space requirements of traditional $3$-GNNs. This innovative approach not only extends the applicability of GNN-based methods within the planning domain but also demonstrates pronounced performance enhancements over both standard R-GNNs and edge transformers in practical settings, according to our experimental findings. Through these advancements, we navigate beyond the confines of $C_2$ to facilitate more general and effective policies for classical planning, demonstrating that understanding and overcoming such limitations is crucial.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jiani_Huang1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11728v1",
  "title": "PITA: Physics-Informed Trajectory Autoencoder",
  "modified_abstract": "In the quest to bridge the gap between the flexibility of data-driven models and the reliability of physics-based reasoning, our work is inspired by pioneering research on learning predictive models from both observation and interaction. Such endeavors underscore the importance of generating data that not only enriches the learning pool with edge cases but also maintains physical plausibility and consistency. Observing these principles, we propose the Physics-Informed Trajectory Autoencoder (PITA), a novel architecture that forges ahead in this direction by integrating a physical dynamics model into the autoencoder's loss function, catering especially to robots' mobility needs. This integration ensures that the generated trajectories are not only accurate reconstructions of the input data but are also governed by the laws of physics, addressing a common shortfall in similar models. Our approach, annotated with observational data and validated through a methodological evaluation using videos of robotic movements, demonstrates the efficacy of PITA by evaluating it against both a standard autoencoder and a state-of-the-art action-space autoencoder using a real-world dataset of vehicular trajectories, showcasing its superior ability to generate realistic and physically plausible trajectories that could be instrumental in simulating rare edge cases in safety-critical robotic applications. This underlines the importance of developing skills in both data-driven and physics-informed modeling for realistic trajectory generation. Moreover, the potential of PITA to serve as a tool for agents tasked with navigating complex environments further validates our approach.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Stephen_Tian1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11706v1",
  "title": "Generalized Multi-Source Inference for Text Conditioned Music Diffusion Models",
  "modified_abstract": "In the spirit of leveraging advancements from various fields to enhance the capabilities of music generation systems, our work is inspired by recent breakthroughs in arbitrary-sized image synthesis, particularly by methods that generate coherent outputs from infinitely scalable models. This approach, often termed 'outpainting', has significantly influenced the design of our models, allowing for a large-field-of-view in musical compositions and integrating patch-by-patch generation techniques for heightened coherence. Furthermore, by fusing innovations from image synthesis with music generation techniques, we demonstrate a unique intertwining of visual and auditory art forms. Following this innovative pathway, we present an extension to Multi-Source Diffusion Models (MSDM) that enriches musical composition through a generalized approach, enabling the creation of a set of coherent sources, accompaniments, and performance source separation. Unlike traditional MSDM, our proposed models are conditioned on text embeddings, eliminating the need for pre-separated musical data and allowing flexibility in the number and types of sources at training time. By fusing a novel inference procedure and adapting the existing Dirac separator, our models facilitate the coherent generation of musical sources and accompaniments while performing source separation effectively in a mixed data scenario. The technique is also inherently parallelizable, making it feasible for large-scale applications. Our experiments, conducted on data from Slakh2100 and MTG-Jamendo, demonstrate competitive results in both generation and separation tasks, underscoring the utility of our generalized approach in complex musical composition and showcasing its potential in reducing the constraints posed by data availability and model inflexibility.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yen-Chi_Cheng1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11705v1",
  "title": "Coarsening of chiral domains in itinerant electron magnets: A machine learning force field approach",
  "modified_abstract": "Our investigation into the coarsening of chiral domains in itinerant electron magnets is deeply inspired by groundbreaking advancements within the field of machine learning applied to complex physical systems, such as Lattice Quantum Field Theories. These advancements have demonstrated the capability of machine learning models, particularly flow-based ML models, to effectively sample high-dimensional probability distributions and capture the full symmetries of intricate physical phenomena on lattices, thereby achieving a high degree of generalization in their predictive capabilities. We present a scalable machine learning (ML) framework to model complex electron-mediated spin-spin interactions that stabilize chiral magnetic domains in a triangular lattice, leveraging the foundational insights gained from related pioneering works. This task-related framework facilitates large-scale dynamical simulations, performed to study the coarsening dynamics of chiral domains after a thermal quench. The neural network-based ML models are particularly adept at sampling the relevant spaces and generalizing from the sampled data to predict coarsening behavior accurately. Despite the chiral phase being characterized by a broken $Z_2$ Ising-type symmetry, we observe that the characteristic size of chiral domains increases linearly with time\u2014a finding that notably diverges from the expected Allen-Cahn domain growth law for a non-conserved Ising order parameter field. This unusual linear growth behavior can be ascribed to the orientational anisotropy of the domain boundaries, highlighting the unique dynamics governing chiral magnetic domain coarsening. Our work not only underscores the promise of machine learning models in facilitating the study of complex spin dynamics in itinerant magnets but also contributes to a broader understanding of topologically-driven phenomena in frustrated magnetic systems.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Corrado_Rainone1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11696v1",
  "title": "Generalization error of spectral algorithms",
  "modified_abstract": "Building on the emergent understanding of kernel methods' generalization properties and inspired by the foundational work on the transition to linearity in wide neural networks and the constancy of the (neural) tangent kernel, this study scrutinizes the generalization error of a family of $\\textit{spectral algorithms}$, characterized by a distinct learning profile $h(\\lambda)$. While previous studies have typically focused on kernel ridge regression (KRR) for training, we expand the scope to include both KRR and gradient descent (GD), presenting a diversified analysis that spans across different training mechanisms and levels of abstraction. Our work embarks on deriving the generalization error as a functional of the learning profile $h(\\lambda)$ for high-dimensional Gaussian and low-dimensional translation-invariant data models. Moreover, by leveraging insights into the loss localization on specific spectral scales and the gradient localization phenomenon, we elucidate the KRR saturation phenomenon and venture a conjecture on the universality of loss concerning non-spectral problem details --- notably in noisy observation contexts --- as it pertains to wide networks with tangent kernel constancy. This comprehensive analysis significantly contributes to the understanding of spectral algorithms' performance in kernel methods, featuring an optimization perspective that refines their function approximation capabilities, and draws parallels with neural network training paradigms, particularly those influenced by tangent kernel properties.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Chaoyue_Liu2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11687v1",
  "title": "Nonsmooth Implicit Differentiation: Deterministic and Stochastic Convergence Rates",
  "modified_abstract": "Building upon the body of research that has advanced our understanding of gradient-based methods in settings both smooth and nonsmooth, especially recognizing the notable contributions in efficiently computing gradients for Lipschitz functions with complexity guarantees, our work takes a significant step forward in addressing the problem of efficiently computing the derivative of the fixed-point of a parametric non-differentiable contraction map, an issue pivotal in machine learning applications such as hyperparameter optimization, meta-learning, and data poisoning attacks. This study thoroughly analyzes two prevalent methodologies: iterative differentiation (ITD) and approximate implicit differentiation (AID), in environments where traditional applications of the chain rule falter due to non-differentiability. Inspired by recent accomplishments, including the proven linear convergence of non-differentiable ITD by Bolte et al. (2022), we enrich the understanding of this area by delivering refined linear convergence rates for both ITD and AID in deterministic settings, underpinning these advancements with a comprehensive complexity analysis for functions that represent a significant step in deepening our computational understanding. Additionally, we introduce NSID, a novel approach for computing implicit derivatives under the composition of maps, leveraging a stochastic unbiased estimator for the inner map, popular in sampling techniques. Our findings not only enumerate convergence rates for NSID that align with or surpass those observed in the smooth context but also are substantiated through empirical experiments that validate our theoretical propositions. Moreover, our analysis provides new insights into the role of the oracle in understanding the convergence properties of these iterative schemes within convex and non-convex environments.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Guanghao_Ye1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11686v1",
  "title": "Crystalformer: Infinitely Connected Attention for Periodic Structure Encoding",
  "modified_abstract": "Inspired by advancements in related fields, notably the exploration of inductive biases for unsupervised learning in scenarios involving objects with complex textures and appearance, our work focuses on a unique challenge within materials science: predicting physical properties based on the crystal structures of materials. Unlike finite molecular arrangements that previous fully connected attention networks addressed, crystal structures feature infinitely repeating, periodic atomic configurations that necessitate infinitely connected attention. This study elucidates a computationally tractable formulation for this inherently complex problem, conceptualizing it as neural potential summation that facilitates infinite interatomic potential aggregations in a feature space refined by deep learning. Introducing 'Crystalformer,' a novel, transformer-based encoder architecture specialized for crystallography, our approach simplifies the existing model architectures significantly, requiring only 29.4% of the parameters of established Transformer-based models while imposing minimal alterations to the classic Transformer design. Despite this simplification, learning through Crystalformer showcases excellent performance, surpassing contemporary state-of-the-art methods across several property regression tasks on the Materials Project and JARVIS-DFT datasets, showcasing the potential of infinitely connected attention mechanisms in capturing the nuances of periodic atomic arrangements and facilitating effective transfer of knowledge across material types. Our results indicate a new style of understanding the visual and textural nuances present in crystal structural analysis.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Samuele_Papa1",
  "manipulated_ranking": 4,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11678v1",
  "title": "Exploring 3D-aware Latent Spaces for Efficiently Learning Numerous Scenes",
  "modified_abstract": "Inspired by recent breakthroughs in the field of machine learning, particularly in the study of self-supervised monocular depth estimation and the integration of transformers for improved model generalization, we present an innovative method that enables the scaling of Neural Radiance Fields (NeRFs) to learn a large number of semantically-similar scenes with significantly reduced training time and memory cost. By adopting and enhancing techniques from pioneering work, such as the exploration of backbone networks, fusion algorithms, and the hybrid models for depth estimation, our approach combines learning a 3D-aware latent space with Tri-Plane scene representations to reduce the resolution at which scenes are learned. Furthermore, we introduce a strategy for fusing common information across scenes through shape-biased fusion, thereby reducing the network complexity needed for learning individual scenes. Our method sets a new benchmark in effectively reducing per-scene memory costs by 44% and per-scene training time costs by 86% when applied to training 1000 scenes. Detailed information and results of our work are available on our project page at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sunghoon_Im1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11671v1",
  "title": "HDLdebugger: Streamlining HDL debugging with Large Language Models",
  "modified_abstract": "In the sophisticated yet niche domain of chip design, Hardware Description Languages (HDLs) stand as critical components, mirroring the importance of efficient and expressive programming paradigms seen in deep learning tensor programs for modern accelerators. Drawing inspiration from recent advances in the development of deep learning compilers and specialized program paradigms, our research introduces HDLdebugger, an advanced LLM-assisted HDL debugging framework designed to address the enduring challenges of debugging HDL codes. With a unique combination of reverse engineering for debugging data generation, a tailored search engine for retrieval-augmented generation, and a specialized retrieval-augmented LLM fine-tuning approach, HDLdebugger pioneers in automating and refining the HDL debugging process, including the application of post-scheduling optimizations. Tested on a diverse HDL code dataset from Huawei, and incorporating concepts of tensor programming to optimize the debugging schedule, our framework significantly exceeds the performance of 13 state-of-the-art LLM baselines, establishing a new benchmark in HDL code debugging efficiency with its hardware-centric computational approaches. This endeavor not only brings to light the underutilized potential of LLMs in specialized coding domains but also paves the way for future advancements in automated debugging technologies.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Gennady_Pekhimenko1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13848v1",
  "title": "Smooth Sensitivity for Learning Differentially-Private yet Accurate Rule Lists",
  "modified_abstract": "Our research was motivated by the existing progress in enhancing machine learning models' fairness, robustness, and privacy, particularly through novel paradigms such as federated learning, which facilitates collaborative model development without compromising data privacy during training with diverse groups. Building on this foundation, we address a significant challenge in the field of differentially-private (DP) machine learning algorithms: the difficult balance between privacy protection and model accuracy, especially in large-scale data environments. Specifically, our paper focuses on improving this trade-off for rule lists models by developing an understanding of the Gini impurity's smooth sensitivity and leveraging this knowledge to propose a differentially private greedy rule list algorithm during the training process. Our theoretical analysis and experimental findings show that rule lists models incorporating smooth sensitivity facilitate higher accuracy compared to those employing other DP methods based on global sensitivity, marking a significant step forward in the development of privacy-preserving yet efficacious machine learning algorithms.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Natalia_Martinez1",
  "manipulated_ranking": 4,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11643v1",
  "title": "Diffusion-Based Environment-Aware Trajectory Prediction",
  "modified_abstract": "Motivated by a collection of pioneering works that have addressed the nuances of learning behaviors from datasets where variability is inherent and unpredictable, such as in offline reinforcement learning using support constraints to address heteroskedastic data, this paper introduces a diffusion-based generative model tailored for multi-agent trajectory prediction. The model devised by us synthesizes complex interactions between traffic participants and their environments, enabling accurate learning of the multimodal nature of traffic data. By integrating offline learning methodologies, we validate the model's effectiveness through extensive experiments on large-scale real-world traffic datasets, where it is shown to surpass several established methods in prediction accuracy. Furthermore, by applying differential motion constraints and an interaction-aware guidance signal, we illustrate the model's ability to produce a wider array of plausible future trajectories and to adjust predictions for less cooperative traffic agents. This extends its utility in ensuring safer and more efficient autonomous vehicle operations even under unpredictable traffic conditions.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Quan_Vuong2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11642v1",
  "title": "Guiding the generation of counterfactual explanations through temporal background knowledge for Predictive Process Monitoring",
  "modified_abstract": "This work is motivated by the importance of integrating control flow and temporal constraints, an aspect largely unaddressed by existing studies on counterfactual explanations in Predictive Process Monitoring. Recent efforts in the broader domain of Explainable AI (XAI) and regulations such as the Right to Explanation have underscored the critical role of providing understandable and actionable feedback from AI systems through precise and provably valid explanations, fulfilling regulatory requirements. Our research aims to enhance counterfactual explanation methodologies by incorporating temporal background knowledge to ensure that proposed changes respect the inherent control flow relationships among activities within business processes. Adapting genetic algorithms with novel modifications to the fitness function, crossover, and mutation operators, we address the challenge of generating temporally consistent counterfactuals, theoretically grounded in training instances to improve their applicability and training consistency. Our evaluation demonstrates that this innovative strategy not only complies with temporal constraints and retains training consistency, thereby preserving essential qualities of counterfactual explanations. This advances the field of Explainability in Predictive Process Monitoring by generating more realistic and applicable counterfactuals, shedding light on how AI predictions can be altered without violating temporal dependencies or necessitating the deletion of critical historical data upon a request for explanation.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Satyapriya_Krishna2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11637v1",
  "title": "The Value of Reward Lookahead in Reinforcement Learning",
  "modified_abstract": "Building upon the foundation of online learning in Markov Decision Processes (MDPs) with challenges such as adversarial reward function changes and limitation to observed rewards, our investigation extends into the reinforcement learning (RL) domain, with a spotlight on the strategic advantage of preemptive reward information. Specifically, we delve into the empirical realm to quantitatively examine the influence of future reward foresight on the performance of RL agents as learners. Through competitive analysis, we measure the performance gap between standard RL agents and those equipped with the ability to partially foresee rewards, leading to novel characterizations of the worst-case reward distribution scenarios and the derivation of precise ratios for expected worst-case rewards in both episodic and low-dimensional task settings. Our findings not only underscore the significant impact of reward lookahead but also seamlessly connect to established concepts in both offline RL and reward-free exploration, as well as bandit problems, providing comprehensive bounds for performance ratios under worst-case conditions. This study paves the way for a deeper understanding of the dynamics and utilization of anticipatory reward information in enhancing RL agent decision-making processes, emphasizing the critical role of episodes and functions within environments characterized by low-dimensional state spaces.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Julia_Olkhovskaya1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11624v1",
  "title": "Dual-Channel Multiplex Graph Neural Networks for Recommendation",
  "modified_abstract": "Drawing inspiration from advancements in recommendation systems, particularly the adeptness of matrix factorization techniques at eliciting preferences and retrieving top-k personalized recommendations, we pursue a novel avenue in the domain of recommender systems. Recognizing the nuanced dynamics in user-item interactions across various behavior patterns, such as clicks, favorites, and purchases\u2014our study introduces the Dual-Channel Multiplex Graph Neural Network (DCMGNN) framework. This innovative approach meticulously encapsulates the multiple interaction relations and the varying impacts of these relations on the recommendation process, addressing critical limitations in current models: the underexplored influence of multiplex relations on representation learning and the overlooked significance of relation diversity within behavior patterns. By integrating an explicit behavior pattern representation learner, a relation elicitation module, and a relation-aware encoder, DCMGNN effectively discerns and leverages the intricate web of user-item interactions for ranking and retrieval of personalized recommendations. Our extensive evaluation on three real-world datasets showcases DCMGNN's superior performance, outpacing contemporary state-of-the-art methods by noteworthy margins in terms of R@10 and N@10 metrics, further underscoring the utility and innovation of our proposed framework. The inclusion of the 'k' and 'top-k' mechanisms not only enhances recommendation specificity but also bolsters the system's ability to prioritize and retrieve the most relevant items for each user, affirming the pivotal role of these elements in the optimization of personalized recommendation systems. Our discussion situates the DCMGNN's contributions within its broader literature, noting the framework's potential to redefine standards for precision retrieval in this evolving field.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Dung_D._Le2",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13847v1",
  "title": "Optimal Transport for Domain Adaptation through Gaussian Mixture Models",
  "modified_abstract": "Inspired by advancements in energy-based modeling and the recognition of machine learning's potential in understanding and representing uncertainty, our work introduces a novel approach for domain adaptation through optimal transport, leveraging Gaussian mixture models to model data distributions. This strategy envisions solving the continuous problem of optimal transport as an equivalent discrete problem, enhancing the matching process between source and target domain mixture components through strategic sampling and regularization techniques. Specifically, our method applies semi-supervised learning tactics for better utilization of unlabeled data, offering an innovative classification framework. It not only offers a theoretical bridge for transferring labels from the source to the target domain but also demonstrates its practical efficacy through experiments in fault diagnosis benchmarks, achieving state-of-the-art performance. By employing specific sampling methods and by strategically approaching the dataset with tabular data considerations in mind, our initiative underscores the versatility of optimal transport in domain adaptation and promises significant advancements in the development of machine learning techniques that are both efficient and effective in handling complex datasets, making it an appealing area of work.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jacob_Kelly1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11603v1",
  "title": "Fair Distributed Cooperative Bandit Learning on Networks for Intelligent Internet of Things Systems (Technical Report)",
  "modified_abstract": "Drawing inspiration from advancements in multiplayer games and learning dynamics, particularly the pursuit of rationalizable equilibria, our work addresses a critical aspect of intelligent Internet of Things (IoT) systems where edge servers need to navigate complex decisions regarding data collection and task completions. In this paper, we introduce a novel multiplayer multi-armed bandit model tailored for intelligent IoT systems designed to optimize data collection processes and integrate fairness into decision making. Through the development of a distributed cooperative bandit algorithm, DC-ULCB, we propose a mechanism that enables edge servers to collaboratively choose sensors, maximizing data collection rates while ensuring equitable task distribution across the network. Our analytical work elucidates the algorithm's performance by showcasing logarithmic instance-dependent upper bounds on reward regret and fairness regret, emphasizing its sample-efficient nature and the inherent complexity of managing such systems. Extensive simulations further affirm the superiority of DC-ULCB over existing algorithms in balancing reward maximization with fairness, marking a significant step forward in the design of fair distributed cooperative learning systems within the context of intelligent IoT. The integration of players into our model underscores the critical role of multi-agent cooperation and competition in achieving optimal solutions.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Dingwen_Kong1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11574v1",
  "title": "Offline Multitask Representation Learning for Reinforcement Learning",
  "modified_abstract": "This work is motivated by advancements in reinforcement learning (RL) and the quest for efficient learning frameworks, particularly influenced by the exploration of hierarchical Bayesian models for decision-making under uncertainty, as demonstrated by previous studies on the generalization capabilities of contextual bandit frameworks. Specifically, we delve into offline multitask representation learning within RL, focusing on how a learner can harness an offline dataset from diverse tasks, possessing a unified representation, to learn this shared representation effectively and act upon it in various RL scenarios. Our investigation leads to the theoretical exploration of offline multitask low-rank RL and the introduction of a novel algorithm, MORL, tailored for this purpose. Moreover, we analyze the application of the learned representation in downstream RL tasks across reward-free, offline, and online scenarios, empirically unveiling the advantages of leveraging previously acquired representations over initiating the learning process afresh for each new task's low-rank model. Our findings underscore the potency and applicability of multitask representation learning in enhancing the efficiency and robustness of reinforcement learning processes, particularly in addressing complex problems within the contextual bandit, mixed-effect model, and regret minimization frameworks, while also considering the action-selection process and priors in the model development.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Imad_AOUALI1",
  "manipulated_ranking": 5,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11565v1",
  "title": "Decentralized Stochastic Subgradient Methods for Nonsmooth Nonconvex Optimization",
  "modified_abstract": "This paper focuses on the challenge of decentralized optimization with nonconvex and nonsmooth objectives, a critical area in the optimization of neural networks for robust and efficient computation across various domains. Inspired by seminal works and recent advancements in optimization methodology, particularly those addressing min-max problems and games via approximation of the Proximal Point Method, we introduce a comprehensive framework named DSM to analyze and ensure the global convergence of decentralized stochastic subgradient methods. Our framework demonstrates how a broad spectrum of existing decentralized subgradient methods, including variations such as DSGD, DSGD-T, and DSGDm, can be analyzed under a unified lens. We further innovate by incorporating SignSGD into DSGDm as part of DSM, showcasing its applicability and effectiveness within our convergence analysis for nonsmooth nonconvex optimization problems in various domains. Our convergence proofs, supported by preliminary numerical experiments, mark a pioneering step in establishing global convergence guarantees and rates for these methods in the domain of nonsmooth neural network training. This propels the theory ahead to meet the challenges of practical applications, including those in gaming and other strategic scenarios.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Stratis_Skoulakis2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11544v2",
  "title": "RL in Markov Games with Independent Function Approximation: Improved Sample Complexity Bound under the Local Access Model",
  "modified_abstract": "Efficiently learning equilibria in general-sum Markov games presents profound challenges, particularly within large state and action spaces, and is exacerbated by the curse of multi-agency. This combined complexity has been addressed in diverse contexts, including recent explorations into Markov Potential Games, which examine multi-agent coordination within both state-dependent and independent dynamics, reflecting upon the concept of domination in coordination games. Our work builds upon these foundational studies by leveraging independent linear function classes to approximate each agent's marginal $Q$-value, addressing prior limitations such as suboptimal sample complexity bounds related to the desired accuracy $\\varepsilon$ or action space size. We propose a novel algorithm, Lin-Confident-FTRL, designed for learning coarse correlated equilibria (CCE) in normal-form games with optimal accuracy bounds and reduced dependency on the action space size, utilizing local access to a simulator for efficient environment interaction and applying a gradient-based approach in our algorithm design. This algorithm significantly advances the field by offering an optimal $O(\\epsilon^{-2})$ accuracy bound - a noteworthy improvement that circumvents linear dependencies on the action space and scales polynomially with crucial problem parameters. Furthermore, our analytical framework notably extends virtual policy iteration techniques from the single-agent local planning realm to multi-agent settings, establishing a computationally efficient approach with tighter sample complexity bounds under the local access model.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Stefanos_Leonardos1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11537v1",
  "title": "Semantic Prompting with Image-Token for Continual Learning",
  "modified_abstract": "Building on the advancements in continual learning and the success of Vision Transformers (ViTs), our study introduces a novel concept in the realm of prompt-based learning, which is an emerging paradigm that leverages pre-trained models, especially networks like transformers, to learn new tasks flexibly and efficiently. This is particularly inspired by the dynamic field of computational vision where novel methods like Meta-attention for ViT-backed continual learning have significantly advanced our ability to tackle rapidly evolving tasks by adapting pre-trained networks without the detrimental effects, such as catastrophic forgetting or performance degradation, typically observed with conventional approaches. Our innovation, I-Prompt, provides a task-agnostic approach focusing on the visual semantic information of image tokens to circumvent the limitations of task prediction. We propose semantic prompt matching to determine prompts based on the similarities between tokens, and image token-level prompting, which applies prompts directly to image tokens in the intermediate layers, thus enhancing performance across diverse tasks while substantially reducing training times and computational overhead. Networks employing our technique suffer less from the common pitfalls of continual learning, such as forgetting previously learned tasks. Our comprehensive experiments across four benchmarks demonstrate the robustness and efficiency of our approach, offering a compelling alternative to state-of-the-art methods, including those based on convolutional learning strategies, and opening avenues for more effective and versatile continual learning strategies. The technique employs mask-based refining of prompts and leverages learned attention mechanisms to adaptively focus on task-relevant parts of image tokens, ensuring continual learning progression without forgetting previously learned information.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Haofei_Zhang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11536v1",
  "title": "OCR is All you need: Importing Multi-Modality into Image-based Defect Detection System",
  "modified_abstract": "In the domain of automatic optical inspection (AOI), the adoption of OCR (Optical Character Recognition) techniques as elaborated in our methodology is inspired by the evolving landscape of computational vision, particularly the advancements in object detection such as those presented in weakly-supervised camouflage object detection methods. These methods have demonstrated the potential of leveraging less intensive annotation approaches for significant performance gains, including the use of boundary detection to delineate obscure or camouflaged defects. AOI, a cornerstone in manufacturing quality control, leverages high-resolution imaging for anomaly detection through pattern analysis but faces challenges like limited sample sizes and variations in imaging conditions, which impair model accuracy. Recognizing the unexploited wealth of machine or image-embedded mechanism-parameter information, we propose an external modality-guided data mining framework, leveraging OCR to unearth statistical features within images, thus enriching defect detection capabilities. Our network, OANet (OCR-AOI-Net), aligns and synergizes features extracted from different modalities, including those tagged as 'scribbles' by operators to indicate areas of interest, enhancing semantic representation fusion across views. The addition of feature refinement processes and a gating function optimizes feature combination, improving inference and decision-making robustness in our cross-view network architecture. Experimental validation attests to our approach's efficacy, markedly elevating the recall rate of defect detection in challenging conditions, setting new benchmarks for the field.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ruozhen_He1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11532v1",
  "title": "Out-of-Distribution Detection Should Use Conformal Prediction (and Vice-versa?)",
  "modified_abstract": "This work is situated within the rapidly evolving field of machine learning, drawing inspiration from prior efforts to delineate more effective methodologies for outlier detection in complex data scenarios, such as benchmarking unsupervised outlier node detection on static attributed graphs. Out-Of-Distribution (OOD) detection has predominantly focused on developing scores to efficiently separate OOD data from In Distribution (ID) data, while Conformal Prediction (CP) utilizes non-conformity scores to construct prediction sets with probabilistic coverage guarantees. Given this backdrop, we propose leveraging CP to enhance the evaluation of OOD scores. Particularly, we highlight how standard OOD benchmark settings might overestimate performance due to the finite sample size of test datasets. Incorporating insights from (Bates et al., 2022), we introduce new conformal AUROC and conformal FRP@TPR95 metrics that correct these overestimations, providing probabilistic conservativeness guarantees. Our analysis demonstrates the impact of these corrections across key OOD and anomaly detection benchmarks, including OpenOOD (Yang et al., 2022) and ADBench (Han et al., 2022), benchmarking neural network models on attributed graphs. Further, we explore the reciprocity between OOD and CP, showing how utilizing OOD scores as non-conformity scores can enhance CP methods. This research underscores the potential synergy between OOD and CP, positing that their integration could reveal deeper insights into score design and interpretation. Code and resources related to this paper are available at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Xueying_Ding1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11522v1",
  "title": "LOOPer: A Learned Automatic Code Optimizer For Polyhedral Compilers",
  "modified_abstract": "In the realm of polyhedral compilation, selecting the most profitable transformations that lead to significant speedups remains a challenge. This issue has led to the exploration of machine learning techniques for building cost models that can guide the optimization and compilation process. Motivated by previous works demonstrating the potential of integrating ML with compiler technologies, such as the pioneering co-design of algorithms and compilation techniques that leverage neural network sparsity through pruning for improved runtime execution, our paper introduces LOOPer. LOOPer represents an advancement in polyhedral auto-scheduling, utilizing a deep-learning-based cost model that supports a comprehensive range of affine transformations and programs. This model notably incorporates attention mechanisms to better prioritize optimization tasks and identify the most impactful areas for improvement. It is capable of optimizing programs with multiple loop nests and both rectangular and non-rectangular iteration domains, thereby extending the applicability of polyhedral auto-scheduling to a broader spectrum of programs and compilation tasks. Our evaluation on the Polybench benchmark shows that LOOPer achieves significant speedups, with a geometric mean speedup of 1.59x over Tiramisu and competitive performance against Pluto, demonstrating its efficacy as a next-generation code optimizer in polyhedral compilers. Designed to act as an intelligent scheduler, LOOPer seamlessly integrates into the polyhedral compilation workflow, highlighting its utility in automating optimization decisions and modeling them through deep learning for enhanced compilation strategies.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Fu-Ming_Guo2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11521v1",
  "title": "A Data-driven Approach for Rapid Detection of Aeroelastic Modes from Flutter Flight Test Based on Limited Sensor Measurements",
  "modified_abstract": "This study is inspired by the foundational work in the domain of signal processing and system identification, including novel approaches to deal with intersecting singularities in multi-structured estimation. It focuses on the unique challenges of flutter flight tests, which necessitate the evaluation of airframes' aeroelastic stability through artificial excitation on aircraft lifting surfaces. The detection and analysis of aeroelastic modes are complicated by factors such as noise contamination, turbulence, non-optimal excitation of modes, and sensor malfunctions, which can make the extraction process time-consuming and unreliable. To address these challenges, we introduce a methodology that leverages a time-delay embedded Dynamic Mode Decomposition technique enhanced by Robust Principal Component Analysis and a sparsity-promoting regularizer criterion for the automatic and optimal selection of sparse modes with emphasis on circumventing intersecting singularities and maximizing the efficacy of multi-structured data analysis. Furthermore, our approach utilizes inference based on the captured response data, facilitating rapid identification of aeroelastic modes. Utilizing anonymized flutter flight test data, our methodology, grounded in convex optimization, does not require prior knowledge of the input excitation and depends solely on responses captured by accelerometer channels. The inclusion of a compressed sensing algorithm empowers our method to function effectively with a limited number of sensors, significantly improving robustness and real-time applicability during flutter test campaigns.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Emile_R_Richard1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11520v1",
  "title": "State-Separated SARSA: A Practical Sequential Decision-Making Algorithm with Recovering Rewards",
  "modified_abstract": "In the landscape of reinforcement learning (RL), the assumption of constant rewards in multi-armed bandit algorithms is often invalidated by real-world complexities, notably in scenarios where rewards must recover over time\u2014a concept highlighted by the study of recovering bandits. Inspired by studies on scalable Markov Decision Processes (MDPs) and the underexplored aspects of polynomial mixing times which crucially affect the functionality of continual learning in RL, our research introduces the State-Separate SARSA (SS-SARSA) algorithm. This novel RL solution is designed for such dynamic environments, effectively handling the variability of reward structures by treating rounds as states, thereby streamlining the learning process through a significant reduction in state-space complexity and scaling issues. SS-SARSA not only makes fewer assumptions about reward distribution but also offers an efficient algorithmic solution with lower computational cost, proving its asymptotic convergence to an optimum policy under mild conditions. Continual simulation studies across a variety of games and sequential decision-making tasks of varying duration validate the superiority of SS-SARSA in practice, demonstrating its practicality for sequential decision-making in environments with recovering rewards and its pivotal role in the development of reinforcement learning.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Gopeshh_Raaj_Subbaraj1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11505v1",
  "title": "Covid-19 detection from CT scans using EfficientNet and Attention mechanism",
  "modified_abstract": "In the context of the ongoing COVID-19 pandemic, the efficiency and accuracy of diagnostic processes are paramount, especially within the healthcare industry. Building on the momentum from existing work that explores innovative deep learning approaches for medical image analysis, such as leveraging federated learning with Vision Transformers for COVID-19 diagnosis on chest X-ray images, our study introduces a novel deep learning model-based pipeline aimed at enhancing COVID-19 detection from lung Computed Tomography (CT) scan images. Addressing the challenges posed by manual diagnosis and the inefficiencies associated with high patient volumes and numerous images per patient, our work proposes the integration of EfficientNet with an Attention mechanism, complemented by a pre-processing step to automate the detection process. Learning from split data in a protocol that amalgamates distributed and data-centralized methodologies, our approach is validated through participation in the Domain adaptation, Explainability, and Fairness in AI for Medical Image Analysis Workshop and COVID-19 Diagnosis Competition (DEF-AI-MIA COV19D), where our designed pipeline demonstrated superior performance compared to last year's participants on the validation set of the competition dataset. This indicates not only the feasibility of using advanced neural network architectures like EfficientNet for the critical task of COVID-19 detection but also highlights the potential for approaches that incorporate attention mechanisms to significantly improve the accuracy of such diagnostic tools. Furthermore, adopting a client-centered approach in data sharing strategies alongside this learning and training framework can pave the way for groundbreaking advancements in medical image analysis.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jeongsol_Kim1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11497v1",
  "title": "Do CLIPs Always Generalize Better than ImageNet Models?",
  "modified_abstract": "Large vision language models, such as CLIPs, have revolutionized modern machine learning with their transformer-based architectures. CLIPs have demonstrated great generalizability under distribution shifts, supported by an increasing body of literature, especially when incorporating cross-attention mechanisms for segmentation and object recognition. However, the evaluation datasets for CLIPs, despite being queries for various scenarios, are variations primarily designed for ImageNet benchmarks, which may not fully reflect the extent to which CLIPs, e.g., pre-trained on LAION, are robust to spurious correlations involving segmentation issues. To bridge the gap, we collect a real-world dataset called CounterAnimal that contains realistic spurious features found in animal photos and employ k-means clustering to differentiate between the common group: comprising animals on common backgrounds, and the counter group: including animals on unusual backgrounds. The network's performance drops from the common to counter groups quantify the reliance of models on spurious features (i.e., backgrounds) to predict the animals. We find that CLIPs trained on either LAION or the OpenAI data exhibit notable performance drops on the counter group, indicating issues with robustness and segmentation under novel conditions. Surprisingly, we observe that single-modal models trained on ImageNet are more robust than CLIPs. We provide both theoretical and empirical explanations for why CLIPs still learn spurious features. Our findings suggest that distribution shifts remain an open problem for CLIPs, and one needs to be cautious about test setups when evaluating foundation models pre-trained on a significantly different scale and distribution. Clustering of data based on common vs. counter backgrounds further illustrates the challenges CLIPs face in generalizing well across different environments.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Liang-Chieh_Chen1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11495v1",
  "title": "Semantic-Enhanced Representation Learning for Road Networks with Temporal Dynamics",
  "modified_abstract": "Inspired by the significant advancements in the field of graph neural networks (GNNs), particularly those addressing the challenges of modeling dynamic and heterogeneous structures as exemplified by the Heterogeneous Graph Transformer (HGT), our study introduces a novel framework termed Toast for learning general-purpose representations of road networks. To this end, we also present DyToast, an enhanced version designed to integrate temporal dynamics more effectively for improved performance on time-sensitive downstream tasks. Our approach entails encoding two key semantic characteristics intrinsic to road networks: traffic patterns and traveling semantics. This is achieved by refining the skip-gram module to include auxiliary objectives that predict the traffic context relative to specific road segments, and by leveraging trajectory data with pre-training strategies rooted in Transformer architecture to distill traveling semantics more accurately. The encoding process is complemented by an attention mechanism that efficiently focuses on critical aspects of road networks. DyToast builds on this foundation by employing unified trigonometric functions for encoding temporal evolution and dynamic nature of road networks more effectively, alongside mini-batch processing for enhanced scalability. Through these innovations, we aim to generate representations that encapsulate a comprehensive knowledge spectrum within road networks, supporting a wide array of applications that range from those based on individual graph segments to those reliant on trajectory analysis across various tasks. The superiority of our proposed framework is validated through extensive experiments on two real-world graphs across three tasks, wherein it consistently surpasses state-of-the-art baselines by a significant margin.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ziniu_Hu1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11491v1",
  "title": "Uncertainty-Calibrated Test-Time Model Adaptation without Forgetting",
  "modified_abstract": "This study extends the frontier of Test-time adaptation (TTA) by addressing the critical limitations observed in prior methods, motivated by the pioneering works that delve into enhancing model efficiency and reliability, such as the Neural Tangent Kernel Spectrum-Aware Pruning (NTK-SAP), which leverages the underpinnings of neural tangent kernel theory to improve neural network pruning. Our work, while distinct, shares a similar ethos in mitigating efficiency and performance issues but within the domain of TTA. We pinpoint two central challenges: 1) the prohibitive optimization costs incurred by backpropagating for each test sample in traditional TTA strategies, and 2) the tendency of these strategies to cause performance deterioration on in-distribution data\u2014a phenomenon known as forgetting\u2014upon attempting to enhance out-of-distribution robustness. To overcome these, we introduce the Efficient Anti-Forgetting Test-Time Adaptation (EATA) approach, which innovatively employs an active sample selection criterion for test-time entropy minimization and integrates a Fisher regularizer to preserve model integrity against significant parameter shifts. EATA also maximizes network connectivity and metric learning principles to maintain high performance on in-distribution data while adapting to new tasks across large-scale networks. To address the critical issue of overconfident predictions resulting from uncalibrated entropy optimization, we advance our methodology through EATA with Calibration (EATA-C). EATA-C differentiates between model uncertainty and inherent data uncertainty to finely tune prediction confidence levels, utilizing divergence loss for promoting prediction consistency and a novel min-max entropy regularizer for dynamic confidence calibration in large models. Our extensive evaluation across image classification and semantic segmentation tasks underlines EATA-C's superior ability to facilitate calibrated, adaptive, and anti-forgetting model performance in the face of distribution shifts, leveraging contemporary methods and large network architectures.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yite_Wang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13846v1",
  "title": "A Clustering Method with Graph Maximum Decoding Information",
  "modified_abstract": "Drawing inspiration from recent advancements in the field of graph representation learning, particularly the SpecTRA approach to spectral transformers for graphs, our study introduces a novel clustering method that aims to harness the untapped potential of graph models. Our method, named Clustering method for Maximizing Decoding Information (CMDI), aims at addressing the overlooked aspects of uncertainty and embedded structural information within graph-based clustering analyses. Unlike traditional methods, CMDI innovatively integrates two-dimensional structural information theory, encompassing graph structure extraction and graph vertex partitioning into the clustering process, while ensuring frequency invariance in its analysis. Additionally, by implementing advanced filtering techniques, CMDI refines the data that feeds into the clustering algorithm, sharpening the focus on the most information-rich elements. This innovative approach allows for the reformulation of graph partitioning as an abstract clustering problem, where the goal is to leverage maximum decoding information to minimize the uncertainty associated with random vertex visits. Empirical evaluations on three real-world datasets demonstrate that CMDI not only outperforms classical baseline methods in terms of the decoding information ratio (DI-R) but also exhibits superior efficiency, especially when considering prior knowledge in modern machine learning architectures, including those implementing transformers and employing multi-channeled input strategies. These findings highlight CMDI's potential to significantly enhance decoding information quality and computational efficiency in terms of frequency invariance, establishing it as an impactful addition to graph-based clustering analyses.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Anson_Bastos1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11483v1",
  "title": "Open-World Semi-Supervised Learning for Node Classification",
  "modified_abstract": "Influenced by advances and challenges in semi-supervised learning (SSL), especially in contexts with imbalanced class distributions, this study tackles the practical yet under-explored problem of open-world semi-supervised learning (Open-world SSL) for node classification. This area of study classifies unlabeled nodes into seen classes or multiple novel classes, highlighting the issue of intra-class variance imbalance between seen and novel classes, which can detrimentally affect model performance. Our work is motivated by the observation that pre-trained feature encoders, though beneficial, pose the challenge of generalization across various types of graph data using different strategies. Consequently, our paper introduces OpenIMA, an IMbalance-Aware classifier for Open-world semi-supervised node classification, which innovatively employs contrastive learning with bias-reduced pseudo labels to train the node classification model from scratch, incorporating class-balanced minibatch strategies. Through comprehensive experiments on seven popular graph benchmarks, we demonstrate the effectiveness of OpenIMA, using various metrics suited for majority and minority classes and effectively handling imbalanced distributions by employing minibatch techniques. The source code is available on GitHub [omitted for de-identification], contributing to the growing need for effective methods that do not depend on pre-trained graph encoders.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Lee_Hyuck1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11482v1",
  "title": "SeisFusion: Constrained Diffusion Model with Input Guidance for 3D Seismic Data Interpolation and Reconstruction",
  "modified_abstract": "Given the critical importance of seismic data for understanding geophysical phenomena, particularly in the context of earthquake forecasting and detection, our work offers a novel approach to seismic data reconstruction. Traditional methods in seismic data reconstruction are limited by the need for multiple empirical parameters and struggle with large-scale continuous missing data, a challenge magnified by the complexity of seismic events and their precursors as noted in related foundational research on earthquake forecasting. With advancements in deep learning, there is a significant opportunity to improve upon these traditional methods. Our proposal, SeisFusion, leverages a novel diffusion model reconstruction framework specifically designed for 3D seismic data, overcoming the limitations of existing convolutional neural networks by incorporating conditional supervision constraints. This novel approach allows for the reconstruction of seismic datasets with complex missing patterns, enhancing the accuracy and consistency of these reconstructions, and holds significant societal implications by potentially improving seismic forecasting and preparedness. A 3D neural network architecture integrated into the diffusion model permits the extension to 3D space, and the inclusion of missing data in the generation process addresses a broad spectrum of missing patterns. Our method demonstrates superior reconstruction accuracy on both field and synthetic datasets through comprehensive ablation studies. We make our implementation available at [omitted for de-identification], inviting further exploration and validation of our approach in real-world applications.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Debvrat_Varshney1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11477v1",
  "title": "Span-Based Optimal Sample Complexity for Weakly Communicating and General Average Reward MDPs",
  "modified_abstract": "Informed by existing research on learning efficiencies in both completely and partially observable Markov Decision Processes (MDPs and POMDPs), our study targets the sample complexity of learning an $\\epsilon$-optimal policy within average-reward Markov decision processes (MDPs) under a generative model. Earlier endeavors have intricately explored representation learning, estimation techniques, and tractable planning in POMDPs, revealing a foundational understanding of efficient algorithm design and sample complexity in high-dimensional spaces. Building upon these insights, we establish a new complexity bound of $\\tilde{O}(SA\\frac{H}{\\epsilon^2})$ for weakly communicating MDPs, introducing the first minimax optimal (up to logarithmic factors) results across critical parameters (state spaces, actions, bias function span, and approximation margin $\\epsilon$). Our findings surpass previous results by either avoiding assumptions of uniformly bounded mixing times for all policies or by improving parameter dependencies. Extending beyond weakly communicating MDPs, we dive into the general average-reward setting, proposing a new transient time parameter $B$, and aligning with a complexity bound of $\\tilde{O}(SA\\frac{B+H}{\\epsilon^2})$. Our advancements underpin a novel approach by connecting average-reward MDPs to discounted MDP frameworks, requiring innovative methods in planning and estimation to ascertain optimized bounds. Specifically, our analysis refines bounds for $\\gamma$-discounted MDPs in weak and general contexts, notably circumventing established lower bounds for arbitrary $\\gamma$-discounted MDPs. Significantly, our work also offers tighter bounds based on the span and transient time parameters rather than traditional mixing time or MDP diameter perspectives. These contributions stand poised to offer broader applicability and insights into MDP theory and applications.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Xuezhou_Zhang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11472v1",
  "title": "Accelerating String-Key Learned Index Structures via Memoization-based Incremental Training",
  "modified_abstract": "Our work builds upon the foundational principles established by prior research in machine learning and database systems, including advancements in hyperparameter optimization which have highlighted the potential for algorithmic efficiency through techniques like power-law scaling and multi-fidelity estimation. Learned indexes represent a novel convergence of machine learning models, specifically neural networks, and traditional index structures, aiming to optimize the mapping between keys and positions within databases. These models necessitate frequent updates to incorporate changes, a process often hindered by the computational complexity associated with variable-length string keys. In response to this challenge, we introduce SIA, a system that harmonizes algorithmic innovations with hardware acceleration through FPGA to implement a memoization-based incremental training scheme. This scheme, distinctive for its efficiency in processing updates, significantly reduces the computational burden by reusing previously computed matrix decomposition results. Our comprehensive evaluation against leading learned index systems, such as ALEX, LIPP, and SIndex, using benchmarks on diverse datasets demonstrates SIA's capacity to deliver up to 3.4x enhanced throughput and improved performance on real-world datasets, marking a substantial leap towards addressing the scalability and performance bottlenecks inherent in learned index systems.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Arlind_Kadra1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11464v1",
  "title": "FedSPU: Personalized Federated Learning for Resource-constrained Devices with Stochastic Parameter Update",
  "modified_abstract": "In the context of Personalized Federated Learning (PFL) for IoT applications, which handles high-volume, non-iid client data while ensuring data privacy through encryption, our work is inspired by advancements and challenges highlighted in previous studies, such as the Generative Adversarial Federated Model, which focuses on collaborative learning across heterogeneous data sources without data exchange. These studies underscore the computational and privacy challenges faced in federated settings, especially when dealing with complex neural models and diverse data distributions. Addressing the specific challenge of computation and communication bottlenecks in heterogeneous edge devices, we introduce Federated Stochastic Parameter Update (FedSPU), a novel strategy that contrasts with the conventional federated dropout approach. FedSPU preserves the full neural model architecture on each device but employs a stochastic mechanism to freeze a percentage of neurons during training, thus aligning with the personalized needs of each client without introducing bias towards non-iid local data. Our experimental findings demonstrate a significant outperformance of federated dropout by an average of 7.57% in accuracy and a substantial reduction in training time by 24.8% to 70.4% through an early stopping scheme, enabling more efficient label prediction in various IoT application verticals and offering a promising direction for PFL implementations on resource-constrained devices. This approach efficiently manages gradient updates and computational resources, catering to the low-power specifications of parties involved in the federated network.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Leying_Guan1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11449v1",
  "title": "Graph Partial Label Learning with Potential Cause Discovering",
  "modified_abstract": "This investigation is inspired by recent advancements in Graph Neural Networks (GNNs), particularly by the insight that even unmodified Transformers can act as powerful graph learners, efficiently processing graph-structured data without the need for graph-specific modifications. Building on this foundation, we tackle the challenge of effectively annotating graph data for training GNN models, which is complicated by the inherent complexity and interconnectedness of the network. Our proposed method, a novel graph representation learning approach, is designed to enable GNN models to discern critical information in the presence of noisy labels within Partially Labeled Learning (PLL) scenarios. To enhance our model's precision, we incorporate embeddings of graph nodes, assuming not just their isolated form but their complex interactions as tokens of broader structural narratives and leverage message-passing mechanisms to process these tokens within the graph. PLL, a form of weakly supervised learning, involves instances labeled with candidate sets that include both the correct label and additional, non-target labels. Our methodology emphasizes potential cause extraction to identify data with a higher likelihood of causal links to the labels, coupled with auxiliary training on the refined data to mitigate label noise. Supported by theoretical analysis, learning theory, and validated through comprehensive evaluations and ablation studies on diverse datasets, our approach demonstrates enhanced performance, underscoring its potential in advancing applications reliant on complex graph-structured data and its variants. Beyond its initial implementation, our method offers a framework for further exploration into efficient learning paradigms for graph-structured information.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sungjun_Cho1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11446v1",
  "title": "LLM Guided Evolution -- The Automation of Models Advancing Models",
  "modified_abstract": "In the evolving landscape of machine learning, where traditional model development adopts a structured approach through abstraction layers, such as AutoML leveraging tree-based or Cartesian genetic programming, our study introduces a pioneering methodology we name \"Guided Evolution\" (GE). GE innovates by harnessing the computational intellect of Large Language Models (LLMs) to directly modify and evolve code, inspired by the efficiency and transformative capacity seen in tasks like data-to-text and text-to-data conversion. Through a novel \"Evolution of Thought\" (EoT) technique, GE enables LLMs to introspect and adapt based on the outcomes of prior mutations, effecting a self-enhancing loop that refines the model evolution strategy for various machine learning tasks. By utilizing LLMs' aptitude for generating a multitude of diverse responses from intricately constructed prompts and their ability to fine-tune model temperament, GE promotes genetic diversity in the execution of machine learning tasks - a cornerstone for evolutionary algorithms. This integration not only expedites the evolutionary process but also imbues it with expert-like inventiveness and insight for machine learning tasks such as the organization and interpretation of tables through text-to-data transformations. Demonstrating its potential, GE's application to evolve the ExquisiteNetV2 model autonomously led to enhanced variants, showing a notable accuracy improvement from 92.52% to 93.34% while preserving model compactness. Thus, illustrating LLMs' transformative role in expediting and autonomizing the model design pipeline for self-improving technology solutions.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Song_Duong1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12109v1",
  "title": "GCAM: Gaussian and causal-attention model of food fine-grained recognition",
  "modified_abstract": "Leveraging the insights gained from recent advancements in few-shot classification, particularly the careful analysis of joint distributions for enhanced representation capability as seen in methods like deep Brownian Distance Covariance, our research introduces an innovative approach to food recognition challenges. Current strategies struggle to distinguish between visually similar food items, a gap our work aims to bridge by developing a Gaussian and causal-attention model specifically tailored for fine-grained object recognition and categorization. Our model applies Gaussian features to target regions and extracts intricate details and vectors from objects to improve feature mapping within a defined space. To address data drifts from uneven distributions, we utilize counterfactual reasoning, analyzing the effects of our image attention mechanism on network predictions and improving attention weights for finer image recognition nuances. Our novel, learnable loss strategy also aids in balancing training stability across different modules, leading to enhanced target recognition accuracy. Validated across four relevant benchmarks, our GCAM model not only outperforms current state-of-the-art methods on the ETH-FOOD101, UECFOOD256, and Vireo-FOOD172 datasets but also achieves top-tier results on the CUB-200 dataset, marking a significant advancement in the field of fine-grained food recognition and categorization.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Peihua_Li1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11432v1",
  "title": "Demystifying Deep Reinforcement Learning-Based Autonomous Vehicle Decision-Making",
  "modified_abstract": "This work is informed by the substantial progress in the domain of autonomous vehicles (AVs), particularly the advancements in safe, robust autonomy and the system-level design aspects that facilitate broader adoption of this technology. With the advent of universal function approximators in the domain of reinforcement learning, deep reinforcement learning (DRL) has found extensive practical applications, chiefly among them being decision-making in automated driving tasks. Despite its potential, the opaque nature of these models constitutes a significant barrier to their real-world implementation in AVs. Hence, our research emphasizes enhancing the interpretability of an attention-based DRL framework through a safe-by-design methodology. Leveraging a continuous proximal policy optimization-based DRL algorithm as the baseline and integrating a multi-head attention framework within an open-source AV simulation, we illuminate the interpretability of the trained models by offering analytical techniques that elucidate explainability and causality for spatial and temporal correlations related to motion patterns. Our findings reveal that certain weights encode crucial positional data of neighboring vehicles and the leader vehicle, proving a causal link with the ego vehicle's actions. These insights pave the way for rigorous testing strategies that allow practitioners to test and understand the decision processes of DRL algorithms in autonomous driving, thus addressing a pivotal challenge in the deployment of autonomous driving technologies. The integration of perception mechanisms further aids in decoding complex environmental cues, promoting a comprehensive safety and testing protocol.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Stefano_V_Albrecht1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13845v1",
  "title": "Learning to better see the unseen: Broad-Deep Mixed Anti-Forgetting Framework for Incremental Zero-Shot Fault Diagnosis",
  "modified_abstract": "Our study innovates within the zero-shot fault diagnosis (ZSFD) domain, inspired by the significant progress in supervised learning approaches such as Gaussian Processes (GPs), which, while effective in large dataset scenarios, reveal limitations in their adaptability and robustness, particularly in safety-critical applications. Specifically, recognizing the dynamic nature of industrial processes, we address the challenge of enabling ZSFD systems to continuously adapt to new fault categories and attributes without forgetting previously learned diagnostic capabilities. To this end, we introduce the incremental ZSFD (IZSFD) paradigm and propose a novel broad-deep mixed anti-forgetting framework (BDMAFF) tailored for this purpose, seeking to provide guarantees for avoiding the forgetting of critical diagnostic knowledge amidst sparse training data scenarios. BDMAFF integrates category increment and attribute increment within both traditional ZSFD and generalized ZSFD paradigms, offering an effective strategy to accommodate evolving streams of sparse training data through a unique synergy of feature memory. Utilizing a deep generative model with anti-forgetting strategies, and attribute prototype memory, inspired by broad learning systems, BDMAFF seeks to preserve diagnostic capability over unseen faults. Our framework stands out by enabling the incremental learning of new fault categories and attributes without the necessity of retaining all historical training samples, facilitated by a memory-driven iterative update strategy for the diagnosis model that guarantees the sustainability of the diagnostic capabilities. The efficacy of BDMAFF in avoiding the forgetting phenomenon and its application in real-world settings, as well as its evaluation through regression analysis compared to traditional methods, is demonstrated through experiments on a hydraulic system and the Tennessee-Eastman benchmark process, indicating its capacity to handle large scale and complex fault diagnosis tasks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Barbara_Rakitsch1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11425v1",
  "title": "Narrative Feature or Structured Feature? A Study of Large Language Models to Identify Cancer Patients at Risk of Heart Failure",
  "modified_abstract": "In an era where machine learning (ML) can dramatically improve patient outcomes and treatment safety, our study investigates the potential of ML models, including traditional ML, Time-Aware long short-term memory (T-LSTM), and large language models (LLMs), to identify cancer patients at risk of heart failure (HF) using electronic health records (EHRs). Drawing inspiration from data-driven approaches in health care, such as AI systems designed to predict patient deterioration from COVID-19 using routine clinical variables, imaging data, and incorporating neural networks, our research pivots to the realm of oncology. We focus on utilizing novel narrative features derived from structured medical codes in EHRs to forecast cardiotoxicity-induced HF in patients treated for lung, breast, and colorectal cancers. Analyzing a cohort of 12,806 patients from the University of Florida Health, with 1,602 individuals developing HF post-cancer treatment, we found the LLM, GatorTron-3.9B, enhanced by deep learning and boosting techniques, to significantly outperform existing models. The GatorTron-3.9B model achieved the highest F1 scores, surpassing traditional support vector machines boosted by 39%, T-LSTM by 7%, and BERT by 5.6%, showcasing the effectiveness of narrative features in enhancing identification accuracy. This highlights the critical role of advanced ML models and feature selection in improving health outcomes by enabling timely risk interventions.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Aakash_Kaku1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11418v1",
  "title": "Variational Sampling of Temporal Trajectories",
  "modified_abstract": "This work is inspired by recent advancements in the utilization of simulation-based inference (SBI) for parameter estimation in complex models, which have highlighted the intricacies of modeling and understanding dynamic systems across various scientific fields, including the electrophysiological studies of neuron behavior. In the context of a deterministic temporal process that can be delineated by its trajectory, comprising both an initial condition $z_0 \\in \\mathcal{Z}$ and a transition function $f: (\\mathcal{Z}, \\mathcal{T}) \\to \\mathcal{Z}$, we propose a novel methodology for sampling and statistical inference of these trajectories using neural networks, with several practical implications for improving workflows in simulation and analysis tasks. Existing approaches often employ differential equations or recurrent neural networks to model transition functions. However, these mechanistic methods have been limited in their ability to sample trajectories and conduct statistical inference comprehensively, often restricted by the parameterization of models. Our approach introduces a novel mechanism for learning the distribution of trajectories by explicitly parameterizing the transition function $f$ within a function space. This framework not only facilitates efficient synthesis of novel trajectories but also offers direct utility for inference tasks, including uncertainty estimation, likelihood evaluations, post-hoc analyses, and out-of-distribution detection for atypical trajectories. The implications of our methodology are vast, offering valuable tools for simulation, evaluation, and reinforcement learning applications.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yves_Bernaerts1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11415v1",
  "title": "DreamSampler: Unifying Diffusion Sampling and Score Distillation for Image Manipulation",
  "modified_abstract": "Inspired by significant strides in image manipulation through generative models and the limitations identified in latent diffusion models (LDMs) for tasks such as generating new clothing on a wearer while maintaining structural coherence, we introduce DreamSampler. This innovative framework unifies the strengths of reverse sampling and score distillation, methodologies that have become instrumental in the field of image manipulation using LDMs. DreamSampler, a model-agnostic approach applicable to any LDM architecture, facilitates both distillation and reverse sampling with added guidance for more effective image editing and reconstruction in clothing image editing, particularly focusing on the body of the wearer. Through our adversarial learning network, DreamSampler demonstrates superior performance compared to existing models in extensive experiments, including those focusing on clothing image editing, and SVG reconstruction. Notably, DreamSampler enables new applications in the realm of image manipulation, even in complex situations like generating a photograph-quality image from sketches, by efficiently interpreting compositional descriptions.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Shizhan_Zhu1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11408v1",
  "title": "Layer-diverse Negative Sampling for Graph Neural Networks",
  "modified_abstract": "In response to the evolving landscape of graph data processing, where traditional Graph Neural Networks (GNNs) primarily leverage information from immediate neighbors but struggle with over-smoothing and over-squashing, our research introduces a novel layer-diverse negative sampling method for GNNs. Drawing inspiration from recent breakthroughs that challenge conventional wisdom in graph learning---notably, the use of Transformers as powerful graph network learners without the need for graph-specific modifications---we propose a unique mechanism that employs a sampling matrix within a determinantal point process. This approach not only transforms the candidate set into a more favorable space from which negative samples for message-passing propagation are drawn but also introduces a space-squeezing method aimed at fostering layer-wise diversity among negative samples in multi-layer GNNs. The methodology inherently boosts the embeddings' quality by ensuring diversity and dynamism in sample selection, thereby enhancing the learning process of GNNs. Moreover, by establishing firm baselines, we underscore the effectiveness of our methodology in ameliorating the challenges of over-smoothing and over-squashing, with the dynamic incorporation of negative samples offering a promising avenue for augmenting the graph's topological structure, thereby enhancing the expressiveness and learning performance of GNNs. Our approach proves to be invariant to different layers of GNNs, which is pivotal in combating homogeneity in learned representations. Token-wise analysis further validates our claims, presenting a detailed insight into the enhancement of GNNs through our proposed sampling technique. The repetition of utilizing graph and graph-specific strategies, along with baselines comparisons and token analysis, significantly elucidates the contribution of our work towards advancing the field of graph-based deep learning.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sungjun_Cho1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11407v1",
  "title": "Divide-and-Conquer Posterior Sampling for Denoising Diffusion Priors",
  "modified_abstract": "Building upon the recent attention garnered by Denoising Diffusion Models (DDM) for their potential in solving inverse Bayesian problems, our research is partly inspired by the challenges encountered in score-based data generation, particularly the issue of managing score mismatch as elucidated in previous studies. Recognizing the limitations in sampling from the posterior distribution of DDM, which previous works attempted to address through approximations that bias the drift term of the diffusion, our approach diverges by leveraging the inherent structure of DDM priors. We propose a strategy that segments the complex problem of posterior sampling into a series of intermediate, more tractable sampling challenges, thereby achieving a reduction in approximation error when compared to existing methodologies. Our empirical log evaluation, spanning synthetic examples to various image restoration tasks and targeted generation techniques, validates the advanced reconstruction capability of our method, asserting its efficacy not only in training robust systems but also in improving classifier performance in general linear inverse problems through generation and robust evaluation protocols.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yi-Chen_Lo1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11395v2",
  "title": "Automated data processing and feature engineering for deep learning and big data applications: a survey",
  "modified_abstract": "This survey is motivated by the significant trajectory of artificial intelligence's (AI) evolution, primarily through the lens of supervised deep learning's advancements in using large-scale data for driving progress. It particularly draws inspiration from research uncovering the complexities of data heterogeneity in shaping the generalization performance and fairness of machine learning models, as seen in domains such as agriculture and healthcare. In such a rapidly evolving field, the task of manually preprocessing data and implementing feature engineering becomes increasingly untenable, highlighting the necessity for automation in data processing across deep learning pipelines. This work presents an extensive review on the automation of data processing tasks, encompassing data preprocessing\u2014such as cleaning, labeling, and handling missing data\u2014data augmentation, and feature engineering through optimization techniques and automated methods including feature extraction, construction, and selection. Moreover, the emphasis on optimization throughout the machine learning pipeline has revolutionized the ability to transform raw data into informative features for big data applications by streamlining every stage of data preparation, thereby enhancing prediction capabilities and addressing the needs of diverse sub-populations. Furthermore, this survey examines AutoML's role in enhancing the efficiency and effectiveness of deep learning systems by optimizing and leveraging the entire machine learning pipeline, thereby setting the stage for future developments in making deep learning applications more accessible and capable of handling the nuanced demands of big data.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jiayun_Wu1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13844v1",
  "title": "Scheduled Knowledge Acquisition on Lightweight Vector Symbolic Architectures for Brain-Computer Interfaces",
  "modified_abstract": "Leveraging previous breakthroughs in symbolic representation learning, such as the GEDI framework, which marries the generative and discriminative training paradigms for enhanced symbolic representations, our work explores the intersection of lightweight computing and brain-computer interfaces (BCIs). BCIs are designed to be lightweight and responsive in real-time to provide users timely feedback. While classical feature engineering is computationally efficient, it often suffers from low accuracy. In contrast, although deep neural networks (DNNs) offer improved accuracy, they are computationally demanding and induce significant latency. As a viable alternative, the low-dimensional computing (LDC) classifier based on vector symbolic architecture (VSA) presents a compelling small model size with accuracy superior to classical methods, albeit not at par with modern DNNs. This paper addresses the accuracy gap by proposing a novel scheduled knowledge distillation method that utilizes a self-supervised curriculum data approach with pre-training and clustering objectives to refine the learning process. It allows the student model, implemented using LDC/VSA, to progressively assimilate knowledge from the teacher model, regulated by an $\\alpha$ scheduler, thereby optimizing both accuracy and hardware efficiency for BCI applications requiring low latency. Our empirical evaluations demonstrate a more favorable balance between accuracy and efficiency in comparison to existing methods, thereby advancing the capabilities of compact BCI devices.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Emanuele_Sansone1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11391v1",
  "title": "Investigating the Benefits of Projection Head for Representation Learning",
  "modified_abstract": "Our research builds upon the fundamental question of how to extract high-quality representations in machine learning, a concern echoed in related works such as the development of the Harmonious Bottleneck on two Orthogonal dimensions (HBO) for MobileNets, which seeks to optimize convolutional neural network architectures for accuracy and efficiency in resource-constrained environments. Similar to how HBO aims to improve representation by encoding feature interdependencies across spatial and channel dimensions, our work explores the value of a projection head in enriching representation learning, particularly in object recognition, person re-identification, and detection tasks in computer vision. Despite its proven practical effectiveness in various vision tasks, the underlying reasons for the success of projection heads remain poorly understood, with pre-projection representations not directly optimized by the loss function. This work rigorously postulates a theoretical foundation explaining the efficacy of projection heads. We examine linear models trained with self-supervised contrastive loss and identify that the implicit bias of training algorithms catalyzes layer-wise progressive feature weighting, resulting in progressively unequal features. This embodies the notion that lower layers harbor normalized and less specialized representations, bilaterally benefiting from data augmentation and specific input features which are theoretically and experimentally shown to be beneficial under scenarios influenced by data augmentation and input features' diversity. Moreover, we delve into the effects of introducing non-linearity into the network, presenting how it enables lower layers to capture features absent in higher ones, which could be pivotal benchmarks in advancing the frontiers in person re-identification. We validate our hypotheses and theoretical framework through empirical experiments on various datasets including CIFAR-10/100, UrbanCars, and different versions of ImageNet, also proposing a potential alternative to the projection head that offers a more interpretable and controlled design, considering the channels' contribution to the learning process. Our research not only elucidates the mechanism behind the projection head's success but also contributes to the broader discourse on optimizing convolutional neural networks for superior representation learning.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Anbang_Yao1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11385v1",
  "title": "Stochastic approach for elliptic problems in perforated domains",
  "modified_abstract": "Drawing inspiration from previous efforts that harnessed innovative algorithmic approaches and error analysis in the training of deep neural networks (DNNs) for engineering and scientific applications, our work advances the field of solving partial differential equations (PDEs) within perforated domains using a neural network-based mesh-free methodology. A wide range of applications in science and engineering involve a PDE model in a domain with perforations, such as perforated metals or air filters, where computational challenges emerge due to the scale of geometries of perforations. Our proposed method is robust and efficient, adept at capturing the various configuration scales, including the averaged macroscopic behavior induced by small perforations, through the innovative use of a derivative-free loss method that leverages a stochastic representation or the Feynman-Kac formulation. This approach, deep in mathematical rigor, incorporates bounds on behavior of solutions to elliptic problems in perforated domains and ensures improved generalization capabilities in the solutions identified by the DNN training process. Emphasizing practical applicability and the design of the computational framework, we also adapt the Neumann boundary condition within the derivative-free loss framework to adeptly manage the interface between the domain and perforations. The efficacy of our approach is supported by a suite of stringent numerical tests, showcasing its capability in efficiently navigating the complexities posed by various scales of perforation and serving as a surrogate for more traditional mesh-based analysis methods.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~T._Konstantin_Rusch1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11366v2",
  "title": "JORA: JAX Tensor-Parallel LoRA Library for Retrieval Augmented Fine-Tuning",
  "modified_abstract": "The advent of sophisticated approaches in machine learning for tasks such as text-guided synthesis of artistic images, retrieval-augmented diffusion models, and speech generation has underscored the critical challenge of scaling Large Language Models (LLMs) for retrieval-based tasks, particularly Retrieval Augmented Generation (RAG). These challenges include significant memory constraints during fine-tuning, especially with extensive prompt-engineering sequences. Existing open-source libraries offer support for full-model inference and fine-tuning across multiple GPUs but do not fully address the need for efficient parameter distribution for the retrieved context. Our framework, aimed at addressing this limitation, introduces a novel avenue for PEFT-compatible fine-tuning of Llama-2 models, harnessing the capacities of JAX just-in-time (JIT) compilation and tensor-sharding for resource-efficient computational distribution during the diffusion and training phases. This innovation not only accelerates fine-tuning processes but also significantly lowers memory demands, enhancing scalability and practicality of fine-tuning LLMs for complex RAG tasks, even on hardware with limited GPU capacities. Comparative experiments reveal our framework's performance exceeds that of prevalent Hugging Face/DeepSpeed implementations by more than 12-fold in runtime on a quad-GPU setup while halving the VRAM requirement per GPU through efficient tensor-parallel synthesis and retrieval mechanisms. Code is available at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Robin_Rombach1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11363v1",
  "title": "IGANN Sparse: Bridging Sparsity and Interpretability with Non-linear Insight",
  "modified_abstract": "Feature selection is a critical component in predictive analytics that significantly affects the prediction accuracy and interpretability of models. Intrinsic methods for feature selection are built directly into model learning, providing a fast and attractive option for large amounts of data. Machine learning algorithms, such as penalized regression models (e.g., lasso) are the most common choice when it comes to in-built feature selection. However, they fail to capture non-linear relationships, which ultimately affects their ability to predict outcomes in intricate datasets. In this paper, we propose IGANN Sparse, a novel machine learning model from the family of generalized additive models, which promotes sparsity through a non-linear feature selection process enhanced by a customized kernel during training to ensure smoothness in the learned functions and convergence of the model towards optimal solutions. This framework ensures interpretability through improved model sparsity without sacrificing predictive performance. Moreover, IGANN Sparse introduces an innovative approach to classification problems in datasets with continuous variables, serving as an exploratory tool and potential surrogate for traditional models in domains characterized by complex patterns and potentially smooth transitions between categories. Our ongoing research is directed at a thorough evaluation of the IGANN Sparse model, including user studies that allow assessing how well users of the model can benefit from the reduced number of features and the influence of neighboring data points on learning. This will allow for a deeper understanding of the interactions between linear vs. non-linear modeling, number of selected features, and predictive performance.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Vivien_Cabannes1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11353v1",
  "title": "Solvent-Aware 2D NMR Prediction: Leveraging Multi-Tasking Training and Iterative Self-Training Strategies",
  "modified_abstract": "This study is informed by the growing interest and need for more accurate prediction methods within scientific research realms, particularly in Nuclear magnetic resonance (NMR) spectroscopy, a tool paramount for gaining insights into molecules' structural, electronic, and dynamic properties. Inspired by the recent advancements in machine learning (ML) for hyperparameter optimization and transfer learning, as detailed in works such as the establishment of HPO-B, a large-scale reproducible benchmark for black-box hyperparameter optimization (HPO) based on OpenML, we propose a novel approach to address the existing gap in two-dimensional (2D) NMR spectrum prediction. Our iterative self-training (IST) strategy alongside a Multi-Task Training (MTT) methodology seeks to overcome the challenges posed by the scarcity of annotated training datasets and resources for 2D NMR, with a focus on improving reproducibility and leveraging meta-datasets. Utilizing annotated 1D NMR data for $^{1}\\text{H}$ and $^{13}\\text{C}$ spectra in an initial pre-training phase, our model subsequently employs pseudo-annotations for unlabelled 2D NMR spectra, iteratively refining its predictive capabilities through measures designed to evaluate and enhance learning efficiency and the efficient collection of spectroscopic data. This method not only allows for enhanced prediction of 2D NMR shifts and peak assignments in experimental spectra but also demonstrates the model's effectiveness across a broad spectrum of molecules, including mediums-sized and large polymers. Notably, the reproducibility and thorough evaluation of our training strategy and the IST methodology underscore the reliability and advancement in the prediction capabilities. Our findings showcase a significant step forward in the application of ML to chemical spectroscopy, opening new avenues for precise molecular structure identification.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Martin_Wistuba1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11351v1",
  "title": "An SDP-based Branch-and-Cut Algorithm for Biclustering",
  "modified_abstract": "This paper introduces an innovative approach to biclustering, a critical analytical method enhanced by drawing inspiration from recent advancements in network alignment and optimal transport frameworks. Our work on the $k$-densest-disjoint biclique problem, aiming to identify $k$ disjoint complete bipartite subgraphs (bicliques) within a weighted complete bipartite graph, echoes the need to optimize complex structures within data sets, much like recent methodologies that have revolutionized network alignment through the use of position-aware regularized optimal transport and loss function regularization to tackle inaccuracies inherent in data representation. We tackle this challenge using a tailored branch-and-cut algorithm, leveraging a semidefinite programming (SDP) relaxation augmented with crafted valid inequalities to tighten the upper bound. This is solved in a cutting-plane fashion applying a proximal method, while the lower bound benefits from a maximum weight matching rounding procedure that capitalizes on the series of relaxation solutions at each node. Our computational findings, derived from both synthetic and real-world datasets, indicate that our specialized algorithm can manage tasks approximately 20 times larger than those possible with general-purpose solvers, indicating a significant leap in the scalability and efficiency of solving biclustering problems. The application of ranking-based metrics in our evaluation process underscores our algorithm's capability to produce highly relevant biclusters.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zhichen_Zeng1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11350v1",
  "title": "Robustness of the data-driven approach in limited angle tomography",
  "modified_abstract": "Inspired by the latest advancements in image reconstruction techniques, especially those utilizing alternative forms of light and imaging data such as PDMS algorithm's innovative use of polarimetric information for enhanced geometry in dense SLAM, our study probes the efficacy of data-driven models, specifically deep neural networks, in tackling the inversion challenge posed by the limited angle Radon transform. This notoriously difficult problem, marked by its ill-posed nature, has been a long-standing obstacle in obtaining accurate reconstructions from incomplete data. Incorporating photometric data and exploiting depthmap, stereo, and monocular techniques, our approach not only pre-segmented the task but also significantly improved the quality of reconstructions through a tailored optimization process. We offer a mathematical explanation for why the data-driven approach can outperform traditional methods, demonstrating an enhanced capability to reconstruct more information in a stable manner. This not only contributes to the literature by providing a robust solution to a classic problem but also broadens the horizon for the application of neural networks in solving complex, ill-posed problems across various fields.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Feitong_Tan1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11348v1",
  "title": "COLEP: Certifiably Robust Learning-Reasoning Conformal Prediction via Probabilistic Circuits",
  "modified_abstract": "Informed by the challenges and techniques explored in related works, such as the nuanced interplay between model transparency and privacy risks highlighted in the study of attribute inference attacks, our work introduces the certifiably robust learning-reasoning conformal prediction framework (COLEP) via probabilistic circuits. COLEP leverages a dual-component approach consisting of a data-driven learning component that trains statistical models to learn various semantic concepts and a reasoning component that encodes knowledge to characterize relationships among these learned models for logical reasoning. Employing probabilistic circuits within the reasoning component allows for exact and efficient reasoning over distributions. The training process for COLEP reduces the risks associated with attribute inference attacks by minimizing unwanted information leakage. Theoretically, we establish end-to-end certification of prediction coverage for COLEP against bounded adversarial perturbations and provide certified coverage accounting for the finite size of the calibration set, which often relies on incomplete records or data. Moreover, we demonstrably show that COLEP outperforms single-model approaches in terms of prediction coverage and accuracy when the utility of knowledge models is non-trivial. Imputation techniques integrated into the training phase to handle incomplete records, enhancing COLEP's applicability in real-world scenarios. Empirical assessments further validate the certified coverage's validity and tightness, with COLEP exhibiting up to 12% improvement in certified coverage on datasets such as GTSRB, 9% training improvement on CIFAR-10, and 14% on AwA2. This research not only addresses critical gaps in ensuring model robustness but also sets a new benchmark for achieving conformal prediction in adversarially vulnerable settings.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Bargav_Jayaraman1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11345v1",
  "title": "Independent RL for Cooperative-Competitive Agents: A Mean-Field Perspective",
  "modified_abstract": "Informed by substantial advancements in the understanding and application of Reinforcement Learning (RL) in structured environments, such as the translation of classical bandit algorithms to structured bandit settings and enhancing reward mechanisms through exploration strategies, this paper ventures into exploring RL within a complex dynamic of cooperative-competitive interactions among agents. Specifically, we address RL where agents are grouped into teams that cooperate internally while engaging in general-sum (non-zero sum) competition across teams. To theoretically ground our method for achieving a Nash equilibrium in this context, we adopt a linear-quadratic structure and tackle the non-stationarity resulting from multi-agent interactions by employing a mean-field approach, estimating the where the number of agents in each team is considered infinite. This leads us to study General-Sum Linear-Quadratic Mean-Field Type Games (GS-MFTGs) and to characterize the Nash equilibrium under a standard invertibility condition. Furthermore, we propose the Multi-player Receding-horizon Natural Policy Gradient (MRPG) algorithm that enables teams to independently minimize cumulative costs by effectively selecting a subset of actions. This algorithm's convergence to a global Nash equilibrium, despite the problem's non-convexity, is validated through a novel decomposition approach using backward recursive discrete-time Hamilton-Jacobi-Isaacs equations. Experiments demonstrate the practical value of our approach, emphasizing regret minimization and how these algorithms can further guide future research in multi-agent RL. Our findings also hint at the potential utility of incorporating recommendation systems into the RL framework to further enhance agent performance through systematic exploration.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Shreyas_Chaudhari1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11343v1",
  "title": "Federated Transfer Learning with Differential Privacy",
  "modified_abstract": "In the burgeoning field of federated learning, our endeavor is primarily motivated by a recent thrust to blend privacy with utility in distributed learning scenarios, as exemplified by advancements in differentially private aggregation protocols in the shuffle model. Building on these insights, this paper takes a novel step in addressing both data heterogeneity and privacy within the federated transfer learning framework. Our approach aims to enhance learning on a target data set by leveraging information from multiple heterogeneous source data sets while adhering to rigorous privacy constraints. We introduce the concept of \\textit{federated differential privacy}, an innovative privacy model that provides robust privacy guarantees without necessitating a trusted central server. Through this lens, we explore three classical statistical problems: univariate mean estimation, low-dimensional linear regression, and high-dimensional linear regression, using machine learning techniques to draw close parallels between theoretical models and practical applications. Our analysis reveals that federated differential privacy navigates an optimal path between the established paradigms of local and central differential privacy models. In doing so, we not only elucidate the inherent costs associated with privacy and data heterogeneity in federated settings but also underscore the significant potential for knowledge transfer across disparate data sets, facilitated by our methodological innovations.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Amer_Sinha1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11338v1",
  "title": "Ensembling and Test Augmentation for Covid-19 Detection and Covid-19 Domain Adaptation from 3D CT-Scans",
  "modified_abstract": "Building upon the burgeoning interest in leveraging artificial intelligence (AI) for medical image analysis, as indicated by the foundational works in domain adaptation techniques for 3D object detection, this paper significantly adds to the 4th COV19D competition's dialogue on Covid-19 Detection and Covid-19 Domain Adaptation Challenges. It outlines an innovative approach combining lung segmentation, Covid-19 infection segmentation with a recent CNN-based segmentation architecture (PDAtt-Unet), and employing an ensemble of 3D CNN backbones (Customized Hybrid-DeCoVNet, pretrained 3D-Resnet-18, and 3D-Resnet-50) for improved disease recognition. A novel concatenation methodology of grayscale input with segmented regions mimics color channels, enhancing model input quality. Additionally, the study explores ensemble strategies and test augmentation to, supported by self-training methods, drastically improve diagnostic performances, reflected by a significant enhancement in F1-score of 14% over baseline studies. The integration of adaptation and self-training methodologies, alongside advanced AI techniques, showcases a robust methodology for not only accurate Covid-19 detection but also effective adaptation and quality-aware analysis in medical image analysis. [The code repository URL has been omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jihan_Yang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11332v1",
  "title": "Graph Neural Network based Double Machine Learning Estimator of Network Causal Effects",
  "modified_abstract": "In the realm of causal effect inference within social networks, where data is entangled with complex interdependencies among individuals, our work is propelled by insightful advancements like those in probabilistic models for studying causality, including the implementation of sum-product networks that embrace the challenge of tractable inference. We propose a cutting-edge approach that synergizes graph neural networks with double machine learning to navigate through the intricacies of social network data, thereby enabling the precise estimation of direct and peer effects from observational networks through interventional learning. Our methodology leverages graph isomorphism networks, in tandem with double machine learning, to adeptly adjust for network-derived confounders and consistently derive the targeted interventional causal effects. We attest to our estimator's robustness in learning, showcasing its asymptotic normality and semiparametric efficiency. Through exhaustive comparisons with four advanced baseline methods over three semi-synthetic social network datasets, we highlight our approach's competitive or superior performance in estimating causal effects accurately. A practical validation through a case study on the influence of Self-Help Group participation on financial risk tolerance further elucidates our method's applicative usefulness in sectors such as health, revealing significant direct effects that validate our methodological and probabilistic advancements in the field of social network analysis. We also delve into the implications of network sparsity on the fidelity of our estimations.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Athresh_Karanam1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11330v1",
  "title": "Improving Dialogue Agents by Decomposing One Global Explicit Annotation with Local Implicit Multimodal Feedback",
  "modified_abstract": "Inspired by the concept of enriching machine understanding through multimodal inputs, as seen in previous works like MERLOT, which leveraged a large corpus of YouTube videos for self-supervised learning to garner temporal commonsense, multimodal reasoning, video cognition, and rich representations of conversational dynamics, our research introduces an innovative approach for enhancing Large Language Model (LLM)-based dialogue agents. We focus on aligning dialogue agents using global (dialogue-level) rewards while incorporating naturally occurring multimodal signals, including visual cues from video and audio, as well as images that bear significant recognition capabilities for conversational context understanding. Our method, referred to as GELI, decomposes human-provided global explicit (GE) session-level rewards into local, turn-level rewards using local implicit (LI) multimodal reward signals derived from a diverse corpus of videos, images, and models. This decomposition allows for the cross-modal shaping of rewards, fundamentally improving the dialogue agent's ability to understand and respond within conversational contexts through an implicit cognition process. By integrating this decomposed reward model into the standard RHLF pipeline and analyzing various representations of dialogue content, we demonstrate through both quantitative and qualitative human studies that our GELI approach consistently surpasses baseline methods across various conversational metrics, marking a significant advancement in the dialogue agent's performance by integrating nuanced, real-world multimodal feedback.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jack_Hessel1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11314v1",
  "title": "Reasoning in Transformers -- Mitigating Spurious Correlations and Reasoning Shortcuts",
  "modified_abstract": "Building on foundational advancements such as those demonstrated by the Neural Abstract Reasoner, which leveraged spectral regularization to enhance neural networks' reasoning and logical inference capabilities, our research explores how transformers, particularly renowned for their efficacy in natural language processing, handle the challenges of logical reasoning amid the presence of spurious correlations. Specifically, this study investigates the extent to which transformers, with their novel architecture, can be trained to a) approximate reasoning within propositional logic while b) sidestepping the all-too-common pitfall of reasoning shortcuts facilitated by spurious correlations within data. Utilizing a dataset with known spurious correlations, such as between truth and the number of rules in a problem, we augment this data with proofs and employ two distinct models for our analysis: a generative transformer model, WP-BART, trained on comprehensive problems and their proofs showcasing its neural architecture, and a neuro-symbolic model, SIP-BART, which combines the generative capabilities of BART with a symbolic proof checker and focuses on individual proof steps. Our findings reveal that SIP-BART successfully circumvents reasoning shortcuts that WP-BART falls prey to, demonstrating superior generalization and neural memory in abstract reasoning tasks. Further examination of SIP-BART highlights novel reasoning inaccuracies not previously documented, leading to the formulation of a taxonomy categorizing four distinct types of reasoning pitfalls when leveraging pre-trained language models for logical reasoning tasks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Svetlin_Penkov1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11299v1",
  "title": "SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant",
  "modified_abstract": "Drawing inspiration from the innovative application of neural architectures to dissect RGB-D images for nuanced 3D object detection and few-shot concept classification, this paper seeks to address the challenge of aligning pre-trained vision encoders with large language models, a notable bottleneck in achieving seamless cross-modality interaction in vision-language tasks. Leveraging rich contextual information inherently present in visual data within dynamic environments, we present the first attempt to train a model on self-supervised learning to generate high-quality questions, thereby unlocking the underexplored potential within visual instruction data. Our novel framework, SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant, exhibits an advanced level of generalized visual understanding by proficiently generating flexible and meaningful image-related questions, grounded in visual clues and prior language knowledge in various applications, including but not limited to 3D object detection, few-shot classification, and compositional learning tasks. By fine-tuning SQ-LLaVA on higher-quality instruction data, we demonstrate a consistent performance improvement over traditional visual-instruction tuning methods, underscoring the efficacy of self-questioning techniques in deepening the model's comprehension of visual content across diverse contexts and visual formation systems, including but not limited to 3D object detection and few-shot classification.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Shamit_Lal1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11297v1",
  "title": "A Modified Word Saliency-Based Adversarial Attack on Text Classification Models",
  "modified_abstract": "Drawing inspiration from the foundational works that have advanced our understanding of attention mechanisms in deep neural networks, particularly their empirical success and theoretical underpinnings in text classification, this paper introduces a novel adversarial attack method targeting text classification models, termed the Modified Word Saliency-based Adversarial Attack (MWSAA). The technique leverages the concept of word saliency and attention to strategically perturb input texts in an effort to deceive classification models while preserving semantic coherence. MWSAA refines the traditional adversarial attack approach during the gradient descent training phase, thereby augmenting its capacity to elude detection by sophisticated classification systems. The method involves an analysis of identifying salient words in the input text through a saliency estimation process, which singles out words most influential to the model's decision-making process. These salient words are subsequently modified using carefully crafted perturbations, with guidance from semantic similarity metrics to ensure that the resulting text remains coherent and true to its original meaning. The dynamics of how words are selected and modified are critical to this success. Empirically, evaluations on various text classification datasets underscore the effectiveness of MWSAA in generating adversarial examples that successfully fool state-of-the-art models. Comparative analysis with existing techniques further establishes MWSAA\u2019s superior performance in simultaneously achieving a high attack success rate and preserving text coherence, thereby reinforcing the importance of structured training protocols in the development of adversarial techniques.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Haoye_Lu1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11292v1",
  "title": "Multi-Relational Graph Neural Network for Out-of-Domain Link Prediction",
  "modified_abstract": "Inspired by the advancement in graph neural networks (GNN) and the novel regularization techniques that aim to leverage the underlying structure of data in expressive ways, such as the Fiedler regularization approach, our research extends into the realm of dynamic multi-relational graphs. These graphs serve as a nuanced relational representation, encapsulating entities, their various types of relations, and the evolution of these relationships over time. In addressing predictive tasks over such complex, time-varying data structures, our work introduces a multi-relational GNN model that harnesses spectral theory and variational approaches to improve the connectivity and predictive accuracy across unseen domains. Our model leverages the spectral properties of graphs and approximation techniques to capture the nuanced dynamics and relationships, while variational techniques and bounds on performance enhance its adaptability and performance. By utilizing a theory-driven form of analysis and comparison to existing methodologies, our approach offers a robust framework for understanding and predicting the evolution of relationships in dynamic, multi-relational graphs.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Edric_Tam1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13843v1",
  "title": "Machine Learning and Vision Transformers for Thyroid Carcinoma Diagnosis: A review",
  "modified_abstract": "The increasing reliance on artificial intelligence (AI) to address the complexities of modern healthcare, particularly in diagnostic challenges like thyroid cancer (TC), forms the bedrock of our review. Inspired by advancements in algorithmic strategies for medical image analysis, such as the pioneering \u2018switching loss\u2019 function designed to enhance nucleus detection in histopathology by dealing with class imbalance and improving segmentation, our work explores the intersection of machine learning (ML), big data, and especially vision transformers in evaluating TC prognosis and assessing malignancy risk. This review provides an in-depth overview of AI-based approaches, centering on transformers\u2019 role in TC diagnostics, offering a novel categorization of these methodologies centered on AI algorithms, including deep learning models, the framework objectives, and computing environments supported by mini-batch processing. It critically analyzes the features of available TC datasets and the importance of segmentation in enhancing the accuracy of TC diagnosis. The review emphasizes AI's pivotal role in enhancing TC diagnosis and treatment through varying analytical modes (supervised, unsupervised, or mixed approaches), including detailed analysis of various types of losses to optimize model performance. Moreover, attention is given to how vision transformers continue to redefine the paradigms of medical diagnostics and disease management, drawing parallels with significant contributions like the switching loss function in other areas of medical image processing focusing on nucleus segmentation. The review discusses current progress and persistent challenges, paving the way for future research directions in ML-driven diagnostic systems for TC.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Gaurav_Patel2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13014v1",
  "title": "General Line Coordinates in 3D",
  "modified_abstract": "Building on the notion that interactive and interpretable visual pattern discovery can significantly enhance machine learning by allowing end users to intuitively navigate and understand complex data sets, this work introduces a novel approach to 3D visualization. Inspired by advancements in high-dimensional variational inference that shed light on balancing approximation accuracy with user interaction in model development, our paper leverages 3D General Line Coordinates (GLC) to facilitate lossless information preservation from high-dimensional spaces within a 3D visualization framework. The variational technique's ability to handle posteriors effectively informs our visualization strategy, offering an insightful bridge between abstract statistical concepts and their geometric representations. We present a system that integrates Shifted Paired Coordinates (SPC), Shifted Tripled Coordinates (STC), General Line Coordinates-Linear (GLC-L), and employs sampling methods to support interactive visual pattern discovery in ways that transcend the limitations of traditional 2D visualizations. This system not only enables a seamless transition from 2D to 3D visualizations for a clearer pattern recognition but also empowers users to identify optimal data viewing positions and control model overgeneralization, fostering a deeper, attribute-specific data analysis that is accessible to non-expert users.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Akash_Kumar_Dhaka1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13013v1",
  "title": "Hierarchical Classification for Intrusion Detection System: Effective Design and Empirical Analysis",
  "modified_abstract": "Prompted by the evolving landscape of cybersecurity threats amidst the proliferation of network technologies such as the Internet of Things (IoT), this study seeks to enhance the detection capabilities of Intrusion Detection Systems (IDS) against novel cyberattacks. Motivated by prior advancements in network technology and the need for refined security measures, including an improvement in the understanding of congestion control in IoT protocols, our research explores the potential of hierarchical classification as a novel approach in IDS. Unlike the predominantly used flat multi-class classification, which struggles to effectively differentiate between normal traffic and various types of attacks, hierarchical classification leverages the inherent structure of cyberattack types, grouping granular subtypes into overarching categories. Through a detailed empirical analysis employing a three-level hierarchical model across 10 different datasets and classification algorithms, we demonstrate that while the overall performance of hierarchical and flat classifications remains comparable in detecting normal and various attack types accurately, the hierarchical method notably reduces the misclassification of attacks as normal traffic\u2014a critical advantage for the protection of essential systems. This empirical control over misclassification underscores the importance of congestion control and analysis in the effective design and improvement of IDS. Our findings further suggest that the adjustment and control of communication protocols play a significant role in enhancing IDS efficacy.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Rahul_Sunil_Bhalerao1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11265v1",
  "title": "Forging the Forger: An Attempt to Improve Authorship Verification via Data Augmentation",
  "modified_abstract": "This study is situated within the emerging challenges of Authorship Verification (AV), inspired notably by advancements in areas that dabble in adversarial robustness and novel data manipulation techniques, as evidenced by the exploration of monotonicity in model training and its implications on model performance in various tasks, including regularization strategies. Authorship Verification is a critical text classification endeavor that seeks to determine the true authorship of text documents, a task complicated by adversaries aiming to either conceal their stylistic markers or mimic another\u2019s style. Our research explores the augmentation of classifier training sets with synthetic examples that are designed to mirror the style of the author of interest, with the aim of enhancing the classifier's ability to withstand adversarial attempts via methods that include minimization of differences between real and synthetic data. We delve into the efficacy of this approach through a comparative analysis of multiple generator architectures\u2014namely, Recurrent Neural Networks, small-scale transformers, and the GPT model\u2014paired with two distinct training methodologies grounded in Language Models and Wasserstein Generative Adversarial Networks, employing mixtures of data augmentation techniques. Extensive evaluations conducted across five datasets, including three curated to simulate adversarial scenarios, and employing Support Vector Machines and Convolutional Neural Networks as learning models, unfortunately, yielded mixed results. Despite achieving some success in adversarial settings, the inconsistency of benefits presented by our methodology indicates the need for further investigation into the practicality of its application in real-world AV challenges.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Joao_Monteiro1",
  "manipulated_ranking": 5,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11262v1",
  "title": "Understanding Diffusion Models by Feynman's Path Integral",
  "modified_abstract": "Our research advances the understanding of score-based diffusion models, which have become instrumental in image generation tasks, by drawing inspiration from a diverse range of previous works, including pioneering studies on unsupervised generative 3D shape learning from natural images. Such studies have laid the groundwork for exploring complex generative models without relying on traditional supervisory signals. In this context, we introduce a novel formulation of diffusion models using Feynman's path integral, originally developed for quantum physics, to address the performance disparities between stochastic and deterministic sampling schemes (i.e., the probability flow ODEs) observed in these models. This approach enables us to learn intricate patterns and shapes inherent in complex datasets, particularly emphasizing faces in our generative tasks. Our approach provides a comprehensive description of score-based generative models, demonstrates the derivation of backward stochastic differential equations, and lays out network training protocols including annotations to generator mappings. Additionally, we introduce an interpolating parameter that connects stochastic and deterministic sampling schemes, analogous to Planck's constant in quantum physics, which allows us to apply the Wentzel-Kramers-Brillouin (WKB) expansion to evaluate the negative log-likelihood. This analogy grants us new perspectives and tools to assess the performance disparities between different sampling schemes, thereby exposing critical insights into the architectural and training nuances of these networks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Attila_Szabo1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11261v1",
  "title": "A Lie Group Approach to Riemannian Batch Normalization",
  "modified_abstract": "Inspired by advances in adaptive gradient methods for Deep Neural Networks (DNNs) that have focused primarily on improving learning rates and convergence through various means such as block-diagonal matrix adaptation and spectrum-clipping, our work extends these foundational contributions to the domain of manifold-valued measurements, crucial in many computer vision and machine learning tasks. We propose a unified framework for Riemannian Batch Normalization (RBN) on Lie groups to address the gap in Riemannian normalization methods, which have often been derived in an ad-hoc manner and limited to specific manifolds. Our theoretical contributions ensure control over both the Riemannian mean and variance, particularly focusing on Symmetric Positive Definite (SPD) manifolds with three distinct Lie group structures due to their direct adaptation of the normal distribution to the geometry of these spaces. To further enrich the model's efficiency, we incorporate layer-wise adaptation techniques that utilize diagonal gradient adjustments for improved performance across networks. We explore the deformation concept to generalize the Lie group structures on SPD manifolds into three parameterized families, leading to specific normalization layers through the diagonal adaptation for SPD neural networks. The effectiveness of our approach is validated through experiments in radar recognition, human action recognition, and EEG classification, with our code available at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jihun_Yun2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11259v1",
  "title": "A learning-based solution approach to the application placement problem in mobile edge computing under uncertainty",
  "modified_abstract": "This research is motivated by the advancements in learning-based methods for complex optimization problems under uncertainty, akin to the progress made in fields like neural ordinary differential equations, where learning-based methods such as GNesterovNODEs significantly improve efficiency and accuracy. Addressing the challenge of application placement in mobile edge computing involves navigating a high-dimensional problem space fraught with uncertainty, requiring an approach that can efficiently maximize quality of service while adhering to technical constraints. In this context, we propose utilizing machine learning to emulate optimal solutions for dynamically allocating user requests to edge servers, considering factors such as spatial positions and request rates. The study formulates the problem through a two-stage stochastic programming and employs Support Vector Machines (SVM) and Multi-layer Perceptron (MLP) as learning models to approximate solution variables for request allocation. Our methodology involves generating extensive training data encompassing varying user locations and request rates, allowing the learning models to effectively mimic the optimization model's decisions. The efficacy of this approach is evidenced by an execution effectiveness exceeding 80%, showcasing the potential of machine learning models to significantly streamline solutions times for high-dimensional problems in mobile edge computing under conditions of uncertainty, thus leading towards an accelerated solution process. This accelerated process underlines the capability of learning-based solutions in enhancing the allocation of applications to edge servers, presenting a substantial improvement over traditional optimization techniques.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Thieu_Vo1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12106v1",
  "title": "Circular Belief Propagation for Approximate Probabilistic Inference",
  "modified_abstract": "Inspired by preceding works that tackle high-dimensional optimization problems and sparse data recovery through innovative algorithmic approaches, such as Iteratively Reweighted Least Squares (IRLS), this paper introduces Circular Belief Propagation (CBP), an advanced probabilistic inference algorithm. CBP extends traditional Belief Propagation (BP) techniques by overcoming their primary limitation of only being exact on cycle-free graphs. Through the incorporation of mechanisms for detecting and canceling spurious correlations and belief amplifications caused by cycles, as well as integrating non-smooth optimization techniques for low-rank approximations, CBP enhances the algorithm's applicability and performance in handling complex probabilistic graphs, even in cases that demand computational feasibility. Our numerical experiments validate that CBP not only converges to accurate solutions but also exhibits excellent convergence and optimization performance over BP in scenarios involving high-dimensional signal space, demonstrating its competitive edge in several learning contexts rooted in the convergence theory. This opens new avenues for research in both neuroscience and artificial intelligence domains.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Claudio_Mayrink_Verdun1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11230v1",
  "title": "Simple 2D Convolutional Neural Network-based Approach for COVID-19 Detection",
  "modified_abstract": "Our research is motivated by the urgent need for effective COVID-19 detection tools and the potential of deep learning to address this requirement through the analysis of lung Computed Tomography (CT) images. Given the challenges posed by classic deep learning methods, including dealing with varying slice counts and resolutions in CT images due to the use of diverse scanning equipment typically requiring predictions on single slices to be combined for a comprehensive outcome but failing to learn features specific to each slice, our study proposes a novel Spatial-Slice Feature Learning (SSFL++) framework designed for CT scan analysis. This framework significantly reduces data redundancy by filtering out out-of-distribution (OOD) data within the entire CT scan and selecting key spatial-slice features, achieving a 70% reduction of data redundancy. Additionally, we have integrated a Kernel-Density-based slice Sampling (KDS) method, enhancing the stability during both the training and inference phases. This, in turn, accelerates convergence and improves the overall performance of our simple, yet effective, EfficientNet-2D (E2D) model through a more focused selector in the learning process. The effectiveness of our approach is empirically validated on the COVID-19-CT-DB datasets made available by the DEF-AI-MIA workshop. The simplicity and potency of our approach resonates with the emerging trend of seeking balance between performance and resource demand in medical imaging model selection, acknowledging the necessity to restrain the carbon footprint alongside optimizing accuracy and efficiency in the learning and selection process.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Raghavendra_Selvan1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11227v1",
  "title": "Cheap Ways of Extracting Clinical Markers from Texts",
  "modified_abstract": "In an age where the adaptability and dynamic nature of language is critically examined through various distributional tools and methodologies, such as detecting corpus-level language shifts and domain adaptation, our research targets the practical application of these linguistic advancements within the medical field. Specifically, this paper outlines the UniBuc Archaeology team's contribution to the CLPsych's 2024 Shared Task, which focuses on identifying textual evidence supporting assigned suicide risk levels through innovative clustering techniques and adaptation strategies for language models. Our exploration evaluates the efficacy of Large Language Models (LLM) against a memory and resource-efficient alternative that employs a traditional machine learning (GOML) pipeline. This pipeline adapts through the use of a tf-idf vectorizer and a logistic regression classifier for extracting relevant text spans, anchored by clinical markers, while the more resource-intensive LLM approach generates comprehensive summaries, directed by chain-of-thought reasoning, to highlight clinical markers. The incorporation of graph-based methodologies to enhance clustering accuracy and embeddings alignment further underscores the versatility of linguistic analysis in practical healthcare applications. This comparative study not only seeks to enhance the efficiency and accessibility of extracting clinical markers from texts but also underscores the importance of leveraging linguistic research for practical, life-saving applications in healthcare.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Vin_Sachidananda1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11220v2",
  "title": "CPA-Enhancer: Chain-of-Thought Prompted Adaptive Enhancer for Object Detection under Unknown Degradations",
  "modified_abstract": "Building upon insights from a breadth of works in object detection and semi-supervised learning, such as exploring the robustness and efficiency of utilizing uncertainty-guided pseudo labels for video object detection, our work introduces a pioneering approach in the realm of object detection under unknown degradations. Recognizing the limitations inherent in existing methods that require prior knowledge of degradation types and necessitate training separate network models for each, we propose the CPA-Enhancer, a novel chain-of-thought (CoT) prompted adaptive enhancer. This approach marks the first utilization of CoT prompting to dynamically adapt enhancement strategies for degraded images in object detection tasks, without the prerequisite of knowing the degradation type. Positioned as a versatile, plug-and-play model, the CPA-Enhancer is designed to easily integrate with any generic detectors, providing significant improvements in object detection on degraded images. It also bolsters performance in semi-supervised learning scenarios with a mix of labeled and unlabeled data by generating uncertainty-guided pseudo labels. Our empirical research demonstrates that CPA-Enhancer not only advances the state of the art in object detection amidst unknown degradations but also significantly enhances the performance in other downstream vision tasks affected by such degradations, including video object detection.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yongri_Piao1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11206v1",
  "title": "CBR -- Boosting Adaptive Classification By Retrieval of Encrypted Network Traffic with Out-of-distribution",
  "modified_abstract": "Informed by numerous advancements in efficient edge computation and selective query frameworks for enhancing machine learning (ML) applications on edge devices, our exploration into network security through encrypted traffic classification brings forth a novel perspective. This paper introduces Adaptive Classification By Retrieval (CBR), a novel approach in encrypted network traffic classification leveraging an Artificial Neural Network (ANN)-based method with deep supervision, circumventing the conventional challenges of fixed-class ML or Deep Learning applications - notably, the misclassification when encountering unknown classes. Unlike traditional methods that necessitate frequent retraining with the advent of new classes, our approach allows dynamic adaptation and identification of both existing and new classes through learning mechanisms demonstrating fewer shots requirements, thereby reducing latency in model updating. Incorporating examples from the edge computing environment, we highlight edge-specific challenges such as real-time data processing and analysis, which are exacerbated by slow uploading times for new data samples. Moreover, by effectively querying encrypted traffic data, our method enhances classification processes with notable reductions in latency. The comparative analysis reveals that our CBR approach aligns closely with results from Random Forest (RF) techniques, showing only up to a 5% discrepancy in performance metrics, with a slight decrement in accuracy for samples pertaining to new classes. This methodology underlines the potential of CBR as both a standalone and complementary tool in the classification toolkit for real-time encrypted traffic analysis, showcasing the adaptability and efficiency of our method in a landscape that continuously evolves, including cloud computing scenarios.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Igor_Fedorov1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11204v1",
  "title": "Partitioned Neural Network Training via Synthetic Intermediate Labels",
  "modified_abstract": "In an era where artificial neural network models, especially those in deep learning, rapidly grow in complexity and size, training these models becomes increasingly resource-intensive, highlighting the need for innovative, energy-efficient training methodologies. This study is propelled by insightful precedents such as recent breakthroughs in Spiking Neural Networks (SNNs), which exhibit neuromorphic computing characteristics and have introduced novel training methods like online training through time (OTTT) to achieve high performance while mitigating memory consumption and crafting spike-based representations, despite facing challenges such as large memory requirements and inconsistency with the online property of biological learning. Our work ventures into a pioneering approach of partitioning neural network models across GPUs and utilizing synthetic intermediate labels for training individual model segments. This technique, optimized for both on-chip hardware limitations and computational efficiency, is designed to reduce the memory overhead and computational load, thereby addressing the significant challenges posed by GPU memory constraints and the extensive communication data between partitions in existing strategies like data parallelism, model parallelism, and pipeline parallelism. Through an innovative application of generating synthetic intermediate labels for segmented training, complemented by spike-based tracking for performance assessment, we demonstrate the potential of this method by partitioning a 6-layer fully connected neural network into two parts and evaluating its performance on the extended MNIST dataset. The results underscore the effectiveness of our proposed approach in maintaining model accuracy while creating a significant reduction in memory and computational demands, showcasing it as a step towards more resource-efficient training of large-scale neural networks and further enriching the landscape of neural network training methodologies.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Qingyan_Meng1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11199v1",
  "title": "Graph Unitary Message Passing",
  "modified_abstract": "Informed by the strides in addressing graph-related challenges, such as those tackled by the Neural Bellman-Ford Networks for link prediction through innovative path-based representation learning frameworks, we introduce Graph Unitary Message Passing (GUMP). This novel approach aims to mitigate the oversquashing problem inherent in Graph Neural Networks (GNNs) by leveraging a unitary adjacency matrix for enhanced message passing, marking a shift towards both transductive and inductive learning paradigms. To develop GUMP, we initially transform general graphs to have unitary adjacency matrices while preserving structural bias, catering to both inductive and multi-relational graph learning tasks. Subsequently, we employ a unitary projection algorithm, supported by operators that ensure the intrinsic path-based structure of these matrices, ensuring GUMP's permutation-equivariance and bolstering its learning mechanisms. Through empirical evaluation across a variety of graph learning tasks, our findings validate GUMP's capacity to significantly enhance performance in prediction tasks, thereby affirming the method's potential in refining and extending the utility of GNNs.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Louis-Pascal_A._C._Xhonneux1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13010v1",
  "title": "A Dual-Tier Adaptive One-Class Classification IDS for Emerging Cyberthreats",
  "modified_abstract": "In response to the constantly evolving landscape of cyber threats and leveraging lessons from cutting-edge research in machine learning that includes novel methods like leave-one-out conditional mutual information (loo-CMI) for evaluating the generalization of ML models, this paper introduces a one-class classification-driven Intrusion Detection System (IDS) that is particularly adept at tackling the challenges posed by new and unforeseen cyberattacks. Our system is designed to address the shortcomings of traditional machine learning-based IDSs, which struggle to identify novel attack vectors due to the dynamic nature of cyber threats and the scarcity of labeled attack data. By proposing a dual-tier framework where the first tier differentiates between normal and potentially harmful activities and the second tier utilizes a multi-classification mechanism along with a clustering approach and cross-validation leave-one-out approaches to distinguish between known and unknown attacks, our model not only fortifies defenses against currently recognized threats but also ensures robust generalization and learns from unseen attack patterns to enhance its predictive capabilities over time. The application of conditional logic and one-class classifiers (OCC) in both tiers mitigates the issue of data scarcity and effectively separates unknown attacks from known ones, making our approach not only innovative but also highly adaptable to the fast-paced evolution of cyber threats. Our evaluations highlight the potential of this framework for effective real-world deployment, setting a benchmark for future research in IDS.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Aditya_Golatkar1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11180v1",
  "title": "usfAD Based Effective Unknown Attack Detection Focused IDS Framework",
  "modified_abstract": "In addressing the escalating issues of cyber threats within diverse networked environments, such as the Internet of Things (IoT) and Industrial Internet of Things (IIoT), our research contributes to the critical domain of Intrusion Detection Systems (IDS). This work is motivated by the pioneering strategies in semi-supervised and self-supervised learning, such as those discussed in recent advances like CADet, which leverages contrastive learning for anomaly detection without requiring direct access to out-of-distribution examples. Drawing from this foundation, our study introduces two innovative semi-supervised learning strategies for developing IDS capable of identifying unknown or zero-day attacks without the necessity for attack samples during training. These methods include: 1) deploying a supervised machine learning model trained with randomly generated synthetic attack samples, influenced by the principles of adversarial training, notably utilizing perturbations, and 2) crafting a One Class Classification (OCC) model trained purely on benign network traffic, utilizing contrastive learning techniques to differentiate between normal and anomalous patterns, which are further enhanced by strategic data augmentations. Through extensive experimentation with 10 recent benchmark IDS datasets, our findings showcase the superior performance of the OCC model, particularly our proposed usfAD (Uniformly Scattered Feature-based Anomaly Detection) technique, which relies heavily on self-supervised learning principles. This approach not only surpasses conventional supervised classification methods but also outperforms other OCC techniques, proving its efficacy in real-world scenarios for detecting previously unseen attacks. The success of usfAD underlines the importance of crafting models that can learn to identify anomalies without explicit labels for each specific attack.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Joao_Monteiro1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13842v1",
  "title": "Analyzing the Variations in Emergency Department Boarding and Testing the Transferability of Forecasting Models across COVID-19 Pandemic Waves in Hong Kong: Hybrid CNN-LSTM approach to quantifying building-level socioecological risk",
  "modified_abstract": "Motivated by the prior advancements in machine learning techniques for efficient data processing and model optimization, such as the development of dataset pruning methods that enable more effective sample selection and optimization-based techniques for training increasingly generalizable models, our study addresses the critical issue of emergency department (ED) boarding. Specifically, we examine the variations in ED boarding times in Hong Kong during the COVID-19 pandemic and assess the transferability of forecasting models across different pandemic waves using a hybrid convolutional neural network (CNN)-Long short-term memory (LSTM) approach. We utilized public-domain data from Hong Kong's Hospital Authority, Department of Health, and Housing Authority to apply our model with an enhanced focus on dataset pruning and effective sample selection to ensure the reliability of our forecasts. Our analysis aimed to identify which phase of the COVID-19 pandemic most significantly impacted the healthcare system, uncovering a stable pattern of interconnectedness among its components through deep transfer learning methodology. Our findings indicate that the greatest proportion of days with ED boarding was between the fourth and fifth waves of the pandemic, where the forecasting model based on features representing the built environment and sociodemographic profiles of residential buildings, combined with the historical time series of ED boarding and case counts, performed best. Moreover, when applying the model developed from data between the fourth and fifth waves to other waves via deep transfer learning, we observed enhanced performance compared to indigenous models. This study not only provides insights into ED boarding dynamics during the pandemic but also demonstrates the potential of hybrid CNN-LSTM models in forecasting complex healthcare system metrics and ensuring the generalization of findings across different scenarios, increasingly proving to be a crucial tool in anticipatory healthcare management.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zeke_Xie1",
  "manipulated_ranking": 4,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11175v1",
  "title": "Prior-dependent analysis of posterior sampling reinforcement learning with function approximation",
  "modified_abstract": "This work is motivated by the pressing need to enhance randomized exploration in reinforcement learning (RL), with a keen interest in function approximation techniques and their implications for the operational efficacy of RL models. Our investigation is inspired by foundational achievements that have explored episodic reinforcement learning in continuous state-action spaces, including episodes and planning strategies, and the seminal work on the bilinear exponential family of MDPs, which introduced order-optimal regret bounds and tractable exploration/planning algorithms. Against this backdrop, we advance the discourse by establishing the first prior-dependent Bayesian regret bound for RL with function approximation, modeled by linear mixture MDPs in a specific setting. By employing a value-targeted model learning perspective that encompasses a decoupling argument and a variance reduction technique alongside estimators for approximating the model's dynamics, our methodology circumvents traditional dependencies on confidence sets and concentration inequalities, thereby formalizing Bayesian regret bounds with greater efficacy for continuous planning in RL across episodes of varying length. Our analysis respectfully acknowledges the episodic nature of reinforcement learning and contextualizes our contributions within a broader family of reinforcement learning theories, contributing to the discourse on algorithms and episodes in the continuous planning space.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~reda_ouhamma1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11173v1",
  "title": "Multi-Objective Evolutionary Neural Architecture Search for Recurrent Neural Networks",
  "modified_abstract": "Drawing inspiration from the sophisticated domain of automated machine learning (AutoML), particularly in the automation of deep learning architectures that require significant computational expense and domain expertise, our research extends the frontier of neural architecture search (NAS) to the complex realm of recurrent neural networks (RNNs). While neural architecture search has shown promise in automating the design of neural network architectures, yielding designs that eclipse manually constructed alternatives in performance, the focus has predominantly been on feedforward architectures, leaving the territory of multi-objective RNN design relatively uncharted. RNNs, pivotal in sequential data modeling and a backbone of natural language processing tasks, encapsulate a vast potential for optimization in model accuracy, learning efficiency, and architectural complexity. Recognizing the real-world trade-offs between model accuracy and computational efficiency, we introduce a multi-objective evolutionary algorithm tailored for RNN architecture search. Our research leverages approximate network morphisms and automated strategies, driven by state-of-the-art algorithms, to navigate the optimization of the search space towards RNN architectures that balance performance metrics with reduced computational demands effectively. This endeavor enriches the current understanding and capabilities of NAS and learning automation, pointing towards a future where the automated design of RNN architectures can be accomplished with nuanced consideration of multiple objectives, underpinning advancements in both learning efficiency and performance.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Tejaswini_Pedapati1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11166v1",
  "title": "Pencil: Private and Extensible Collaborative Learning without the Non-Colluding Assumption",
  "modified_abstract": "Amidst growing data privacy concerns, collaborative neural network training faces significant hurdles, especially when data ownership and model training responsibilities are decentralized. This work is inspired by diverse efforts in the machine learning community that have explored federated learning (FL), privacy-preserving ML through cryptographic constructs like homomorphic encryption (HE) and secure multiparty computation (MPC), as well as strategies to counter the challenges of non-iid data in federated learning, to propose more holistic and inclusive solutions. However, existing approaches, such as FL, neglect model privacy, whereas HE and MPC-related solutions, though improving privacy, are limited by their assumptions, such as non-colluding servers\u2014a constraint hardly met in real-world scenarios. 'Pencil' is introduced as a pioneering framework that ensures private and extensible collaborative learning, offering three primary benefits\u2014data privacy, model privacy, and extensibility across multiple clients, all achieved without resting on the non-colluding assumption. Incorporating several novel cryptographic protocols and deep learning techniques, Pencil is meticulously designed as a scalable solution that leverages an efficient two-party protocol as its foundation, allowing seamless sharing and transitions between different data providers without incurring additional costs. Through rigorous security, privacy analyses, and comprehensive evaluations on large datasets, including image data collected from clients', we illustrate that Pencil not only retains nearly identical test accuracies compared to models trained in plaintext but also significantly lowers the training overhead by efficiently handling large, latent dataset features, achieving substantial improvements in throughput and communication efficiency while ensuring resilience against both existing and adaptive attacks, differentially protecting the privacy of all participants.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Huancheng_Chen1",
  "manipulated_ranking": 4,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11163v1",
  "title": "A Selective Review on Statistical Methods for Massive Data Computation: Distributed Computing, Subsampling, and Minibatch Techniques",
  "modified_abstract": "This paper is a discourse on the evolution of statistical computation to tackle the gargantuan datasets that have become a mainstay in the modern data-driven world, inspired by significant breakthroughs such as the EF-BV theory which bridges the gap between error feedback and variance reduction in distributed optimization. Our review zeroes in on three pivotal domains: distributed computing, where datasets transcend the memory capacity of a single machine necessitating a networked computational approach coupled with data compression and top-k communication techniques to manage the bandwidth and communication constraints; subsampling methods designed for datasets that, while small enough to reside on a single computer, prove onerous to process in totality due to memory constraints; and minibatch gradient techniques that optimize deep learning models through efficient data processing batches, demonstrating significant convergence properties in the journey towards algorithmic efficiency. The convergence of these domains under the aegis of massive data computation, supported by concrete proof of concept and innovative methods, offers a comprehensive perspective to address and navigate the complexities posed by large-scale data processing, underscored by evolutionary strides in error-feedback optimization, algorithm efficiency, and learning methodologies.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Laurent_Condat1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11162v1",
  "title": "CGI-DM: Digital Copyright Authentication for Diffusion Models via Contrasting Gradient Inversion",
  "modified_abstract": "Motivated by the extensive body of work in image processing and generation, including novel techniques for exploratory decoding of compressed images and their decompression, our study introduces CGI-DM (Contrasting Gradient Inversion for Diffusion Models), a groundbreaking approach for digital copyright authentication in the realm of advanced image generation. CGI-DM leverages the algorithms involved in decompression and employs an exploration of unique architectural design nuances to achieve its objectives. Diffusion Models (DMs) have significantly advanced as tools for few-shot image generation, where a pretrained model is fine-tuned on a small set of images to capture a specific style or object. However, the proliferation of these models raises copyright concerns due to the potential use of unauthorized data. CGI-DM mitigates these concerns by presenting a novel methodology that leverages the conceptual differences between pretrained and fine-tuned models for image recovery and copyright authentication through a unique exploration and comprising of algorithms. Our approach involves removing partial information of an image and recovering missing details through the exploitation of KL divergence between the latent variables of the two models, followed by a recovery process using Monte Carlo sampling and Projected Gradient Descent (PGD) within a specific architectural design. The fidelity of the original and recovered images serves as a strong indicator of potential copyright infringements. Extensive empirical evaluations on WikiArt and Dreambooth datasets validate the high accuracy of our method, outperforming existing techniques in the field. Code implementation is available at [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yuval_Bahat2",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11144v1",
  "title": "Is Mamba Effective for Time Series Forecasting?",
  "modified_abstract": "Drawing inspiration from the rich vein of research into state space models (SSMs) and their historical robustness in understanding serial correlations within sequential data across various fields like neuroscience, psychology, and econometrics, our study focuses on the realm of time series forecasting (TSF). The Transformer model, despite its capacity in capturing global contexts and long-range dependencies, faces challenges stemming from its computational inefficiencies, sparking interest in alternatives like Mamba. Unlike conventional models, Mamba's allure comes from its handling of complex, unstructured dependencies with near-linear complexity, having shown promising results in text and image processing tasks. This encourages us to probe Mamba's utility in TSF, leading to the development of two SSM-based models, S-Mamba and D-Mamba, employing the innovative Mamba Block to discern serial variate correlations and dynamic structure effectively. Our findings evidence that, besides ensuring superior performance, S-Mamba and D-Mamba contribute to significant savings in GPU memory and training time, showcasing their dynamic adaptability and estimation strength as estimators in the TSF context, addressing both non-asymptotic and serial dependency challenges with their ability to model variational dynamics. Through exhaustive experimental analysis, this paper illuminates the comparative utility of Mamba against the Transformer within TSF, marking a novel trajectory of inquiry in the field. Code is available at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Song_Wei1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11125v1",
  "title": "Machine learning-based system reliability analysis with Gaussian Process Regression",
  "modified_abstract": "This work is rooted in the evolving domain of machine learning-based reliability analysis, drawing inspiration from existing literature on Gaussian Process (GP) Regression, such as explorations into scalable GP regression that notably refine predictive uncertainties. Emphasizing computational efficiency and accuracy, our research proposes several theorems to explore the theoretically optimal learning strategy for reliability analysis, including a focus on both considering and neglecting correlations among candidate design samples. We demonstrate that the widely recognized U learning function can be reinterpreted as the optimal learning function in scenarios that disregard the Kriging correlation. Moreover, we delve into the theoretical framework for the optimal learning strategy involving sequential multiple training samples enrichment,\u00a0articulated through Bayesian estimates, posterior distributions, and associated loss functions with a variational approach. Our simulations suggest that strategies incorporating the Kriging correlation and utilizing a combination of scalable approaches and large datasets prevail over those that omit it and other contemporary learning functions in literature, achieving a superior reduction in the number of function evaluations of performance through inference. Nonetheless, implementing these optimal, variational strategies is computationally demanding, highlighting a significant area for further investigation.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Geoff_Pleiss1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11114v1",
  "title": "Phasic Diversity Optimization for Population-Based Reinforcement Learning",
  "modified_abstract": "Motivated by the aim to surpass existing diversity Reinforcement Learning (RL) techniques, which primarily utilize augmented loss functions to achieve a balance between reward and diversity, our work introduces the Phasic Diversity Optimization (PDO) algorithm. This innovative approach is informed by the exploration of complex dynamics in co-operative multi-agent RL systems, such as those highlighted in related research like backdoor attacks and poisoning in RL, which demonstrate the nuanced challenges of optimizing for both performance and safety in RL environments, including vulnerability to various types of attacks. PDO, a Population-Based Training framework, distinctively decouples reward and diversity optimization by dividing training into a structured pipeline of separate phases, allowing for a specialization of objectives that addresses the inherent limitations found in conventional methods, such as the dynamic distribution of reward signals for Multi-armed Bandits or the conflict between quality and diversity. Our algorithm furthers learning into this domain by creating an aggressive diversity optimization strategy in its auxiliary phase, which enhances performance in simulated games scenarios without deterring from main objectives. We validate PDO through extensive experimentation in a unique dogfight scenario for aerial agents and simulations in MuJoCo, demonstrating superior performance compared to traditional baselines. The findings underscore the efficacy of our method, indicating a promising direction for achieving advanced optimization in population-based RL systems.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Xueluan_Gong1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11106v1",
  "title": "Self-Supervised Quantization-Aware Knowledge Distillation",
  "modified_abstract": "Inspired by the evolution of self-supervised learning methods and their potential to circumvent the need for labeled data, as demonstrated in works like Label-free Concept Bottleneck Models, our study proposes a novel integrative framework, Self-Supervised Quantization-Aware Knowledge Distillation (SQAKD), that marries Quantization-aware training (QAT) and Knowledge Distillation (KD) to craft low-bit deep learning models efficiently. The current landscape of applying KD to QAT is marred by the need for extensive hyper-parameter tuning, reliance on labeled data, and computationally heavy training routines. SQAKD innovatively overcomes these hurdles by unifying the forward and backward dynamics of quantization functions for enhanced adaptability across various QAT applications, including those involving large-scale neural network architectures, and by formulating a co-optimization problem that minimizes both the KL-Loss (for KD) and the discretization error (for quantization) without the need for label supervision, addressing the performance bottleneck. Our comprehensive evaluations affirm that SQAKD significantly surpasses existing QAT and KD methods across multiple model architectures, validating its utility in human-independent learning environments and its application in concept-based learning scenarios. Our code is available at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Tuomas_Oikarinen1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11103v1",
  "title": "ProgGen: Generating Named Entity Recognition Datasets Step-by-step with Self-Reflexive Large Language Models",
  "modified_abstract": "Drawing inspiration from the contemporary challenges in tracking entities within textual data\u2014a crucial concept explored through extensive work, including the study on tracking entities in open domain procedural text\u2014our research introduces a novel approach that leverages Large Language Models (LLMs) for enhancing named entity recognition (NER) dataset generation. Despite the impressive adaptability of LLMs across numerous domains, their performance in structured knowledge extraction tasks like NER often remains wanting. Our methodology innovates by using class-conditional prompts in combination with self-reflection by LLMs on their domain-specific knowledge (e.g., emotions and categories in movie reviews) to yield highly relevant attributes for NER datasets. By first generating entity terms and subsequently constructing NER context around them, we mitigate the typical challenges LLMs face with complex structures and for achieving greater accuracy and completeness. Through experimental validation across diverse domains, our approach showcases not only superior performance to traditional data generation methods but also achieves greater cost-efficiency than conventional alternatives.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Dheeraj_Rajagopal1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11100v1",
  "title": "Graph Expansion in Pruned Recurrent Neural Network Layers Preserve Performance",
  "modified_abstract": "Taking inspiration from seminal works in graph-based deep learning, particularly those exploring the efficiency of graph neural networks under conditions of sparsity and limited labeled data, our research focuses on the application of graph expansion principles in the pruning of recurrent neural networks (RNNs) including LSTMs. By maintaining the expansion property, characterized by strong connectivity despite sparsity, we demonstrate that it is possible to prune RNNs significantly while ensuring the preservation of performance across a variety of tasks, even under low-labeling conditions. Our investigation into the spectral gap and layerwise expansion properties of the pruned networks, along with a detailed analysis of the bipartite layers within time unfolded recurrent network graphs, reveals that these graph properties are crucial for maintaining classification accuracy on benchmark datasets such as MNIST, CIFAR-10, and Google speech command data. Our findings highlight the potential of leveraging graph theoretical principles and diffusion processes in designing efficient, resource-conscious recurrent network architectures without sacrificing task performance, especially networks specifically tailored for various tasks during the training phase. Additionally, this approach aligns with current trends emphasizing the importance of efficient learning mechanisms in the processing of vast amounts of labeled deep training data.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Hedi_Xia2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11093v1",
  "title": "Learning-Based Pricing and Matching for Two-Sided Queues",
  "modified_abstract": "This study is motivated by and contributes to the burgeoning field of dynamic systems optimization within diverse two-sided markets, such as ride-sharing platforms, which require delicate balancing between supply and demand under uncertainty. Drawing upon insights from the exploration of general function approximation in games, which often includes two-player and zero-sum scenarios, and decision-making under uncertainty in various settings, we concentrate on a dynamic system with multiple types of customers and servers, each forming separate queues that establish a bipartite graph of customer and server queues. The platform's objective is to design pricing and matching algorithms that maximize profit while managing the demand and supply dynamics through unknown functions, alongside maintaining queue lengths below a certain threshold that considers the system's capacity. To address these complex challenges, we introduce a learning-based pricing algorithm that combines gradient-free stochastic projected gradient ascent with bisection search, and a longest-queue-first matching strategy. Our algorithm's efficacy is underscored by a theoretically substantiated sublinear regret $\\tilde{O}(T^{5/6})$ and queue-length control $\\tilde{O}(T^{2/3})$, where $T$ is the operational time horizon, alongside a rigorous analysis of the trade-off between regret and queue-length that sheds light on the operational flexibility in managing two-sided platforms in zero-sum settings. Thus, our work not only advances the theoretical understanding of learning-based pricing and matching in two-sided queues but also offers a practically viable strategy for platform management under dynamic settings.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Baihe_Huang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11090v1",
  "title": "Brain-on-Switch: Towards Advanced Intelligent Network Data Plane via NN-Driven Traffic Analysis at Line-Speed",
  "modified_abstract": "Driven by the transformative potential of emerging programmable networks, this work is inspired by advancements across various domains such as Autonomous Mobility-on-Demand (AMoD) systems, where learning-based models like graph neural networks are leveraged for decision-making in dynamically coordinated fleets. Leveraging this innovative spirit, our research introduces Brain-on-Switch (BoS), a novel initiative revolutionizing the Intelligent Network Data Plane (INDP) by introducing Neural Network (NN) driven traffic analysis capable of operating at line-speed. BoS illuminates the path for transportation networks, enhancing the learning capabilities within network hardware to unprecedented levels. Historically, INDP research has predominantly focused on deploying tree/forest models, despite their inherent limitations due to the computational constraints of network hardware. BoS transcends these limitations by employing NN models, such as Recurrent Neural Networks (RNNs) and transformers, designed for sequential data processing, thereby circumventing the need for complex on-the-fly feature computations. This adaptation not only enhances representation capabilities within the data plane but also addresses the challenge of adapting the recurrent computational schemes vital to RNN inference for compatibility with the match-action paradigm native to network data planes. Our contributions include (i) a data plane-friendly RNN architecture enabling the execution of unlimited RNN timesteps within constrained data plane stages for real-time inference and (ii) a complementary off-switch transformer-based traffic analysis module potentially relevant in urban and inter-city travel scenarios, to enhance overall system performance. The implementation of BoS, using a P4 programmable switch, and its extensive evaluation across multiple traffic analysis tasks demonstrates notable improvements over existing state-of-the-art models in both analysis accuracy and scalability.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~James_Harrison1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11087v2",
  "title": "Incorporating Higher-order Structural Information for Graph Clustering",
  "modified_abstract": "Our investigation takes inspiration from the burgeoning field of graph analysis, where recent explorations, such as the Brain Network Transformer, have demonstrated the efficacy of Transformer-based models in handling complex data structures like brain networks by leveraging both the graph-level and node-level information with an emphasis on attention mechanisms. In the domain of data mining, where clustering plays a crucial role, this research proposes an innovative graph clustering network aimed at fully utilizing graph structural information, a dimension most existing methods overlook, especially the critical higher-order structures that define distant connections within the same cluster. To achieve this, we introduce a graph mutual infomax module designed to maximize mutual information between graph and node representations effectively, supplemented by a trinary self-supervised module that incorporates modularity as a structural constraint, distinguishing our approach in the landscape of deep clustering methods. The design of this model is deliberately intended to capture the intricate details of higher-order structural information, and its functional significance is central to our study. The effectiveness of our model is confirmed by its superior performance against several state-of-the-art methods across diverse datasets, and an extensive evaluation is conducted to assess its potential. Our findings also underscore the model's capability to advance the understanding and implementation of graph-based data analysis in both generic datasets and specific applications such as brains, potentially contributing to the identification and understanding of various disorders.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Hejie_Cui1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11082v1",
  "title": "RobustSentEmbed: Robust Sentence Embeddings Using Adversarial Self-Supervised Contrastive Learning",
  "modified_abstract": "Building upon the insights from the recent advancements in large-scale multi-task and transfer learning approaches, as exemplified by works like the introduction of models capable of handling an extensive array of supervised NLP tasks, this paper presents RobustSentEmbed, a novel self-supervised sentence embedding framework. Its design centers on enhancing both the generalization capability and robustness of pre-trained language models (PLMs) against various adversarial attacks across multiple domains. By innovatively integrating high-risk adversarial perturbations into a new objective function and employing denoising techniques, RobustSentEmbed successfully generates sentence embeddings that excel in quality and durability, while massively scaling up the robustness. Co-training strategies were leveraged to further amplify the robustness and generalization across distinct NLP tasks, which alongside large-scale processing, underscores our framework's efficacy. Empirical evaluations attest to its superior performance, especially highlighting a dramatic decrease in vulnerability to adversarial attacks, exemplified by halving the success rate of the BERTAttack, alongside noticeable improvements in tasks measuring semantic textual similarity and transfer learning effectiveness. Furthermore, rigorous experiments underpinning our framework's efficacy in a diverse collection of texts.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jai_Gupta1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11079v1",
  "title": "Bridging Expert Knowledge with Deep Learning Techniques for Just-In-Time Defect Prediction",
  "modified_abstract": "Our study addresses the growing importance of Just-In-Time (JIT) defect prediction within the software engineering domain, particularly the challenge of optimizing feature representation for predictive accuracy. This challenge resonates across various fields, illustrated by prior efforts such as the development of 'Deequ', a tool for automating the verification of data quality at scale in data science, and the rigorous reviews of datasets for anomaly detection. Drawing inspiration from related work that leverages expert knowledge for enhancing data processing capabilities, this research proposes a novel approach to JIT defect prediction. It combines the complementary strengths of simple models, grounded in expert knowledge and encapsulated through hand-crafted features, with the semantic depth of complex models derived from deep learning techniques. To achieve a synergistic integration of these methodologies, we introduce a model fusion framework named SimCom++, which employs both early and late fusion strategies for feature and decision-level integration, respectively. Throughout this process, validation plays a crucial role in ensuring the robustness and reliability of the predictive models. Our experimental investigations validate that SimCom++ significantly surpasses existing baselines, highlighting the potential of hybrid models in leveraging the dual advantages of expert knowledge, automated feature extraction, and thorough validation processes for advanced predictive tasks. The iterative process of model refinement and validation incrementally advances the field, demonstrating the effectiveness and necessity of integrating deep learning techniques with manual verification and reviews for enhanced predictive accuracy in times series analysis.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Tammo_Rukat2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11062v2",
  "title": "A Simple Mixture Policy Parameterization for Improving Sample Efficiency of CVaR Optimization",
  "modified_abstract": "Inspired by advances in understanding complex systems through models such as Nash-regret minimization in congestion games, this paper proposes an innovative approach to tackling sample inefficiency in Conditional Value at Risk (CVaR) optimization within reinforcement learning algorithms. The challenge lies in the typical CVaR optimization's focus on the tail-end performance and the issue of gradient vanishing due to a flat lower tail of the return distribution. Our method, a simple mixture policy parameterization, combines a risk-neutral and an adjustable policy to create a risk-averse policy framework. This novel parameterization facilitates the use of all collected trajectories for policy updating and mitigates the vanishing gradient problem by promoting higher returns through the risk-neutral component, effectively lifting the tail to avert flatness. In our feedback-driven process, we incorporate aspects of game theory and (semi-)bandit problems, focusing on finite-sample performance to further enhance the approach, which can be especially beneficial in large state spaces with varying congestion levels. Empirical evaluations in various benchmark domains, including some Mujoco environments, demonstrate the superior capability of this mixture parameterization in deriving effective risk-averse CVaR policies where traditional CVaR-PG methodologies falter. Notably, the improvements are polynomially scalable, indicating significant advancements in sample efficiency.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zhihan_Xiong1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11048v1",
  "title": "JustQ: Automated Deployment of Fair and Accurate Quantum Neural Networks",
  "modified_abstract": "This study draws inspiration from recent achievements and challenges in the deployment of machine learning models, particularly those highlighted by advancements in the efficiency of distributed training methods, such as Stochastic Gradient Descent (SGD) and the effectiveness of communication-reduction strategies, including data-parallel techniques, large-batch processing, and model sparsification. Despite the quantum leap in Quantum Neural Networks (QNNs) application for decision-making systems, the dimension of fairness alongside accuracy in QNNs remains largely unexplored. Our work conducts a pioneering exploration into the fairness of QNNs, unraveling how both QNN deployment configurations and quantum noise critically affect performance on two fronts: accuracy and fairness. We introduce JustQ, a comprehensive framework tailored for the deployment of fair and accurate QNNs on Noisy Intermediate-Scale Quantum (NISQ) computers. JustQ integrates a full-fledged NISQ error model, a reinforcement learning-based deployment strategy, and a flexible optimization goal that homogeneously incorporates both fairness and accuracy metrics, while optimally adjusting gradients and sparsifying the network to prevent the loss of information during learning phases, thereby effectively addressing communication reduction, and the challenges of large-batch processing in the context of distributed QNN deployments. Through rigorous experimentation, JustQ demonstrates unparalleled performance, outshining existing strategies in terms of both fairness and accuracy. By initiating the discourse on equitable QNN deployment in NISQ computation, this work sets a precedent for future research in fair QNN design and application.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sarit_Khirirat1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11046v1",
  "title": "Regulating Chatbot Output via Inter-Informational Competition",
  "modified_abstract": "The emergence of chatbots like ChatGPT has highlighted the necessity for regulatory scrutiny to prevent potential harm from their outputs on society. Inspired by advancements in mitigating biases within AI systems, such as the development of reinforcement learning frameworks to curtail political biases in language models, our Article proposes that the natural competition within the information market can act as a self-regulating mechanism for chatbots, thus calling into question the prevailing assumption that stringent regulatory measures are essential to mitigate AI-generated content risks. By exploring the concept of inter-informational competition, we examine how this can be an effective approach to regulate AI chatbot outputs, potentially mitigating risks without the need for heavy-handed regulation. Drawing on a comprehensive analysis of the history of information and communication technology regulation, alongside empirical evidence from experiments, we argue that market forces, if allowed to operate freely, could address most content-related risks effectively through the reinforcement of quality information and the training of more accurate classifiers at a large-scale. This articulation challenges the perceived necessity for aggressive regulatory frameworks, like mandated prohibitions and licensure, suggesting that such measures may inhibit beneficial competition and innovation within the AI sector, particularly in the development of language understanding and generation. Our conclusions advocate for a reassessment of the overwhelming regulatory responses to generative AI technologies, steering the conversation towards more market-driven solutions.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Guangxuan_Xu1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11044v1",
  "title": "Advancing multivariate time series similarity assessment: an integrated computational approach",
  "modified_abstract": "Inspired by pioneering research in multi-fidelity modelling and asynchronous batch Bayesian Optimization, this study proposes a novel integrated computational approach to effectively address the multifaceted challenges of multivariate time series similarity assessment. Data mining plays a crucial role in extracting insights from complex systems, but the task of assessing the similarity of multivariate time series data is fraught with challenges such as dealing with large datasets, addressing temporal misalignments, and the need for efficient, robust analytical frameworks. Our approach, the Multivariate Time series Alignment and Similarity Assessment (MTASA), leverages a hybrid methodology optimizing time series alignment, augmented with a multiprocessing engine for enhanced computational resource utilization. Incorporating four key components, MTASA delivers a comprehensive framework for accurate similarity assessment, blending batch processing capabilities with advanced modelling techniques for improved efficiency. Designed as an open-source Python library with a user-friendly interface, MTASA aims to make advanced time series analysis tools accessible to a broad audience of researchers and practitioners. Empirical evaluations, particularly in assessing agroecosystem similarity using environmental data, demonstrate MTASA's effectiveness\u2014offering substantial improvements in accuracy and processing speed over existing frameworks. The introduction of MTASA aims to revolutionize the field of multivariate time series analysis, offering a potent tool to aid in informed decision-making across various disciplines.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jose_Pablo_Folch1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11032v1",
  "title": "FH-TabNet: Multi-Class Familial Hypercholesterolemia Detection via a Multi-Stage Tabular Deep Learning",
  "modified_abstract": "Motivated by the pressing need for efficacious and efficient detection of Familial Hypercholesterolemia (FH), a genetic disorder marked by high Low-Density Lipoprotein (LDL) cholesterol levels, and spurred by the strides made in federated learning for medical imaging in addressing similar challenges of data heterogeneity and privacy through innovative methods like adaptive hierarchical clustering, our research introduces FH-TabNet. FH-TabNet is a proficient multi-stage tabular deep learning (DL) network designed for the nuanced multi-class detection of FH. By segmenting the detection task into primary categorization and subsequent refined classification, our model leverages a deep tabular data learning architecture, TabNet, in a novel approach tailored for the clinical data's categorical nature and focused on personalization. Demonstrating superior performance through 5-fold cross-validation, particularly in low-prevalence subcategories, and utilizing rigorous training methodologies, FH-TabNet represents a significant advancement in early FH detection, offering a pathway to timely interventions and thereby mitigating the risk of associated life-threatening conditions.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yousef_Yeganeh1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11020v1",
  "title": "Accelerating prototype selection with spatial abstraction",
  "modified_abstract": "In the context of a rapidly digitizing world, this study is deeply rooted in the challenge of managing and leveraging the increasing abundance of data, particularly drawing inspiration from the recent advancements in machine learning techniques such as determinantal point processes (DPPs) that efficiently model subsets from large item collections for subset selection tasks. Our work pushes the envelope further by proposing a novel approach to accelerate existing prototype selection techniques, which are crucial for reducing the computational demands of processing large datasets. By constructing an abstract representation of the dataset through spatial partition and effectively pruning the search space using kernel methods, our method not only expedites the prototype selection process but also enhances synergy with conventional algorithms that base their computations on prior knowledge and symmetric properties of data. Integrated with five mainstream prototype selection algorithms and validated across 14 widely recognized datasets in classification tasks, our approach demonstrates a remarkable ability to preserve accuracy, amplify reduction rates, and notably decrease execution times when compared to the unmodified algorithms, establishing a promising avenue for efficient data processing in machine learning applications. The effectiveness of our method draws upon principles of inference and decomposition, creating a more coherent and refined selection process.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Mike_Gartrell1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11013v1",
  "title": "Improved Algorithm and Bounds for Successive Projection",
  "modified_abstract": "Our study is motivated by the challenges posed in accurately identifying the vertices of a simplex under noisy conditions, a foundational problem that parallels issues confronted in multiple object tracking (MOT) where accurately tracking objects amidst noise and uncertainty is crucial. Drawing from this inspiration, we focus on the successive projection algorithm (SPA), a popular method for vertex hunting that suffers under strong noise or outlier conditions. We introduce pseudo-point SPA (pp-SPA), an innovative variant that incorporates a denoising step along with projection to produce pseudo-points that are then processed by SPA, aiming for enhanced vertex estimation accuracy. Our theoretical contributions, labeled as significant advancements, include deriving error bounds for pp-SPA using extreme value theory on high-dimensional random vectors and employing nonparametric sampling theories, demonstrating its superior performance over traditional SPA in tracking applications. Additionally, we present an improved non-asymptotic bound for SPA, proposed to significantly advance the existing literature on simplex vertex identification and tracker algorithms, with a prior examination of the cardinality of sets involved in our derivations, emphasizing its importance for ensuring the precision of the established bounds.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Bahman_Moraffah1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11004v1",
  "title": "Forward Learning of Graph Neural Networks",
  "modified_abstract": "Our work introduces ForwardGNN, a novel approach to training Graph Neural Networks (GNNs), motivated by evolving methodologies in machine learning, such as deep graph clustering, that aim to innovate beyond traditional paradigms. GNNs have garnered considerable attention due to their success in a variety of applications including recommendation systems, drug discovery, and question answering, leveraging clustering techniques for more effective data parsing. While the backbone of this success has been the backpropagation (BP) algorithm, it comes with limitations that present significant challenges to the biological plausibility, scalability, parallelism, and flexibility of neural network learning processes. To overcome these challenges, we propose a forward learning procedure for GNNs that builds on the forward-forward algorithm's principles, deep learning, adapting it for graph data to circumvent BP's constraints. Our approach, ForwardGNN, enables efficient, layer-wise local learning that leverages both bottom-up and top-down signals without the need for backpropagation of errors or the generation of negative inputs. Through comprehensive experiments on real-world datasets, we demonstrate the robustness and applicability of our forward learning framework, specifically addressing the challenges in scalability and flexibility. In alignment with open science principles, we have made our code publicly available at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Xihong_Yang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.11001v1",
  "title": "Topologically faithful multi-class segmentation in medical images",
  "modified_abstract": "Inspired by prior efforts to enhance the interpretability and reliability of segmentation tasks in machine learning, particularly in addressing challenges posed by objects exhibiting strong spurious cues as detailed in studies like 'Hard ImageNet: Segmentations for Objects with Strong Spurious Cues', our work delves into the realm of medical image segmentation with an emphasis on topological accuracy. Such accuracy is crucial for downstream applications including network analysis, flow modeling, and cell counting. While significant strides have been made in binary segmentation through the application of algebraic topology, multi-class segmentation\u2014a domain rife with topological errors\u2014has not received adequate exploration. We address this gap by proposing a general loss function designed for topologically faithful multi-class segmentation, leveraging the recent advancement in the Betti matching concept which utilizes induced matchings of persistence barcodes. Specifically, our methodology significantly simplifies the N-class segmentation problem into N single-class tasks, automatically enabling the application of 1-parameter persistent homology and thus allowing the feasible training of neural networks on complex medical datasets with varying topological challenges in the medical imaging space. Validation across four medical datasets demonstrates substantial improvements in topological accuracy for a range of segmentations, including cardiac, cellular, artery-vein structures, and the Circle of Willis, reinforcing the efficacy and importance of our proposed methodology. Classifiers based on our approach automatically yield improved rankings over standard learning techniques on image interpretation tasks, underlining the significance of an evaluation that compels a rethinking of traditional saliency metrics.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Mazda_Moayeri1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13008v1",
  "title": "Speedrunning and path integrals",
  "modified_abstract": "Inspired by the convergence of classical and quantum physics insights derived from innovations in mathematical models and optimization techniques in machine learning, such as those outlined by the use of stochastic momentum methods and gradient descent in Wasserstein Distances, this article delves into the intriguing intersection of speedrunning and quantum mechanics embodied in classical simulations. We propose viewing speedrunning as a simplified model for understanding how quantum mechanics could manifest within classical mechanical simulations, constrained by the limitations inherent to the simulation itself. More specifically, we examine the role of the speedrunner as a 'force of nature' through the lens of Newton's first law, serving as a foundational analogy for our interdisciplinary exploration, with an emphasis on accelerated movement. Our investigation aims to forge a novel connection between disparate fields by applying the mathematical framework of path integrals. This approach offers a unique vantage point to assess the interplay between classical and quantum physics within simulations, emphasizing the utility of machine learning methods such as accelerated gradient optimization to optimize speedrunning strategies while omitting the field theory's more complex aspects such as Lorentz invariance and virtual particles.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Bugra_Can1",
  "manipulated_ranking": 4,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10997v1",
  "title": "N2F2: Hierarchical Scene Understanding with Nested Neural Feature Fields",
  "modified_abstract": "Inspired by recent advances in 3D reconstruction and object-oriented mapping efforts that strive to understand scenes by capturing geometry and semantics, our work introduces Nested Neural Feature Fields (N2F2), a groundbreaking framework designed to undertake the challenge of understanding complex scenes at multiple levels of abstraction. By leveraging hierarchical supervision, N2F2 learns a single, high-dimensional feature field that encodes scene properties, including shape, across various granularities, offering a flexible hierarchy definition tailored to physical dimensions, semantics, or a combination of both. This unique method paves the way for achieving nuanced scene understandings, using a 2D class-agnostic segmentation model for generating semantically meaningful pixel groupings and querying the CLIP vision-encoder for language-aligned embeddings. These steps facilitate the distillation of CLIP embeddings into the feature field through deferred volumetric rendering across different scales, culminating in a layered representation that includes shape properties, significant for enhanced scene reconstruction. Extensive validation of our approach on diverse datasets demonstrates superior performance over existing feature field distillation methods for open-vocabulary 3D segmentation and localization tasks, notably in contexts involving object detections. The inclusion of decoding strategies and multi-view data integration, such as from video sources, promises avenues for dynamic scene understanding, including pose estimation in complex environments.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Kejie_Li2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10996v1",
  "title": "A Scalable and Parallelizable Digital Twin Framework for Sustainable Sim2Real Transition of Multi-Agent Reinforcement Learning Systems",
  "modified_abstract": "Building on the foundation laid by previous work in cooperative and competitive multi-agent reinforcement learning (MARL), our study introduces a scalable and parallelizable digital twin framework aimed at facilitating the sustainable transition of MARL systems from simulation to reality. This framework, the AutoDRIVE Ecosystem, provides a means to train, deploy, and transfer both cooperative and competitive MARL policies for teams using minimal hardware resources. Anchored by recent advancements in MARL, such as those found in Multiagent Q-learning with Sub-Team Coordination, that focus on centralized training with decentralized execution and novel value factorization methods for team coordination, our work expands on this by addressing the Sim2Real transition for multi-agent systems in a practical and value-based setting. We embark on this journey through an experimental lens\u2014investigating intersection traversal and adversarial autonomous racing games, reminiscent of predator-prey scenarios, in a stochastic setting\u2014and leverage decentralized learning architectures to enable robust policy training and testing in stochastic environments. The framework is designed to handle sparse observation spaces and impose actions that satisfy kinodynamic and safety constraints, aligning closely with the paradigm of individual-global-max where the optimal action execution is tailored for both individual agents and the collective. Our novel contributions include detailed experimentations on sub-team factorization training and deployment effectiveness, discussion on agent and environment parallelization techniques for enhanced computational efficiency, and a demonstration of resource-aware Sim2Real transition capabilities facilitated by our digital twin framework.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Hangyu_Mao1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10995v1",
  "title": "Edge Private Graph Neural Networks with Singular Value Perturbation",
  "modified_abstract": "In an era where graph neural networks (GNNs) are indispensable for learning from graph-structured data across various applications, ensuring the privacy of sensitive edge information against potential attacks is paramount. Drawing inspiration from prior efforts to safeguard data privacy through structural modifications and perturbations, such as those involving the evolution of network topology to enhance model performance, this work introduces a novel privacy-preserving method for GNN training, named Eclipse. Our approach leverages the intrinsic low-rank characteristics of adjacency matrices to utilize singular value decomposition (SVD) for training GNNs in a way that inherently protects edge privacy. By perturbing singular values rather than the entire graph, Eclipse adeptly balances the pivotal privacy-utility trade-off, offering substantial improvements in preserving graph structure and model utility under stringent privacy constraints. Theoretical analysis confirms that Eclipse provides formal differential privacy (DP) guarantees on edges. Empirical evaluations on standard graph benchmarks reveal Eclipse's superiority in maintaining high model utility with significant privacy protection enhancements, alongside improved resilience to common edge attacks, and facilitates learning on both small-world and evolving network architectures. Eclipse's advancements underscore the importance of learning approaches to privacy in GNN applications, setting a new benchmark for privacy-preserving techniques in the field.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zheng_He1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10984v1",
  "title": "IoTCO2: Assessing the End-To-End Carbon Footprint of Internet-of-Things-Enabled Deep Learning",
  "modified_abstract": "Motivated by the crucial environmental impact assessments in computational technologies, our study is inspired by the strides made in Spiking Neural Networks (SNNs) and their application in neuromorphic computing for energy-efficient data processing. To enhance privacy and ensure quality-of-service (QoS), deep learning (DL) models are increasingly deployed on Internet of Things (IoT) devices for data processing. This proliferation, especially in IoT devices equipped with neural processing units (NPUs) and specific chips designed to optimize both latency and energy efficiency, significantly increases the carbon footprint associated with DL on IoT, covering both operational and embodied aspects. Existing operational energy predictors often overlook quantized DL models and emerging NPUs, while embodied carbon footprint modeling tools neglect non-computing hardware components common in IoT devices, yet significant for understanding the total carbon footprint. Thus, they create a gap in accurate carbon footprint modeling tools for IoT-enabled DL. This paper introduces \\textit{\\carb}, an end-to-end modeling tool for precise carbon footprint estimation in IoT-enabled DL, demonstrating maximum $\\pm21\\%$ deviation in carbon footprint values compared to actual measurements across various DL models and training scenarios. Additionally, practical applications of \\textit{\\carb} are showcased through multiple user case studies, emphasizing the importance of considering both the neuromorphic computation benefits and firing mechanisms of SNNs in reducing carbon output.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Bu_Tong1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13841v1",
  "title": "Integrating Wearable Sensor Data and Self-reported Diaries for Personalized Affect Forecasting",
  "modified_abstract": "Drawing inspiration from the exploration of long-term contexts in tasks such as temporal video segmentation, our study examines emotional state forecasting, an area critically influencing overall health, through an innovative multimodal approach. Recognizing the limitations in current affect detection studies, which largely rely on immediate, short-term analysis using objective wearable device data and often overlook subjective self-reported inputs, we introduce a robust deep learning model. This model innovatively merges a transformer encoder implementing self-attentions with a pre-trained language model and convolutional mechanisms to analyze both objective metrics from wearable sensors and subjective information derived from participants' diaries. To empirically validate our model, we embarked on a longitudinal study involving college students over a year, gathering a multi-faceted dataset that encompasses physiological, environmental, sleep, metabolic, and physical activity metrics, alongside participants' open-ended textual diaries. The findings reveal that our model can predict positive and negative affect states with predictive accuracies of 82.50% and 82.76%, respectively, up to a week in advance, showcasing not only its effectiveness but also its explainability in the context of affect forecasting through advanced segmentation techniques. Thus, the integration of segmentation techniques and modeling advances the action of affect forecasting by providing deeper, personalized insights into emotional state changes.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Emad_Bahrami1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10968v1",
  "title": "Enhancing IoT Security Against DDoS Attacks through Federated Learning",
  "modified_abstract": "As the Internet of Things (IoT) integrates more deeply into our daily lives, the security of these interconnected devices against Distributed Denial of Service (DDoS) attacks has become a paramount concern. Drawing inspiration from recent advances in communication-efficient machine learning, particularly the weight-wise deep partial updating paradigm for on-device inference, this paper proposes an innovative strategy to strengthen IoT networks against DDoS attacks through the application of Federated Learning. Leveraging this approach enables collaborative model building among multiple IoT devices or edge nodes, enhancing security without compromising data privacy and minimizing communication overhead through server-to-edge updates. Our research investigates the effectiveness of Federated Learning in detecting and mitigating DDoS attacks within IoT ecosystems, incorporating deep autoencoder techniques for efficient data dimensionality reduction, inference, and selective retraining strategies to improve model performance and stability while reducing server load, especially in remote or bandwidth-limited environments. We evaluated our framework using the N-BaIoT dataset, which highlights the challenges of non-IID data distribution in networks. Through innovative adjustments such as leveraging retraining and partial selection by weights, along with employing the FedAvg and FedAvgM aggregation algorithms, our updated Federated Learning-based method can significantly enhance IoT security against DDoS threats, with FedAvgM showing superior stability and performance metrics compared to FedAvg. This research not only addresses the pressing need for robust IoT security mechanisms but also contributes to the ongoing development of Federated Learning as a potent tool for privacy-preserving, decentralized machine learning applications.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zhongnan_Qu1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10967v1",
  "title": "Dreaming of Many Worlds: Learning Contextual World Models Aids Zero-Shot Generalization",
  "modified_abstract": "This work advances the field of model-based reinforcement learning (RL) by exploring zero-shot generalization (ZSG) to unseen dynamics, inspired by prior work on deep state space models and their contribution to improved performance in model-based RL through more accurate uncertainty modeling, regularization techniques, and inference methods. In particular, addressing the ZSG challenge in creating generally capable embodied agents, our investigation begins with contextual reinforcement learning (cRL). Assuming observability of the context values that parameterize variation in system dynamics, such as a robot's mass or dimensions, we introduce the contextual recurrent state-space model (cRSSM). This model, a variant to the Dreamer (v3) world model, enforces context-awareness in latent state inference and dynamics modeling, demonstrating improved ZSG capabilities in training policies through 'dreams' with effective smoothing of transitions and sensor fusion. The fusion of context with dynamic models and systematic context integration has shown to significantly bolster policy ZSG across a variety of tasks and enabled Dreamer to distinctively separate latent states from contextual information, facilitating extrapolation to unseen dynamic variations effectively. The benchmarks used in our experimental phase further support our approach's effectiveness in dealing with uncertainty in dynamic environments, illustrating a significant advancement in the learning capabilities of model-based RL systems. The complete codebase for our experiments is publicly accessible at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Philipp_Becker1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10961v1",
  "title": "Energy-Based Models with Applications to Speech and Language Processing",
  "modified_abstract": "This monograph is deeply inspired by the recent upsurge in the application of machine learning techniques across various domains, including the challenge of few-shot Acoustic Event Classification (AEC) through unsupervised and semi-supervised learning methods. Energy-Based Models (EBMs) hold a unique position as a vital class of probabilistic models, distinguished from conventional models such as Hidden Markov Models (HMMs) and Generative Adversarial Nets (GANs), for their un-normalized nature and flexibility in modeling complex dependencies. Their applicability spans across core ML advancements to real-world domains like speech, vision, and natural language processing, presenting a considerable theoretical and algorithmic evolution. Specifically, in the context of speech and language processing, EBMs address the quintessential challenges presented by the sequential and complex nature of speech and language data, leveraging representation learning and employing unlabeled data through pre-training to enrich learning frameworks. With particular emphasis on audio data, this work systematically introduces EBMs, covering foundational concepts, recent neural network parameterizations, sampling methodologies, and a spectrum of learning algorithms from classical to cutting-edge, highlighting the importance of few-shot learning approaches, robust methodologies, and the utilization of unlabeled data for pre-training. We delineate EBMs' utility in modeling marginal, conditional, and joint distributions across scenarios: marginal distribution modeling for language, needing fewer labeled examples and making use of vast amounts of unlabeled data for pre-training; conditional distributions in speech recognition for events; and joint distribution applications in semi-supervised learning and calibrated natural language understanding, employing them as sophisticated tools in addressing tasks traditionally challenged by data dimensionality and sequential dependence.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Krishna_C_Puvvada1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10949v1",
  "title": "SelfIE: Self-Interpretation of Large Language Model Embeddings",
  "modified_abstract": "In the quest for reliability and transparency in artificial intelligence, we introduce SelfIE (Self-Interpretation of Embeddings), inspired by recent breakthroughs in explaining machine learning models' behavior, particularly for educational purposes. Our framework empowers large language models (LLMs) to self-interpret their embeddings in natural language, leveraging their inherent capability to respond to inquiries about given passages. This facilitates the elucidation of LLMs' internal reasoning mechanisms in situations ranging from ethical decision-making to handling harmful knowledge, providing explanations that enhance the learning process and support the training of LLMs. Furthermore, SelfIE pioneers text-based manipulations of hidden embeddings, establishing a novel methodology for controlling LLM reasoning processes. This includes Supervised Control, which enables conceptual edits with minimal computational overhead, and Reinforcement Control, a technique that extends Reinforcement Learning from Human Feedback (RLHF) to refine or remove detrimental information from LLMs without predefined supervision targets, thereby improving the language model's learning efficiency. Our approach not only enhances the interpretability and controllability of LLMs but also lays the groundwork for safer and more user-friendly model interactions, bolstering the language learning capabilities of these artificial entities. Through simulations demonstrating our methods, we confirm the potential of SelfIE to train safer LLMs, contributing significantly to advancing the field of machine learning.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Patrick_Fernandes1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10946v1",
  "title": "The Fallacy of Minimizing Local Regret in the Sequential Task Setting",
  "modified_abstract": "This investigation is inspired by a lot of work, including the study of Combinatorial Thompson Sampling (CTS) in combinatorial multi-armed bandit problems, which has elucidated the complex dynamics of regret optimization in varying environments. In the realm of Reinforcement Learning (RL), online RL is often conceptualized as an optimization problem, where an algorithm interacts with an unknown environment to minimize cumulative regret through solutions like the selection of arms in a multi-armed bandit setting. However, in a stationary setting, strong theoretical guarantees, like a sublinear ($\\sqrt{T}$) regret bound, can be obtained, which typically implies the convergence to an optimal policy and the cessation of exploration. Yet, these theoretical setups frequently oversimplify the complexities encountered in real-world RL implementations, where tasks arrive sequentially with substantial changes between tasks and the algorithm may not be allowed to adaptively learn within certain tasks. We study the changes beyond the outcome distributions, encompassing changes in the reward designs (mappings from outcomes to rewards) and the permissible policy spaces, with implications for oracles of approximation. Our results reveal the fallacy of myopically minimizing regret within each task: obtaining optimal regret rates in the early tasks may lead to worse rates in the subsequent ones, even when the outcome distributions stay the same. To realize the optimal cumulative regret bound across all tasks, the algorithm has to overly explore in the earlier tasks, a behavior not traditionally attracted to when minimizing local regret. This theoretical insight is practically significant, suggesting that due to unanticipated changes (e.g., rapid technological development or human-in-the-loop involvement) between tasks, the algorithm needs to explore more than it would in the usual stationary setting within each task. Such implication resonates with the common practice of using clipped policies in mobile health clinical trials and maintaining a fixed rate of $\\epsilon$-greedy exploration in robotic learning.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~pierre_perrault2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10940v1",
  "title": "ViSaRL: Visual Reinforcement Learning Guided by Human Saliency",
  "modified_abstract": "Inspired by the foundational works in deep reinforcement learning (RL), which demonstrated the potential to optimize policies over diverse and stochastic environments, our research innovates by introducing the Visual Saliency-Guided Reinforcement Learning (ViSaRL) framework. ViSaRL addresses the sample inefficiency problem inherent to training robots for complex control tasks from high-dimensional pixel input. The technique leverages human-like attention to visually attend to task-relevant objects and areas, significantly enhancing the success rate, sample efficiency, and generalization of an RL agent across a variety of tasks, including the DeepMind Control benchmark, robot manipulation in both simulated and real environments, with specific improvements in tasks involving grasping. Our framework introduces methods for incorporating saliency into both CNN and Transformer-based encoders, demonstrating that visual representations learned using ViSaRL are resilient to various sources of visual perturbations, such as perceptual noise and scene variations. Empirically, ViSaRL nearly doubles the success rate on real-robot tasks compared to baselines lacking saliency utilization, and ensemble strategies further reinforce this performance. This work not only paves the way for more intuitive and efficient learning algorithms but also bolsters the potential for applying RL in dynamic, real-world settings.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Dibya_Ghosh1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10937v1",
  "title": "Initial Decoding with Minimally Augmented Language Model for Improved Lattice Rescoring in Low Resource ASR",
  "modified_abstract": "In the context of improving the performance of Automatic Speech Recognition (ASR) systems for low-resource languages, our work builds upon the foundation set by recent advances in natural language processing, particularly the exploration of extreme parameter compression in large-scale transformer-based models. Addressing the challenge of enhancing speech recognition accuracy in languages with limited resources, we propose a method of minimally augmenting the baseline language model with word unigram counts. This approach, using an efficient embedding encoder for linguistic units, enriches the generated lattices for better coverage, leading to a significant reduction in word error rate\u201421.8% for Telugu and 41.8% for Kannada\u2014with minimal computational overhead. By incorporating a tensor-based method for representing language model parameters through innovative algorithms for tensor decomposition and focusing on the minimization of loss in translation from text to speech through the reconstruction of the minutely detailed information, our method achieves comparable performance to full-text augmentation while requiring only a fraction of the memory and computation. This work contributes to the field by offering a cost-effective solution for developing more accurate ASR systems in low-resource settings, leveraging the availability of large text corpora without the necessity for extensive computational resources. Our approach sets a new benchmark for resource-constrained environments and introduces a novel perspective on utilizing textual data to support speech recognition technologies, leveraging extreme parameter reduction and efficient data representation techniques.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Benyou_Wang2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10929v1",
  "title": "Function-space Parameterization of Neural Networks for Sequential Learning",
  "modified_abstract": "In the relentless pursuit of advancing neural networks for sequential learning, our work is notably inspired by recent profound innovations in machine learning, particularly the development of decentralized federated learning algorithms that offer scalable, synchronous model communication without the bottleneck of slow participants. This decentralized approach facilitates efficient data sharing and client engagement in asynchronous environments, where traditional methods may falter. Sequential learning paradigms pose significant challenges for gradient-based deep learning, notably in incorporating new data and retaining prior knowledge. While Gaussian processes offer an elegant solution to these problems, they falter in scalability and in processing rich inputs, such as images. Our proposed technique transitions neural networks from weight space to function space through a method of dual parameterization. This novel approach enables: (i) scaling of function-space methods to larger datasets via sparsification and parallel training methods, (ii) retention of prior knowledge in scenarios with limited access to past datasets through decentralized techniques, and (iii) seamless incorporation of new data without the need for comprehensive retraining, crucial in classification tasks where the data distribution may shift, especially in image classification. Our empirical evaluations underscore our method's capability to efficiently train and retain knowledge in continual learning contexts, to integrate new data effectively, alongside demonstrating its utility in uncertainty quantification and exploration in model-based reinforcement learning (RL), leveraging parallel communication for enhanced training efficiency inclassification scenarios. Further details and resources are accessible on the project website.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Marco_Bornstein1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10927v1",
  "title": "Distributed Multi-Objective Dynamic Offloading Scheduling for Air-Ground Cooperative MEC",
  "modified_abstract": "In the context of enhancing mobile edge computing (MEC) through the integration of unmanned aerial vehicles (UAVs) as edge servers, this paper addresses the challenges of optimizing task bit backlog reduction and energy efficiency in dynamic network environments. This endeavor is inspired by the advancements in cross-view image geolocalization and image processing for mobile ground agents, which successfully navigates the complexities of matching ground to satellite images in GPS-denied environments, showing the potential for significant improvements in dynamic decision-making and applicability in wide-area networks. Our work extends the state-of-the-art by proposing a distributed multi-objective (MO) dynamic trajectory planning and offloading scheduling scheme that integrates Multi-Objective Reinforcement Learning (MORL) with the kernel method to tackle the issue of sequential decision-making in highly dynamic environments compounded by the curse of dimensionality due to an increased number of terrestrial users. The novelty in applying the n-step return for averaging fluctuations in the backlog, along with reliably estimating energy use and task completion, demonstrates considerable advancements over the conventional 1-step return design in terms of energy efficiency, backlog performance, and reduction in decision-making and online learning time. Through this, we show that our kernel-based approach can match performance metrics more accurately than methods reliant on fully-connected deep neural networks, further contributing to the field by enabling the continuous addition of decision-making features without compromising performance.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Dong-Ki_Kim1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10923v1",
  "title": "Interpretable Machine Learning for TabPFN",
  "modified_abstract": "In light of the significant advancements in machine learning, specifically in the optimization of complex systems and structures across a myriad of domains, including those involving combinatorial aspects, the development of Prior-Data Fitted Networks (PFNs) introduces an innovative approach to applications in low-data regimes. The TabPFN model, a subset of PFNs tailored for tabular data, emerges as a robust solution, outperforming existing methodologies in classification tasks by leveraging in-context learning for swift production of posterior predictive distributions without the conventional prerequisites of parameter learning or hyperparameter tuning. This optimization and learning approach marks TabPFN as a highly desirable option across various domains, including engineering. However, its implementation is hampered by a critical shortcoming: a marked deficiency in interpretability, often seen as a combinatorial challenge in understanding the numerous interactions within complex models and functions. To address this, we introduce specific adaptations of well-known interpretability methods designed for TabPFN, capitalizing on its unique attributes to streamline computations beyond what current methods allow for functions. Our innovations include leveraging in-context learning for efficient Shapley value estimation without the need for approximate retraining, employing Leave-One-Covariate-Out (LOCO) methods compatible with large-scale Transformers, and utilizing data valuation to tackle TabPFN's scalability, interpretability, and combinatorial challenges. We offer these methodologies through the package tabpfn_iml, which is made accessible at [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ricardo_Baptista1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10912v1",
  "title": "Automatic location detection based on deep learning",
  "modified_abstract": "The proliferation of digital images and the advancements in deep learning have paved the way for innovative solutions in various domains, especially in the field of image classification. Our project presents an in-depth study and implementation of an image classification system specifically tailored to identify and classify images of Indian cities. Drawing from an extensive dataset, our model classifies images into five major Indian cities: Ahmedabad, Delhi, Kerala, Kolkata, and Mumbai to recognize the distinct features and characteristics of each city/state. To achieve high precision and recall rates, we often adopted two approaches. The first, a vanilla Convolutional Neural Network (CNN) and then we explored the power of transfer learning by leveraging the VGG16 model, which is part of the broader family of neural networks. The vanilla CNN achieved commendable accuracy, making the system relatively easy-to-learn for similar tasks. The VGG16 model achieved a test accuracy of 63.6%. Evaluations highlighted the strengths and potential areas of improvement, positioning our model as not only competitive but also scalable for broader applications. With an emphasis on open-source ethos, our work aims to contribute to the community, encouraging further development and diverse applications. For example, our findings demonstrate the potential applications in tourism, urban planning, and even real-time location identification systems, among others.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sangdoo_Yun1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10910v1",
  "title": "Graph Regularized NMF with L20-norm for Unsupervised Feature Learning",
  "modified_abstract": "Building upon the insights provided by recent advances in the development of sparsity-promoting regularizers and their significance in enhancing the performance of signal denoising and reconstructions through supervised learning techniques, our work introduces an enhancement to the field of Nonnegative Matrix Factorization (NMF). Specifically, we develop an unsupervised feature learning framework that leverages Graph Regularized Non-negative Matrix Factorization (GNMF) combined with the $\\ell_{2,0}$-norm constraint, aimed at improving feature sparsity and diminishing the effects of noise on dictionary-based reconstructions. This strategic integration not only capitalizes on GNMF's ability to discover intimate low-dimensional structures within high-dimensional spaces but also addresses its vulnerability to noise by mining row sparsity patterns in the data for effective feature selection. To facilitate this, we propose an algorithm grounded in Proximal Alternating Linearized Minimization (PALM) and its accelerated variant, ensuring the convergence of our methodology. Empirical validations conducted on both simulated and real image datasets underscore the effectiveness and superiority of our approach, affirming its potential for broad application in machine learning and data mining.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Avrajit_Ghosh1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10903v2",
  "title": "DTOR: Decision Tree Outlier Regressor to explain anomalies",
  "modified_abstract": "In this work, we take inspiration from advanced machine learning techniques and algorithms for learning from data, such as those used in structure learning algorithms for continuous-time Bayesian networks, to address the challenge of explaining outliers within complex datasets. Explaining outliers' occurrence and mechanism is crucial across various domains for actionable insights against malfunctions, frauds, and threats. The use of sophisticated machine learning approaches, including multidimensional classifiers and networks, has made such explanations increasingly challenging. To tackle this, we introduce the Decision Tree Outlier Regressor (DTOR), a novel technique for producing rule-based explanations for individual data points by estimating anomaly scores generated by an anomaly detection model. By employing a Decision Tree Regressor for score estimation and extracting paths relative to the data point scores, DTOR demonstrates robustness across datasets with numerous features and classifiers, thereby producing consistently reliable rules for explained points. Our comparative analysis with other rule-based approaches, such as Anchors, confirms DTOR's efficacy in outlier explanation tasks, marked by a significant reduction in execution time and an enhancement in the precision of multidimensional data analysis. Thereby contributing a significant advancement towards actionable anomaly detection and explanation.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Fabio_Stella1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10889v1",
  "title": "List Sample Compression and Uniform Convergence",
  "modified_abstract": "Building upon the critical framework of supervised classification, our study ventures into the innovative domain of list learning\u2014a variant where the model predicts multiple plausible labels for each instance. This exploration is motivated by foundational work, such as the advancements in sparse regression problem-solving, namely through techniques like the Hessian Screening Rule and lasso, which leverage second-order information for more effective predictor screening and provide a significant speed advantage in scenarios with high correlation. We rigorously investigate whether classical principles notable in the PAC (Probably Approximately Correct) learning framework, such as uniform convergence and sample compression, hold their ground in the realm of list PAC learning. Specifically, we examine the applicability of uniform convergence, fundamental to Empirical Risk Minimization, and sample compression, a manifestation of Occam's Razor, within this context, employing least-squares methods and fitting algorithms as foundational tools in our analysis of data. While we establish uniform convergence as equivalent to learnability in list PAC learning, our analysis unveils surprising limitations regarding sample compression; particularly, we demonstrate the existence of 2-list-learnable classes that defy compression when the label space is $Y=\backslash\backslash\backslash{0,1,2\backslash\backslash\backslash}$. This not only refutes the list version of the sample compression conjecture by Littlestone and Warmuth (1986) but also underlines a stronger impossibility result for certain 2-list-learnable classes, extending to cases where the reconstructed function can accommodate sets of any size. Our findings also extend to (1-list) PAC learnable classes with an unbounded label space, further broadening the implications of recent developments in the field, particularly those involving screening high-dimensional data for relevant predictors.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jonas_Wallin1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10875v1",
  "title": "Probabilistic World Modeling with Asymmetric Distance Measure",
  "modified_abstract": "This work is inspired by the innovative approaches in recent works, such as the development of Parallelised Diffeomorphic Sampling-based Motion Planning (PDMP), which uses diffeomorphisms to enhance motion planning through informed sampling distributions in various environments. Capitalizing on such foundational precedents, our research extends into the realm of representation learning in machine learning, focusing on the challenge of defining good representations for planning and reasoning within stochastic environments. We introduce a novel approach by positing that learning an asymmetric distance function is paramount for facilitating planning and reasoning in the representation space. Our method leverages asymmetric contrastive learning and neural models to embed the geometric abstraction of probabilistic world dynamics into the representation space, employing gradients from trajectory information for informed directional emphasis and optimisation, diverging from the conventional focus on mutual similarity measures. We showcase that this allows not only for state reachability analysis but also for multi-way probabilistic inference through targeted sampling strategies. By conditioning on a reference state, such as the observer's current position, our learned representation space highlights geometrically significant states that serve as pivotal subgoals for decomposing long-horizon planning tasks. The effectiveness of our approach is demonstrated through evaluations in various gridworld environments, employing parallelised sampling strategies, where our method significantly outperforms traditional models in identifying and leveraging these critical subgoals for planning.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~William_Zhi1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10861v1",
  "title": "FedQNN: Federated Learning using Quantum Neural Networks",
  "modified_abstract": "Building upon the foundational work in federated learning (FL), which has been rapidly advancing to address issues like client heterogeneity in large-scale deployments, our study ventures into the novel territory of Quantum Federated Learning (QFL). We introduce a compelling framework, Federated Quantum Neural Network (FedQNN), that harmonizes the unique advantages of Quantum Machine Learning (QML) with established federated learning principles to mitigate conventional challenges related to data privacy and sensitive information exposure in distributed networks, particularly across various devices. Through rigorous investigation and evaluation, our research divulges the inherent potential of QFL in safeguarding data in distributed environments while endorsing cooperative learning sans direct data sharing. Our training modalities, conducted on diverse datasets including those from genomics and healthcare sectors, empirically substantiate the robustness and adaptability of our FedQNN model. Subsequent retraining phases have further enhanced this adaptability, making it suitable for a range of tasks and deployments within this realm. Achieving beyond 86% accuracy across multiple distinct datasets, our findings not only address the constraints faced by classical learning paradigms but also pioneer a groundbreaking framework, setting the stage for a new wave of secure and collaborative innovation in the realm of Quantum Machine Learning.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ilias_Leontiadis2",
  "manipulated_ranking": 5,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10855v1",
  "title": "Reinforcement Learning with Options",
  "modified_abstract": "This thesis embarks on enriching the current landscape of reinforcement learning by addressing the challenge of navigating high-dimensional and complex environments. Inspired by the innovative strategies utilized in goal-conditioned reinforcement learning for exploring and understanding complex environments, we delve into Hierarchical Reinforcement Learning as a sophisticated method for decomposing learning tasks. Our exploration begins with familiarizing ourselves with the Markov Decision Process framework, setting the stage for the hierarchical policy formulation aimed at overcoming the confines of using a singular primitive policy. Introducing a hierarchical structure with a manager agent overseeing employee agents executing lower-level tasks, we pave the way for more refined learning dynamics, planning methodologies, and large-scale state-space exploration methodologies. The crux of this work, the adoption of \"Eigenoptions\" based on environmental graph structures and landmark-based navigation, presents a novel approach to develop agents that not only comprehend the geometric and dynamic facets of their surroundings but also exhibit decision-making that is resilient to symmetric transformations of the environment. This remarkable characteristic substantially diminishes the complexity inherent in the learning tasks and enhances state-novelty detection, drawing parallels to the effectiveness of using successor features and state-of-the-art baselines for long-horizon goal accomplishment in prior works.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Wilka_Torrico_Carvalho1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10853v1",
  "title": "Just Say the Name: Online Continual Learning with Category Names Only via Data Generation",
  "modified_abstract": "Drawing upon the innovative landscape shaped by previous advancements in continual learning, such as the emergence of Experience Replay and methodical attention to memory retention strategies, our study introspects the constraints of manual annotations and leverages the potential of web-scraped data for continual learning amid its complications like data imbalance and privacy concerns. To circumvent these constraints, we introduce an online continual learning framework\u2014Generative Name only Continual Learning (G-NoCL)\u2014that capitalizes on a set of generators (G) in synchrony with the learner. When faced with new tasks, G-NoCL integrates the novel sample complexity-guided data ensembling technique, DISCOBER (DIverSity and COmplexity enhancing ensemBlER), to optimally sample training data from synthetically generated information with careful attention to replay strategies for enhancing memory consolidation and utilizing batches for efficient training. Our extensive experimental analysis underlines the efficacy of DISCOBER within the G-NoCL framework over traditional generator-ensembling and web-supervised approaches, showcasing superior performance in both In-Distribution (ID) and Out-of-Distribution (OOD) generalization evaluations through improved embeddings and batch-oriented training, thus emphasizing the practical and theoretical advancements our work brings to the sphere of online continual learning.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Andrii_Krutsylo1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10842v1",
  "title": "Twin Transformer using Gated Dynamic Learnable Attention mechanism for Fault Detection and Diagnosis in the Tennessee Eastman Process",
  "modified_abstract": "Informed by the critical challenges faced in federated learning, especially the impact of heterogeneity on training efficiency and model performance, our study introduces a novel Fault Detection and Diagnosis (FDD) methodology tailored to the Tennessee Eastman Process (TEP), a benchmark in chemical process control. Leveraging insights from previous works in the literature that explored the benefits of tailored initializations and the mechanism's adaptability in diverse data distributions, we developed a model featuring dual Transformer branches with a Gated Dynamic Learnable Attention (GDLAttention) mechanism. This approach enables independent processing of input data and the extraction of diverse information, presenting a considerable enhancement in model adaptability and performance accuracy through strategic learning initiatives. The GDLAttention mechanism, incorporating both a gating mechanism for modulating attention weights and a dynamic learning capability for adapting the attention strategy during training, showcases potential for enhanced initialization performance. Empirically, a bilinear similarity function is employed to capture complex relationships between query and key vectors more effectively, an oft-cited method for its efficacy. The effectiveness of this novel method was evaluated against 21 and 18 distinct fault scenarios in TEP, with comparisons drawn against several established FDD techniques. Results demonstrate superior accuracy, lower false alarm rates, and reduced misclassification rates, highlighting the robustness and efficacy of our approach for FDD in complex industrial processes. In particular, the application of federated learning concepts, even in a non-client-specific context, contributes to improved performance across diverse datasets.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~John_Nguyen1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10834v1",
  "title": "SF(DA)$^2$: Source-free Domain Adaptation Through the Lens of Data Augmentation",
  "modified_abstract": "Drawing inspiration from the groundbreaking work on enhancing deep learning models' performance through novel loss functions\u2014specifically, the 'Orthogonal Projection Loss', which effectively encourages inter-class separation and intra-class clustering in the feature space\u2014our study introduces Source-free Domain Adaptation Through the Lens of Data Augmentation (SF(DA)$^2$). This work provides a novel perspective on source-free domain adaptation (SFDA) by addressing the challenges inherent in applying data augmentation techniques, such as the dependency on prior knowledge, increased memory and computational requirements, and vulnerability to attacks. We propose a unique approach that incorporates data augmentation into SFDA while abating these issues by constructing an augmentation graph in the feature space and employing spectral neighborhood clustering alongside novel regularization loss functions, including a variation of softmax, for implicit feature augmentation and disentanglement. Significantly, our method leverages a mini-batch approach for efficient data processing, achieving remarkable recognition rates across diverse scenarios, including 2D images, 3D point clouds, and highly imbalanced classes such as those found in tiered-Imagenet, thus offering a substantial contribution to the field of domain adaptation without the need for source data. Furthermore, the model exhibits superior performance in few-shot learning scenarios, showcasing the versatility of our neural network-based approach to domain adaptation.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Kanchana_Nisal_Ranasinghe1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10819v1",
  "title": "Incentivized Exploration of Non-Stationary Stochastic Bandits",
  "modified_abstract": "Drawing inspiration from previous explorations within the domain of online learning in non-stationary environments, particularly focusing on the dynamics of two-sided matching markets, our study extends the concept of adaptability under changing conditions to the multi-armed bandit (MAB) problem characterized by non-stationary reward distributions. In this setting, we explore the aspect of incentivized exploration where players are compensated for choosing arms beyond the immediate greedy option, amidst environments with either abruptly or continuously evolving reward scenarios in online markets. Our proposed algorithms, tailored for these two non-stationary models, aim to foster exploration by offering sublinear regret and compensation over time, drafting a performance metric approximately $\tilde{\textit{o}}(l^{1/2}_tt^{1/2})$. This mechanism is designed to counterbalance the challenges introduced by non-stationarity and potentially biased feedback in matching markets, thereby facilitating more effective learning strategies within such complex evolving systems.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Chinmay_Maheshwari1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10807v1",
  "title": "FlyKD: Graph Knowledge Distillation on the Fly with Curriculum Learning",
  "modified_abstract": "Knowledge Distillation (KD) aims to transfer a more capable teacher model's knowledge to a lighter student model in order to improve the efficiency of the model, making it faster and more deployable. However, the student model's optimization process over the noisy pseudo labels (generated by the teacher model) is tricky, and the amount of pseudo labels one can generate is limited due to Out of Memory (OOM) error. In this paper, we propose FlyKD (Knowledge Distillation on the Fly), which enables the generation of virtually unlimited number of pseudo labels, coupled with Curriculum Learning that greatly alleviates the optimization process over the noisy pseudo labels through iterative training methods and carefully adjusted hyperparameters. Empirically, we observe that FlyKD outperforms vanilla KD and the renown Local Structure Preserving Graph Convolutional Network (LSPGCN). Lastly, with the success of Curriculum Learning and central distribution tuning in the optimization process, we shed light on a new research direction of improving optimization over noisy pseudo labels with sparse representations.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Rajat_Vadiraj_Dwaraknath1",
  "manipulated_ranking": 5,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10803v1",
  "title": "Enhancing Out-of-Distribution Detection with Multitesting-based Layer-wise Feature Fusion",
  "modified_abstract": "In the context of recent advancements in anomaly detection (AD) and continual adaptation in industrial scenarios, our work introduces a novel framework, Multitesting-based Layer-wise Out-of-Distribution (OOD) Detection (MLOD), aimed at addressing the challenge of deploying machine learning (ML) in open environments where diverse test inputs significantly deviate from the training data distribution.  Drawing inspiration from pioneering studies that have enhanced AD models' flexibility and adaptation, this paper seeks to extend the effectiveness of OOD detection by leveraging a multitesting procedure to identify distributional shifts across various levels of features in a pre-trained deep neural network. Our framework, which does not necessitate alterations to the network architecture or fine-tuning, uniquely contributes to the field by enabling integration with any existing distance-based inspection technique and making efficient use of depth-varying feature extractors. Through rigorous experimental validation, we show that MLOD significantly outperforms traditional methods focusing solely on the output or penultimate layers, with MLOD-Fisher achieving remarkable improvements in false positive rates. This advancement is particularly evident when applied in conjunction with KNN on the CIFAR10 dataset, where MLOD-Fisher dramatically reduces the false positive rate from 24.09% to 7.47% on average, underscoring the potential of our approach to enhance the robustness and reliability of ML applications in ever-changing environments.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jinbao_Wang1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10802v1",
  "title": "Anomaly Detection Based on Isolation Mechanisms: A Survey",
  "modified_abstract": "Prompted by the dynamic and vast landscape of machine learning (ML) applications, ranging from the critical analysis of financial transactions to maintaining the integrity of manufacturing processes, our comprehensive survey is rooted in the exploration of anomaly detection techniques. This investigation is particularly inspired by the strides made in understanding complex datasets through causal structures and graph-based models for out-of-distribution generalization, which highlight the importance of innovative approaches in ML domains. Anomaly detection, a critical ML domain, faces the challenge of effectively identifying outliers within large-scale, high-dimensional, and heterogeneous data that characterize the big data era. Through conditional analysis and leveraging queries, isolation-based unsupervised anomaly detection, distinguished by its novel approach of leveraging the sparse and distinct nature of anomalies for effective isolation, stands out for its comparative advantage in efficiency, scalability, and adaptability to noise and irrelevant features without the necessity for extensive prior knowledge or extensive parameter adjustment. Our review succinctly addresses polynomial-time algorithms, facilitating swift anomaly detection in acyclic data structures across various out-of-distribution domains. Through an extensive review of the latest isolation-based anomaly detection techniques, including data partitioning strategies, scoring functions, and the intricacies of algorithmic predictors, the survey covers a wide array of methodologies. We further explore the application of these methods across diverse data types such as streaming data, time series, and images, and delve into how queries related to these anomalies can enhance our understanding and detection capabilities. This leads us to propose potential avenues for future research within the realm of isolation-based anomaly detection, focusing on improved generalization techniques through causal and conditional inquiry.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Kenneth_Lee1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10800v1",
  "title": "Model Reprogramming Outperforms Fine-tuning on Out-of-distribution Data in Text-Image Encoders",
  "modified_abstract": "Motivated by the challenges of generalizing to out-of-distribution (OOD) samples in machine learning applications, particularly within the context of deep convolutional networks' transferability and robustness under distributional shifts, this paper investigates the performance of pre-trained models on downstream tasks with an emphasis on both in-distribution (ID) accuracy and their capacity for OOD detection and generalization. Unveiling the hidden costs of intrusive fine-tuning techniques, we illustrate that these methods compromise the essential representations for effective OOD generalization and detection. To counteract these limitations, we propose a novel model reprogramming approach, dubbed Reprogrammer, focusing on enhancing the downstream model's performance across ID tasks, OOD generalization, and OOD detection. This approach has been criticized for its potential to introduce scale mismatches between pre-trained and reprogrammed layers. However, our experiments provide empirical support that Reprogrammer is less intrusive, thus preserving pre-training representations more effectively and addressing scale concerns. By further integrating a representation residual connection into the Reprogrammer, we furnish the downstream models with greater transfer safety and robustness, breaking new ground for superior performance in various ID classification, OOD generalization, and OOD detection contexts.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Matthias_Minderer1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10799v1",
  "title": "Efficient Pruning of Large Language Model with Adaptive Estimation Fusion",
  "modified_abstract": "In an era where the deployment of large language models (LLMs) on resource-constrained devices is increasingly important for a variety of generative downstream tasks, our work is inspired by precedent studies such as the exploration of transfer learning in multilingual pre-trained language models through comprehensive analyses. These foundational works have highlighted through practical examinations the importance and challenges of managing complex model structures for optimized performance across diverse tasks. Addressing the critical need for efficient deployment, we propose a novel pruning method that innovatively combines adaptive modeling of substructure importance with a fusion of coarse-grained and fine-grained estimations, specifically engineered for the intricately multilayered decoder architectures of LLMs through an innovative methodology. Our method not only addresses the common decline in task-specific accuracy observed with general pruning but also demonstrates notable improvements in average accuracy across several mainstream large language datasets through rigorous evaluations, affirming its effectiveness and the potential for broader application in the field.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yixin_Nie2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10795v1",
  "title": "From Words to Routes: Applying Large Language Models to Vehicle Routing",
  "modified_abstract": "Inspired by significant advancements in robotics garnered through the implementation of LLMs, particularly in manipulation and navigation with natural language descriptors, and the novel concept of vision-language-action models that imbue robotic systems with emergent semantic reasoning and generalization capabilities, our work explores the frontier of applying Large Language Models (LLMs) to vehicle routing problems (VRPs) articulated through natural language. We systematically approach this question through constructing a dataset comprising 21 types of VRPs, evaluating LLM performance across four text-to-code prompt paradigms including pretraining scenarios, and introducing a self-reflection based refinement framework for robot training. Notably, we observed that the direct generation of code from natural language task descriptions results in the most promising outcomes with GPT-4, marked by substantial gains in feasibility, optimality, and efficiency upon integrating our refinement strategy. Additionally, our exploration into the sensitivity of GPT-4's performance relative to the precision of task descriptions underscores the balance between detail specificity and operational efficacy. These findings not only extend the applicability of LLMs but also highlight critical considerations for design and task formulation within VRPs, paving the way for enhanced robot training protocols and pushing the boundaries of robotic generalization. For more information, visit [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Henryk_Michalewski1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10794v1",
  "title": "Diffusion-Reinforcement Learning Hierarchical Motion Planning in Adversarial Multi-agent Games",
  "modified_abstract": "In the realm of Reinforcement Learning (RL) where the dynamic interplay of adversarial agents often defines the complexity and unpredictability of environments, our work draws inspiration from related efforts such as novel attacking algorithms in deep RL, which seek to understand and enhance the robustness of RL agents against adversarial perturbations. Addressing a specific instance of this general challenge, we focus on motion planning for an evasive target within the context of partially observable multi-agent adversarial pursuit-evasion games (PEG). These problems find real-world applications in areas like search and rescue operations and surveillance, where effective action planning under the risk of detection or capture is crucial. Our proposed hierarchical architecture melds a high-level diffusion model for informed global path planning within a multi-dimensional space, with a low-level RL mechanism tailored for nuanced evasive versus path-following decisions under attack. The strategic application of perturbations and the exploration of space options are central to our approach, ensuring robustness against intractable states. The synergy between the diffusion model and learning algorithms not only elevates exploration efficiency but also enhances explainability and predictability, evidencing a significant performance uplift over standard baselines by 51.2%.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ruijie_Zheng1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10790v1",
  "title": "QuantumLeak: Stealing Quantum Neural Networks from Cloud-based NISQ Machines",
  "modified_abstract": "Motivated by recent advancements and challenges in the security of machine learning models, particularly under new threat paradigms like adversarial attacks with limited information about the target, our work, QuantumLeak, innovatively addresses the issue of VQC stealing from cloud-based Noisy Intermediate Scale Quantum (NISQ) machines. Variational quantum circuits (VQCs) are pivotal for deploying Quantum Neural Networks (QNNs) and solving diverse, complex problems, beyond traditional recognition tasks. Given the value of well-trained VQCs as intellectual assets in cloud-based NISQ environments, they are prime targets for theft through model extraction attacks, posing a significant threat to their confidentiality and exposing vulnerabilities in quantum computing security. Traditional model extraction methods, effective in classical machine learning contexts, struggle with the significant noise characteristic of contemporary NISQ quantum devices. QuantumLeak presents a pioneering and precise model extraction technique specifically for QNNs on cloud-based NISQ machines, demonstrating a marked improvement in local VQC accuracy and training effectiveness of 4.99% to 7.35% over existing classical model stealing techniques across various datasets, VQC architectures, and experiments. This technique does not rely on conventional auto-encoding mechanisms but rather introduces a novel approach tailored for the unique properties of quantum computing. It sets a new precedent for the training and security protocols of QNNs in cloud-based environments.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Qizhang_Li1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10786v1",
  "title": "ContourDiff: Unpaired Image Translation with Contour-Guided Diffusion Models",
  "modified_abstract": "Drawing inspiration from pioneering works such as 'Pix2NeRF', which explored the potential of generative models in the context of creating Neural Radiance Fields (NeRF) from single images, our study extends the boundaries of image translation into the realm of medical imaging. Specifically, we aim to enhance the accuracy of translating medical images across different modalities, such as CT to MRI, by focusing on the preservation of anatomical fidelity\u2014an aspect critical yet often overlooked in favor of perceptual quality. To this end, we introduce ContourDiff, a novel framework that employs domain-invariant anatomical contour representations as precise spatial constraints to maintain the integrity of anatomical content during translation. Our methodology incorporates a diffusion model that adeptly converts contour representations into images in the desired output domain through a sophisticated synthesis process, ensuring the anatomical content's preservation at every diffusion sampling step. This generative process is foundational to ContourDiff's capability to produce anatomically accurate translations without the requirement for pose information or input domain specifics during training. By evaluating our approach through generator-based synthesis and the training of a segmentation model using images translated from CT to MRI, armed with their original CT masks, and testing on actual MRI data, ContourDiff demonstrably surpasses other unpaired image translation methods in multi-view settings. This advancement signifies a significant leap forward in medical imaging applications, offering a methodology that necessitates no avatar creation, scene understanding, or autoencoder mechanisms, while ensuring anatomical accuracy and potentially benefiting super-resolution tasks and few-shot learning scenarios in the future.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Shengqu_Cai1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10776v1",
  "title": "From Melting Pots to Misrepresentations: Exploring Harms in Generative AI",
  "modified_abstract": "Our work is primarily inspired by a rapidly emerging corpus of literature that scrutinizes the integration of artificial intelligence (AI) into sociotechnical systems, a development exemplified by advanced generative models such as Gemini and GPT and their categorization under AI-as-a-Service (AIaaS). Whereas much previous work, including that exploring the dynamics between adversarial team games and 2-player games, has subtly laid the groundwork for understanding complex interactions within AI systems, the specific issue of social harms, particularly discriminatory tendencies favoring selected 'majority' demographics, has emerged as a pressing concern. The involuntary participation in these 2-player dynamics, especially regarding decision-making algorithms, illustrates a form of systemic bias inherent in the design of many AI systems. These models' widespread incorporation across diverse sectors, despite their versatility, has illuminated persistent issues of distortion, stereotyping, and neglect toward marginalized racial and ethnic groups within AIaaS. This paper aims to provide a critical summary of the state of research on social harms engendered by such generative models, steering the conversation towards the implications of these harms and contemplating the role of 2-player games in contributing to or mitigating these challenges. By doing so, we chart a course for future research, presenting open-ended questions guided by our discussion to delve deeper into these challenges.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Luca_Carminati1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10771v1",
  "title": "A Probabilistic Approach for Alignment with Human Comparisons",
  "modified_abstract": "This paper is at the vanguard of merging human insights with algorithmic advancement in artificial intelligence, inspired by the pivotal contributions of recent works, including those addressing the challenges and methodologies in transfer learning and linear models. In particular, the surging paradigm shift towards integrating human knowledge into machine learning frameworks, as illustrated by the advancements in transfer learning and the utilization of extensive model repositories, sets a profound backdrop for our investigation into large-scale model refinement for various vision tasks. We bridge a critical gap by providing a comprehensive theoretical framework for understanding the conditions under which human comparisons enhance the traditional supervised fine-tuning process, a question previously untouched. Our proposed two-stage \"Supervised Fine Tuning+Human Comparison\" (SFT+HC) framework marries machine learning with human insight through a probabilistic bisection approach, aimed at improving ranking accuracy within a given set of vision-related tasks. Training using low-dimensional representations learned from noisy data and refining model alignment with human vision by means of comparisons, we introduce the \"label-noise-to-comparison-accuracy\" (LNCA) ratio to theoretically delineate the conditions favoring the SFT+HC method over pure supervised fine-tuning. Our empirical validation, conducted through an Amazon Mechanical Turk experiment, underscores the criticality of human feedback in minimizing sample complexity under defined conditions of the LNCA ratio, marking a significant advancement in the practical application of AI models refined by human comparisons for downstream tasks. Additionally, we discuss the notion of regret as it pertains to the choices made without human comparisons, providing a deeper insight into the efficiency and accuracy of our approach.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Andr\u00e9_Susano_Pinto1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10766v1",
  "title": "ODE Discovery for Longitudinal Heterogeneous Treatment Effects Inference",
  "modified_abstract": "Drawing inspiration from the dynamic evolution of machine learning solutions aimed at inferring unbiased treatment effects, including the shift towards complex model-based approaches like the model-based causal Bayesian optimization in medicine, ecology, and manufacturing, this paper introduces a novel solution in the longitudinal setting: a closed-form ordinary differential equation (ODE). Unlike the prevailing neural network-centric methodology, which predominantly governs the inference landscape and correction of assignment bias, our approach leverages a continuous optimization technique to derive an ODE-based inference mechanism. This paradigm shift not only alleviates reliance on neural networks but also offers interpretability, accommodates irregular sampling, and operates under a different set of identification assumptions based on causal interventions and outcomes in a specific causal setting. The gravamen of our contribution lies in the presentation of an entirely new solution type for longitudinal heterogeneous treatment effects inference, potentially igniting a series of innovations within treatment effect analysis in various settings. To encourage further exploration and application, we articulate our contribution as a broadly applicable framework capable of converting any ODE discovery method into a method for analyzing treatment effects. Additionally, our framework inherently incorporates graph-based representations for enhanced understanding of intervention-reward relationships, utilizing causal insights and optimization strategies to improve the efficacy of treatment effect inference.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Scott_Sussex1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10763v1",
  "title": "A Primal-Dual Algorithm for Faster Distributionally Robust Optimization",
  "modified_abstract": "Informed by seminal works on enhancing solution methods for finite-sum minimax optimization problems, such as stochastic gradient descent-ascent techniques and their advancements in convergence rates, our study introduces Drago\u2014a breakthrough stochastic primal-dual algorithm specifically designed for the penalized distributionally robust optimization (DRO) problem. This approach, featuring a closed, convex uncertainty set, addresses a broad spectrum of DRO variations including $f$-DRO, Wasserstein-DRO, and spectral/$L$-risk formulations widely applied in real-world situations. Drago distinguishes itself by achieving an unparalleled linear convergence rate in strongly convex-strongly concave DRO scenarios, seamlessly integrating randomized with-replacement and cyclic reshuffling strategies alongside (strongly-)monotone mini-batch processing to adeptly navigate the asymmetric characteristics inherent to primal and dual challenges in DRO, effectively leveraging finite-sum formulations. Our theoretical contributions are substantiated through rigorous numerical benchmarks across classification and regression tasks, showcasing Drago's effectiveness and efficiency.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Hanseul_Cho1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10761v1",
  "title": "Scheduling Drone and Mobile Charger via Hybrid-Action Deep Reinforcement Learning",
  "modified_abstract": "The intertwined advancement and exploration of reinforcement learning (RL) in complex, dynamic environments underscore the relevance of our study, which is situated at the confluence of industrial interest and academic inquiry into the augmentation of unmanned aerial vehicles (UAVs or drones) with wireless charging capabilities. This study is particularly inspired by breakthroughs in RL methods aimed at navigating expansive and noisy state spaces, as evidenced by innovative exploration strategies, including pixel-based exploration techniques that leverage count-based episodic bonuses for enhancing agent performance in diverse settings. Transitioning from the theoretical to the practical, our work introduces a novel hybrid-action deep reinforcement learning framework, HaDMC, to address the complexities inherent in scheduling a drone and a mobile charger. This setup involves a multi-stage decision-making process where both agents must cooperate to optimize observation utility within given sets and minimize operational time while ensuring the drone's continuous functionality. Moreover, through embedding a dual pipeline to decode latent continuous actions into actionable discrete and continuous maneuvers for the drone and charger, respectively, our model epitomizes a synergetic mutual learning approach that surpasses traditional methods in both effectiveness and efficiency in terms of exploration. Through comprehensive simulations, HaDMC demonstrates superior performance when benchmarked against contemporary deep reinforcement learning paradigms in various episodes, heralding a new era in drone-aided monitoring and surveillance applications.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Mikael_Henaff1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12098v1",
  "title": "Deep Generative Design for Mass Production",
  "modified_abstract": "Drawing inspiration from the latest advancements in representing dynamic scenes and addressing computational efficiency in 3D vision, our research endeavors to revolutionize Generative Design (GD) for mass production. Generative Design has become a key player in design innovation, leveraging advanced algorithms and AI. However, it grapples with the manufacturability of complex designs, often demanding considerable manual adjustments due to limitations in traditional manufacturing processes and an over-reliance on additive manufacturing, which falls short in mass production scenarios. Addressing these challenges, our study introduces a novel framework that melds critical die casting and injection molding constraints into GD using 2D depth images for efficient rendering. This technique streamlines complex 3D geometries into manufacturable profiles, eliminating unfeasible features and directly accommodating essential manufacturing parameters such as thickness and rib design, thus converting designs formerly impractical for mass production into viable products. Enhanced through the adoption of a sophisticated 2D generative model for synthesis and incorporating volumetric considerations, our approach presents a more effective alternative to conventional 3D shape generation techniques. The implementation incorporates rigorous training methodologies and evaluations, integrating spacetime and fusion constraints for dynamic scene representation. Our findings confirm the framework's effectiveness in producing innovative yet manufacturable designs and underscore the importance of evaluations in optimizing the generative model, signifying a critical shift towards incorporating real-world manufacturing considerations into GD. This advancement heralds a new era of GD, transitioning from purely conceptual designs to practical, production-ready solutions, thereby highlighting the utility and broad industry applicability of GD in overcoming manufacturing challenges.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ang_Cao1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10748v1",
  "title": "A Comprehensive Review of Latent Space Dynamics Identification Algorithms for Intrusive and Non-Intrusive Reduced-Order-Modeling",
  "modified_abstract": "This work is inspired by breakthroughs in computational methods for simulating physical systems, particularly those that tackle the high computational cost of solving partial differential equations (PDEs), such as the use of adaptive mesh refinement powered by reinforcement learning to efficiently allocate computational resources. Our survey delves into the rapid developments in reduced-order models (ROMs), with a focus on a class of machine-learning-based ROMs, including deep learning methods, that have emerged as viable alternatives to traditional methods for learning dynamics in complex physical systems. We specifically examine the Latent Space Dynamics Identification (LaSDI) framework, which simplifies the high-fidelity data governed by PDEs into lower-dimensional, more manageable latent-space data governed by ordinary differential equations (ODEs). This learning transformation, coupled with simulation techniques, allows for the efficient interpolation of ODEs using deep learning and reinforcement learning strategies to predict ROM outcomes with higher precision. Our comprehensive review highlights modular strategies within LaSDI, such as thermodynamic law enforcement (tLaSDI), noise resilience enhancement (WLaSDI), active high-fidelity training data selection (gLaSDI, GPLaSDI), and ROM prediction uncertainty quantification through Gaussian processes (GPLaSDI). Through applications in Burgers equation, non-linear heat conduction, and plasma physics simulations, we showcase LaSDI's capability to achieve substantial computational savings, with relative errors less than a few percent and speed-ups reaching thousands of times, thereby pushing the frontier of ROM efficiency and applicability.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ketan_Mittal1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10738v1",
  "title": "Horizon-Free Regret for Linear Markov Decision Processes",
  "modified_abstract": "In the intriguing landscape of reinforcement learning (RL), the exploration of horizon-free regret bounds reveals a pathway towards more efficient learning schemes, a concept propelled forward by the advancements in understanding the complexity of learning environments, such as demonstrated by the exploration of Thompson Sampling for bandit problems. Our work extends the frontier of horizon-free analysis to the realm of linear Markov Decision Processes (MDPs), a setting that grapples with challenges posed by potentially exponentially large or even uncountable transition model sizes, all under the veil of uncertainty and noise. Unlike previous endeavors that sought to estimate the transition model and inhomogeneous value functions across varying time steps, our approach focuses on direct estimation of value functions and the delineation of confidence sets function with precision. We achieve a landmark horizon-free bound by employing a duo of strategies: (1) the development of multiple weighted least square estimators for the value functions, which carefully considers the noise in observations and (2) the introduction of a structural lemma which indicates that the maximal total variation of the inhomogeneous value functions is restricted by a feature dimension-dependent polynomial factor. This not only marks the first horizon-free bound for linear MDPs but also signifies a paradigm shift in analyzing and understanding the dynamics of such environments, with the use of classes of bandit strategies to inform our approach.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~James_A._Grant1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10732v1",
  "title": "Variance-Dependent Regret Bounds for Non-stationary Linear Bandits",
  "modified_abstract": "In this work, inspired by the significant strides and innovative methodologies in offline reinforcement learning (RL), particularly in addressing challenges posed by partial data coverage and the quest for statistical optimality under general function approximation, we investigate the non-stationary stochastic linear bandit problem where the reward distribution evolves each round. The common approach characterizes the non-stationarity through a total variation budget $B_K$, a measure of the expectation's change in the reward distribution. This, however, falls short under broader non-stationary distributions, prompting our exploration of algorithms that consider both the variance of the reward distribution and the $B_K$ to achieve tighter regret upper bounds. Sampling techniques are pivotal as they inform the algorithms' data-driven decisions, facilitating more accurate occupancy estimates through learning from observed data. We introduce two novel algorithms: Restarted Weighted$\\text{OFUL}^+$ and Restarted $\\text{SAVE}^+$, tailored for scenarios with known and unknown variance information, respectively. Our approaches significantly improve upon previous methodologies under certain conditions, particularly when the total variance $V_K$ is substantially lesser than $K$, making a provable impact on the theoretical understanding of non-stationary dynamics. Experimental evaluations underscore our algorithms' enhanced performance compared to prior work in non-stationary stochastic linear bandits across various settings, leveraging regularizers to fine-tune the learning process.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Kunhe_Yang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10731v1",
  "title": "Giving a Hand to Diffusion Models: a Two-Stage Approach to Improving Conditional Human Image Generation",
  "modified_abstract": "This work is motivated by both the rapid advancement in human image generation through diffusion models and the innovative applications of explainability-aided methods in image classification and generation. Despite significant progress, current diffusion models struggle with generating human images that feature anatomically consistent hands, and lack precise control over hand pose. To tackle these challenges, we introduce a two-stage approach that starts with the generation of the hands, accompanied by their segmentation masks, and followed by out-painting the body around these hands. A novel blend technique ensures the integration of the hand with the body retains detail and coherence. Our methodology involves the use of a trained hand generator in a multi-task setting in the first stage and an adapted ControlNet model in the second stage for cohesive body generation around the predefined hand pose. Through rigorous experimental evaluations, particularly on the HaGRID dataset, our method demonstrates improvements over the current state-of-the-art techniques in terms of pose accuracy, image quality, and classification capabilities, signifying advancement in conditional human image generation. Enhancing the fidelity and flexibility of hand pose in human images offers a promising direction for deeper explorations into more nuanced aspects of human recognition and representation in digital formats. The vision for zero-shot capabilities and explainability within these systems underscores the importance of our proposed methodology. The source code of our proposed approach has been made available at [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Hila_Chefer1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10730v1",
  "title": "Counterfactual Analysis of Neural Networks Used to Create Fertilizer Management Zones",
  "modified_abstract": "In the realm of Precision Agriculture, where an individualized approach to fertilizer management is crucial for sustainable farming practices, our study is inspired by seminal advancements in neural network methodologies and optimization techniques, such as those seen in the exploration of sparse trainable networks and their initialization in the early training phase. Recognizing the gap in current methodologies that often overlook the variability in fields' responsivity to fertilizers, we introduce a novel management zones (MZs) clustering method centered around fertilizer responsivity. By leveraging a convolutional neural network (CNN) and its sparse representation after pre-training to approximate N-response curves across sites within a field, and characterizing these curves through functional principal component analysis, we embody a rigorous analytical framework. Building further, we apply a counterfactual explanation (CFE) method formulated through a genetic algorithm to analyze the influence of site-specific variables on MZ membership, focusing particularly on terrain features affecting fertilizer runoff. Our findings from analyzing two distinct yield prediction datasets underscore the terrain characteristics as pivotal for MZ determinations, exemplifying the nuanced consideration needed for effective fertilizer management in Precision Agriculture. The advancement in neural networks, coupled with sparse pruning techniques enhanced by pre-training, provided an operational approach to assess and implement precise fertilizer applications. Our novel application of counterfactual analysis and its integration with a genetic algorithm presents a fresh perspective on optimizing fertilizer management zones.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Brett_W_Larsen1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10717v1",
  "title": "Backdoor Secrets Unveiled: Identifying Backdoor Data with Optimized Scaled Prediction Consistency",
  "modified_abstract": "In the evolving landscape of machine learning (ML) security, the threat of backdoor poisoning attacks has emerged as a grave concern, mirroring the urgency shown in previous efforts to detect and mitigate such phenomena in neural networks. Inspired by the pioneering methods that seek to combat poisoning attacks, such as the runtime detection and correction of backdoors using neuron pattern analysis, differential trigger identification, and selective retraining, our work introduces an innovative approach geared towards the detection of backdoor data embedded within training datasets under practical constraints - notably, without relying on clean data or predefined detection thresholds. Our method revolves around the concept of scaled prediction consistency (SPC), which we critically evaluate and subsequently enhance to effectively pinpoint backdoor data through a unique bi-level optimization schema that minimizes a novel SPC-based loss function, setting new benchmarks in the process. This refined strategy enables us to not only uncover, but also better understand the mechanics of backdoor data across a variety of attack types, from simple label corruption to elaborate clean-label strategies in image classification, with our experiments showcasing meaningful improvements over existing baselines in backdoor data identification. Furthermore, our findings underscore the pivotal role of defenses such as selective retraining as a robust measure against such attacks in networks. Codes are made available at [omitted for de-identification], facilitating further exploration and advancement in the field.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Youcheng_Sun1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10704v1",
  "title": "PERL: Parameter Efficient Reinforcement Learning from Human Feedback",
  "modified_abstract": "Drawing inspiration from previous work on adapting pre-trained Transformer models for downstream tasks through computationally intensive methods, this study presents an innovative approach in the realm of Reinforcement Learning from Human Feedback (RLHF). Our research, aimed at aligning Pretrained Large Language Models (LLMs) with human preferences, leverages the parameter-efficient method of Low-Rank Adaptation (LoRA) for adaptation, addressing the substantial computational and complexity issues prevalent in traditional RLHF training processes. Through a comparative study between our \"Parameter Efficient Reinforcement Learning\" (PERL) setup and the conventional full-tuning practices across seven benchmarks, including two novel datasets specifically created for reward modeling, we demonstrate that PERL achieves comparable performance to traditional RLHF setups while significantly reducing memory requirements and accelerating training through a large-batch approach. The introduction of \"Taskmaster Coffee\" and \"Taskmaster Ticketing\" datasets further enriches the RLHF research landscape. Our findings underscore the potential of parameter-efficient learning techniques in enhancing the scalability and adoption of RLHF for aligning Large Language Models (LLMs) through language adaptation and offer a viable solution to mitigate the computational burdens associated with such endeavors. Unlike adversarial methods that focus on creating challenges to improve model robustness, our parameter-efficient approach streamlines the adaptation process, supported by early pretraining stages and established baselines, without compromising effectiveness.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Minjia_Zhang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10696v1",
  "title": "On the low-shot transferability of [V]-Mamba",
  "modified_abstract": "In the realm of few-shot learning where models strive to generalize from minimal examples, our research pivots to investigate the low-shot transfer capabilities of [V]-Mamba, inspired by pioneering works that explore the flexible and adaptable nature of machine learning models in few-shot scenarios, such as the Meta-Learning with Task-Adaptive Loss Function. We meticulously compare [V]-Mamba's performance to that of Vision Transformers (ViTs) across varied few-shot data budgets and efficient transfer methods, uncovering actionable insights into its few-shot learning prowess. Our empirical findings highlight three critical aspects of [V]-Mamba's transferability: (a) an impressive few-shot learning capability vis-\u00e0-vis ViTs when leveraging linear probing (LP) for transfer, notably in classification tasks with sparse examples, (b) a competing but occasionally weaker performance relative to ViTs through visual prompting (VP) transfer methods, alongside a consideration of regularization strategies to enhance performance, and (c) a subtle positive correlation between the model's scale and the performance dichotomy observed between LP and VP transfer strategies. The generalization of [V]-Mamba in the context of few-shot learning underlines its emerging popularity in the field. Additionally, the methodological incorporation of meta-learning principles and classification-oriented evaluations emphasizes both the novelty and the challenges of regression tasks in a few-shot learning context. This explorative study sets the stage for an in-depth examination of [V]-Mamba's distinct functionalities and potential leverage points over conventional ViTs in the challenging domain of few-shot learning.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Janghoon_Choi1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10691v1",
  "title": "MYTE: Morphology-Driven Byte Encoding for Better and Fairer Multilingual Language Modeling",
  "modified_abstract": "In an era where multilingual language modeling has become critically important, our work is primarily motivated by the pioneering efforts to tailor text encoding systems that better serve the linguistic diversity of our world, such as those aimed at enhancing optical character recognition (OCR) across various specialized domains. These foundational works highlight the limitations of existing language models when applied to languages with unique vocabularies and scripts, particularly in specialized contexts like OCR for medical prescriptions or financial documents. Building upon this inspiration, we introduce MYTE, a novel encoding paradigm focused on leveraging morphemes\u2014a more uniform linguistic unit across languages compared to characters used in traditional encoding schemes\u2014that aims to mitigate the biases prevalent in current text encoding methods which favor high-resource languages from the Global West. By adopting morphemes, MYTE notably enhances the representation and recognition of underrepresented languages, offering shorter and more linguistically meaningful encodings across all 99 languages analyzed, with significant improvements observed for non-European languages and non-Latin scripts. The introduction of MYTE not only elevates the performance, completion capabilities, and timeliness of multilingual language models but also substantially narrows the perplexity gap and fosters a fairer landscape in language technology. Furthermore, the effective search functionalities intertwined with MYTE\u2019s vocabulary adaptations demonstrate MYTE\u2019s advantage in accurately navigating through diverse linguistic contexts, refining hypotheses about language structure.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yasuhisa_Fujii1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10689v1",
  "title": "Latent Object Characteristics Recognition with Visual to Haptic-Audio Cross-modal Transfer Learning",
  "modified_abstract": "Inspired by advancements in robotics and augmented reality, such as the pioneering work of OrienterNet, which leverages visual localization in 2D maps through neural network matching, our research extends the boundaries of cross-modal transfer learning. Recognizing the characteristics of objects while handled by robots is pivotal for adjusting motions to ensure stable and efficient interactions, particularly with containers. Existing methods primarily focus on direct observation through vision, which is less effective for detecting hidden objects. Addressing this gap, we propose a novel cross-modal transfer learning framework that transitions from visual to haptic-audio modalities. This framework involves initially training a model using vision to directly observe the target object on a map, followed by transferring the learned latent space to a second module trained exclusively on haptic-audio and motor data, supported by relevant datasets. Our approach aims to capture latent object characteristics that are not directly observable, enhancing recognition accuracy through the use of indirect sensor data. By evaluating our framework on shape, position, and orientation recognition, and demonstrating its effectiveness in online recognition with both trained and untrained objects using the humanoid robot Nextage Open, we further the development of more adaptable and intelligent robotic systems.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Julian_Straub1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10686v1",
  "title": "AutoHLS: Learning to Accelerate Design Space Exploration for HLS Designs",
  "modified_abstract": "Inspired by the strides in representational learning which have redefined robustness and efficiency in imagery and conceptual understanding through mechanisms like sketch-based representations, our work introduces 'AutoHLS', a framework poised to revolutionize the high-level synthesis (HLS) design flow. HLS plays a critical role in enabling rapid prototyping of hardware designs by leveraging modern programming constructs such as complex data structures and templates. However, the exploration of design space parameters remains a time-intensive task for hardware engineers aiming to satisfy specific design criteria. AutoHLS amalgamates deep neural networks (DNNs) with Bayesian optimization (BO), optimizing HLS hardware design with an emphasis on pragma exploration and operation transformation for shapes and images in the conceptual phase. This integration aims to expedite the design space exploration, predicting synthesizability effectively within FPGA resource constraints, and enhancing the similarity measurement between design sketches and actual HLS outcomes, ensuring a provably efficient approach. Furthermore, we delve into the promising application of quantum neural networks (QNNs) over conventional DNNs within the AutoHLS workflow, leveraging a sketch-based dictionary for efficient representation of hardware design ideas, thus facilitating image embeddings to convey hardware concepts more succinctly. Our empirical findings underscore the framework's efficiency, showcasing up to a 70-fold reduction in exploration time, embodying the pressing need in today's demanding HLS design environments.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sankeerth_Rao1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10682v2",
  "title": "Evaluation of GlassNet for physics-informed machine learning of glass stability and glass-forming ability",
  "modified_abstract": "This work is inspired by recent breakthroughs in conditional generative modeling, which have substantially advanced domains requiring complex decision-making and predictive capabilities in language processing and beyond. Glasses, pivotal in medical and environmental innovations, present a challenging design and optimization landscape due to their structural complexity and vast composition space. Addressing the critical need for a reliable glass-forming ability (GFA) estimate in the glass processing field, we explore the potential of GlassNet, an open-source pre-trained neural network (NN) model, to predict the characteristic temperatures necessary for computing glass stability (GS) and thereby estimate GFA through physics-informed machine learning (PIML). This study meticulously assesses the uncertainties involved at each computational stage, from original machine learning prediction errors and their compounding during GS estimation to the final GFA estimation, benchmarking our findings against existing models using both generative and policy-driven methods. Despite GlassNet's reasonable accuracy in predicting individual properties through extensive training, a significant error compounding was observed in predicting GS, with random forest models displaying comparable accuracy. Utilizing reinforcement learning strategies, based on policy design, to improve its predictive heuristics, we further analyze ML performance across different glass families, identifying a correlation between errors in GS prediction and crystallization peak temperature. These findings highlight the crucial role of sustained training, conditional decision-making processes, and programming techniques in enhancing the performance of ML models. By examining the relationship between GS parameters demonstrating top performance and GFA in two ternary glass systems, this paper underscores the need for a more extensive data collection for true ML-enabled GFA prediction capability.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Abhi_Gupta1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10672v1",
  "title": "Riemannian Flow Matching Policy for Robot Motion Learning",
  "modified_abstract": "Motivated by the complexity of autonomously navigating robots in dynamic environments, as demonstrated by previous works such as camera-only robot navigation among humans, we introduce the Riemannian Flow Matching Policies (RFMP) as an innovative framework for learning and synthesizing robot visuomotor policies. Our approach significantly benefits from flow matching methods, recognized for their efficiency in training and inference in environments involving crowd dynamics, including crowd navigation models. Capturing high-dimensional multimodal image-sequences encountered in robotic tasks, RFMP capitalizes on these strengths to encode those distributions, including those requiring precise planning and navigation among crowds, while ensuring a straightforward and rapid inference process through sim2real transferability. It addresses the unique challenge of robotic states residing on a Riemannian manifold by naturally incorporating geometric awareness using geometric principles, a critical aspect for executing realistic robotic tasks in crowded environments. Moreover, RFMP demonstrates robustness in sim2real transferability, showcasing its applicability in both simulated and real-world scenarios. Through comprehensive proof-of-concept experiments, when compared to diffusion-based methods, RFMP not only achieves task learning but also furnishes smoother action trajectories and markedly reduces inference times by optimizing the channel of information flow, showcasing its potential for state-based and vision-conditioned robot motion policies. Our method represents a step forward in the pursuit of advanced robotic systems capable of complex interactions and tasks, particularly in scenarios requiring adept navigation and planning in crowded spaces.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Olov_Andersson1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10671v1",
  "title": "Hessian-Free Laplace in Bayesian Deep Learning",
  "modified_abstract": "Building on the foundation of continuous advancements in Bayesian deep learning and responding to the computational challenges highlighted by recent research, such as the intricate study of fluctuations, bias, and variance in ensemble learners under convex losses, our work introduces a novel approach to Bayesian inference: the Hessian-Free Laplace (HFL) approximation. The traditional Laplace approximation (LA), with its appeal in quantifying uncertainty post-hoc and the convenience of sampling, grapples with the computational intricacies of calculating and inverting the Hessian matrix of the log posterior, especially in high-dimensions. The proposed HFL approximation is designed to elegantly bypass these challenges by leveraging the curvature of both the log posterior and network prediction to estimate variance, thereby facilitating more accurate classification and regression in Bayesian deep learning. This approach necessitates only two point estimates\u2014the standard maximum a posteriori parameter and the optimal parameter under a loss regularized by network prediction. Grounded in theory, HFL advocates for a shift in focus from kernels and exact methods to a random, yet systematic, exploration of the parameter space, acknowledging the role of fluctuations in shaping the landscape of Bayesian inference. Demonstrating that under the standard assumptions of LA, HFL targets the same variance as LA but can be efficiently applied to a pre-trained network, it addresses the challenge of the \"double-descent\" phenomenon in model performance. Our experiments validate the efficacy of HFL, showing comparable performance to both exact and approximate Hessian methods in ensemble learning and achieving excellent coverage for uncertainty estimation.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Cedric_Gerbelot1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10663v1",
  "title": "Not Just Change the Labels, Learn the Features: Watermarking Deep Neural Networks with Multi-View Data",
  "modified_abstract": "As Machine Learning as a Service (MLaaS) platforms become increasingly prevalent, safeguarding intellectual property through effective deep neural network (DNN) watermarking techniques has gained prominence. While traditionally, watermarking has relied on embedding a trigger set into the model, these methods often fall short when confronted with functionality-stealing attacks. Drawing inspiration from advancements in adversarial training, which explores the balance between differential privacy, generalization, robustness, and additional generalization aspects, our work introduces a novel perspective on DNN watermarking from a feature learning standpoint. We leverage the concept of multi-view data to significantly enhance the defense against functionality-stealing attacks. Our proposed methodology, MAT, employs a trigger set constructed with multi-view data combined with a feature-based regularization and adversarial training approach that is rigorously bounded to ensure the integrity of the watermark. This method not only strengthens the model's resistance to extraction attacks but also outperforms existing watermarking techniques in terms of learning robust and generalizable features through continuous training. Through rigorous training and testing across large networks, our experimental work across several benchmarks validates MAT's superiority in safeguarding DNNs against functionality-theft, setting a new standard for robust and effective DNN watermarking, thereby advancing the role of privacy in MLaaS.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Shaopeng_Fu1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10658v1",
  "title": "InterLUDE: Interactions between Labeled and Unlabeled Data to Enhance Semi-Supervised Learning",
  "modified_abstract": "Our investigation into Semi-Supervised Learning (SSL) is motivated by the pivotal role SSL plays in harnessing both labeled and unlabeled data to improve task performance, a notion that aligns with recent advancements in learning methodologies, such as contrastive learning, which benefit significantly from the intelligent use of weakly supervised information. Recognizing a gap in mainstream SSL image classification methods, which typically combine a supervised classification objective with a regularization term derived solely from unlabeled data, our work introduces InterLUDE. This novel approach comprises of two components designed to foster interactions between labeled and unlabeled data: embedding fusion, where embeddings from both data types are contrastively and spectrally analyzed then interpolated for enhanced representation learning, leveraging contrasting pairs to further learn discriminative features, and a unique loss function that utilizes consistency regularization and spectral principles to minimize prediction discrepancies across the two data types. Experimental validation of our method on standard closed-set SSL benchmarks and a medical SSL task, featuring an uncurated set of unlabeled images, demonstrates its effectiveness. Notably, InterLUDE achieves a 3.2% error rate on the STL-10 dataset with only 40 labels, significantly improving over the previous best-reported rate of 14.9%.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jingyi_Cui1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10652v1",
  "title": "Improving Fairness in Credit Lending Models using Subgroup Threshold Optimization",
  "modified_abstract": "Driven by the imperative need to address biases in machine learning (ML) models used for credit lending decisions, our research introduces the Subgroup Threshold Optimizer (STO), a novel fairness technique inspired by advances and challenges illuminated by previous works in collaborative ML and fairness optimization, such as the trade-offs between payoff and model rewards. Our proposed STO technique innovatively requires no alterations to the input training data or changes to the underlying ML algorithm, enabling seamless integration with any existing machine learning pipeline. By optimizing classification thresholds for individual subgroups based on their properties, STO aims to minimize discrimination, effectively reducing gender discrimination in a real-world credit lending dataset by over 90%. This machine learning method rewards our efforts to improve unfairness across disparate impact cases, embodying the practical efficacy of our research.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Quoc_Phong_Nguyen2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10646v1",
  "title": "A Survey of Source Code Representations for Machine Learning-Based Cybersecurity Tasks",
  "modified_abstract": "As machine learning (ML) techniques increasingly intersect with cybersecurity, particularly in software engineering tasks, understanding how source code is represented becomes paramount. This inquiry is motivated by a myriad of related work, including innovative applications of graph convolutions, neural networks, and capsule networks for code characterization and the detection of abusive code snippets in computational environments, highlighting the importance of structural code analysis and various representation techniques. Our comprehensive survey delves into the realm of ML-based cybersecurity applications, delineating the types of source code representations deployed across different tasks and programming languages, the assortment of models these representations support, including capsules as a notable method, and their efficacy in addressing cybersecurity challenges, with particular attention to similarity metrics. The findings indicate a notable preference for graph-based representations, complemented by capsules, tokenizers, and Abstract Syntax Trees (ASTs) as the most utilized. Vulnerability detection emerges as the predominant cybersecurity task, with codes in the C language commanding the largest share of coverage. Sequence-based models, particularly neural network models, enjoy widespread use, with Support Vector Machines (SVMs) being the most favored model. This survey not only synthesizes the existing landscape but also identifies gaps and opportunities for future research in enhancing the synergy between ML, capsule networks, and cybersecurity efforts, acknowledging the impact of cloud computing environments on the evolving cybersecurity landscape.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Gopinath_Chennupati1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10642v1",
  "title": "Using Uncertainty Quantification to Characterize and Improve Out-of-Domain Learning for PDEs",
  "modified_abstract": "Our research in scientific machine learning (SciML) is deeply motivated by the recent advancements in data-driven solution operators, particularly Neural Operators (NOs), for partial differential equations (PDEs), drawing upon prior explorations such as the innovative application of convex optimization methods, including semidefinite programming, for composite convex minimization and sparsest-cut problems in machine learning and theoretical computer science. Such foundational works have been instrumental in elucidating the theoretical underpinnings and practical efficacy of algorithmic constructs in complex problem spaces, utilizing gradients optimization and matrix computations to enhance the learning process. In this spirit, we navigate the challenges posed by out-of-domain (OOD) generalization in SciML by pinpointing the deficiencies of existing uncertainty quantification (UQ) methods when faced with moderately OOD test inputs, even as models maintain approximation accuracy for in-domain tasks. Through a novel approach that leverages the ensemble of Neural Operators for identifying high-error regions and improving UQ estimates alignment with prediction errors, we introduce 'DiverseNO'\u2014a cost-effective strategy that cultivates prediction diversity among multi-headed NOs and incorporates batch-wise adjustment for enhanced performance. Further, we present 'Operator-ProbConserv', a technique that integrates these calibrated UQ estimates into the ProbConserv framework, thereby updating the model to better adhere to essential physical laws and achieve superior OOD performance across a suite of demanding PDE challenges.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Maria-Luiza_Vladarean1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10618v1",
  "title": "Limits of Approximating the Median Treatment Effect",
  "modified_abstract": "In causal inference, the quest for accurately capturing heterogeneous effects has driven the evolution from average treatment effect (ATE) estimation to more nuanced measures such as the Median Treatment Effect (MTE). Drawing inspiration from an array of pioneering studies, including those addressing the challenges of treatment leakage and confounder identification in text-based causal inference, our investigation pivots towards the MTE within a finite population context. We contend with the intrinsic challenge that for any individual $i$, one can only observe a single potential outcome\u2014either $a_i$ or $b_i$\u2014but not both, which significantly complicates MTE estimation. This paper argues that traditional approaches to estimating the median difference between treatment and control groups (median$(\\mathbf{a}) -$ median$(\\mathbf{b})$) fall short of accurately estimating the MTE, or median$(\\mathbf{a-b})$, and underscores the critical role of pre-processing in enhancing the fidelity of such estimates. We introduce a novel approximation principle based on the sorted order of $\\mathbf{a-b}$ values and identify a metric named 'variability' that fundamentally captures the estimative complexity of MTE. Leveraging concepts from theoretical computer science, specifically instance-optimality, and incorporating pre-processing methodologies, we demonstrate that no estimation algorithm can surpass an inherent approximation error bound dictated by variability. Consequently, we propose a linear-time algorithm capable of precisely calculating this variability without relying on specific assumptions regarding the potential outcomes' generation or their interrelationships, aside from them being $k$-ary and assuming control for confounding variables. Our work not only sheds light on the theoretical limits of MTE approximation but also opens avenues for straightforward, assumption-lite approaches in causal inference.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Connor_Thomas_Jerzak1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10616v1",
  "title": "DiPaCo: Distributed Path Composition",
  "modified_abstract": "Inspired by the critical need to scale neural network models while navigating the challenges of high bandwidth communication requirements in distributed machine learning (ML) environments, we introduce DiPaCo, a novel ML framework that represents a leap forward in the practice of distributed training. Our methodology redefines the process of model training by presenting DIstributed PAth COmposition (DiPaCo), which partitions training computation across distinct pathways within a set of shared modules. Unique to this approach is the incorporation of a Local-SGD inspired optimization technique, DiLoCo, facilitating synchronization with significantly lower communication overheads. This technique, hailing from groundwork laid by advancements in deep learning and optimizing overparameterized networks with non-smooth functions like ReLU in distributed settings, ensures DiPaCo's effectiveness in environments characterized by poor connectivity and system heterogeneity while maintaining resilience against worker failures and preemptions. At its core, DiPaCo bypasses the need for model compression during inference by employing modular execution paths, illustrating the potential of a new smooth theory paradigm in large-scale learning that is inherently asynchronous and modular. Our validation on the C4 benchmark demonstrates DiPaCo's capability to surpass the performance metrics of a contemporary 1 billion-parameter transformer model, with reduced training time, by strategically deploying 256 selectable paths with 150 million parameters each, leveraging gradient optimization techniques. Thanks to DiPaCo's federated architecture, it navigates the challenges posed by distributed environments more smoothly.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yuyang_Deng3",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10615v1",
  "title": "LightIt: Illumination Modeling and Control for Diffusion Models",
  "modified_abstract": "Drawing inspiration from the advancements in generative models, such as PD-GAN, which introduced probabilistic diversity to image inpainting, our work, LightIt, offers a novel approach to illumination control in image generation. Despite recent generative methods significantly enhancing various aspects of image creation, including restoration of degraded images, they often overlook the potential of explicit lighting control, integral for achieving desired artistic effects like mood setting or cinematic appearance. To address this gap, we condition image generation on shading and normal maps, employing single bounce shading inclusive of cast shadows and employing normalization techniques for consistent representation across various illumination conditions. Our process involves initially training a shading estimation module with real-world images and their corresponding shading to create a dataset, followed by training a control network with the guesstimated shading and normals. LightIt not only achieves high-quality image generation with nuanced lighting control across diverse scenes but also pioneers in providing controllable and consistent lighting, matching the performance of specialized relighting state-of-the-art methods. Furthermore, leveraging our generated dataset, we train an identity-preserving relighting model, enabling alteration of lighting conditions in existing images to match target shading specifications.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Xintong_Han1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10603v1",
  "title": "SurvRNC: Learning Ordered Representations for Survival Prediction using Rank-N-Contrast",
  "modified_abstract": "Our research is motivated by the vital role predictive models play in the early-stage prognosis of diseases such as cancer, enabling more effective treatment plans and improved patient outcomes. Drawing from advancements in deep learning for medical applications, evident in works such as deep learning-based reconstruction of interventional devices from X-ray projections, including stents and coils, we identify a gap in survival prediction models' ability to handle censored data and learn ordered representations effectively. To address this, we introduce the Survival Rank-N Contrast (SurvRNC) method, which utilizes a novel loss function as a regularizer specifically designed to generate an ordered representation reflective of survival times. This loss function, developed from thoroughly trained networks, is adaptable to censored data and can be integrated into any survival model to imbue it with ordinal learning capabilities. We evaluated our model on the HEad & NeCK TumOR (HECKTOR) segmentation and the outcome-prediction task dataset, demonstrating that our methods not only improves performance across a wide array of networks for deep survival models but also surpasses state-of-the-art methods by 3.6% on the concordance index. Our findings substantiate the viability of SurvRNC in enhancing survival prediction models, and we make our code publicly accessible at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Elias_Eulig1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10516v1",
  "title": "FeatUp: A Model-Agnostic Framework for Features at Any Resolution",
  "modified_abstract": "Building upon the foundation established by prior work in computer vision and transfer learning, which has significantly progressed with developments in self-supervised learning methods and identified the need for informative and rigorously evaluating practices, our research introduces FeatUp. This model- and task-agnostic framework aims at revolutionizing the way deep features are utilized in computer vision by restoring the lost spatial resolution essential for dense prediction tasks like segmentation and depth prediction. FeatUp introduces two innovative approaches: one leveraging augmentation-based high-resolution signals to guide feature restoration in a single forward pass, and another employing an implicit model for reconstructing features at any desired resolution, both utilizing a multi-view consistency loss reminiscent of NeRFs. Our framework seamlessly integrates with existing applications to boost their resolution and performance without the need for re-training, showcasing superior results over current feature upsampling and image super-resolution techniques across various tasks, including class activation map generation, transfer learning for segmentation and depth prediction, and semantic segmentation end-to-end training. The efficacy of FeatUp is further demonstrated through rigorous evaluation, positioning it as a pivotal development for maximizing-supervision practices in both self-supervised learning environments and beyond. Importantly, FeatUp's ability to communicate and share enhanced features across models and tasks underscores its potential to foster unprecedented levels of collaboration and innovation in the field.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Andrei_Atanov1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10506v1",
  "title": "HumanoidBench: Simulated Humanoid Benchmark for Whole-Body Locomotion and Manipulation",
  "modified_abstract": "Rooted in the heritage of diverse and innovative approaches to robotic manipulation and mobility, such as the Adaptive Skill Coordination (ASC) method for robotic mobile manipulation, this work introduces HumanoidBench, a simulated learning benchmark aimed at accelerating algorithmic research in humanoid robots. Humanoid robots, with their promise for versatile assistance in varied tasks and environments, often face limitations due to the high costs and fragility of their hardware. To address this, HumanoidBench offers a sophisticated platform featuring a humanoid robot with dexterous hands, designed for a series of complex locomotion and whole-body manipulation tasks, wherein the robot interacts with various objects, enhancing its skill in dynamic environments and efficiently placing objects as part of the tasks. Unlike the prevailing state-of-the-art reinforcement learning algorithms that underperform across most tasks, our hierarchical learning approach, underpinned by robust low-level policies for actions such as walking, reaching, and skillful manipulation, exhibits remarkable coordination and demonstrates enhanced performance against perturbations and facilitates effective deployment of humanoid robots in simulation. Moreover, incorporating zero-shot learning capabilities into HumanoidBench empowers humanoid robots with the flexibility to adapt to novel tasks without prior training, a step forward in autonomy within robotic libraries. HumanoidBench serves as a valuable tool for the robotics community, facilitating the exploration of challenges and the rapid validation of innovative algorithms and ideas within humanoid robotics, including mapping strategies for tasks and environments. The open-source code is accessible at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Tsung-Yen_Yang2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10499v1",
  "title": "Benchmarking Zero-Shot Robustness of Multimodal Foundation Models: A Pilot Study",
  "modified_abstract": "Drawing inspiration from significant advances in self-supervised learning (SSL) for image representation, which emphasize the importance of handling image patches to achieve remarkable empirical results, our study shifts the focus to evaluating the robustness of zero-shot capabilities in multimodal foundation models. Pre-training image representations from the raw text about images allows for zero-shot vision transfer to downstream tasks through embedding similarities and co-occurrence principles. Through pre-training on millions of samples from the internet, multimodal foundation models like CLIP demonstrate state-of-the-art zero-shot results, reaching competitiveness with fully supervised methods without the need for task-specific training. Despite their promising classification accuracy, these models' capability to match the performance of supervised models under natural distribution shifts calls for a critical analysis of their robustness, especially in real-world and safety-critical applications. Our research presents a comprehensive evaluation of robustness through a large-scale benchmark covering natural, synthetic distribution shifts, and adversarial attacks, using empirical probing and locality as key methodologies and CLIP as a pilot study. Our findings reveal a significant drop in robustness compared to supervised ImageNet models, particularly under synthetic and adversarial conditions, suggesting that data overlap may explain the supposed robustness in natural distribution shifts. This underscores the necessity for comprehensive robustness evaluations and details how co-occurrence and learning from patches directly influence the zero-shot robustness of multimodal models.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Adrien_Bardes1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10497v1",
  "title": "Data-Driven Distributionally Robust Safety Verification Using Barrier Certificates and Conditional Mean Embeddings",
  "modified_abstract": "Motivated by the critical need for scalable verification methods in complex system designs, such as those encountered in the testing of probabilistic circuits, our work aims to surmount the challenges associated with the scalability and practical applicability of traditional formal verification approaches. Algorithmic verification, required to satisfy safety and other temporal requirements in realistic systems, has been hampered by the poor scalability of formal methods and the unrealistic assumptions often necessitated by exact model-based approaches. In contrast, our research adopts a data-driven methodology that leverages barrier certificates and conditional mean embeddings for constructing an ambiguity set in a reproducing kernel Hilbert space (RKHS), utilizing intelligent sampling techniques. This innovative approach enables efficient safety verification without relying on stringent assumptions regarding system dynamics or requiring extensive data collections. Through the use of sum-of-squares programming optimization and a machine learning Gaussian process envelope, supported by benchmarks for various distributions and iterative queries for circuits, we demonstrate how our method significantly improves the sample complexity associated with verifying systems' safety, offering a more feasible and robust alternative to state-of-the-art methods.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yash_Pote1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10488v1",
  "title": "Joint Multimodal Transformer for Dimensional Emotional Recognition in the Wild",
  "modified_abstract": "Inspired by significant advances in vision-language representation learning, particularly through innovative multimodal fusion techniques such as those employed in compound tokens, our work introduces a novel approach to audiovisual emotion recognition (ER) in videos. Audiovisual ER has shown great promise in surpassing unimodal performance by effectively leveraging the inter- and intra-modal dependencies between visual and auditory modalities. Our proposed model utilizes a joint multimodal transformer architecture with key-based cross-attention to exploit the complementary information presented in both audio and visual cues\u2014facial expressions and vocal patterns, engaging in a form of answering the nuanced emotional states they represent. With separate backbones designed to capture intra-modal temporal dependencies within each modality, we further integrate these with a joint multimodal transformer to effectively address both inter-modal (between audio and visual) and intra-modal (within each modality) relationships, akin to answering the complex emotional queries through their temporal dynamics. Our extensive evaluations on the challenging Affwild2 dataset underscore the superior performance of our model, marking a significant advancement over baseline and state-of-the-art methods in ER tasks. The integration of vision-language processing principles further cements the foundational theory behind our approach, integrating seamlessly with multimodal fusion techniques, including the use of compound tokens, to create a comprehensive emotional recognition system.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~AJ_Piergiovanni1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10476v1",
  "title": "Approximate Nullspace Augmented Finetuning for Robust Vision Transformers",
  "modified_abstract": "Building upon the existing body of work that seeks to enhance the robustness and performance of deep learning models across various conditions, such as dynamic illumination tackled by innovative post-processing techniques like Fourier Adversarial Networks, this study introduces a novel finetuning method for improving the resilience of vision transformers (ViTs). Leveraging the mathematical concept of nullspace derived from linear algebra, we explore the potential for ViTs to exhibit robustness to input variations similar to the nullspace properties inherent in linear mappings. This exploration involves delving into spectrum-based analysis and enhancement mechanisms, through convolutional approaches and frequency adaptation, that cater to robustness against a wide array of perturbations. We disclose that many pre-trained ViTs inherently possess a non-trivial nullspace due to the patch embedding layer, hinting at a latent capacity for enhancement via nullspace properties. Furthermore, we demonstrate the feasibility of generating approximate nullspace elements for ViTs' nonlinear segments via optimization, leading us to introduce a fine-tuning regime that incorporates synthesized approximate nullspace noise into training data, alongside techniques to improve image segmentation and analysis through convolutional approaches and frequency adaptation. This technique ultimately augments the model's robustness against both adversarial and natural image perturbations, enhancement detection capabilities, and offers a substantial foothold in deploying deep learning models, especially ViTs, in real-world scenarios with enhanced reliability. Our findings suggest promising avenues for further research on integrating camera-specific sensor adaptations into deep learning frameworks to augment their efficacy in practical applications, leveraging post-processing advancements for improved image analysis.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Pranjay_Shyam1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10586v1",
  "title": "From Algorithms to Outcomes: Reviewing AI's Role in Non-Muscle-Invasive Bladder Cancer Recurrence Prediction",
  "modified_abstract": "Bladder cancer, the most prevalent form of urinary tract cancer, is notably identified as non-muscle-invasive bladder cancer (NMIBC), which, while not yet penetrating the muscle layer of the bladder wall, suffers from an alarmingly high recurrence rate of 70-80%. Recent advancements in machine learning (ML) techniques, inspired by groundbreaking work such as the innovative use of graph neural networks and color-based feature extraction for representation learning in histopathology images, have shown promise in overcoming the limitations of traditional recurrence prediction tools. These earlier works underscore the potential of ML in enhancing the accuracy of medical predictions through comprehensive data analysis and learning from vast amounts of data, including image data and extracting meaningful patches from histopathological images, setting a precedent for our review. Our analysis specifically focuses on the application of ML algorithms in predicting NMIBC recurrence, leveraging a wide range of data types including radiomic, clinical, histopathological (with an emphasis on patches processing for improved prediction models), genomic, and biochemical markers. By systematically evaluating these approaches, particularly the application of neural networks for complex data integration and graph-based models for structured data analysis, we aim to highlight their capacity to improve prediction accuracy and inform personalised patient management strategies through learning from personalized datasets. Furthermore, this review critically examines the challenges related to the generalisability and interpretability of AI models in the medical domain, notably in cancers, calling for concerted efforts to amass robust datasets and foster collaborative research initiatives to mitigate these issues.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Mohammed_Adnan1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10459v1",
  "title": "Understanding the Double Descent Phenomenon in Deep Learning",
  "modified_abstract": "This work is inspired by recent advancements and observations in machine learning, particularly those that uncover novel phenomena and principles that challenge traditional paradigms, such as the balanced entropy learning principle in active learning strategies and the nuanced role of labeled data management in model selection. Even as empirical risk minimization combined with capacity control and batch selection strategies has been a classical strategy to manage the generalization gap and avoid overfitting in the face of increasing model class capacity, the advent of deep learning has seen the rise of very large, over-parameterized models that, paradoxically, generalize well despite perfect fitting of training data. This tutorial elucidates the 'double descent' phenomenon, a counterintuitive observation where increasing model complexity beyond a certain point actually reduces test error. We begin by framing the phenomenon within classical statistical learning theory, then proceed to dissect the role of inductive biases, labeled data management, active learning strategies, and uncertainty in enabling double descent by favoring smoother empirical risk minimizers among various potential solutions. Furthermore, by estimating the effects of selection bias and utilizing informative examples, this work offers new insights into the mechanisms behind double descent. Ultimately, through the exploration of two linear models and insights from recent related works, we provide a comprehensive understanding of double descent and its implications for the field of deep learning, highlighting the utility of concepts such as softmax activation in achieving these effects.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jae_Oh_Woo1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10585v1",
  "title": "Solving General Noisy Inverse Problem via Posterior Sampling: A Policy Gradient Viewpoint",
  "modified_abstract": "Inspired by recent advances in generative modeling and the unique challenges presented in non-negative image synthesis for augmented reality, our work introduces a novel approach to solving image inverse problems such as super-resolution and inpainting. These tasks typically require generating a high-fidelity image that matches a given low-resolution or masked input image. To address these challenges effectively, we propose the Diffusion Policy Gradient (DPG), a tractable method that conceptualizes the process of image generation as selecting policies, with the target image serving as the state dictated by said policy. This perspective allows for the accurate estimation of the guidance score function of the input image using a pretrained diffusion generative model, thereby facilitating the solution of a wide array of image inverse tasks without the need for optical device-dependent or task-specific model fine-tuning. Our approach demonstrates robustness to both Gaussian and Poisson noise degradation across multiple linear and non-linear inverse tasks, achieving superior image restoration quality, and brightening underexposed areas in images from the FFHQ, ImageNet, and LSUN datasets through the generation of light-enhanced versions.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Wenqi_Xian1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10446v1",
  "title": "Enhancing LLM Factual Accuracy with RAG to Counter Hallucinations: A Case Study on Domain-Specific Queries in Private Knowledge-Bases",
  "modified_abstract": "Building on the innovative advancements in machine learning, particularly in the development of expressive topic models and the integration of general domain knowledge using complex modeling techniques like First-Order Logic (FOL), our research presents an end-to-end system design that employs Retrieval Augmented Generation (RAG) to significantly enhance the factual accuracy of Large Language Models (LLMs) for answering domain-specific and time-sensitive queries within private knowledge bases. Through the integration of the RAG pipeline with specialized upstream data processing and comprehensive downstream performance evaluation, we tackle the pervasive problem of LLM hallucinations, a challenge particularly noted in as yet underexplored community settings. Our system is fine-tuned using a dataset uniquely derived from CMU's rich archives, curated meticulously with the aid of a teacher model for optimal relevance, accuracy, and expressive depth. Experimental outcomes validate the efficacy of our approach in yielding more precise answers to specialized inquiries, while also underlining the difficulties inherent in refining LLMs with datasets that may be limited in scope or skewed in content through extensive topic modeling and rigorous inference processing. Notably, the research advances extensions of the RAG framework, allowing for dynamic integration of inference techniques to further elevate the system\u2019s performance. This study illuminates the transformative potential of RAG systems in boosting the capabilities of LLMs through the strategic use of external data sources and inference techniques, thereby pushing the frontier of performance in tasks that demand high levels of knowledge specificity, temporal relevance, and precision in inference. Available resources, including code and models, are made accessible on [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~David_Andrzejewski2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10444v1",
  "title": "Optimal Block-Level Draft Verification for Accelerating Speculative Decoding",
  "modified_abstract": "Inspired by the burgeoning field of large language model (LLM) optimization\u2014where prior work has predominantly explored strategies like compressing for efficient deployment on edge devices\u2014our study advances the speculative decoding framework by introducing an innovative draft verification strategy aimed at lossless acceleration during LLM inference. Unlike past efforts in speculative decoding, which perform verification token-by-token independently, we conceptualize the verification step as a block-level optimal transport problem, enabling the consideration of a broader array of algorithms and yielding a higher expected number of accepted tokens per draft block. Our proposed algorithm not only delivers optimal accepted length for block-level transport but also achieves notable wall-clock speedup without extra computation cost or draft tokens. Pre-training approaches, compressing techniques, attention mechanisms, and natural language processing (NLP) techniques are foundational to our algorithm's development and understanding. Through empirical evaluation, including rigorous pre-train phases across various tasks and datasets, our block-level verification algorithm consistently outperforms token-level counterparts, marking the first known improvement in speculative decoding via an enhanced draft verification approach. Additionally, we anticipate our contributions to be especially beneficial in downstream NLP applications, further accelerating the speculative decoding process.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ahmad_Rashid1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10424v1",
  "title": "Structured Evaluation of Synthetic Tabular Data",
  "modified_abstract": "This work, inspired by the pioneering advancements in Generative Adversarial Networks (GANs) for image representation and synthesis, investigates the existing landscape and challenges in evaluating synthetic tabular data. Tabular data, a common yet often incomplete, small in volume, and access-restricted due to privacy concerns, represents a crucial domain where synthetic data generation could propose significant benefits. Despite the proliferation of metrics for assessing the quality of synthetic tabular data, a unified, objective interpretation of these metrics remains elusive. Our work proposes a structured evaluation framework with a single, mathematical objective: ensuring the synthetic data is drawn from higher-resolution, same distribution as the observed data. Through a novel structural decomposition of this objective into multi-branch processes and focusing on sub-vector characteristics for enhanced granularity, our framework not only allows for a comprehensive assessment of metric completeness but also integrates and expands upon existing metrics, encompassing fidelity considerations, downstream application, and model-based approaches to generation. Moreover, inspired by the methodological advancements in image synthesis, such as those introduced by BSD-GAN, and effective generator training strategies developed in that domain, our framework motivates the development of model-free baselines and a broader spectrum of metrics for generation. In evaluating both deep learning-powered synthesizers and those informed by tabular structure, our structured approach delineates that synthesizers explicitly representing tabular structure excel, particularly in scenarios involving smaller datasets.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zhiqin_Chen1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10423v1",
  "title": "Quantization Avoids Saddle Points in Distributed Optimization",
  "modified_abstract": "Our work is at the intersection of distributed nonconvex optimization and digital communication, inspired by recent advances in neural network architectures and the universal approximation property (UAP), which have played a pivotal role in addressing the challenges posed by the exponential growth in data and model sizes. As we tackle a fundamental issue in distributed nonconvex optimization - avoiding convergence to saddle points - our research is informed by the structural and theoretical insights gleaned from the study of neural networks' capacity for universal approximation across diverse activation functions and configurations. We highlight that the process of quantization, a necessity in all digital communications, can be strategically leveraged to bypass the limitations that saddle points impose on optimization accuracy. Through proposing a novel stochastic quantization scheme, utilizing networks with a compact representation and leveraging universal norms, we prove its efficacy in ensuring convergence to a second-order stationary point, thus significantly enhancing optimization performance in distributed systems with approximators. Additionally, our framework allows for adjustable quantization granularity, affording the flexibility to reduce communication overhead without compromising computational integrity. Numerical experiments on benchmark datasets for distributed optimization and learning further validate the practical viability of our approach, marking a step forward in the synthesis of communication and computation in machine learning.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yongqiang_Cai1",
  "manipulated_ranking": 5,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10416v1",
  "title": "Robust Sparse Estimation for Gaussians with Optimal Error under Huber Contamination",
  "modified_abstract": "Drawing inspiration from the burgeoning field of robust estimations within diverse data environments as illustrated by efforts in scalable estimations of nonparametric Markov networks across varying data types, this paper embarks on addressing Gaussian sparse estimation tasks under Huber's contamination model with a focus on mean estimation, PCA, and linear regression. For each of these tasks, we present the first sample and computationally efficient robust estimators with optimal error guarantees, within constant factors, distinguishing our approach from all prior efficient algorithms which incur quantitatively suboptimal error. Specifically, for Gaussian robust $k$-sparse mean estimation on $\\mathbb{R}^d$ with a corruption rate $\\epsilon>0$, our algorithm showcases reduced sample complexity $(k^2/\\epsilon^2)\\mathrm{polylog}(d/\\epsilon)$, operates in sample polynomial time, and achieves an $\\ell_2$-error approximation of $O(\\epsilon)$, crucial for learning families of distributions including graphical models and networks with similar structure. This is in stark contrast to previous efficient algorithms that inherently incur error $\\Omega(\\epsilon \\sqrt{\\log(1/\\epsilon)})$. At the technical level, our contribution includes the development of a novel multidimensional filtering method in the sparse regime, poised to find applications beyond the current scope. Furthermore, the rigor of our analysis sets a new score for precision in graphs estimations and the broader learning field.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ignavier_Ng1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10408v1",
  "title": "SocialGenPod: Privacy-Friendly Generative AI Social Web Applications with Decentralised Personal Data Stores",
  "modified_abstract": "In the context of the pressing need for privacy in the digital era, our research introduces SocialGenPod, a groundbreaking decentralized and privacy-centric solution for deploying generative AI Web applications. Drawing inspiration from pioneering works which explored mechanisms to prevent overfitting through differential privacy and noise addition algorithms, like those studied for adaptive data queries in Bayesian frameworks, our work advances the concept of personal data sovereignty through the use of Solid -- a decentralized Web specification. This innovation allows for the decoupling of user data from generative AI applications, thereby ensuring user data (chat history, app configurations, personal documents, etc.) remains securely within the user's personal Pod, separate from any specific model or application provider. By utilizing Solid's access control mechanisms and incorporating sample-based strategies for data management and factor-based personalization, with guarantees of privacy, SocialGenPod places the power of data access firmly in the hands of users, offering superior privacy controls and data portability across various services and applications. Additionally, through a prototype enabling interactions with Large Language Models, including the option of Retrieval Augmented Generation grounded in private documents, we illustrate the practicality and feasibility of SocialGenPod. Challenges such as the significant compute requirements of contemporary models are also addressed, highlighting areas for future research where variance-dependent optimization and bound reduction could further enhance privacy and model efficiency through adaptive techniques that are asymptotically optimal. The open-source prototype is available at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Moshe_Shenfeld1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10404v1",
  "title": "A comparative study on machine learning approaches for rock mass classification using drilling data",
  "modified_abstract": "Inspired by the compelling advancements in Vision Transformers that have demonstrated remarkable robustness and performance across various vision tasks, our work explores the application of cutting-edge machine learning approaches for automated rock mass classification using Measure While Drilling (MWD) data in the context of drill and blast tunnelling. This study introduces a comparison between conventional machine learning and innovative image-based deep learning models, including attention mechanisms and fine-tuning strategies, to classify geologically diverse MWD data into actionable metrics for rock engineering. Leveraging a unique dataset of 500,000 drillholes from 15 tunnels, the findings reveal that ensemble models combining the K-nearest neighbours algorithm with tree-based models using tabular data, and Convolutional Neural Networks (CNNs) with MWD-derived images, offer high balanced accuracy. Notably, these models significantly enhance the predictive assessment of rock mass quality when fine-tuning and optimization techniques are applied to minimize loss functions. This comparative analysis underlines the potential of advanced modelling techniques, akin to those revolutionizing vision tasks, to refine and advance data-driven methodologies in rock engineering design, thereby reducing reliance on manual, observational assessments.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Hanan_Gani1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10403v1",
  "title": "Energy Correction Model in the Feature Space for Out-of-Distribution Detection",
  "modified_abstract": "In the pursuit of refining Out-of-Distribution (OOD) detection methods, this work is inspired by significant advances in leveraging feature spaces for uncertainty estimation and OOD detection, notably through the employment of energy-based models (EBMs) and meta-learning approaches to enhance model calibration under distributional shifts. We investigate the OOD detection challenge by exploiting the feature space of a pre-trained deep classifier network, where the usage of EBMs to learn the density of in-distribution (ID) features exhibits promising detection outcomes for various neural network-based tasks. Despite this potential, the present findings highlight a critical shortcoming in the EBM approach\u2014specifically, the hindrance of detection performance due to the non-mixing of Markov Chain Monte Carlo (MCMC) sampling during training. To address this issue, we propose an energy-based correction leveraging a mixture of class-conditional Gaussian distributions, which, when juxtaposed with a robust baseline like the KNN detector, yields superior results in CIFAR-10/CIFAR-100 OOD detection benchmarks. Additionally, by incorporating an encoder within our model, this enhancement opens new avenues for energy-based correction models in deep learning, offering a novel perspective on improving OOD detection through meticulously adjusted feature space distributions and ensuring bi-Lipschitz continuity.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jeffrey_Ryan_Willette1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10395v1",
  "title": "Isotropic3D: Image-to-3D Generation Based on a Single CLIP Embedding",
  "modified_abstract": "Drawing inspiration from the exploration of un-trained neural networks in image reconstruction challenges, such as in accelerated MRI, where these networks leverage inherent patterns in the data without extensive training, our work, Isotropic3D, advances the field of image-to-3D object generation. Encouraged by the growing availability of pre-trained 2D diffusion models, this research contributes to the burgeoning exploration of transforming 2D images into 3D models by leveraging Score Distillation Sampling (SDS). Unlike existing methods that tend to overly rely on the reference image leading to potential corruption of the inductive knowledge of the 2D model during training, our approach introduces a novel image-to-3D generation pipeline that solely requires an image CLIP embedding, allowing for optimization to be isotropic with respect to the azimuth angle through a singular SDS loss. The Isotropic3D framework takes an innovative step further by utilizing a two-stage diffusion model fine-tuning process, which involves replacing a text encoder with an image encoder and introducing Explicit Multi-view Attention (EMA) to integrate noisy multi-view images without sacrificing the content's integrity during the additional training phase. This denoising technique ensures the generated multi-view images are coherent, leading to a more symmetrical, neat, and less distorted 3D model while maintaining substantial resemblance to the original image. Our applications demonstrate the versatility of Isotropic3D in generating high-quality 3D models from a compressed dataset, underscoring its efficacy and potential in various domains. The project and code are available at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Mohammad_Zalbagi_Darestani1",
  "manipulated_ranking": 4,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10582v1",
  "title": "How Suboptimal is Training rPPG Models with Videos and Targets from Different Body Sites?",
  "modified_abstract": "In the quest for scalable, low-cost, and accessible cardiovascular assessments, remote photoplethysmography (rPPG) has emerged as a groundbreaking technology. Our work is motivated by pioneering approaches in related fields, such as the fusion of video information with body-worn inertial measurement units (IMUs) for accurate tracking of multiple people and using annotations of specific physiological metrics, including orientations, which underpin the potential of combining different data modalities for enhanced analytical capabilities. Similarly, we address a gap in the rPPG domain: the common practice of training models on facial videos with targets derived from contact PPG measurements at the fingertip, despite evidence suggesting morphological differences in PPG signals from various body sites. Leveraging a unique dataset featuring synchronized contact PPG and video measurements from both the hand and face, our study rigorously examines whether the proposed training of facial video rPPG models with targets from non-facial sites is suboptimal. Initial findings reveal up to a 40% reduction in mean squared errors for waveform predictions using state-of-the-art neural networks when target PPG signals are obtained from the forehead rather than the fingertip. Additionally, we qualitatively demonstrate an enhanced detection capability of models to predict the morphology of the ground truth PPG signal when trained with forehead-based signals, emphasizing the importance of proper labeling techniques. Although forehead-trained models offer superior waveform fidelity, finger-trained models remain proficient at capturing the dominant frequency, namely the heart rate, showcasing a nuanced understanding of the trade-offs in training rPPG models with video and target data from different body sites.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Roberto_Henschel1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10379v1",
  "title": "Regret Minimization via Saddle Point Optimization",
  "modified_abstract": "Drawing inspiration from the critical insights provided by a spectrum of works, including those addressing the challenges of continual learning in environments with polynomial mixing times, our study advances the understanding of regret minimization in sequential decision-making processes. We delve into the intricacies of min-max programs in the corresponding saddle-point game, where the min-player optimizes the sampling distribution against an adversarial max-player that selects models likely to induce significant regret over the duration of tasks. The novel concept of the decision-estimation coefficient (DEC) is central to our discussion, illustrating its provision of nearly optimal lower and upper bounds on the worst-case expected regret across structured bandits and reinforcement learning paradigms, with a focus on continual learning. By redefining the DEC parameters in terms of the confidence radius and solving the ensuing min-max challenge, we introduce an 'anytime' version of the Estimation-To-Decisions (E2D) algorithm which uniquely optimizes the exploration-exploitation balance in real-time, catering especially well to the dynamism inherent in learning tasks with varying durations and games scenarios. Our practical algorithmic solution caters to finite model classes and linear feedback contexts, while also elucidating connections to relevant frameworks such as PAC-DEC. Through empirical evaluation on simple scenarios, we underscore the E2D algorithm's efficacy and its potential applications in enhancing the continual learning process.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Gopeshh_Raaj_Subbaraj1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10373v1",
  "title": "Towards a general framework for improving the performance of classifiers using XAI methods",
  "modified_abstract": "In the quest to demystify the black-box nature of modern Artificial Intelligence (AI) models, this paper takes inspiration from a burgeoning field of research focused on understanding model mistakes through the lens of eXplainable Artificial Intelligence (XAI). Particularly, reflecting on insights from studies like ImageNet-X, which unearths consistent failure modes across diverse models by annotating factors of variation such as pose, background, or lighting, our work proposes a novel framework for leveraging XAI methods to enhance the performance of Deep Learning (DL) classifiers in the vision domain. This enhancement is primarily achieved through meticulous annotations and validation processes outlined within our proposed architecture, including data augmentation techniques that introduce variability in a controlled manner. By bypassing the computational burden traditionally associated with retraining, we introduce two distinct learning strategies within this architecture\u2014auto-encoder-based and encoder-decoder-based approaches\u2014and delve into their pivotal characteristics in the context of vision. These strategies, alongside a random sampling method for collecting a diverse dataset, are benchmarked against conventional image classification tasks, underscoring our framework's utility in improving classifier robustness and accuracy. The genesis of this framework is to not only elucidate the internal mechanisms of AI systems but to also use these insights as a springboard for automating performance improvements in a more resource-efficient manner.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Mark_Ibrahim1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10371v1",
  "title": "An Energy-Efficient Ensemble Approach for Mitigating Data Incompleteness in IoT Applications",
  "modified_abstract": "As Machine Learning (ML) becomes integral to IoT-based applications, facing the dynamic, ad-hoc challenges of IoT ecosystems, including data incompleteness due to missing sensor readings caused by sensor failures and/or network disruptions, is critical. This endeavor is inspired by a lineage of work that has navigated the complexities of ML model robustness, specifically those exploring ensemble methods for improved prediction and uncertainty estimation in the face of varying signal quality and completeness, as evidenced by prior investigations into the efficacies of incorporating prior functions and bootstrapping in ensemble learning. Recognizing the power constraints inherent in most IoT systems, our study introduces ENAMLE, a proactive, energy-aware technique designed to mitigate the impact of data incompleteness by leveraging an ensemble of models tailored on strategically selected subsets of sensors. This innovative design focuses on balancing energy consumption with accuracy during the training phase, incorporating extensive bootstrapping techniques to adaptively adjust the ensemble size in response to the degree of missing data and prevailing energy-accuracy trade-offs. Through extensive experiments across diverse datasets and rigorous training regimens, we demonstrate ENAMLE's capacity to enhance energy efficiency while adeptly managing sensor failure scenarios, making a significant step forward in developing ML systems for IoT that are both robust and energy-efficient.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Vikranth_Dwaracherla1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10368v1",
  "title": "Conformal Predictions for Probabilistically Robust Scalable Machine Learning Classification",
  "modified_abstract": "Our work is inspired by the pressing need for probabilistically robust machine learning (ML) models capable of operating under realistic constraints, paralleling efforts in adversarial attack detection where robustness to unpredictable threats remains a paramount goal. Conformal predictions offer a methodology to ensure that learning algorithms are reliable and robust to adversarial manipulations, yet there is room for advancement in defining frameworks that from their inception are tailored for robust classification tasks. This paper bridges the gap by enriching the scalable classifier concept through the integration of statistical order theory, probabilistic learning theory, and projections of data in neural network spaces, thus generalizing classical classifiers with an emphasis on adversarial robustness. We introduce a novel score function based on softmax probabilities and the conformal safety set, a tailored set of input variables that, acting as a detector, can reliably predict the probability of incorrect labeling with an $\\varepsilon$ error margin. Through a cybersecurity application focusing on DNS tunneling attack identification, our work illustrates the practicality of this advanced framework, underscoring its potential in networks delivering probabilistically robust and reliable ML models.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Pablo_Piantanida2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10365v1",
  "title": "Scalable Algorithms for Individual Preference Stable Clustering",
  "modified_abstract": "Our work advances the field of clustering by focusing on individual preference (IP) stability, a concept that merges principles of individual fairness and stability, drawing inspiration from preceding studies like those exploring the online k-median problem with an emphasis on cluster consistency and competitive analysis. We delve into IP stable clustering, operationalized as a clustering being $\\alpha$-IP stable if each data point's average distance to its cluster is at most $\\alpha$ times its average distance to any other cluster, thus providing competitive bounds on individual distances. By leveraging a natural local search algorithm for achieving IP stable clustering, our analysis establishes a $O(\\log n)$-IP stability guarantee, with $n$ representing the number of input points. Furthermore, we optimize the local search technique to achieve scalability, demonstrating it operates in almost linear time, $\\tilde{O}(nk)$, with $k$ denoting the desired number of clusters, thereby offering a practical and efficient solution for IP stable clustering that respects bounds on computational resources. This technique not only fosters better understanding of cluster structures but also yields competitive solutions in terms of operational efficiency and fairness. The inclusion of online considerations and k-median considerations further strengthens the relevance of our approach in real-world applications where data arrives sequentially and decisions must be made dynamically.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Benjamin_Moseley1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10348v1",
  "title": "Denoising Task Difficulty-based Curriculum for Training Diffusion Models",
  "modified_abstract": "In the evolving landscape of generative modeling, diffusion-based approaches have carved out a unique space for their capability to generate high-quality images across a spectrum of complexity. Notably, our research is inspired by advancements in memory models, particularly in how episodic and semantic memories are integrated to enhance learning processes in complex systems, as evidenced by recent breakthroughs in models that optimize memory allocation for better performance in generative tasks. Addressing the ongoing debate on the relative difficulty of denoising tasks across different timesteps, we conduct a methodical investigation into the task difficulties associated with denoising at various timesteps and noise levels. Our analysis, focusing on convergence behavior and shifts in the relative entropy between sequential probability distributions, unveils that denoising at lower timesteps is inherently more challenging due to slower convergence and elevated relative entropy. Leveraging these insights, we pioneer an easy-to-hard learning scheme predicated on curriculum learning principles, significantly deviating from traditional methods by sequentially ordering the timesteps or noise levels from less to more challenging. This structured training approach not only enhances model performance and expedites convergence but also stands in complementary relation to existing state-of-the-art advancements in diffusion model training techniques. Our empirical validation, spanning unconditional, class-conditional, and text-to-image generation tasks, substantiates the practical benefits of our curriculum learning strategy, marking a notable advancement in the training of diffusion-based generative models.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jason_Ramapuram1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10339v1",
  "title": "Generation is better than Modification: Combating High Class Homophily Variance in Graph Anomaly Detection",
  "modified_abstract": "Graph-based anomaly detection stands at the forefront of graph neural network (GNN) research, inspired by pioneering works that have investigated the dynamics of graph structures, including those focusing on structural fairness and the inherent bias within graph convolutional networks (GCNs). Recognizing that the differences in homophily distribution across classes in graph anomaly detection significantly surpass those observed in both homophilic and heterophilic graphs, we, for the first time, put forward a metric dubbed Class Homophily Variance to quantitatively delineate this disparity. Our solution, the Homophily Edge Generation Graph Neural Network (HedGe), diverges from traditional methodologies\u2014which typically hinge on modifying existing relationships within the graph\u2014by advocating for the generation of new connections that exhibit low class homophily variance, leveraging original relationships as supplementary. This augmentation strategy, coupled with our learning process that innovatively samples homophily adjacency matrices from scratch via a self-attention mechanism and prioritizes nodes that, while feature-relevant, remain disconnected in the original graph, enhances graph analysis by fostering intra-community connectedness and fairness. A revamped convolutional loss function further refines the model by penalizing the generation of unnecessary heterophilic edges, bolstering model efficacy through learning optimizations. Our empirical, holistic evaluation across several benchmark datasets highlights HedGe's superior performance in anomaly detection and edgeless node classification, alongside increased robustness against the novel Heterophily Attack by reducing class homophily variance in additional graph classification tasks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Chuan_Shi1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10332v1",
  "title": "GreedyML: A Parallel Algorithm for Maximizing Submodular Functions",
  "modified_abstract": "In the realm of optimization, our work introduces GreedyML, a parallel approximation algorithm for maximizing monotone submodular functions with hereditary constraints, crafted to address the computational challenges posed by large-scale data sets in fields such as data summarization, machine learning, and graph sparsification. Drawing inspiration from the groundbreaking approaches in large-scale optimization like the CS-TSSOS\u2014which leverages correlative and term sparsity in sparse polynomial optimization to achieve scalability and efficiency\u2014our algorithm builds upon the foundation laid by the randomized distributed RandGreedI algorithm, proposed by Barbosa, Ene, Nguyen, and Ward (2015). This innovative approach introduces multiple accumulation steps to mitigate memory constraints and computational bottlenecks inherent to processing large instances. Our comprehensive analysis reveals the strengths of GreedyML in terms of its approximation ratio and time complexity within the BSP computational model, offering insight into its programming efficiency. Extensive evaluations on three distinct problem classes, including industrial applications, and on massive data sets with millions of elements, demonstrate that GreedyML effectively overcomes the limitations of sequential Greedy and distributed RandGreedI algorithms, particularly in memory-constrained environments and for computationally intensive programming tasks. Moreover, our work introduces a novel application of the moment-SOS (sum of squares) hierarchy, which, alongside the semidefinite programming relaxation and sparse matrix techniques, enhances the approximation quality achieved with GreedyML and confirms its efficacy for large-scale submodular function maximization.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Victor_Magron1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10330v1",
  "title": "Towards Non-Adversarial Algorithmic Recourse",
  "modified_abstract": "In the intersection of machine learning (ML) research focusing on adversarial examples and counterfactual explanations, our work is distinctly inspired by the examination of fairness, as it pertains to reducing bias within the domain of fraud detection, and the broader implications this has on high-stakes decision-making in society. Building upon the understanding that adversarial examples and counterfactual explanations operate under different paradigms\u2014the former leading to misclassifications and the latter aimed at providing constructive feedback\u2014we introduce the concept of non-adversarial algorithmic recourse. This concept is pivotal for delineating counterfactual explanations that do not inadvertently mimic adversarial behavior, particularly in scenarios that demand high accountability and specific predictions to ensure fair treatment. By rigorously defining adversarial examples and counterfactual explanations, and analyzing group-wise data, our study elucidates the criteria that precipitate an adversarial outcome versus a genuine recourse. Our experiments, employing common datasets in the context of fraud detection services, navigate through the intricate design choices of objective functions, ML models, and cost functions to infer the nature of the generated recourse, emphasizing that the characterization of whether a recourse is adversarially tinted is more contingent on these design choices than on the dichotomy between recourse and attack algorithms. The findings underscore the significance of selecting robust and precise ML models to foster the generation of non-adversarial, practical recourse that promotes fairness, thereby extending the discourse on ensuring fairness, robustness, and trustworthiness in algorithmic decision-making.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Andr\u00e9_Cruz1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10326v1",
  "title": "CDGP: Automatic Cloze Distractor Generation based on Pre-trained Language Model",
  "modified_abstract": "Drawing inspiration from the advancements in pre-trained language models (PLMs) and their proven efficacy in cross-lingual transferability and zero-shot learning capabilities, this paper introduces an innovative approach, CDGP, for automatic cloze distractor generation. The conventional process of manually designing cloze tests is not only time-consuming but also fraught with challenges, particularly in the selection of plausible wrong options (distractors) that enhance the assessment's effectiveness. By leveraging PLMs, our study explores a novel methodology for automating the generation of candidate distractors through adaptation, thus addressing a major hurdle in creating effective learning assessments. Experimental results demonstrate that our PLM-enhanced model, through further adaptation and pretraining, significantly outperforms existing methods, propelling the state-of-the-art NDCG@10 score from 14.94 to 34.17 across multiple languages. Our findings suggest that integrating PLMs into the distractor generation process, with a particular focus on learning efficiency and multilingual capabilities, can greatly improve the accuracy and efficiency of cloze tests. The code and dataset used in our experiments are made available at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Katharina_Kann1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10318v1",
  "title": "Anytime Neural Architecture Search on Tabular Data",
  "modified_abstract": "This study emerges from the critical need to adapt effortlessly and efficiently to the increasing demand for tabular data analysis, drawing inspiration from pioneering efforts in leveraging machine learning for adaptive solutions in dynamically changing environments, as evidenced by efforts in creating reinforcement learning (RL) agents capable of human-timescale adaptation in open-ended task domains. Recognizing the significant leap forward with RL in learning and adapting to vast, varied task distributions in complex domains, our work pioneers the introduction of ATLAS, the first anytime Neural Architecture Search (NAS) approach devised explicitly for tabular data. ATLAS propels this adaptation into the realm of NAS by incorporating a cutting-edge two-phase filtering-and-refinement optimization scheme, which integrates training-free and training-based architecture evaluation paradigms. Through a novel zero-cost proxy tailored for tabular data and a fixed-budget search algorithm, along with attention-based mechanisms and memory-enhanced techniques for efficient architecture selection, ATLAS efficiently narrows down and refines promising candidate architectures. With the strategic deployment of a budget-aware coordinator, who acts as an agent aware of domain-specific requirements, it delivers high NAS performance adaptable to in-context changes and any time constraints through dynamic adaptation. Our experimental validation showcases ATLAS's capability to drastically cut down search time by up to 82.75x against existing methodologies, achieving outstanding NAS results tailored to tabular data within any allocated time budget and improving outcomes with budget extensions. Additionally, the consideration of user demonstrations for enhancing memory and search strategies represents a future direction to incorporate embodied perspectives for better NAS outcomes.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jack_Parker-Holder1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10288v1",
  "title": "Rough Transformers for Continuous and Efficient Time-Series Modelling",
  "modified_abstract": "Motivated by the computational challenges witnessed in both Neural ODE-based models and Transformer-based architectures when dealing with long-range dependencies in irregularly sampled time-series data\u2014especially in real-world medical settings\u2014our study introduces the Rough Transformer. This model is a novel adaptation that marries the continuous-time representations of Neural ODEs with the efficiency and long-range dependency handling of Transformer models, directly inspired by recent breakthroughs in machine learning for processing non-uniformly sampled data, such as the self-supervised perceptual prediction model for temporal event detection and segmentation in videos. By leveraging advanced segmentation techniques and deep learning training methodologies, the Rough Transformer significantly lowers computational demands through our proposed multi-view signature attention mechanism that augments traditional attention with path signatures, thus capturing both local and global dependencies in the data while adapting seamlessly to variations in sequence length and sampling frequency. Our findings show that Rough Transformers not only outperform traditional models on both synthetic and real-world datasets but also do so with a fraction of the computational cost, showcasing immense potential for applications where real-time data processing, efficiency, and learning from self-supervised task labels in video content are paramount.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ramy_Mounir1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10581v1",
  "title": "Large Language Model-informed ECG Dual Attention Network for Heart Failure Risk Prediction",
  "modified_abstract": "This work is situated at the convergence of advanced predictive models and heart failure (HF) diagnosis, drawing upon historical advances in machine learning efficiency and interpretability to address HF's growing global health burden. Our methodology employs a dual-attention ECG network, leveraging both local temporal dynamics and cross-lead interactions within 12-lead electrocardiograms (ECGs) for early HF prediction. The unique integration of a large language model (LLM) for pretraining with ECG-related data underpins our approach, an inspiration drawn from the machine learning field's exploration of model efficiency through sparse-to-sparse techniques and predictive performance enhancements, such as those achieved through dynamic sparse training, averaging, and ensemble strategies. By tailoring these concepts to heart failure risk prediction within a unified framework, and incorporating literature-informed training processes, our network demonstrates significant advancements by capturing complex ECG features, despite challenges such as data imbalance between risk groups. Furthermore, employing LLM-informed pretraining, sparse techniques, and a novel dual-attention mechanism within subnetworks and ensembles, we not only show notable improvements in predictive performance on specific cohorts from the UK Biobank (UKB-HYP and UKB-MI) but also ensure model interpretability and reliability. Our ensemble model outshines existing models, confirming the potential of LLM-enhanced approaches and ensemble methodologies in clinical risk assessment and advancing heart failure diagnostic methodologies. This pioneering work exemplifies the impactful integration of LLMs and attention mechanisms bolstered by ensemble approaches in medical research.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zahra_Atashgahi1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10281v1",
  "title": "Team Trifecta at Factify5WQA: Setting the Standard in Fact Verification with Fine-Tuning",
  "modified_abstract": "Inspired by innovative advancements in the field of Natural Language Processing (NLP), such as the introduction of the MATINF dataset which has significantly propelled research in classification, question answering, summarization, and the development of relevant datasets, our work presents Pre-CoFactv3, a novel fact verification framework. This framework integrates Question Answering and Text Classification components, employing In-Context Learning, Fine-tuned Large Language Models (LLMs), and our proprietary FakeNet model to address the inherent challenges of fact verification. Through comprehensive experiments that explore various pre-trained LLMs, introduce the FakeNet component, designed for multi-task objectives including answering and summarization, and examine the effectiveness of different ensemble methods for both question answering and summarization tasks, our team, Trifecta, achieved first place in the AAAI-24 Factify 3.0 Workshop. Our framework surpassed the baseline accuracy by an unprecedented 103% and maintained a notable 70% lead over the second competitor, demonstrating the robustness and effectiveness of our approach in advancing the realm of fact verification research. A significant portion of our training data was enriched with human-labeled datasets to enhance the model\u2019s understanding and performance in real-world scenarios.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Canwen_Xu1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10266v1",
  "title": "DSP: Dynamic Sequence Parallelism for Multi-Dimensional Transformers",
  "modified_abstract": "Inspired by the continuing evolution of deep learning architectures, particularly the pursuit of efficiency in video recognition through dynamic and adaptive methods in 3D convolution networks, this paper proposes Dynamic Sequence Parallelism (DSP), a groundbreaking method to address the challenge of scaling large models with long sequences in diverse applications such as language generation, video generation, and multimodal tasks. Unlike existing sequence parallelism methods that are limited by a unidimensional approach, DSP introduces an innovative strategy for multi-dimensional transformer models by dynamically switching the parallelism dimension corresponding to the current computation stage. This adaptation not only leverages the inherent characteristics of multi-dimensional attention mechanisms but also significantly reduces communication overhead, overcoming the limitations of applying traditional single-dimension parallelism techniques to multi-dimensional contexts depth-wisely. Through rigorous experiments and evolutionary training processes, DSP demonstrates remarkable improvements in end-to-end throughput, outperforming prior methods by 42.0% to 216.8% and underscoring its effectiveness in enhancing the efficiency of multi-dimensional transformers in processing complex sequences. Moreover, the growing demand for processing high-quality 3D content in video search engines further validates the need for DSP's adaptable and efficient mechanisms. Code is available at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yichen_Qian1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10259v1",
  "title": "Comprehensive Study Of Predictive Maintenance In Industries Using Classification Models And LSTM Model",
  "modified_abstract": "In today's technology-driven era, the imperative for advanced predictive maintenance across various industries is informed by a rich lineage of breakthroughs in machine learning and deep learning, such as the development of architectures designed to mitigate the vanishing gradient problem like LSTMs and their applications in creating efficient, robust models including novel parametrizations. Our proposed study builds on these advancements, specifically the insightful implementations of \"Passthrough Networks\", to delve into various machine learning classification techniques, including Support Vector Machine (SVM), Random Forest, Logistic Regression, and a focused exploration of an LSTM-based Convolutional Neural Network model, for predicting and analyzing machine performance. SVM classifies data into categories based on their multidimensional space position, while Random Forest employs ensemble learning for classification. Logistic Regression is used to predict the probability of binary outcomes. Our study aims to assess these algorithms' performance in predicting and analyzing machine performance, considering metrics such as accuracy, precision, recall, and F1 score. The findings aim to guide maintenance experts in selecting the most suitable machine learning algorithm for effective and efficient prediction and analysis of machine performance, thereby extending machine lifespan, reducing maintenance costs, and preventing potential accidents or catastrophic events. This research not only emphasizes the critical importance of we embracing the latest computational technologies but also the innovative use of low-rank approximation techniques in enhancing model efficiency.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Antonio_Valerio_Miceli_Barone1",
  "manipulated_ranking": 5,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10253v1",
  "title": "Open Continual Feature Selection via Granular-Ball Knowledge Transfer",
  "modified_abstract": "Drawing inspiration from the advancements in neural network-based feature selection methodologies that have demonstrated effectiveness in handling high-dimensional data and noisy environments, this paper introduces a groundbreaking framework for continual feature selection (CFS) aimed at addressing the challenges posed by dynamic and open environments where new, unknown classes may emerge. Our novel CFS approach integrates the principles of continual learning (CL) with granular-ball computing (GBC) to construct a granular-ball knowledge base for effective detection of unknown classes and seamless transfer of learned knowledge for further feature refinement into a sparse representation space. The framework is divided into initial learning and open learning phases, with the former establishing a foundational knowledge base using granular-balls for multi-granularity representation, and the latter leveraging existing granular-ball insights for adaptation and discovery and assimilation of unknown classes, updating the knowledge base for knowledge transfer, and enhancing feature selection. An optimized feature subset mechanism further refines this process by selectively integrating minimal new features into the pre-existing optimal subset, resulting in a method that surpasses existing state-of-the-art feature selection techniques in both efficacy and efficiency, as validated by comprehensive experiments conducted on various public benchmark datasets. The application of autoencoders in this process offers an additional layer of adaptability and efficiency in managing vast amounts of data, making it particularly suitable for image-related tasks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zahra_Atashgahi1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10250v1",
  "title": "Interpretable Machine Learning for Survival Analysis",
  "modified_abstract": "In the evolving landscape of machine learning (ML), where black box models have proliferated across varied task domains, the imperative for interpretable or explainable ML (IML/XAI) frameworks has become paramount, especially in fields where decisions have profound implications, such as healthcare and survival analysis. Drawing inspiration from advancements in detecting and localizing changes in conditional distributions, our work underscores the urgency and relevance of IML techniques in survival analysis to ensure transparency, accountability, and fairness in clinical and public health decision-making processes. We provide a comprehensive review of the sparse IML methods specifically tuned for survival analysis and extend conventional IML techniques\u2014such as individual conditional expectation (ICE), partial dependence plots (PDP), accumulated local effects (ALE), various feature importance measures, paired with Friedman's H-interaction statistics\u2014to address survival outcomes effectively, with an emphasis on how these models adapt when conditional distributions change. Utilizing a sample of real-world data on the under-5 year mortality task of Ghanaian children from the Demographic and Health Surveys (DHS) Program, this paper serves as a practical guide for researchers, offering detailed methodologies for applying IML techniques to survival analysis tasks, thereby enhancing the interpretability of predictive models and elucidating their decision-making processes in various applications. Our novel approach also introduces the integration of set theory concepts to further delineate the complexities of survival analysis in a more structured form.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Lizhen_Nie1",
  "manipulated_ranking": 4,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10232v1",
  "title": "Matrix Completion via Nonsmooth Regularization of Fully Connected Neural Networks",
  "modified_abstract": "In the field of machine learning, matrix completion has traditionally leveraged the assumption of low-rank matrices to approximate missing values, resulting in a linear approximation framework. However, recent advancements, including those showcasing the power of deep fully connected neural networks (FCNNs), akin to sparsified gradient methods that reduce communication during distributed training, have demonstrated the potential benefits of nonlinear estimators. Inspired by such developments, this paper introduces a novel approach to controlling overfitting in FCNNs by applying nonsmooth regularization, encompassing both the $\\ell_{1}$ norm of intermediate representations and the nuclear norm of weight matrices. Given that these regularization terms introduce a nonsmooth and nonconvex landscape, traditional gradient-based optimization methods falter. We propose a proximal gradient variant designed for these conditions, offering significant communication-reduction in massive data scenarios through efficient gradient computations, and analyze its convergence properties, crucial for the method's efficacy. Additionally, we introduce theoretical bounds to underpin our convergence analysis, reinforcing the robustness of the proposed method. Our methodology includes a phased integration of regularization, initially eschewing such constraints to focus on network training, and gradually increasing their influence, a strategy found essential for the enhanced performance of our deep neural network with nonsmooth regularization (DNN-NSR) algorithm. Comparative simulations reinforce the superiority of our approach over both linear and nonlinear matrix completion methods, underlining the benefits of integrating sparsification insights from distributed deep learning and data-parallel techniques into FCNN regularization.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sarit_Khirirat1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10231v1",
  "title": "Less is More: One-shot Subgraph Reasoning on Large-scale Knowledge Graphs",
  "modified_abstract": "Drawing inspiration from recent advancements in graph neural networks, particularly in efficient node selection and representation within large networks, this work introduces a novel approach to reasoning in large-scale knowledge graphs (KGs) termed one-shot-subgraph link prediction. Existing methodologies in link prediction are hindered by their reliance on the entire KG for deducing new facts, leading to scalability issues that cannot be effectively addressed through traditional sampling methods. By decoupling the prediction process into two steps\u2014(i) extracting a single, query-dependent subgraph, and (ii) performing link prediction within this constrained context\u2014we adopt a more computationally feasible strategy. The use of Personalized PageRank (PPR) as a non-parametric and computation-efficient heuristic is revealed to be effective in identifying potential answers and supporting evidence within this framework, highlighting its centrality in our approach. The task of one-shot-subgraph reasoning not only presents a technical challenge but also represents a significant paradigm shift from conventional methodologies for KGs, requiring novel training strategies and the integration of centrality measures. Furthermore, the proposed method enhances prediction efficiency and loss minimization through the automated discovery of optimal configurations in both data and model spaces, equipped with an encoder-decoder architecture for the embedding process, achieving leading performances across five large-scale benchmarks. This approach circumvents the limitations posed by small-scale KGs and scales effectively to large-scale applications. Code is made available at [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yuhui_Ding1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10220v1",
  "title": "From Chaos to Clarity: Time Series Anomaly Detection in Astronomical Observations",
  "modified_abstract": "In the realm of astronomical research, where the advent of large-scale time series data brings both opportunity and challenges, our work is inspired by groundbreaking methodologies that blend sophisticated architectures for the enhanced classification and depiction of astronomical phenomena. Specifically, building upon the foundation laid by innovative approaches such as Astroformer, which showcases the power of hybrid transformer-convolutional architectures in learning from limited data for classifying galaxy morphologies, we introduce AERO. AERO, with its state-of-the-art design, is a novel two-stage framework designed for unsupervised anomaly detection in astronomical observations of galaxies, a field that demands high precision due to the independent nature of stars juxtaposed with the interference of random concurrent noise. Our framework commences with a transformer-based encoder-decoder architecture leveraging self-attention mechanisms for discerning normal temporal patterns in variate-independent observations. In its second stage, AERO employs an advanced graph neural network, enhanced with window-wise graph structure learning and rigorous training protocols, to adeptly identify and reduce concurrent noise, therefore mitigating the prevalent issue of false alarms and addressing the challenge that this noise presents. Through exhaustive experiments and rigorous training utilizing both synthetic and real-world datasets, AERO has demonstrated superior performance over existing baselines, achieving significant improvements in F1-scores related to galaxy anomalies. This advancement not only speaks to the efficacy of AERO but also contributes to the overarching goal of refining anomaly detection in astronomical data, hence furthering our understanding of the cosmos.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Rishit_Dagli1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10202v1",
  "title": "Learning on JPEG-LDPC Compressed Images: Classifying with Syndromes",
  "modified_abstract": "Drawing inspiration from a breadth of prior works on deep learning and data compression, especially the innovative techniques for network compression and optimization such as Mutual Information-based Neuron Trimming (MINT), and pruning, our research investigates the feasibility of direct learning on compressed data. Specifically, we explore the use of entropic coding via Low-Density Parity Check (LDPC) codes in the context of goal-oriented communications, where the receiver's primary concern is to apply a Deep-Learning model, rather than to reconstruct the original data. Unlike conventional entropic coding methods like Huffman and Arithmetic coding that disrupt the data structure and obstruct learning without decoding, LDPC codes maintain an exploitable structure for Deep Learning models, including in adversarial scenarios. We employ Gated Recurrent Unit (GRU), a class of Recurrent Neural Networks (RNNs), with varying architectures, trained specifically for image classification on LDPC-coded bit-planes. Our findings reveal a significant improvement in classification performance from LDPC-coded data compared to Huffman and Arithmetic coding, with a notably smaller model size required, thereby proving the effectiveness of compression-via-pruning in classification without the need for any form of decompression prior to model application. This underlines not only the compression efficacy but also the strategic advantage of leveraging architecture-based innovations such as filter's strength in dealing with compressed data, setting new benchmarks in compression and model efficiency.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Madan_Ravi_Ganesh1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10190v1",
  "title": "Perceptual Quality-based Model Training under Annotator Label Uncertainty",
  "modified_abstract": "This work is inspired by a recognition of the pervasive challenges inherent in data labeling\u2014a process fundamental to training machine learning models, where annotator label uncertainty can significantly detract from training efficacy. Such challenges have been explored in contexts as varied as crowd counting with partial annotations in dense scenes, and innovative frameworks seek to optimize learning from limited data inputs, enhancing overall performance. In our research, we delve into the implications of annotator label uncertainty for model reliability using diverse datasets, including those with densely annotated instances. Annotator disagreement and the presence of low-quality noisy labels in image datasets are shown to degrade model generalizability and prediction uncertainty. The flow of information through the uncertainty estimation process is critical, yet we evaluate existing uncertainty estimation algorithms and find them lacking in handling the variability introduced by annotator label uncertainty effectively. To address these deficiencies without the need for massive annotation efforts, we propose a novel training framework centered on perceptual quality to more reliably generate multiple labels for training. This framework comprises a performance-driven module focusing on samples with statistically significant irregularities in visual signals for the generation of de-aggregated labels. By leveraging our memory-efficient approach, our method showcases an innovative pathway to bolster model reliability in the face of annotator label uncertainty. Our experimental findings underscore the efficacy of our framework in enhancing model generalizability and mitigating prediction uncertainty, marking a significant advancement in the pursuit of robust machine learning methodologies.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~ziming_zhong1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10187v1",
  "title": "Grasp Anything: Combining Teacher-Augmented Policy Gradient Learning with Instance Segmentation to Grasp Arbitrary Objects",
  "modified_abstract": "Drawing from the extensive body of work addressing the longstanding issue of interactive grasping and inspired by the recent advancements like TOCH, which refine 3D hand-object interactions through novel spatio-temporal representations, our research contributes to the field of robot learning by tackling one of its most challenging problems: interactive grasping from clutter with a level of dexterity akin to human capabilities. This complex problem involves navigating the intricacies of visual perception and precise motor skills, alongside their intricate interplay with interaction dynamics and the essential contact points between the robotic hands and objects. We introduce Teacher-Augmented Policy Gradient (TAPG), a novel two-stage learning framework that melds the strengths of reinforcement learning with policy distillation and manifold understanding. Starting with a teacher policy proficient in motor control based on object pose information and understanding object-manifold correspondences, TAPG pioneers an approach for the guided yet adaptive learning of a sensorimotor policy, leveraging object segmentation and employing denoising techniques to refine learning signals. We utilize the Segment Anything Model for promptable object segmentation, achieving zero-shot transfer from simulation to real-world robotic applications, enabling our trained policies to adeptly grasp a diverse array of objects from cluttered scenarios, both in simulation and reality, guided by human-understandable prompts and using trackers to enhance grasp success rates. Moreover, our approach demonstrates robust zero-shot transfer capabilities for novel objects, significantly denoising the learning process and ensuring effective contact without prior object knowledge. Additionally, the incorporation of reconstruction strategies further enhances the fidelity of object grasping. Videos showcasing our experimental results are accessible at [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Bharat_Lal_Bhatnagar1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10182v1",
  "title": "Reliable uncertainty with cheaper neural network ensembles: a case study in industrial parts classification",
  "modified_abstract": "Inspired by foundational research efforts such as ModelPred, which delves into the intricate dynamics between training data alterations and resultant model modifications, this study, set in the realm of operations research (OR), embarks on tackling the critical challenge of reliable uncertainty quantification in neural network (NN) applications. While NNs have demonstrated exceptional performance in a variety of fields, including image classification, they often falter by producing overconfident yet incorrect predictions in out-of-distribution (OOD) scenarios - a gap that uncertainty estimation meticulously addresses through regularization techniques. Our investigation is pioneering in conducting a comprehensive comparison among a conventional single NN, a deep ensemble, and three innovative efficient NN ensemble methods\u2014namely snapshot, batch, and multi-input multi-output ensembles\u2014in search of cost-effective, yet reliable alternatives. Furthermore, we introduce the novel Diversity Quality metric to assess ensemble performance across both in-distribution and OOD data within a unified framework. Applied to the practical context of industrial parts classification\u2014an endeavour critical for the maintenance efficiency of industrial plants\u2014our findings distinctively mark the batch ensemble as a standout; it not only demonstrates superior performance in uncertainty estimation and predictive accuracy but also underscores remarkable computational efficiency and memory savings compared to the deep ensemble. The dataset used for training and the interpretability of the models' predictions enhance the application's reliability in real-world scenarios.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Hoang_Anh_Just1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10175v1",
  "title": "A Short Survey on Importance Weighting for Machine Learning",
  "modified_abstract": "Building upon foundational research that spans multiple domains of machine learning and statistics, this short survey seeks to encapsulate the pivotal role and broad applications of importance weighting\u2014a fundamental statistical technique for adjusting the objective function or probability distribution based on the instance's importance. The survey contextualizes its significance through the lens of addressing distribution shift in supervised learning, a critical challenge where the training and test distributions differ. Notably, by highlighting the connection to recent breakthroughs, such as the theoretical advancements in ridge-regularized empirical risk minimization within high-dimensional settings, our review underscores importance weighting's utility across various research fronts and its potential to confer statistically desirable properties through density ratio-based adjustments. Thus, we aim to synthesize these perspectives to offer a comprehensive overview of importance weighting\u2019s contribution to ensuring the robustness and efficacy of machine learning methodologies.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Hossein_Taheri1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10168v1",
  "title": "Explainability through uncertainty: Trustworthy decision-making with neural networks",
  "modified_abstract": "This work is positioned at the intersection of uncertainty estimation and the emerging field of Explainable Artificial Intelligence (XAI), inspired by the critical need for interpretable and safe machine learning models as evidenced in recent literature. Such a need has become particularly pressing with the development of models and methods aiming to quantitatively link interpretability with the safety and reliability, including tighter bounds, of ML applications, for instance, in understanding the maximum deviation of predictions from a safe baseline. Uncertainty, a ubiquitous aspect of machine learning models, and particularly neural networks' tendency towards overconfidence, poses a significant challenge, especially under distribution shifts where model performance degrades unknowingly. This treatise introduces a general framework for uncertainty estimation that serves not only as a methodology for making neural networks more interpretable but also enhances their trustworthiness in decision-making scenarios, crucial for operations research and application areas such as educational data mining and potentially in high-stakes decisions like mortgage approval. Contributions are manifold: (i) we establish uncertainty estimation as a potent XAI technique offering both local and model-wide explanations; (ii) we advocate for a classification with a rejection option that leverages human expertise on uncertain predictions to avert misclassifications; (iii) we demonstrate the utility of our framework through a case study in educational data mining affected by distribution shifts, applying optimization functions and potentially leveraging ensembles and tree-based models for more robust decision-making. By grounding uncertainty as a facet of XAI, we pave the way for more accountable, actionable, and robust ML systems.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Kush_R._Varshney1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10164v1",
  "title": "CoReEcho: Continuous Representation Learning for 2D+time Echocardiography Analysis",
  "modified_abstract": "Building upon previous advancements in deep learning (DL) that have significantly enhanced automatic medical image analysis across various modalities, such as the implementation of physics-embedded neural networks for solving complex boundary value problems using sophisticated solvers, our work introduces CoReEcho. This novel training framework is specifically designed to address the less explainable nature of representations learned through the conventional end-to-end training pipelines in echocardiography analysis. The gap in capturing the continuous relation among echocardiogram clips, potentially leading to spurious correlations and limited generalization, is our focus. By emphasizing continuous representation learning tailored for direct ejection fraction (EF) regression and the prediction of heart shapes and functions, CoReEcho not only demonstrates superior performance with a Mean Absolute Error (MAE) of 3.90 and a R-Squared (R2) of 82.44 on the EchoNet-Dynamic, the largest echocardiography dataset, but also ensures that the learned features are complex, robust, and generalizable across related downstream tasks, partially due to the effective deployment of neural solvers and prediction mechanisms. For facilitating further research and replication, the code is publicly available at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Masanobu_Horie1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10160v1",
  "title": "Online Policy Learning from Offline Preferences",
  "modified_abstract": "Building upon the foundation set by prior works in offline reinforcement learning (RL) that deals with the unique challenges of learning from heterogeneously sourced data, this study ventures into the domain of preference-based reinforcement learning (PbRL) with an innovative approach. It focuses on leveraging both offline preferences\u2014human feedback collected for offline data\u2014and introduces the novel concept of virtual preferences. These virtual preferences are created by comparing the agent's current behaviors against those in the offline data through a process akin to sampling, aiming to approximation the ideal decision-making process. The motivation for this bridging is to mitigate the generalizability issues that arise when the learned reward function, based on offline preferences, encounters behaviors absent in the offline data. This framework ensures that the reward function remains closely aligned with evolving agent behaviors, perturbed by novel situations, and addresses a critical problem in PbRL. Through empirical evaluation on continuous control tasks, the effectiveness of integrating virtual preferences into PbRL is demonstrated, marking an advancement in the field and suggesting avenues for further research in this area. The study opens up new decompositions of the problem at hand, ensuring a more robust approach to policy learning.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Chengshuai_Shi1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10158v1",
  "title": "Functional Graph Convolutional Networks: A unified multi-task and multi-modal learning framework to facilitate health and social-care insights",
  "modified_abstract": "Drawing inspiration from the foundational challenges highlighted in Graph Neural Network (GNN) research, particularly the bottleneck effect in information propagation, this paper introduces the novel Functional Graph Convolutional Network (funGCN) framework. This integration of Functional Data Analysis with Graph Convolutional Networks innovatively addresses the complexities of multi-task and multi-modal learning in the domains of digital health and longitudinal studies by breaking the barriers of conventional analysis. FunGCN advances beyond traditional approaches by offering a unified framework adept at handling multivariate longitudinal data across multiple entities, ensuring interpretability even with restricted sample sizes, and overcoming training challenges associated with model overfitting and underfitting. Key innovations include the introduction of task-specific embedding components for diverse data modalities and capabilities spanning classification, regression, and forecasting by incorporating a prior knowledge graph that enhances data interpretation, facilitating a more informed decision-making process. Furthermore, by accommodating the demands of growing datasets, funGCN sets a new standard in scalability and flexibility. The effectiveness of the funGCN framework is substantiated through simulations and real-data application, underscoring its potential to enrich health care and social support systems by facilitating comprehensive health and well-being insights. Code is available at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Eran_Yahav1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10153v1",
  "title": "Improving Medical Multi-modal Contrastive Learning with Expert Annotations",
  "modified_abstract": "Our study, eCLIP, builds on the foundation of contrastive multi-modal learning, drawing inspiration from pioneering works such as those that developed large-scale, multi-modal datasets by interleaving images and text to train vision and language models. Addressing specific challenges in the medical imaging domain, eCLIP incorporates expert annotations in the form of radiologist eye-gaze heatmaps to mitigate issues of data scarcity and the \"modality gap\"-- a significant disparity between image and text embeddings. By integrating a heatmap processor and employing mixup and other forms of publicly sourced augmentation, eCLIP enhances the learning effectiveness of the CLIP model architecture without necessitating core modifications. It selectively demonstrates consistent improvements across various tasks such as zero-shot inference, cross-modal retrieval, and the generation of radiology reports, by leveraging a frozen Large Language Model and effectively utilizing tokens and sequences within its language processing components. This enhancement in embedding quality, revealed through detailed evaluations, underscores eCLIP's ability to effectively utilize high-quality annotations and images for an enriched multi-modal analysis, catering to the nuanced requirements of medical imaging. Ultimately, our approach represents a significant release of potential for multi-modal learning systems in healthcare, highlighting the importance of leveraging expert annotations and publicly available resources.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jack_Hessel1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10144v1",
  "title": "NLP Verification: Towards a General Methodology for Certifying Robustness",
  "modified_abstract": "Building upon the rich history of advancements in Natural Language Processing (NLP), which have revolutionized tasks from paraphrasing to abductive reasoning through innovative approaches like unsupervised paraphrasing and the Reflective Decoding method, our study seeks to consolidate the fragmented landscape of NLP verification. This research is motivated by the substantial success that deep neural networks, especially those that are pretrained, have exhibited in NLP and aims to address the crucial need for their safety and reliability in safety-critical contexts. We propose a general verification methodology that encompasses emerging challenges and advancements in the field, including the generation of more intuitive validation processes under varied levels of supervision and parallel processing techniques for efficiency. Our contributions are twofold: firstly, we offer a general characterization of verifiable subspaces resulting from embedding sentences into continuous spaces, tackling the issue of semantic generalizability and enhancing the reflections on how sentences are understood through generative models; secondly, we analyze the embedding gap's effects, proposing practical NLP methods for generation and introducing the metric of falsifiability of semantic subspaces. Together, these contributions propose a foundation for a unified, robust NLP verification methodology, pushing forward the development of this critical domain.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Peter_West1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10578v1",
  "title": "Generative Modelling of Stochastic Rotating Shallow Water Noise",
  "modified_abstract": "Building on the notion that the complex dynamics within fluid systems can be better understood through stochastic modeling, as exemplified in our exploration of the scale mixtures of Neural Network Gaussian Processes (NNGP) for enhancing our understanding of neural networks as stochastic processes, this paper extends the methodology to the domain of environmental sciences, particularly in the calibration of noise in fluid dynamics' stochastic partial differential equations. This innovation is crucial for the accurate estimation of uncertainty in weather and climate predictions, which necessitates the stochastic parameterization of sub-grid scale processes. Departing from the previously employed principal component analysis (PCA) technique, which was predicated on the assumption that the increments of the stochastic parameterization are normally distributed, we introduce a generative model approach. This mixture model technique enables the avoidance of imposing normal distribution constraints on the increments, thereby yielding a more flexible and accurate representation of the underlying processes. By applying this methodology to a stochastic rotating shallow water model and focusing on the model's elevation variable as the input data, our numerical simulations uncover the non-Gaussian nature of the noise. The results, characterized by favorable RMSE, CRPS scores, and forecast rank histograms, underscore the generative model's efficacy and its potential for broader application in modeling environmental systems beyond traditional tasks or classification, with implications for improving posterior-inference in environmental forecasting.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~EungGu_Yun1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10123v1",
  "title": "Regularization-Based Efficient Continual Learning in Deep State-Space Models",
  "modified_abstract": "In the context of enhancing the adaptability and efficiency of deep state-space models (DSSMs) for dynamic systems, our work is spurred by the challenge of preventing catastrophic forgetting in these models\u2014a challenge not unfamiliar in the realms of reinforcement learning, particularly in safety-critical applications where efficient and safe policy optimization, exploration, and reward/safety trade-offs are paramount. Despite the prolific adoption of DSSMs due to their robust modeling capabilities, the field has traditionally focused on single-task learning scenarios, leaving the potential for multi-task adaptability largely untapped. Addressing this, we introduce continual learning DSSMs (CLDSSMs) that seamlessly adapt to new tasks within the same or different dynamic systems without the need for retraining on historical data, thus overcoming the issue of catastrophic forgetting. By integrating state-of-the-art regularization-based continual learning methods and leveraging approximations for optimization, CLDSSMs achieve efficient updates with fixed computational and memory overheads, enabling them to handle multiple dynamic systems concurrently in a safer manner with more explicit safety guarantees. Our extensive evaluation on real-world datasets showcases the superiority of our proposed models over traditional DSSMs, highlighting their ability to quickly adapt to new tasks while retaining previously learned information in safety-critical systems, thereby marking a significant step forward in the pursuit of efficient and versatile continual learning frameworks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yunyue_Wei1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10110v1",
  "title": "Meta Operator for Complex Query Answering on Knowledge Graphs",
  "modified_abstract": "Inspired by recent advances in learning representations of subgraphs under the challenge of partial observation, our research introduces a novel perspective on learning-based Complex Query Answering (CQA) on knowledge graphs, which are inherently incomplete. Recognizing the inherent complexity in directly traversing these incomplete knowledge graphs for answering complex queries, prior work has largely focused on multi-task learning approaches that necessitate extensive training samples and thorough observation of the node-structures within. Our work diverges by investigating the compositional structure of complex queries, positing that focusing on the different types of logical operators\u2014rather than the diverse types of complex queries\u2014holds the key to enhancing model generalizability. To this end, we propose an innovative meta-learning algorithm specifically designed to learn meta-operators from a limited dataset, operating at a subgraph-level, and adeptly adapt them across varying instances of operators encountered in complex queries. Our empirical findings, evaluated through rigorous evaluation protocols, demonstrate the superior efficacy of learning meta-operators over conventional methods, including standard CQA models, in addressing the challenge of incomplete knowledge in knowledge graphs and related tasks, with observable improvements in task performance. The successful conduct of this research marks a significant step forward in the field.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Dongkwan_Kim1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10105v1",
  "title": "Belief Aided Navigation using Bayesian Reinforcement Learning for Avoiding Humans in Blind Spots",
  "modified_abstract": "This research advances the emerging field of socially aware mobile robot navigation through the development of the BNBRL+ algorithm. Inspired by cutting-edge methods such as the MultiXNet model, which significantly enhances the understanding and prediction of the surroundings in self-driving vehicles by leveraging lidar sensor data for motion prediction, our work similarly acknowledges the complexity of interacting with dynamic environments, particularly with humans. The BNBRL+ algorithm, grounded in the partially observable Markov decision process framework, introduces an innovative approach to navigating crowded spaces by leveraging belief algorithms combined with Bayesian neural networks. This allows for probabilistic inference based on the positional data of humans, thereby addressing challenges related to occlusions and blind spots. By incorporating social norms within its reward function, BNBRL+ not only navigates effectively through spaces with limited visibility but also dynamically avoids human obstacles, jointly improving the safety and functionality of autonomous navigational systems in complex environments. The algorithm's efficacy in multimodal predicting human movement and traffic scenarios positions it at the state-of-the-art in wearable and mobile navigation aids, particularly in aspects where traditional methods fall short.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Henggang_Cui1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10097v1",
  "title": "Adaptive Random Feature Regularization on Fine-tuning Deep Neural Networks",
  "modified_abstract": "This paper introduces adaptive random feature regularization (AdaRand), a novel method inscribed in our ongoing effort to improve deep neural network fine-tuning performance without falling into the pitfalls of overfitting, especially when working with small target datasets. Drawing inspiration from a suite of previous works, particularly those focusing on the calibration of both classification and regression models through mechanisms such as contrastive loss, quantile regularization, and entropy-based methods, our approach stands out by obviating the need for auxiliary source information and mitigating the computational burden typically associated with such measures. AdaRand, leveraging novel network architectures, dynamically aligns the distribution of feature vectors against class-conditional Gaussian random reference vectors. It is inherently trainable, continuously adjusting to feature extractor updates, thereby effectively calibrating predictions by ensuring the accuracy of probability estimation in deep learning models. This method uniquely addresses the distribution gap in feature spaces, making it a computationally efficient solution to enhance fine-tuning processes. Our empirical analysis, demonstrating AdaRand's superiority in calibration over other fine-tuning regularization methods that rely on extensive auxiliary data and computational resources, effectively prevents overconfident predictions and shows promising paths for more adaptive, efficient fine-tuning methodologies in deep learning applications.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Saiteja_Utpala1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10089v2",
  "title": "Approximation and bounding techniques for the Fisher-Rao distances",
  "modified_abstract": "This work contributes to the mathematical foundation underpinning statistical models, which is motivated by recent progress in non-asymptotic analysis of sampling algorithms used in nonconvex optimization. Specifically, our research is inspired by advancements in understanding algorithms' behaviors in challenging optimization landscapes, focusing on the Fisher-Rao distance\u2014a cornerstone metric in information geometry, defined as the Riemannian geodesic distance induced by the Fisher information metric. We explore numerically robust approximation and bounding techniques for this metric, with an eye towards improving the convergence rates of these algorithms. Our contributions include: reporting generic upper bounds based on closed-form 1D Fisher-Rao distances of submodels; describing approximation schemes for cases with and without closed-form geodesics; presenting a method for an arbitrarily small additive error in approximations, given pregeodesics and tight bounds; analyzing Fisher metrics as Hessian metrics for tighter upper bounds; applying these techniques to elliptical distribution families; proposing new distances based on Fisher-Rao lengths or Birkhoff/Hilbert projective cone distance; and exploring a group-theoretic approach for statistical transformation models. Furthermore, our exploration includes a consideration of real-time data tracking processes, enabling a nuanced understanding of data stream processing in nonasymptotic sampling analysis. Through these varied methods, we aim to enrich the toolbox available for analyzing and applying information geometrical methods in real-world statistical and machine learning applications.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Omer_Deniz_Akyildiz1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10075v2",
  "title": "A survey of synthetic data augmentation methods in computer vision",
  "modified_abstract": "The landscape of computer vision, particularly with advanced modeling and deep convolutional neural network (CNN) applications, is perpetually evolving with newer methodologies that overcome the rudimentary boundaries of data collection and utilization. This evolution is significantly propelled by synthetic data augmentation methods, which answer the perennial question of acquiring sufficient, varied data for specialized tasks. Our survey is inspired by pioneering works such as 'SinNeRF: Training Neural Radiance Fields on Complex Scenes from a Single Image', wherein challenges associated with data scarcity and representativeness are addressed through innovative approaches such as multi-view synthesis and radiance field adjustment. This paper presents an extensive review of synthetic data augmentation techniques, covering a wide array of methods including realistic 3D graphics modeling, neural style transfer (NST), differential neural rendering, and generative artificial intelligence (AI) techniques such as generative adversarial networks (GANs) and variational autoencoders (VAEs). We delve into the significant data generation and augmentation techniques within each method class, their general scope, specific use-cases, existing limitations, and potential solutions for sparse data scenarios. Additionally, we provide a summary of common synthetic datasets for training computer vision models, highlighting their key features, application domains, and supported tasks. The objective of this survey is to furnish the reader with comprehensive insights into synthetic data augmentation methods and their effectiveness, thereby paving the way for future explorations and innovations in the field. With this work, we aim to equip readers with a profound understanding of the existing methods, challenges, and advancements in synthetic data augmentation, initiating a foundation for subsequent research and application enhancements.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Dejia_Xu1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10070v1",
  "title": "A Structure-Preserving Kernel Method for Learning Hamiltonian Systems",
  "modified_abstract": "This work is inspired by recent discoveries in the domain of deep learning, particularly the phenomena observed during the training of overparameterized networks for classification tasks, known as \"neural collapse\", which have encouraged the exploration of the underlying manifolds and optimization landscapes in machine learning models. We introduce a structure-preserving kernel ridge regression method aimed at accurately recovering potentially high-dimensional and nonlinear Hamiltonian functions from noisy observations of Hamiltonian vector fields, thereby providing a novel mathematical framework that extends traditional kernel regression methods to accommodate complex loss functions involving gradients and normalization techniques. By leveraging manifold learning techniques and incorporating normalization directly within our deep learning-based loss functions, our approach not only surpasses other methodologies in numerical performance but also enriches the theoretical underpinnings of kernel methods and deep learning classification strategies in machine learning. Additionally, by proving a differential reproducing property and a Representer Theorem within this new context, and exploring the relationship between our kernel estimator and the Gaussian posterior mean estimator, we provide a comprehensive error analysis that includes fixed and adaptive regularization parameters. This not only demonstrates the method's efficacy through a series of numerical experiments on various tasks, including those that may involve collapse phenomena in overparameterized nonconvex models but also contributes both methodologically by adapting kernel regression for gradient-involving loss functions and substantively by offering a refined tool for understanding Hamiltonian systems.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Qing_Qu2",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10063v1",
  "title": "Unified Projection-Free Algorithms for Adversarial DR-Submodular Optimization",
  "modified_abstract": "In this paper, we introduce unified projection-free Frank-Wolfe type algorithms for adversarial continuous DR-submodular optimization. This work is inspired by significant advances in federated learning (FL), especially in addressing challenges related to intermittent client participation and computation/communication efficiency through a unified convergence analysis. Similarly, our algorithms, which are relevant across various learning scenarios such as full information and (semi-)bandit feedback, monotone and non-monotone functions, multiple constraints, and types of stochastic queries, utilize intermittent updates and sophisticated averaging techniques to enhance performance, efficiency, and participant engagement in the learning process. For every problem considered in the non-monotone setting, our proposed algorithms are either the first with proven sub-linear $\\alpha$-regret bounds or have better $\\alpha$-regret bounds than the state of the art, where $\\alpha$ is a corresponding approximation bound in the offline setting. In the monotone setting, by leveraging intermittent feedback and advanced averaging in parameter updates, our approach delivers state-of-the-art sub-linear $\\alpha$-regret bounds among projection-free algorithms in 7 of the 8 considered cases, while matching the result of the remaining case. Additionally, this paper ventures into the less-explored domains of semi-bandit and bandit feedback for adversarial DR-submodular optimization, thereby advancing the understanding of this optimization area and its related learning challenges through convergence analysis.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Mingyue_Ji1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12094v1",
  "title": "Are LLMs Good Cryptic Crossword Solvers?",
  "modified_abstract": "Drawing inspiration from recent advances in NLP models such as GraPPa, which aimed at augmenting grammar for better semantic parsing of tables through innovative pre-training strategies, our study explores the capability of large language models (LLMs) in tackling the intellectually stimulating domain of cryptic crosswords. Cryptic crosswords present a unique challenge, necessitating not only general knowledge but the solver's prowess in linguistic manipulation and wordplay. While previous research highlights the complexity of solving such puzzles for modern NLP models, the efficacy of LLMs\u2014including LLaMA2, Mistral, and ChatGPT\u2014remains untested in this context. This paper steps into this gap by benchmarking the performance of these LLMs on cryptic crossword puzzles, thereby providing insights into their capabilities and limitations in contrast to human solvers. Our analysis leverages a diverse range of datasets, including table-related contents, to ensure comprehensiveness in evaluation. By applying parsers developed through sophisticated pre-training methodologies, we assess whether LLMs can understand and generate solutions that effectively tackle the nuanced demands of cryptic crosswords.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~bailin_wang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10045v1",
  "title": "Towards Adversarially Robust Dataset Distillation by Curvature Regularization",
  "modified_abstract": "Inspired by recent findings that overparameterization in deep learning implicitly reduces variance, challenging the benefits of explicit regularization, our study shifts focus within the domain of dataset distillation (DD). This technology allows datasets to be compacted significantly while retaining the essential distributional information for models, particularly neural networks, to achieve comparable accuracy with reduced computational demands and lower dimensions of data. While prior efforts have enhanced the accuracy of models trained on distilled datasets, adding dimensions to the evaluation, our research introduces an innovative angle by embedding adversarial robustness into DD. Proposing a novel method that incorporates curvature regularization and manipulates loss gradients into the distillation process, our approach achieves superior outcomes in both accuracy and adversarial robustness, challenging the traditional loss-margin trade-off, and without the heavy computational overhead typical of standard adversarial training techniques, which often involve adding noise to data as a form of augmentation. Through comprehensive empirical evaluations, we demonstrate the efficacy of our method in producing robust distilled datasets capable of withstanding various adversarial attacks, thereby providing a more robust and computationally efficient alternative to standard adversarial training that is provably beneficial.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Alexandru_Tifrea1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10042v1",
  "title": "Accurate and Data-Efficient Micro-XRD Phase Identification Using Multi-Task Learning: Application to Hydrothermal Fluids",
  "modified_abstract": "Inspired by recent advancements in representation learning, particularly in discerning complex patterns within data, our study introduces a groundbreaking application of deep learning via multitask learning (MTL) architecture for the analysis of micro-X-ray diffraction ({\\mu}-XRD) patterns from hydrothermal fluid environments. Traditional methods for analyzing these highly distorted patterns are time-consuming and dependent on extensive preprocessing and abundant labeled experimental data. Leveraging insights from related work, such as the use of coupled autoencoders for mixture representation learning, our MTL models are designed to efficiently identify phase information in {\\mu}-XRD patterns, reducing the imperative for labeled experimental data and circumventing the preprocessing steps. Notably, MTL models demonstrated superior accuracy to binary classification convolutional neural networks (CNNs) and further benefited from a tailored cross-entropy loss function. Crucially, MTL models capable of analyzing raw, unmasked {\\mu}-XRD patterns achieved nearly equivalent performance to those trained on preprocessed data, evidencing a minimal compromise on accuracy. This study underscores the potential of advanced deep learning architectures, such as MTL, to automate labor-intensive data handling tasks, simplify the analysis of distorted XRD patterns, and diminish dependency on laborious experimental datasets preparation.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yeganeh_Marghi1",
  "manipulated_ranking": 4,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10576v1",
  "title": "Ignore Me But Don't Replace Me: Utilizing Non-Linguistic Elements for Pretraining on the Cybersecurity Domain",
  "modified_abstract": "Drawing upon the pioneering works in the fields of pretraining methodologies and knowledge distillation, such as the transformative practices of utilizing machine-authored commonsense knowledge graphs for training domain-specific models, our research takes a novel approach in the cybersecurity domain to address the challenge of incorporating non-linguistic elements (NLEs), like URLs and hash values, in the pretraining phase. Cybersecurity information, characterized by its technical complexity and structured yet unstructured text, necessitates engineering automation for effective cyber threat intelligence. Conventionally, engineering pretraining language models on in-domain corpora has been a crucial move towards obtaining domain specificity, yet the prevalent methodologies often dismiss NLEs as noise without probing their potential utility, especially within the cybersecurity context. By proposing and evaluating different pretraining methodologies, including selective masked language modeling (MLM) and a joint training approach referred to as 'teacher' training for NLE token classification with a focus on the causal relationships between cybersecurity events, graph-based techniques, and the authorial intent behind cyber threat narratives, our work not only questions the efficacy of conventional approaches but also introduces CyBERTuned\u2014a domain-customized language model. CyBERTuned showcases superior performance on a variety of cybersecurity tasks compared to other pre-learned models (PLMs), thereby offering a promising alternative to the traditional treatment of NLEs in domain-specific language model training.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Chandra_Bhagavatula1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10024v1",
  "title": "MR-MT3: Memory Retaining Multi-Track Music Transcription to Mitigate Instrument Leakage",
  "modified_abstract": "In this work, we build upon the MT3 model, addressing its instrument leakage issues, which have been a longstanding challenge similar to problems in other complex domains such as neural manifold estimation in neuroscience, a field deeply concerned with understanding the co-variability of brain activities and their characterization in trajectories. Leveraging the state-of-the-art (SOTA) token-based multi-instrument automatic music transcription (AMT) capabilities of MT3, we introduce MR-MT3. This enhanced version incorporates a memory retention mechanism, prior token sampling, and token shuffling to mitigate instrument leakage, evaluated using the Slakh2100 dataset. Our approach demonstrates improved onset F1 scores and reduced instrument leakage. We also introduce and employ new evaluation metrics such as the instrument leakage ratio and the instrument detection F1 score for a more comprehensive and nuanced assessment of transcription quality, translating the complexity of musical expressions into more defined population behaviors and partitioning. Further, by assessing MT3's performance on single-instrument monophonic datasets like ComMU and NSynth, we explore the issue of domain overfitting, a consideration crucial for refining future token-based multi-instrument AMT models. Our findings underscore the complexity of the manifold on which musical expressions lie, emphasizing the co-variability and population behavior of musical activity. The source code is shared to inspire and facilitate ongoing research efforts directed towards overcoming the challenges of multi-instrument transcription.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Pedro_Herrero-Vidal1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10015v1",
  "title": "Linear optimal transport subspaces for point set classification",
  "modified_abstract": "Our investigation builds upon an interesting foundation of handling set-structured data, inspired by significant previous works such as advancements in sets clustering and its novel approaches to tackle the sets-$k$-means problem through core-set construction and $\\textit{epsilon}$-approximations. The challenges of native, unordered, and permutation-invariant set structure space, particularly under spatial deformations, drive our development of a novel framework for point set classification. We introduce an innovative approach leveraging the Linear Optimal Transport (LOT) transform for the linear embedding of set-structured data, that exhibits resilience to affine spatial deformations found in various datasets. By capitalizing on the mathematical attributes of the LOT transform and employing a sophisticated classification algorithm, our method constructs a convex data space that can accommodate variances in point sets, including different centers, greatly simplifying the sets-$k$-means classification task. Our approach is characterized by label efficiency, non-iterative behavior, and the absence of hyper-parameter tuning, offering competitive accuracies across diverse point set classification tasks when juxtaposed with contemporary methods. Additionally, our method demonstrates robustness in out-of-distribution scenarios, highlighting its potential in handling deformation discrepancies between training and test distributions effectively.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Murad_Tukan1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10013v1",
  "title": "LyZNet: A Lightweight Python Tool for Learning and Verifying Neural Lyapunov Functions and Regions of Attraction",
  "modified_abstract": "Drawing inspiration from the innovative utilization of neural networks for learning dynamic systems, as observed in advanced solutions for learning surrogate models in advection-dominated systems, we present LyZNet, a lightweight Python framework tailored for the integrated learning and verification of neural Lyapunov functions for stability analysis. This tool leverages physics-informed neural networks (PINNs) to address Zubov's equation for learning neural Lyapunov functions and employs satisfiability modulo theories (SMT) solvers for verification, offering a novel approach to ascertain verified regions of attraction that closely align with the true domain of attraction. This achievement is facilitated through the adaptation of Zubov's partial differential equation (PDE) within the PINN framework, thereby efficiently navigating the non-convex and smooth nature of the optimization challenges inherent in the analysis of dynamic systems. Notably, LyZNet, by utilizing surrogate and learning methodologies, surpasses the limitations of traditional convex optimization methods, like semidefinite programming, in identifying the domain of attraction, especially for systems where these methods falter, and it leverages high-fidelity simulations to enhance this process. Furthermore, the tool enhances its applicability via automatic decomposition of coupled nonlinear systems into manageable low-dimensional subsystems and latent spaces for compositional verification while rendering complex regions of stability more comprehensible. The utility and effectiveness of LyZNet are underscored through its application to a variety of systems, ranging from intricate low-dimensional to challenging high-dimensional nonlinear systems, and the study of their stability trajectories. The continuous work and hypernetwork-based learning facilitated by LyZNet in the field of stability analysis exemplify its potential as an efficacious tool. The repository of the tool is accessible at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Leonardo_Zepeda-Nunez1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10006v1",
  "title": "Graph Enhanced Reinforcement Learning for Effective Group Formation in Collaborative Problem Solving",
  "modified_abstract": "Motivated by the pressing need to optimize collaborative efforts in diverse settings and inspired by recent advances in understanding multi-agent coordination from multi-task offline data, this study tackles the complex challenge of forming effective groups in collaborative problem-solving environments. By integrating principles from graph theory with reinforcement learning techniques, we introduce a groundbreaking framework that structures participants into a graph format, with nodes representing individuals and edges representing their interactions. This framework conceptualizes each participant as an autonomous agent tasked with learning optimal group dynamics through reinforcement learning, deriving knowledge from collaborative experiences. Employing clustering techniques, our approach delineates distinct group structures, facilitating theoretical and practical insights into achieving superior collaborative efficiency and minimizing conflict. This research extends beyond traditional team-building methodologies, offering a novel, data-driven perspective to enhancing group effectiveness within various collaborative contexts such as organizational teams and educational settings. Furthermore, by bridging graph theory and reinforcement learning with collaborative group formation, this work pioneers the exploration of these computational techniques in social and behavioral sciences, laying the groundwork for empirical validation in subsequent studies.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Fuxiang_Zhang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09976v1",
  "title": "AD3: Implicit Action is the Key for World Models to Distinguish the Diverse Visual Distractors",
  "modified_abstract": "Inspired by advancements in model-based methods for visual control and the transformative pre-training paradigms in vision-and-language navigation tasks, our work introduces the Implicit Action Generator (IAG) and a novel algorithm named the implicit Action-informed Diverse visual Distractors Distinguisher (AD3). This methodology signifies a departure from traditional focus areas by addressing the challenge of homogeneous distractors that closely resemble controllable agents, a largely unexplored facet in previous research that primarily attended to heterogeneous distractors like noisy background videos. By learning the implicit actions of visual distractors, IAG facilitates the training of separated world models through AD3, leveraging action inference to enhance the distinction between task-relevant and irrelevant components in visual control tasks. Our methodological approach, harnessing the power of pre-trained models and integrating language processing capabilities for improved navigation, achieves superior performance across a spectrum of visual control scenarios featuring both heterogeneous and homogeneous distractors. By benchmarking against existing vision-and-language models and utilizing image-text-action combinations, our empirical validation showcases the indispensable role of implicit actions in distinguishing diverse visual distractors effectively. The integration of self-supervised techniques with state-of-the-art vision-and-language paradigms enables our models to navigate complex environments more efficiently, showcasing significant advancements in the field.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Xiujun_Li1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09974v1",
  "title": "GET: Unlocking the Multi-modal Potential of CLIP for Generalized Category Discovery",
  "modified_abstract": "In the pursuit of improving Generalized Category Discovery (GCD), this work is inspired by advancements in multi-modal learning, particularly by previous efforts that addressed the challenges of multi-modal Few-Shot Object Detection (FSOD) through the innovative use of meta-learning and cross-modal prompting. These foundational works spotlight the unprecedented benefits of integrating visual and semantic modalities for object detection, which in turn motivated our exploration into leveraging text information alongside visual data in GCD tasks. Our novel Text Embedding Synthesizer (TES) addresses the impediment of unavailable class names in unlabelled data by transforming visual embeddings of images into textual tokens using CLIP's aligned vision-language features, enabling the generation of pseudo text embeddings. By instituting a dual-branch framework that fosters the mutual enhancement of visual and semantic information, our methodology not only enhances the interaction between visual and text embedding spaces but also capitalizes on the multi-modal capabilities of CLIP to set new benchmarks in GCD, significantly outperforming state-of-the-art methods across all GCD benchmarks, incorporating prior knowledge and few-shot learning processes facilitated by a meta-learning-based approach. The advancement in prompt-based techniques has particularly informed our approach to detection, allowing for more seamless cross-modal communication and discovery processes. The code for our pioneering approach will be released at [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Guangxing_Han1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10573v1",
  "title": "Medical Unlearnable Examples: Securing Medical Data from Unauthorized Training via Sparsity-Aware Local Masking",
  "modified_abstract": "Amidst the transformative wave of artificial intelligence (AI) in healthcare, our work is inspired by the pioneering efforts in the field of privacy-preserving techniques, particularly in multi-task learning for healthcare, finance, and IoT domains. These efforts have aimed to strike a balance between the utility of shared sensitive data and the protection against its unauthorized exploitation. Therefore, to augment the security measures for sensitive medical data against unauthorized training, which threatens the integrity of medical AI advancements, we introduce a novel Sparsity-Aware Local Masking (SALM) method. Unlike prior approaches that may not fully consider the unique challenges posed by the sparse nature of medical images, our method selectively perturbs significant pixel regions, concentrating on local areas to effectively reduce the perturbation search space and enhance privacy through optimization of the masking process. This approach makes our method highly efficient and optimizes tasks related to data protection. Our localized strategy not only enhances the efficiency but also the effectiveness of protecting biomedical datasets, without compromising their clinical utility. Through extensive evaluations across diverse datasets and model architectures, our computing method within SALM demonstrably outperforms existing data protection methods, establishing new benchmarks in safeguarding medical data for AI applications while maintaining its precious utility for medical research and practice. Furthermore, our approach is also in line with distributed computing and federated learning principles, emphasizing distributed data handling without directly sharing sensitive information, thus presenting a personalized solution to privacy problems in the medical field.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Shengyuan_Hu2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10572v1",
  "title": "Discovering Invariant Neighborhood Patterns for Heterophilic Graphs",
  "modified_abstract": "In an era where the understanding and analysis of diverse real-world networks are pivotal, our investigation is particularly inspired by the challenges posed by non-homophilous graphs, for which the prevailing homophily-based assumptions\u2014nodes from the same class are more likely to be linked\u2014do not hold. These complexities are exemplified in the identification and alignment of patterns within such graphs, as explored in previous studies on graph alignment, isomorphism, and the processing of Erd\u0151s\u2013R\u00e9nyi random graphs. Addressing the critical issue of distribution shifts on non-homophilous graphs, we introduce a novel Invariant Neighborhood Pattern Learning (INPL) framework that employs an algorithm designed to navigate these complexities. INPL employs an Adaptive Neighborhood Propagation (ANP) module to adeptly manage the neighborhood pattern distribution shifts characteristic of non-homophilous graphs, and an Invariant Non-Homophilous Graph Learning (INHGL) module to impose constraints on ANP, thus learning invariant graph representations effectively. Our message-passing mechanism within ANP enhances the aligning and recovery of invariant patterns, allowing for robust learning even on Erd\u0151s\u2013R\u00e9nyi random graphs with their intrinsic structural randomness. By incorporating sparse polynomial transformations within INPL, we further ensure the efficient processing and recovery of complex, sparse graph structures. Our extensive experimental analyses on real-world non-homophilous graphs demonstrate INPL\u2019s superior capability in mitigating distribution shifts, propelling it to state-of-the-art performance for learning on large non-homophilous graphs.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Guilhem_Semerjian1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09969v1",
  "title": "Prediction of Vessel Arrival Time to Pilotage Area Using Multi-Data Fusion and Deep Learning",
  "modified_abstract": "Building upon the broadly recognized importance of accurate temporal predictions in the realm of physical systems, as highlighted by advancements in multi-scale physical representations for approximating Partial Differential Equations (PDEs) solutions with Graph Neural Operators, this paper contributes to the field by investigating the prediction of vessels' arrival time to the pilotage area using a novel blend of multi-data fusion and deep learning techniques. Firstly, we extract the vessel arrival contour utilizing Multivariate Kernel Density Estimation (MKDE) combined with clustering methodologies. Secondly, we propose a comprehensive data fusion approach that integrates Automatic Identification System (AIS), pilotage booking information, and meteorological data for enhanced latent feature extraction and modeling. Thirdly, we design and implement a Temporal Convolutional Network (TCN) framework, embedding it with a residual mechanism, to unravel the implicit patterns governing vessel arrivals. Our empirical investigations, developed and conducted on real-world datasets from Singapore, yield promising outcomes: the fusion of pilotage booking and meteorological data notably boosts prediction accuracy, with the former being particularly influential; discrete embedding of meteorological data demonstrates superiority over its continuous counterpart; the TCN model outshines established baseline methodologies in forecasting tasks, achieving Mean Absolute Error (MAE) values ranging between 4.58 and 4.86 minutes; and approximately 89.41% to 90.61% of the prediction residuals are confined within a 10-minute tolerance range, asserting the model's precision, reliability, and capacity to describe complex temporal dynamics.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~L\u00e9on_Migus1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09953v1",
  "title": "Online GNN Evaluation Under Test-time Graph Distribution Shifts",
  "modified_abstract": "Inspired by the transformative potential of Graph Convolutional Networks (GCNs) in learning representations of attributed graphs and the challenges posed by large-scale graph data processing, our work tackles the crucial yet understudied problem of online GNN evaluation amidst test-time graph distribution shifts. This research area is guided by the advances in graph sampling techniques, such as those introduced by GraphSAINT for efficient inductive learning on large graphs in minibatches, which underscore the need for evaluating GNNs' performance on real-world, unlabeled graphs with an attributed nature. Our study proposes a novel approach, termed Learning Behavior Discrepancy (LeBeD) score, to effectively gauge the generalization errors of well-trained GNNs under test-time distribution shifts. By integrating insights from both node prediction and structural reconstruction perspectives, and employing an attention mechanism at each layer of the GNN architecture for refined feature processing of nodes within each minibatch, LeBeD serves as a comprehensive tool for online evaluation, emphasizing the adaptability of GNNs to unseen test graph distributions. Through extensive experiments on real-world datasets exhibiting various graph distribution shifts, our method not only verifies the effectiveness of the proposed LeBeD score but also showcases its strong correlation with ground-truth test errors, thus contributing valuable techniques for the reliable deployment and evaluation of GNNs in dynamic environments.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ajitesh_Srivastava1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09942v1",
  "title": "Attention-Enhanced Hybrid Feature Aggregation Network for 3D Brain Tumor Segmentation",
  "modified_abstract": "In the pursuit of advancing the automated detection of glioblastoma, a highly aggressive and malignant brain tumor, through Artificial Intelligence (AI)-driven healthcare solutions, our research draws inspiration from recent innovations and methods in visual recognition, with particular attention to improving feature extraction and aggregation for precise segmentation tasks. The proposed GLIMS model represents a novel architecture that leverages the multi-scale, attention-guided principles empirically validated in transformative works such as GPViT, a vision transformer emphasizing efficient global information exchange amongst high-resolution features. Specifically, our GLIMS model integrates a multi-scale feature extraction mechanism and Swin Transformer blocks within a U-Net-shaped framework, optimized for 3D brain tumor segmentation across three critical regions: Enhancing Tumor (ET), Tumor Core (TC), and Whole Tumor (WT). By embedding attention mechanisms, decoder modules, and utilizing transformer tokens, our approach fine-tunes the importance of characteristic attributes at varying depths of the model, thereby enhancing the feature aggregation process and embodying superior segmentation capability with attention to critical details. Perceiving nuances in the model's architecture allows for refined adjustments that contribute to its elevated performance. Evaluated using the Brain Tumor Segmentation Challenge (BraTS) validation set, our model demonstrates promising results with up to 92.19, 87.75, and 83.18 percent Dice Scores for WT, TC, and ET, respectively. Hierarchical supervision techniques further augment the training process, ensuring more precise and efficient learning. For broader application and future development, these details along with the code are made publicly accessible at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Chenhongyi_Yang3",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09940v1",
  "title": "Global Convergence Guarantees for Federated Policy Gradient Methods with Adversaries",
  "modified_abstract": "Inspired by the advancements in centralized value function methodologies for cooperative multiagent reinforcement learning, evidenced by works such as SMIX(\n) which address the exponential increase in joint action space with the number of agents, this study extends into the federated learning domain to tackle unique challenges posed by adversarial agents. Federated Reinforcement Learning (FRL) allows multiple agents to collaboratively build a decision-making policy without sharing raw trajectories. However, adversarial agents within these setups can significantly undermine the reinforcement learning process, threatening the centralized coordination and benchmark performance levels previously achievable. Our paper introduces a policy gradient-based method designed to be robust against adversaries capable of sending arbitrary loss values to the server, marking the first time global convergence guarantees with general parametrization have been established under such adversarial settings for multiagent reinforcement learning systems. Demonstrating resilience, our results show that with adversaries, the sample complexity reaches an order of $\tilde{\text{O}}\text{left}( \frac{1}{\text{epsilon}^2} \text{left}( \frac{1}{N-f} + \frac{f^2}{(N-f)^2}\text{right)}\text{right)}$ where $N$ represents the total number of agents and $f$ the number of adversarial agents. This framework not only addresses the emergent challenge of adversarial behavior in federated setups but also opens new avenues for creating robust federated learning systems, setting new benchmarks in the process.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Xiaoyang_Tan2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09930v1",
  "title": "Quality-Diversity Actor-Critic: Learning High-Performing and Diverse Behaviors via Value and Successor Features Critics",
  "modified_abstract": "A key aspect of intelligence is the ability to demonstrate a broad spectrum of behaviors for adapting to unexpected situations. Over the past decade, advancements in deep reinforcement learning have led to groundbreaking achievements in solving complex continuous control tasks, emphasizing the importance of efficient decision-making in network structures. However, most approaches return only one solution specialized for solving a specific problem, lacking the capability to dynamically adapt their decision-making to novel scenarios. We introduce Quality-Diversity Actor-Critic (QDAC), an off-policy actor-critic deep reinforcement learning algorithm that leverages a value function critic and a successor features critic to learn high-performing and diverse behaviors, even in partially observable environments requiring strategic memory management for effective problem-solving. In this framework, the actor, serving as an agent, optimizes an objective that seamlessly unifies both critics using constrained optimization to (1) maximize return, while (2) executing diverse skills with strategic reasoning to enhance its problem-solving capabilities. Compared with other Quality-Diversity methods for solving continuous control locomotion tasks, QDAC achieves significantly higher performance and more diverse behaviors on six challenging continuous control locomotion tasks. We also demonstrate that, by reasoning about the learned skills and employing effective memory strategies, we can harness them to adapt better than other baselines to five perturbed environments, showcasing our approach's unique problem-solving versatility. Finally, qualitative analyses showcase a range of remarkable behaviors, available at: http://bit.ly/qdac.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Athirai_Aravazhi_Irissappane2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09919v1",
  "title": "Recurrent Drafter for Fast Speculative Decoding in Large Language Models",
  "modified_abstract": "Motivated by the challenges encountered in optimizing large language model operations, particularly in speculative decoding, this study presents an innovative methodology that draws inspiration from both classical techniques and contemporary approaches such as Medusa. Our solution strategically blends the tactical essence of speculative decoding with a pioneering single-model strategy, introducing a lightweight, recurrently dependent draft head to enhance the efficiency and agility of speculative decoding for tasks like long-dialogue and long-document summarizing. This novel approach is characterized by its streamlined architecture, which simplifies the speculative decoding process by eliminating the need for a data-dependent tree attention structure exclusive to inference, as observed in Medusa. Through empirical evaluations conducted on a range of widely-utilized open-source language models, our research not only substantiates the effectiveness of our proposed method in abstractive summarization generator functions but also elucidates the multiple trade-offs associated with this specific decoding strategy. The dedication to optimizing long-dialogue summarization within large language models emphasizes the potential impacts and benefits of this methodology, seeking to contribute extensively to the field's ongoing discourse. This work is propelled by technological advancements and theoretical insights evidenced in related works, underscoring the relevance of summarizing tasks in achieving breakthroughs in model efficiency and output quality, especially in latent feature exploration.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ansong_Ni1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09918v1",
  "title": "Attention-based Class-Conditioned Alignment for Multi-Source Domain Adaptive Object Detection",
  "modified_abstract": "Inspired by the dynamic landscape of domain adaptation and the proven efficacy of self-learning techniques under distribution shifts, our work presents a novel approach to Multi-Source Domain Adaptation (MSDA) within the realm of object detection (OD), addressing the inherent limitations of current methods that primarily execute feature alignment in a class-agnostic manner. Recognizing that objects exhibit unique modal information attributed to variations in appearance across domains, and building upon recent advancements in prototype-based class-wise alignment\u2014which, however, grapple with the challenges posed by noisy pseudo-labels, imbalance in data distribution, and entropy in feature representation\u2014we introduce an attention-based, class-conditioned alignment scheme. This approach incorporates self-supervised learning for refining pseudo-labels through enhanced self-learning training sessions and ensures that the attention mechanism consistently focuses on the most relevant features across epochs, facilitating the focused training of domain-invariant, class-specific instance representations. Thus, we effectively tackle the pitfalls of prior strategies, consistently addressing the dynamics of domain shifts. Experimental evaluations conducted on renowned benchmark MSDA datasets for machine learning classification showcase our methodology's superiority in outperforming existing state-of-the-art techniques, underscoring its resilience against class imbalance and its ability to adapt hyperparameters for optimal performance. Our code is accessible at [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Evgenia_Rusak1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09904v1",
  "title": "FedComLoc: Communication-Efficient Distributed Training of Sparse and Quantized Models",
  "modified_abstract": "In the evolving landscape of Federated Learning (FL), which is poised at the intersection of heterogeneous data processing and privacy preservation, communication cost emerges as a formidable challenge. Motivated by the burgeoning need to address this bottleneck and the foundational advances made by the \\emph{Scaffnew} algorithm in reducing communication complexity, our work, FedComLoc (Federated Compressed and Local Training), pioneers the integration of effective compression techniques into \\emph{Scaffnew} to further elevate communication efficiency. By incorporating the TopK compressor and quantization, alongside differential privacy measures, we demonstrate through extensive training experiments the capability of FedComLoc in significantly curtailing communication overheads within heterogeneous settings where device memory constraints and trainable parameter optimization are critical. This achievement is particularly relevant given the rising demands for data privacy and efficient pipeline management in FL. Drawing inspiration from prior undertakings in distributed learning, we tackle the optimization of large models under constraints, such as those imposed by differential privacy in the regime of billions of parameters, often deployed in parallel computing environments.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zhiqi_Bu1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09901v1",
  "title": "Robust Subgraph Learning by Monitoring Early Training Representations",
  "modified_abstract": "In the realm of graph neural networks (GNNs), the critical issue of adversarial attacks underscores the emphasis on methodologies that ensure performance alongside adversarial robustness in graph-based decision-making. Inspired by pioneering studies on the interpretability and influence of graph elements in machine learning models, such as the use of influence functions in graph convolution networks (GCNs) to measure the impact of graph modifications and estimation techniques for evaluating influence, this paper introduces a novel method for increasing the robustness of GNNs against adversarial attacks. Named SHERD (Subgraph Learning Hale through Early Training Representation Distances), our technique utilizes the dynamics of partially trained GCNs through convolution operations to detect and sideline susceptible nodes, creating a fortified subgraph without compromising node classification accuracy. Through extensive experiments with citation networks and microanatomical cell graphs, and enhanced by strategic re-training, SHERD not only improves adversarial resilience but also enhances node classification performance, outstripping several established baselines in both efficiency and accuracy. Furthermore, the application of post-hoc analysis solidifies our understanding of how early representations contribute to the overall robustness and learning process provided by our method.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zizhang_Chen1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09898v1",
  "title": "TimeMachine: A Time Series is Worth 4 Mambas for Long-term Forecasting",
  "modified_abstract": "Building upon the foundations laid by innovative approaches such as the probabilistic sequential matrix factorization (PSMF), which addressed factorization of time-varying and non-stationary datasets through nonlinear Gaussian state-space models, we introduce TimeMachine, an innovative model that overcomes the challenges of long-term time-series forecasting including capturing long-term dependencies, achieving linear scalability, and maintaining computational efficiency. TimeMachine incorporates the Mamba model, a state-of-the-art state-space model for filtering and factorizing multivariate time series data with finesse. By exploiting the inherent characteristics of time series data for generating salient contextual cues at various scales, and utilizing a probabilistic integrated approach that combines four Mamba models for both factorization and imputing missing data in scenarios of channel-mixing and channel-independence effectively, TimeMachine sets a new benchmark in predictive abilities across global and local contexts at differing scales. Our experimental validations, using benchmark datasets, exhibit TimeMachine's superiority in extended forecasting accuracy, factorization scalability, and memory efficiency through periodic validations and applications of filters to refine forecasting precision. Code availability has been moved to: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Omer_Deniz_Akyildiz1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09891v2",
  "title": "Fisher Mask Nodes for Language Model Merging",
  "modified_abstract": "This study is motivated by the intersection of two significant trends in machine learning: the ubiquitous adoption of pre-trained models like BERT for enhancing downstream task performance in natural language processing, and the emergence of model merging as a promising solution for multi-task learning scenarios. Addressing the limitation of task-specific fine-tuned models which excel at individual tasks yet require additional training or ensembling for multi-task scenarios, we introduce a novel model merging method for Transformers. This method leverages insights from the field of model merging, particularly techniques informed by Fisher-weighted averaging and the strategic use of Fisher information for model pruning. By applying Fisher information to mask nodes within the Transformer architecture, we create a computationally efficient weighted-averaging scheme. Our approach demonstrates consistent and significant performance improvements across various models in the BERT family, surpassing traditional Fisher-weighted averaging methods in both efficiency and efficacy, with up to a 57.4x speedup in computational time and baseline performance improvements of up to +6.5. The findings underscore the effectiveness of our method in contemporary multi-task learning frameworks and highlight its potential applicability and scalability to new model architectures and learning scenarios.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Wilson_Ma1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09889v1",
  "title": "Generalization of Scaled Deep ResNets in the Mean-Field Regime",
  "modified_abstract": "Our work extends the burgeoning dialogue on the intricate mechanisms underpinning neural network efficacy, specifically focusing on the acclaimed ResNet's deep architecture. This exploration is inspired by recent advancements in understanding the nonparametric estimation of operators in infinitely dimensional spaces, an area critical for widespread applications in machine learning, imaging science, and mathematical modeling. Unlike previous inquiries which often limit themselves to the lazy training regime, we delve into the properties of \\emph{scaled} ResNet within the theoretical framework of infinitely deep and wide networks\u2014aptly described by a partial differential equation in the large-neural network limit, or the \\emph{mean-field} regime. Through a fresh lens, our investigation introduces a time-variant, distribution-dependent Gram matrix, replacing the conventional time-invariant matrix, to derive novel generalization bounds. We further offer a lower bound on the Gram matrix's minimum eigenvalue in the mean-field regime. In addition, our discourse encompasses the dynamics of Kullback-Leibler (KL) divergence, achieving linear convergence of the empirical error alongside an upper limit estimation of the KL divergence across parameter distributions. Lastly, by harnessing Rademacher complexity, we present uniform convergence for the generalization bound. These insights significantly enrich our comprehension of deep ResNet's generalization capabilities beyond the lazy training regime, propelling forward our grasp of deep learning's foundational traits.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Wenjing_Liao1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09871v1",
  "title": "ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Image",
  "modified_abstract": "Motivated by advancements and challenges in various fields of computer vision, such as unsupervised 3D point cloud registration from raw RGB-D video, this work introduces ThermoHands\u2014a new benchmark for thermal image-based egocentric 3D hand pose estimation. The groundwork laid by prior efforts in deriving insights from differentiable rendering and geometric consistency informs our exploration into overcoming hurdles like varying lighting and obstructions (e.g., handwear), which are commonplace in 3D hand pose estimation. Our benchmark boasts a diverse dataset from 28 subjects engaged in hand-object and hand-virtual interactions, meticulously annotated with 3D hand poses via an automated process that leverages a powerful stream of data processing. We also present a pioneering baseline method, TheFormer, which employs dual transformer modules, tailored for effective egocentric 3D hand pose estimation in thermal imagery through learning mappings from thermal images to 3D annotations and utilizing cloud-based learning approaches for model training. Experimental analyses, inclusive of various cloud-based learning approaches, reveal TheFormer's superior performance, underscoring the potential of thermal imaging as a robust medium for 3D hand pose estimation under adverse conditions. Notably, our work provides essential insights into the rendering, registration, and learning processes necessary for advancing video-based egocentric hand pose estimation, while opening potential interdisciplinary avenues with fields such as robotics through datasets and application insights.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Mohamed_El_Banani1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09869v1",
  "title": "Mind the GAP: Improving Robustness to Subpopulation Shifts with Group-Aware Priors",
  "modified_abstract": "Triggered by the pressing need for models that maintain high performance under diverse and shifting data landscapes, our work is fueled by insights gleaned from examining the intricacies of robustness and generalization in machine learning models against subpopulation shifts\u2014a challenge highlighted and addressed by preceding studies like ORIENT, which explores efficiency in domain adaptation through innovative data subset selection methods. Our paper introduces a novel technique emphasizing the development of group-aware prior (GAP) distributions over neural network parameters, ingeniously designed to bolster model generalization in the face of subpopulation shifts. By proposing a straightforward group-aware prior necessitating only minimal data with group annotations for adaptation, we demonstrate how leveraging such GAPs can significantly enhance performance, even with minor adjustments to the final layer of an existing model. The conceptual elegance and complementarity of GAPs to prevailing methods, including attribute pseudo-labeling and data reweighting, underscore our contribution to advancing Bayesian inference as a powerful ally for achieving model robustness to subpopulation shifts. Furthermore, the adaptability of our group-aware prior technique suggests its potential for efficient query set construction and submodular function optimization in further studies, through subset optimization. This work not only reaffirms the role of source data in fine-tuning models but also highlights the importance of submodular optimization in selecting the most informative data subsets for domain-specific adaptation.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Krishnateja_Killamsetty1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09867v1",
  "title": "iBRF: Improved Balanced Random Forest Classifier",
  "modified_abstract": "Class imbalance remains a pervasive issue in a wide range of classification tasks across various real-world applications, an issue that has sparked considerable innovation in data resampling techniques. This work is specifically inspired by advancements in addressing data discrepancies through novel sampling methods, such as those proposed in density ratio estimation, which aim to reconcile data disparities in machine learning environments by accurately estimating densities of different classes. We recognize the value of ensemble learning and propose an enhancement to the Balanced Random Forest (BRF) classifier to improve prediction performance in the face of imbalanced datasets. In contrast to the original BRF's reliance on Random Undersampling (RUS), our approach integrates a novel hybrid sampling technique with the ensemble framework to mitigate data loss and enhance class distribution balance. The resulting iBRF: improved Balanced Random Forest classifier, not only exemplifies a significant advancement in handling imbalanced classification but also outperforms existing classifier methods. Our experimental evaluation across 44 imbalanced datasets demonstrates the iBRF's superior performance, achieving higher Mean Matthews Correlation Coefficient (MCC) and F1 scores compared to the original BRF, thereby establishing the potential of our proposed algorithm as an effective technique in the realm of imbalanced learning. The use of densities and estimation techniques is essential in achieving these results, clearly showing our method\u2019s ability to effectively handle complex imbalanced data scenarios. Furthermore, our experimental design took into consideration the divergence of class distributions, making our learning process adept at navigating the challenges presented by imbalanced data.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Benjamin_Rhodes1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09863v1",
  "title": "A Conceptual Framework For White Box Neural Networks",
  "modified_abstract": "Inspired by significant strides in the understanding of neural networks and their visual processing capabilities, this paper introduces semantic features as a novel conceptual framework for the development of fully explainable neural network layers, drawing upon insights such as the recognized efficacy of divisive normalization strategies in improving image recognition performance. The incorporation of both divisive and canonical normalizations, alongside an average pooling strategy, presents a tangible proof of concept in the context of a relevant MNIST subproblem; this work details a model comprising four such layers with a total of 4.8K learnable parameters. The model not only achieves human-level adversarial test accuracy without any form of adversarial training but also necessitates minimal hyperparameter tuning, showcasing remarkable efficiency in manifold representation and significantly improved interpretability in recognition tasks. Moreover, it can be swiftly trained on a single CPU. Given its inherent interpretability, minimal requirement for external normalization techniques, and promising results, this approach suggests a potential paradigm shift towards the realization of democratized, truly generalizable white-box neural networks. Code is available at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Michelle_Miller3",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09859v1",
  "title": "MAMBA: an Effective World Model Approach for Meta-Reinforcement Learning",
  "modified_abstract": "Drawing upon the foundational work in model-based reinforcement learning (RL) and the novel concepts introduced by recent advancements in meta-RL, especially in dealing with low-dimensional task distributions and the nuanced exploration strategies, we introduce MAMBA, a pioneering model-based approach tailored specifically for meta-reinforcement learning (meta-RL). Our methodology amalgamates the proven strengths of model-based RL's superior navigation through partially observable MDPs, with the nuanced advantages that meta-RL frameworks offer for rapid learning adaptation across varied tasks. This integration leverages conditioning on prior trajectories and utilizes datasets effectively to enhance learning through practice and maximize return. MAMBA demonstrates unparalleled efficacy and sample efficiency (improving up to 15 times) on standard meta-RL benchmark domains, employs state-of-the-art algorithms, and showcases its potential in handling more complex, higher-dimensional domains, thus making strides toward achieving generalizability in real-world applications, including those that may involve offline RL scenarios.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~David_Brandfonbrener1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09857v1",
  "title": "Few-Shot Class Incremental Learning with Attention-Aware Self-Adaptive Prompt",
  "modified_abstract": "Drawing inspiration from existing methodologies that address the challenges of learning in constrained environments with limited data, including the creative use of uncertainty measures in domain adaptation and object detection tasks, our study introduces a novel Few-Shot Class-Incremental Learning (FSCIL) framework dubbed as Attention-aware Self-adaptive Prompt (ASP). Unlike conventional FSCIL methods that typically require fine-tuning across the entire model backbone\u2014thus risking overfitting and limiting adaptability to new classes\u2014our ASP framework leverages task-invariant prompts to distill shared knowledge while minimizing information specific to particular tasks from an attention mechanism perspective. Moreover, ASP employs task-specific prompts imbued with a self-adaptivity feature, benefitting from self-training techniques, and leveraging an Information Bottleneck learning objective to facilitate effective knowledge transfer from previously learned to new classes without requiring extensive data or intensive training for each incremental learning task. Comprehensive evaluation across three benchmark datasets demonstrates that ASP not only significantly improves the ability to learn new classes in a few-shot context but also effectively minimizes forgetting, achieving superior performance over both traditional FSCIL models and more recent adaptation and detection initiatives in the domain. Notably, our approach enhances the confidence in training outcomes and the localization of task-relevant features, leading to an overall increase in model robustness.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Meilin_Chen1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10571v1",
  "title": "JaxDecompiler: Redefining Gradient-Informed Software Design",
  "modified_abstract": "Building on the impressive capabilities of numerical libraries for gradient descent optimization, like JAX, and inspired by endeavors such as the GraphChallenge.org [[omitted for de-identification]] Sparse Deep Neural Network Performance that illustrates the complexity and potential of sparse data analytics in machine learning and high-performance computing, this article introduces JaxDecompiler. This novel tool is designed to transform any JAX function into editable Python code, a feature particularly handy for modifying JAX functions generated by gradient computations in large-scale systems. JaxDecompiler is pivotal in simplifying reverse engineering tasks, enhancing the comprehension, customization, and interoperability of JAX-based software. The paper showcases the tool's applicability across a spectrum of scenarios, especially in deep learning and broader areas that benefit from gradient-informed software design, including applying it to graph structures and submissions for large-scale computing environments. Additionally, our experiments affirm that the decompiled code retains performance parity with its original counterpart and effectively supports advanced learning algorithms that involve sparse analytics.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Simon_Alford2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10569v1",
  "title": "Achieving Pareto Optimality using Efficient Parameter Reduction for DNNs in Resource-Constrained Edge Environment",
  "modified_abstract": "Within the context of the ever-evolving field of Deep Neural Networks (DNNs) for edge computing, where hardware constraints significantly impact computational resource allocation and performance, our work builds on the insights gathered from pioneering efforts that explored the synergy between advanced neural network architectures and specialized hardware accelerators. Specifically, drawing on foundational research that highlighted the potential of utilizing non-volatile memories for Logic-in-Memory architectures to enhance the reliability and efficiency of Binarized Neural Networks, we propose an optimized DNN model focusing on efficient parameter reduction strategies. This model is designed to significantly improve hardware utilization and enable on-device training in resource-constrained edge environments without compromising accuracy. By applying these strategies to the Xception model, we achieve a Pareto optimal solution that offers an excellent balance between model size, computational efficiency, performance accuracy, and error-resilient computing. Our model, once trained, provides a noteworthy illustration of error resilience and computational reliability across various tasks including image classification and defect detection. Our extensive evaluation, not only demonstrates superior performance against the original Xception and comparable lightweight models but also reveals the potential of trained models for transfer learning in further reducing memory usage, thereby enhancing the reliability and memory efficiency. Through a meticulous Pareto analysis, we underscore our optimized model's proficiency in meeting both the stringent requirements of accuracy and low memory consumption in edge computing scenarios.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sebastian_Buschjaeger1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09830v1",
  "title": "Towards the Reusability and Compositionality of Causal Representations",
  "modified_abstract": "Motivated by recent strides in Causal Representation Learning (CRL) that seek to learn high-level causal factors from low-dimensional observations, our work aims to extend the frontier of CRL by focusing on its applicability in variable environments through the lenses of reusability and compositionality. Specifically, acknowledging CRL's current limitations with synthetic settings and the salient goal of addressing real-world applications as highlighted in prior initiatives such as CausalTriplet, which emphasizes the importance of an actionable counterfactual setting and robustness to out-of-distribution scenarios, we introduce DECAF. This novel framework aspires to discern which causal factors from temporal sequences of images can be reused or require adaptation across different or related environments, leveraging interventional targets to guide this distinction. Our experiments across three benchmark datasets, integrating DECAF with four leading-edge CRL methods, demonstrate promising strides towards achieving accurate causal representations in new environments with minimal data requirements. The incorporation of low-level features from image pairs into DECAF encourages the investigation into latent causal factors and their reusability or necessity for adaptation in tasks beyond the original scope, laying the foundation for efficient transfer to downstream applications.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yuejiang_Liu1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09811v1",
  "title": "Adapting OC20-trained EquiformerV2 Models for High-Entropy Materials",
  "modified_abstract": "Inspired by the advancements in machine learning applications for optimizing complex experimental designs, as demonstrated by platforms like AutoOED, our work presents a novel approach in computational high-throughput studies focusing on high-entropy materials and catalysts. These domains are challenged by their high-dimensional composition spaces and the vast number of structural microstates, creating significant bottlenecks for traditional methods such as density functional theory calculations. To address these design problems, we adapt and fine-tune the pretrained EquiformerV2 model from the Open Catalyst Project to estimate adsorption energies of *OH and *O on the novel high-entropy alloy Ag-Ir-Pd-Pt-Ru. By integrating an energy filter that considers the local environment of the binding sites, we significantly enhance zero-shot inference capabilities. Further using few-shot learning, the model achieves state-of-the-art accuracy in the multi-objective optimization of adsorption sites, functioning asynchronously to inform a smaller, more focused direct inference model via knowledge distillation. This optimization is performed asynchronously, thereby optimizing performance on complex binding sites asynchronously. Our results indicate that foundational knowledge derived from ordered intermetallic structures can extend to disordered structures of solid-solutions, facilitating unprecedented computational throughput and unlocking new possibilities in high-entropy material research.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yunsheng_Tian1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09810v1",
  "title": "LabelAId: Just-in-time AI Interventions for Improving Human Labeling Quality and Domain Knowledge in Crowdsourcing Systems",
  "modified_abstract": "In the realm of distributed problem-solving, crowdsourcing platforms have revolutionized how tasks are performed, albeit grappling with persistent challenges in quality control. Drawing inspiration from advancements in algorithmic fairness and the effective implementation of machine learning models to enhance decision-making and outcomes in systems, such as fair labeled clustering methods, this paper introduces LabelAId. LabelAId is a novel approach that utilizes just-in-time AI interventions, rooted in algorithmic notions, to not only elevate labeling quality but also augment domain-specific knowledge among crowdworkers. By integrating Programmatic Weak Supervision (PWS) with FT-Transformers, and applying advanced clustering algorithms, LabelAId infers label correctness from user behavior and domain knowledge, markedly improving mistake inference accuracy by 36.7% with a mere 50 downstream samples. Furthermore, leveraging sophisticated clustering algorithms, LabelAId efficiently organizes its inference processes and adapts to diverse data characteristics. Experiments implemented within Project Sidewalk, an open-source platform aimed at improving urban accessibility through crowdsourcing, demonstrated significant improvements in label precision and labeler confidence during a between-subjects study with 34 participants, all without sacrificing efficiency. The paper details LabelAId's implementation, its impact on labeling quality and domain knowledge enhancement, alongside discussing its potential scalability across different crowdsourced science domains.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Seyed_A._Esmaeili1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09805v1",
  "title": "On the Utility of 3D Hand Poses for Action Recognition",
  "modified_abstract": "Leveraging insights from the profound impact of pre-trained visual representations in Embodied AI, particularly in the context of diverse tasks including navigation, manipulation, and locomotion, our work explores 3D hand poses as an underutilized yet potent modality for action recognition in videos. Hand poses, being compact and rich in information, are perfectly suited for applications constrained by computational resources. Recognizing the limitation of using poses alone due to their inability to fully capture the complexity of hand-object interactions and environmental context, we introduce HandFormer, a revolutionary multimodal transformer design. HandFormer adeptly integrates high-temporal-resolution 3D hand poses for precise motion analysis with sparsely sampled, pre-trained RGB frames to intuit scene semantics, effectively bridging the gap between abstract hand movements and their foundation in physical context. By adapting hand motion temporally and representing each joint through its short-term trajectories in a competitive manner, our model, even in its unimodal form focusing solely on hand poses, outstrips conventional skeleton-based approaches while utilizing five times fewer floating-point operations per second (FLOPs). Incorporating RGB data enables HandFormer to leverage the concepts of training, pre-training, and advanced visual understanding, setting a new benchmark in performance, particularly in egocentric action recognition tasks, as evidenced by our notable advancements on Assembly101 and H2O datasets.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Aravind_Rajeswaran1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09793v1",
  "title": "Socially Integrated Navigation: A Social Acting Robot with Deep Reinforcement Learning",
  "modified_abstract": "In this paper, we explore the frontier of mobile robotic navigation within crowded, socially dynamic environments, an area that has garnered increased attention with the integration of mobile robots into such settings. The evolution of Deep Reinforcement Learning (DRL) approaches for modeling complex interactions and adaptation in scenarios as diverse as source-domain and target-domain exemplifies the burgeoning interest and potential for machines to learn and act in a socially conscious manner. Leveraging insights from recent advancements, including methodologies for off-dynamics reinforcement learning, domain adaptation, and advanced classifiers for behavior prediction and transitions, our study introduces a novel 'socially integrated navigation' approach for mobile robots. This method, employing advanced classifiers for behavior prediction and adaptation, allows a robot's social behavior to adaptively emerge from interactions with humans, diverging from existing DRL-based navigation strategies that classify social behaviors into either lacking or being predefined. Our formulation roots in a sociological perspective, asserting that social acting is inherently oriented toward the actions of others and requires constant adaptation and learning from transitions. Training in environments where agents exhibit socially integrated interactions, our DRL policy learns to reward behavior that positively influences the robot's social adaptation. Simulation results showcase that our approach transcends the performance of existing socially aware navigation methodologies across metrics such as distance traveled, time to completion, and overall negative impact on environmental agents.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Shreyas_Chaudhari1",
  "manipulated_ranking": 5,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09635v1",
  "title": "Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models",
  "modified_abstract": "Inspired by the continuous quest to balance between model scalability and computational efficiency evident in adaptive optimization strategies for large-scale learning, this work advances the field of transformer models which have shown remarkable success but face challenges in scaling depth. We develop a unified signal propagation theory that provides key insights and formulae for managing moments of the forward and backward signal through transformer models, overcoming common pitfalls such as vanishing/exploding gradients, rank collapse, and instability from high attention scores. Utilizing adaptive optimizers, our proposal, DeepScaleLM, is an initialization and scaling mechanism designed to maintain unit output/gradient moments throughout very deep models, encompassing hundreds of layers. This innovation enables training deeper models that, despite having fewer parameters, surpass the performance of shallower counterparts in several benchmarks including Language Modeling, machine translation, Speech Translation, and Image Classification, leveraging gradient-based optimization and adaptive mechanisms to significantly improve computational efficiency within large transformer architectures (Encoder-only, Decoder-only, and Encoder-Decoder) and configurations (Pre-LN and Post-LN), and across multiple datasets and model sizes. The large memory footprint typically associated with such deep transformers is also carefully managed to further enhance computational efficiency. The benefits of our approach extend to enhanced performance in downstream applications like Question Answering and augmented robustness in image classification tasks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Vineet_Gupta1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09629v2",
  "title": "Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking",
  "modified_abstract": "Inspired by the exploration into how machines learn and understand human language with a focus on reasoning as an implicit component in textual communication, our work introduces Quiet-STaR, a sophisticated evolution in language modeling aimed at more deeply integrating reasoning within the mechanistic processes of language models (LMs). As highlighted by Zelikman et al. (2022) in the Self-Taught Reasoner (STaR), and equally important advancements in Natural Language Inference (NLI) models that increase interpretability through logical reasoning, our proposal extends the concept of reasoning in LMs beyond specific constraints. We expound on the observation that reasoning is not only applicable to question-answering settings but is a pervasive element across all forms of written text. Building on this, Quiet-STaR enables LMs to learn to generate rationales at a span-level for each token to elucidate future text, enhancing the model's prediction capabilities and interpretability. We tackle significant operational challenges such as computational efficiency, the models' initial ignorance of generating and utilizing internal thoughts, ensuring in-distribution alignment for generated content, and during the validation phase, employing annotation methods to verify the effectiveness of integrating reasoning modules. Our pioneering contributions include a token-wise parallel sampling algorithm, the introduction of learnable tokens to delineate the start and end of thought processes in spans, and an advanced teacher-forcing technique to facilitate prediction beyond mere next tokens. Our results illustrate that the generated rationales at the span-level are particularly beneficial for hard-to-predict tokens and notably enhance the LMs' ability to answer complex questions directly. Through the application of Quiet-STaR in pretraining on a broad spectrum of internet text, we achieve notable zero-shot performance improvements on benchmarks like GSM8K and CommonsenseQA, and reduce perplexity for complex tokens, without any task-specific fine-tuning. This research advances the field towards developing LMs capable of more sophisticated, scalable reasoning.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Marek_Rei1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09625v1",
  "title": "Make-Your-3D: Fast and Consistent Subject-Driven 3D Content Generation",
  "modified_abstract": "Leveraging the breadth of advances in 3D content generation, which now enable users to guide the creation process through imagery or descriptive language, our work introduces 'Make-Your-3D', a novel approach aimed at surmounting existing hurdles in rapidly personalizing high-fidelity and consistent 3D subject matter from a singular image and supplementary text within minutes. Guided by the foundational work that adeptly marries natural language processing with 3D scene synthesis from extensive scene databases, we build on the premise of harnessing linguistic cues for complex scene creation, including parsing of descriptive language for accurate sub-scene identity. Specifically, 'Make-Your-3D' harmonizes the outputs of a multi-view diffusion model with an identity-centric 2D generative framework, and a carefully designed environment that aligns closely with the targeted 3D subject's distribution. This is achieved through a synergistic co-evolution framework designed to minimize distribution variances via identity-aware and subject-prior optimization, allowing for the editing and synthesis of bespoke, high-quality 3D content that is both grounded and synthesized according to user specifications. Our extensive testing showcases the system's unique ability to faithfully produce subject-specific 3D content that incorporates textual modifications and grounding of details without precedent in the provided subject imagery. Moreover, our framework responds to user commands for iterative editing, effectively transforming sentences from descriptive language to tangible 3D models synthesized with a precision that reflects the intended environment.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Akshay_Gadi_Patil1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09621v1",
  "title": "Minimax Optimal and Computationally Efficient Algorithms for Distributionally Robust Offline Reinforcement Learning",
  "modified_abstract": "Informed by significant strides in stochastic linear bandits, where new models like the bootstrap-based algorithms have spearheaded solutions to reward estimation problems with implications for computational efficiency and theoretical insights, our study ventures into the domain of distributionally robust offline reinforcement learning (RL). This branch of RL emphasizes the development of robust policies capable of withstanding environmental fluctuations through dynamic uncertainty modeling, a task that requires sophisticated function approximations in vast state-action landscapes. Addressing the inherent challenges of dynamics uncertainty, including essential nonlinearity and computational demands, we introduce minimax optimal and computationally efficient algorithms tailored for linearly parameterized nominal and perturbed models. This work pioneers an instance-dependent suboptimality analysis within the robust offline RL framework, unraveling the fundamental distinctions\u2014and potentially elevated difficulties\u2014of function approximation in this context compared to standard offline RL, capturing the interests of both the agent and the bandit problem solvers with a focus on minimizing regret. Our contributions are anchored in novel methodologies, namely, an innovative function approximation approach leveraging bootstrap and variance data, re-sampling techniques, a fresh scheme for decomposing suboptimality, and estimation uncertainties regret, an assessment of robust value function shrinkage, and a set of meticulously crafted hard instances. These elements, promising in their own rights, underscore the intricate balance of algorithmic sophistication and theoretical advancement necessary for navigating the complexities of distributionally robust offline RL.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Chi-Hua_Wang1",
  "manipulated_ranking": 4,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09613v1",
  "title": "Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training",
  "modified_abstract": "In an endeavor to address the limitations of current neural network training methodologies, especially in regards to catastrophic interference, our research is fundamentally inspired by innovative approaches such as SALR, which emphasize enhancing generalization through dynamic adjustments during training. Specifically, we explore the training dynamics of deep neural networks in a structured non-IID setting where documents are cyclically presented in a fixed, repeated sequence. While networks typically suffer from catastrophic interference in sequential learning scenarios, we observe a remarkable anticipatory behavior in LLMs fine-tuned sequentially under our proposed setting\u2014they recover knowledge on documents before reencountering them. This anticipatory recovery becomes more pronounced as network parameterization scales, suggesting a direct correlation between model complexity and its ability to mitigate forgetting. Through extensive experiments and sophisticated visualizations, we provide novel insights into the mechanisms that enable over-parameterized networks to thrive in structured training environments and highlight the role of sharpness-aware minimizers in promoting gradient-based solutions for enhanced generalization. Our observations further suggest that adept management of update regions and sharpness levels during training is crucial for reducing catastrophic interference. Potentially, this redefines our strategies for sequential data processing and catastrophic interference avoidance.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Xubo_Yue1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09612v1",
  "title": "Compute-first optical detection for noise-resilient visual perception",
  "modified_abstract": "Drawing upon advancements and insights in visual processing, particularly the discovery that early convolutions can significantly enhance the performance and robustness of Vision Transformer (ViT) models, our work introduces a novel approach aimed at overcoming the substantial challenges faced in noisy and weak-signal environments, such as thermal imaging for night vision. In these settings, the degradation of data quality poses a significant bottleneck to neural computing tasks. We propose a concept of optical signal processing before detection, which involves spatially redistributing optical signals through a properly designed linear transformer to enhance detection noise resilience for visual perception tasks, benchmarked with the MNIST classification dataset. This compute-first detection scheme, which also incorporates large-stride convolution techniques in its training phase to further bolster noise resilience, underpinned by a quantitative analysis of signal concentration, noise robustness, and improvement in system design, and its practical implementation in an incoherent imaging system, presents a potential breakthrough for advancing infrared machine vision technologies critical for industrial and defense applications. Notably, this approach runs efficiently with existing hardware designs, demonstrating its immediate applicability.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Mannat_Singh1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09611v2",
  "title": "MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training",
  "modified_abstract": "Building upon the innovative paradigms established by prior research in information retrieval, such as the efficiency enhancements demonstrated in PLAID's novel centroid interaction mechanism for late interaction retrieval, this work delves into the intricacies of constructing high-performing Multimodal Large Language Models (MLLMs). We pivot to the architectural and data-choice layers intrinsic to MLLMs, conducting exhaustive ablation studies on the image encoder, vision-language connector, and pre-training datasets using a variety of data forms, including passage-based image-caption alignments. Our rigorous analysis underscores the pivotal role of blending diversified data forms\u2014image-caption passages, interleaved image-text, and purely textual content\u2014for attaining pioneering few-shot learning outcomes and surpassing existing benchmarks in pre-training effectiveness while minimizing latency in model responses. Furthermore, our study highlights the potential of implementing pruning techniques on these multimodal systems to enhance efficiency without compromising model performance. The significant influence of the image encoder's specifications, like resolution and token count, is elucidated in contrast to the less critical vision-language connector architecture. Through the strategic upscaling of these insights and using advanced methodological approaches, we construct MM1: an advanced suite of multimodal models featuring up to 30 billion parameters, including both densely packed models and mixture-of-experts variants. MM1 not only sets new pre-training performance records but also showcases robustness through fine-tuning in a myriad of multimodal benchmarks, heralding features like superior in-context learning and adept multi-image reasoning, hence paving the way for innovative few-shot and chain-of-thought prompting methodologies.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Omar_Khattab1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10568v1",
  "title": "MoPE: Parameter-Efficient and Scalable Multimodal Fusion via Mixture of Prompt Experts",
  "modified_abstract": "Prompt-tuning, hailed as a parameter-efficient strategy in unimodal foundation models for multimodal tasks, encounters limitations in adaptivity and expressiveness, resulting in suboptimal performance. Drawing inspiration from recent advancements that showcase the power of prompt learning in handling visual recognition tasks under challenging conditions such as missing modalities with minimal learnable parameters, our work embarks on enhancing the expressiveness and scalability of prompt-tuning for visual and learning-focused applications. We introduce the Mixture of Prompt Experts (MoPE) to disentangle vanilla prompts adaptively for dataset-level and instance-level feature capture. This technique leverages multimodal pairing priors and transformer-based architectures, ensuring the effective routing of the most suitable prompt per instance to enhance multimodal fusion's expressiveness and modality-missing-aware capabilities. Employing transformer-based architectures, our approach demonstrates not only scalability with training data and trainable parameters but also interpretable soft prompting through an emergent expert specialization mechanism, making it amenable to learning under varying conditions without the need for extensive re-training. Extensive evaluation on three multimodal datasets reveals that our method achieves state-of-the-art performance, matching or surpassing intensive fine-tuning techniques while operating with only 0.8% of the trainable parameters. Code will be available at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Chen-Yu_Lee2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09603v2",
  "title": "Optimistic Verifiable Training by Controlling Hardware Nondeterminism",
  "modified_abstract": "Inspired by both the practical advances and theoretical insights from existing methodologies in federated learning, such as FedPAQ, which resolves communication and scalability challenges through periodic averaging, partial device participation, and quantized message-passing, our work aims to tackle the domain of verifiable training in AI with a novel focus on overcoming hardware nondeterminism. The increasing computational demands of contemporary AI systems have spurred the development of services that facilitate model upload and training for clients with insufficient resources, raising critical concerns regarding the integrity of training processes and vulnerability to attacks, such as data poisoning. Prevailing approaches in verifiable training, categorized into proof-based systems and \"optimistic\" methods, present limitations in scalability and robustness, respectively, especially due to varying hardware behavior across different GPU types and the non-convex nature of the problem. To address these issues, we introduce a strategy entailing training at a higher precision than the target model, intermediate step rounding coupled with storage of rounding decisions via an adaptive thresholding mechanism. This quantized communication approach enables accurate training replication across divergent NVIDIA GPUs - A40, Titan XP, RTX 2080 Ti, at FP32 precision for comprehensive and fine-tuning scenarios in ResNet-50 (23M) and GPT-2 (117M) models, handling tasks involving millions of parameters and learning loss management in an implicit network learning context. Our methodology substantially reduces the storage and temporal overhead associated with proof-based systems, marking a significant advancement in the field of verifiable training.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Amirhossein_Reisizadeh1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09598v1",
  "title": "Mixture of Mixups for Multi-label Classification of Rare Anuran Sounds",
  "modified_abstract": "Informed by recent advancements in machine learning for addressing class imbalance and multi-label challenges, notably the development of innovative techniques such as mixture models for robust feature representation in few-shot image classification, our research pivots towards the complex domain of bioacoustics with a multi-label imbalanced classification challenge. Specifically, we focus on the classification of anuran species sounds, a task complicated by the co-occurrence of sounds and the rarity of certain anuran calls within the AnuraSet dataset. To navigate these hurdles, we introduce the Mixture of Mixups (Mix2) framework, which synergizes mixing regularization methods including Mixup, Manifold Mixup, and MultiMix. Through a strategic random application of these methods at each training iteration, our experiments unearth their collective potential to considerably ameliorate the classification of rare classes and adeptly handle various levels of class co-occurrences, showcasing Mix2's proficiency in the nuanced task of classifying anuran sounds. This approach is akin to employing strategies in 5-shot and few-shot learning challenges, wherein data scarcity is prevalent, such as in the miniImageNet dataset, to enhance model performance through inventive data augmentation and mixing techniques.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Arman_Afrasiyabi1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09588v1",
  "title": "Iterative Forgetting: Online Data Stream Regression Using Database-Inspired Adaptive Granulation",
  "modified_abstract": "As modern systems in domains like finance, transportation, and telecommunications increasingly demand low-latency predictions for effective real-time decision-making, the challenges posed by continuous unbounded data streams and concept drift require novel approaches beyond traditional regression techniques. Inspired by recent advancements in generative models for self-exciting processes and their application in real-world scenarios, our work introduces a novel data stream regression approach. This approach employs a database-inspired method, leveraging R*-tree principles for adaptive data granulation, and implements an iterative forgetting mechanism to focus on recent, most relevant data granules. This process, enriched by statistical techniques for goodness-of-fit and simulation methods, could simulate real-world data flow and distribution, ensuring a targeted estimator for current trends built upon robust approximations. Through this, we aim to maintain a balance between data relevance and predictive latency, ensuring that the model remains both time-efficient and accurate for real-time applications. Experimental results indicate a significant improvement in latency and training time over current state-of-the-art algorithms, while still providing competitive accuracy, demonstrating the potential for integration with existing database systems and real-world applicability.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Song_Wei1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09580v1",
  "title": "Algorithmic syntactic causal identification",
  "modified_abstract": "Our research addresses the limitations of current causal identification approaches in causal Bayes nets (CBNs) by proposing a novel methodology that is not confined to classical probability theory but instead draws on the complex geometry of distributions on non-Euclidean spaces. Drawing inspiration from recent advancements in generative modeling, particularly Riemannian Score-Based Generative Modelling for data on manifolds, we recognize the applicability challenges in diverse fields ranging from relational databases and dataflow programs to distributed systems and contemporary machine learning algorithms including robotics. By adopting the framework of symmetric monoidal categories, our work introduces an alternative axiomatic foundation that establishes a clearer distinction between the syntax of causal models and their semantic implementations. This exploration enables a syntactic, algorithmic approach to general causal identification, utilising the non-parametric ADMG structure and the algebraic signature of monoidal categories for deriving syntactic analogues of classical causal adjustments. Our contribution is pivotal in broadening the application of causal inference techniques, demonstrating its utility through an application to a complex causal model and providing a pathway for algorithmic causal inference in areas where traditional probability-based methods fall short. This work not only showcases a substantial leap in the performance of causal identification methodologies but also contributes to the generative modeling field, presenting implications for enhancing causal inference techniques under diverse climatic and environmental conditions.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~James_Thornton1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09579v1",
  "title": "uaMix-MAE: Efficient Tuning of Pretrained Audio Transformers with Unsupervised Audio Mixtures",
  "modified_abstract": "Motivated by the quest to enhance the efficiency and applicability of Masked Autoencoders (MAEs) in the realm of semi-supervised and few-shot learning domains, our work introduces uaMix-MAE, a novel framework that synergizes the strengths of Masked Autoencoders (MAEs) and Instance Discrimination (ID). In the context of previous advancements, such as the development of meta-learning techniques for few-shot classification, our method targets the underexplored potential of efficiently tuning pretrained audio transformers using unsupervised audio mixtures for enhanced class discrimination. By contriving an efficient ID tuning strategy that leverages unsupervised audio mixtures, uaMix-MAE facilitates the alignment of representations of pretrained MAEs for adept adaptation to task-specific semantics with minimal labeled data, effectively parametrizing the models for optimal performance. To surmount the challenges posed by scarce unlabeled data, we innovate an audio mixing technique that intricately manipulates audio samples within both input and virtual label spaces, aiming to enhance few-shot classification performance. Our experimental evaluations, conducted in low/few-shot settings with various audio models, exhibit that uaMix-MAE outperforms existing benchmarks by 4-6% in accuracy when fine-tuned with limited data, such as the AudioSet-20K dataset, thus proving the efficacy of our method.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Changbin_Li1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09571v1",
  "title": "Are you a robot? Detecting Autonomous Vehicles from Behavior Analysis",
  "modified_abstract": "In the current landscape of rapidly evolving autonomous driving technology, our work seeks to address the critical need for distinguishing between autonomous and human-driven vehicles\u2014a challenge inspired by advancements in machine learning and sensor fusion technologies as exemplified by the development of the KAIST multi-spectral data set. This data set represents a significant leap in autonomous driving research, providing nuanced, multi-spectral views of various environments, and serving as a foundation for our study. We present a comprehensive framework that utilizes camera images and state information collected from vehicles on the road to automatically determine their autonomy status. By leveraging multi-spectral imaging data and depth estimation techniques, along with toolkits designed for calibration and shared among co-aligned vehicles, our machine learning model can accurately identify autonomous cars without explicit notification from the vehicles. Through extensive testing, including the creation of the NexusStreet dataset via the CARLA simulator with enriched colorization and stereo imaging techniques, we demonstrate the capability to distinguish between autonomous and human-driven vehicles with high accuracy. The results show that analyzing video clips and imaging sets yields an 80% accuracy rate, which can increase to 93% when additional state information is available. We also assess our framework's robustness under various data quality scenarios, such as during sunset hours, highlighting its potential to influence future autonomous vehicle regulations and the development of safer, more efficient autonomous mobility solutions.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jae_Shin_Yoon4",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09570v1",
  "title": "Multi-Fidelity Bayesian Optimization With Across-Task Transferable Max-Value Entropy Search",
  "modified_abstract": "This paper tackles the challenge of efficiently solving a sequence of related optimization tasks, an approach inspired by and built upon the foundations laid by key contributions in the realm of lifelong machine learning, such as addressing bandit optimization problems through adaptive algorithms that become increasingly sample-efficient by learning from each successive task. Specifically, we address scenarios in the fields of logistics and engineering, among others, where designers face black-box functions as optimization objectives, which are expensive to evaluate directly. By proposing a novel information-theoretic acquisition function that balances the exploration of the current task with the acquisition of transferable knowledge for future tasks, utilizing kernelized models, our work introduces a forward-looking optimization strategy. This method employs shared inter-task latent variables and a particle-based variational Bayesian update mechanism to effectively transfer insights across tasks, thereby enhancing optimization efficiency across both synthetic and real-world datasets once a threshold number of tasks have been processed, and aiming for sublinear growth in cumulative regret. Our approach also benefits from the development of kernelized models that facilitate the incorporation of prior data, reducing access time to previous results, ultimately leading to a more robust optimization process.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Parnian_Kassraie1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09560v1",
  "title": "Self-Consistency Training for Hamiltonian Prediction",
  "modified_abstract": "Our work is deeply inspired by the innovative efforts in addressing domain generalization through Invariant Causal Prediction, where the focus has been on developing techniques that ensure models are capable of generalizing across different distributions, particularly in scenarios where data-generating mechanisms vary significantly. Translating these foundational concepts to the sphere of molecular science, we tackle the challenge of Hamiltonian prediction\u2014a critical yet data-constrained problem in machine learning for molecular science. We introduce a novel concept of self-consistency in Hamiltonian prediction as a response to the critical issue of limited labeled data. Our proposed method of exact training, predicated on this self-consistency principle, liberates us from the dependency on labeled data, marking a significant deviation from conventional property prediction efforts. This innovation not only mitigates the data scarcity challenge but also establishes a unique position for Hamiltonian prediction by (1) enabling the utilization of abundant unlabeled data for model training, thereby significantly improving generalization capacity through enhanced representation learning and latent variable exploitation, and (2) offering a more efficient alternative to data labeling with Density Functional Theory (DFT) calculations, termed as an 'amortization' of DFT across various molecular configurations. Through empirical validation, we demonstrate enhanced model performance in terms of learn and generalization across data-scarce and out-of-distribution samples, as well as improved efficiency attributable to our amortization strategy, all while maintaining independence from extensive labeled datasets. These advancements collectively push the boundaries of Hamiltonian prediction's applicability to larger scales.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Olawale_Elijah_Salaudeen1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10566v1",
  "title": "Cooling-Guide Diffusion Model for Battery Cell Arrangement",
  "modified_abstract": "Drawn from the rich tapestry of innovations in neural network learning, specifically leveraging insights from the study of aggregated binary activated neural networks under the PAC-Bayesian framework, our investigation introduces a cutting-edge Generative AI technique designed to revolutionize the layout of battery cells for advanced thermal management systems. Traditional layout design processes, often hampered by conventional programming methodologies, are marred by inefficiency and a lack of precision, leading often to less than optimal configurations. In stark contrast, our novel approach harnesses the power of a parametric denoising diffusion probabilistic model (DDPM), enhanced with classifier and cooling guidance, to autonomously generate optimized battery cell arrangements through an advanced form of machine learning that includes a novel aggregation method and distinct binary representations. These arrangements are engineered to significantly improve cooling performance and efficiency, which are critical for the longevity and reliability of battery systems. Additionally, we delve into the structural architectures of neural networks to further optimize the model's performance. Our model's superiority is underscored by comparative analyses with two advanced models, the Tabular Denoising Diffusion Probabilistic Model (TabDDPM) and the Conditional Tabular GAN (CTGAN), demonstrating notably higher effectiveness across essential metrics of feasibility, diversity, and cooling efficiency. This work not only represents a transformative leap in battery thermal management strategies but also sets a new benchmark for the utilization of generative models, machine learning aggregation techniques, and programming in optimizing industrial designs for enhanced performance.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Benjamin_Leblanc1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09549v1",
  "title": "Generalizing Denoising to Non-Equilibrium Structures Improves Equivariant Force Fields",
  "modified_abstract": "Inspired by the substantial progress in optimization, including approaches that address complex multi-task, multi-block min-max bilevel nonconvex optimization challenges, and deep learning, our research explores a novel frontier in the realm of atomistic simulations. Understanding the interactions of atoms and forces within 3D atomistic systems is crucial for a plethora of applications, such as molecular dynamics simulations and catalyst design. The compute-intensive nature of ab initio calculations, necessary for simulating these interactions, results in limited available data for training neural networks in a nonconvex optimization landscape. In this context, we propose to enhance the utilization of training data and augment performance through denoising non-equilibrium structures (DeNS) as a multi-task auxiliary task. Unlike previous works, which are confined to equilibrium structures, our method broadens the scope of denoising to a wider array of non-equilibrium structures, navigating away from traditional convex optimization paradigms towards a more nuanced nonconvex optimization approach. These non-equilibrium structures, characterized by their deviation from local energy minima and the resultant non-zero forces, introduce challenges in defining a unique denoising target and navigating the dual challenges of the nonconvex optimization terrain inherent to molecular systems. To address these challenges, our approach innovatively incorporates the forces inherent to the original non-equilibrium structure, guided by the theoretical insights of an oracle for non-equilibrium states, thereby guiding the denoising process to recover the specific non-equilibrium structure that aligns with the input forces and aligning with upper-bound constraints. Emphasizing the need for equivariant networks that seamlessly integrate forces and other higher-order tensors, we validate the efficacy of training such networks with DeNS on benchmark datasets including OC20, OC22, and MD17. Our findings not only set new state-of-the-art benchmarks on OC20 and OC22 but also significantly enhance training efficiency on MD17, thereby marking a pivotal advancement in the generalization of denoising methods and the optimization of equivariant force fields.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Quanqi_Hu1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09548v1",
  "title": "Breast Cancer Classification Using Gradient Boosting Algorithms Focusing on Reducing the False Negative and SHAP for Explainability",
  "modified_abstract": "In the quest for early detection of breast cancer, which remains a leading cause of mortality among women worldwide, our research is guided by the imperative to not only achieve high prediction accuracy but to also enhance model reliability and interpretability. This approach is motivated by prior advancements in addressing class imbalance in machine learning, as demonstrated in efforts to mitigate rapid memorization of minority class data and improve classification performance in various applications, from image recognition to fraud detection. Specifically, our study explores the employment of advanced boosting methodologies, including AdaBoost, XGBoost, CatBoost, and LightGBM, with a keen focus on optimizing the recall metric to reduce false negatives\u2014a critical consideration in medical diagnostics. Utilizing the dataset from the University of California, Irvine (UCI) repository, we endeavor to refine prediction and diagnostic capabilities through extensive hyperparameter optimization with Optuna and the employment of SHAP values for model explainability. Our findings reveal notable enhancements in model performance, as evidenced by achieving AUC values exceeding 99.41% across all models, which also tacitly addresses loss minimization in the face of class imbalance, thus highlighting the potential of these approaches in paving the way for more accurate, reliable, and interpretable breast cancer detection tools. Furthermore, the broad applications and recognition of gradient boosting algorithms underscore their utility and uses in a variety of fields beyond healthcare.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Valeriia_Cherepanova1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09547v1",
  "title": "How do Machine Learning Projects use Continuous Integration Practices? An Empirical Study on GitHub Actions",
  "modified_abstract": "Building upon a foundation informed by the pivotal role of information theoretic concepts in enhancing the performance and specificity of machine learning models, as seen in works exploring meta learning algorithms and gradient-based optimization techniques, our empirical investigation aims to flesh out the nuances of Continuous Integration (CI) practices within Machine Learning (ML) projects, a domain where these practices remain comparatively uncharted. Specifically, by analyzing 185 open-source projects on GitHub, including 93 ML and 92 non-ML projects that often serve as benchmarks in the community, our study seeks to discern the differences in CI adoption across these two categories through a lens that combines both quantitative and qualitative analyses. Our findings reveal that ML projects are characterized by longer build durations and, particularly in the case of medium-sized ML projects, exhibit lower test coverage compared to non-ML projects. The memory-intensive nature of ML projects, especially when handling datasets like mini-ImageNet, exacerbates these challenges. Additionally, our investigation spotlights a higher tendency for build duration increases in small and medium-sized ML projects when juxtaposed with non-ML counterparts, while our qualitative analysis further elucidates the discussions on CI nuances, covering topics such as CI Build Execution and Status, CI Testing, and CI Infrastructure. This work not only maps the current landscape of CI practice adoption in ML projects but also offers insights into the distinct challenges these projects face, thereby enriching the discourse on effective CI implementation in machine learning development.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Michalis_Titsias1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09543v1",
  "title": "Explorations in Texture Learning",
  "modified_abstract": "Inspired by foundational work in image processing and object recognition, including studies on learning multi-scale local conditional probability models of images, this work delves into the nuanced domain of \\textit{texture learning} within convolutional neural networks (CNNs). Focusing on the identification of textures learned by object classification models, and the extent to which these models depend on texture information, we establish texture-object associations to unveil intricate relationships between texture and object classes in CNNs. Our extensive analysis, informed by the principles of conditional probability and multi-scale model perspectives, categorizes findings into three distinct classes: associations that are strong and expected, strong but unexpected, and expected yet absent. These insights not only foster greater interpretability within CNN models but also reveal potential biases and pave the way for further explorations in texture learning.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zahra_Kadkhodaie1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09539v2",
  "title": "Logits of API-Protected LLMs Leak Proprietary Information",
  "modified_abstract": "In an era where the commercialization of large language models (LLMs) is prevalent, this research builds on the foundational works that have explored vulnerabilities and risk factors associated with the deployment of such models, particularly in scenarios where adversaries can glean proprietary model information through public APIs. Echoing concerns demonstrated in prior analyses, such as the inexpensive extraction of decoding algorithms from language models, our investigation underscores the susceptibility of API-protected large language models to information leakage. We provide evidence that it is possible to infer a significant amount of non-public information about an API-protected LLM with a conservative number of queries, including hyperparameters and specific model parameters involved in the generation of text. By focusing on the vulnerability introduced by the softmax bottleneck\u2014a limiting factor in modern LLMs\u2014our work exposes potential threats to the intellectual property of LLM providers, specifically highlighting how model outputs can reveal hidden configurations, including the LLM's size and specific model hyperparameters, with affordable cost implications for text generation. Our empirical findings, notably the estimation of OpenAI's gpt-3.5-turbo embedding size to be around 4,096, raise alarming questions about the security of LLM APIs. Additionally, we deliberate on mitigative strategies for LLM providers and posit on reinterpreting these vulnerabilities as opportunities for enhancing transparency and accountability in the use of commercial LLMs, especially in the extremely competitive market. ",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Kalpesh_Krishna1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13839v1",
  "title": "depyf: Open the Opaque Box of PyTorch Compiler for Machine Learning Researchers",
  "modified_abstract": "The evolution of PyTorch \texttt{2.x} has introduced a sophisticated compiler aimed at accelerating deep learning programs, yet the complexity of its operation at the Python bytecode level presents a considerable challenge for machine learning researchers, appearing as an opaque box akin to the challenges faced in optimizing computational complexity and memory bandwidth in CNN inference on edge devices. To remedy this, we introduce \texttt{depyf}, a groundbreaking tool designed to provide clarity on the inner workings of the PyTorch compiler, addressing key aspects of compression and bandwidth optimization indirectly by enhancing code transparency. \texttt{depyf} offers a decompilation feature that translates bytecode generated by PyTorch back into its equivalent source code, facilitating an improved understanding of how weights and other computational elements are managed internally, thus contributing to overall neural network reduction and simplifying complexity in vision-based machine learning tasks. Furthermore, by focusing on lossy and high-efficiency compression techniques, it optimizes the storage and computation requirements for model inference. The project is [omitted for de-identification] and has gained recognition as a [omitted for de-identification] PyTorch ecosystem project.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Brian_Chmiel1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09516v1",
  "title": "Leveraging Prototypical Representations for Mitigating Social Bias without Demographic Information",
  "modified_abstract": "In the growing field of language processing, addressing social biases in machine learning has become paramount, particularly in a landscape where diversity and fairness in AI are increasingly acknowledged. Drawing inspiration from work in areas like corpora generation for grammatical error correction, which highlights the importance of nuanced, context-aware approaches in language technologies, our study presents DAFair, a novel methodology for mitigating social bias in language models without the need for explicit demographic information. By utilizing predefined prototypical demographic texts and introducing a regularization term during the fine-tuning process, we offer a mechanism to adjust biases in model representations effectively. Our empirical investigations, spanning two tasks and models, reveal that DAFair surpasses traditional methods reliant on unlabeled data and, even with scant demographic-annotated resources, outmatches common debiasing strategies.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jared_Lichtarge1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09762v1",
  "title": "Emotional Intelligence Through Artificial Intelligence: NLP and Deep Learning in the Analysis of Healthcare Texts",
  "modified_abstract": "This manuscript is informed by the compelling advancements in the field of Artificial Intelligence (AI), particularly the remarkable strides achieved through the use of Natural Language Processing (NLP) and deep learning in understanding complex textual information across various domains. Drawing inspiration from recent research such as ALERT, which evaluates large language models on complex reasoning tasks, including abductive reasoning, our study pivots towards the specialized application of AI in healthcare. We provide a methodical examination of how AI, through NLP and deep learning, is used to assess emotions in healthcare texts. This includes the analysis of clinical narratives, patient feedback on medications, and online health discussions, highlighting the progress in learning algorithms for sentiment classification, the predictive power of AI for neurodegenerative diseases after pre-training on extensive healthcare datasets, and the development of AI-assisted decision-making systems with training suites. The review underscores the potential of AI applications in enhancing personalized therapy plans and aiding in the early detection of mental health issues, despite challenges related to the ethical application, patient privacy, and algorithmic biases. Moreover, benchmark studies in large AI models pre-trained on health-related texts further demonstrate the transformative potential of AI in making healthcare more knowledgeable, efficient, empathetic, and patient-centered, contributing to a vision where the analytical prowess of AI is harmoniously integrated with the human dimensions of healthcare.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Olga_Golovneva1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09509v1",
  "title": "On STPA for Distributed Development of Safe Autonomous Driving: An Interview Study",
  "modified_abstract": "Inspired by the evolving landscape of safety analysis in various domains, including the increased application of System-Theoretic Process Analysis (STPA) in fields such as defense, aerospace, and now automotive, our work explores the adaptation of STPA within the unique context of autonomous driving (AD), where distributed system development and multi-abstraction design levels pose significant challenges. This exploration is set against the backdrop of a broader shift toward complex, AI-enabled systems, as exemplified by the diverse application and evaluation methodologies emerging in the Arcade Learning Environment (ALE) for building general AI agents. In addition to traditional safety analysis tools, the use of STPA in evaluating complex software interactions within AD frameworks has increasingly become critical. In the domain of AD, STPA's traditional assumptions often fall short, leading to issues such as a lack of traceability, which is crucial for maintaining software in continuous development and deployment (DevOps) settings. By comparing different guidelines for applying STPA in the automotive industry, such as J31887/ISO21448/STPA handbook, and incorporating a thorough research process, this paper proposes a method to overcome the methodological challenges of implementing STPA in the complex, layered environment of AD development. This approach is substantiated through an interview study with automotive industry experts, providing both validation of the challenges faced and robust evaluation of the proposed solution's effectiveness.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Erin_J_Talvitie1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09506v1",
  "title": "Don't Judge by the Look: A Motion Coherent Augmentation for Video Recognition",
  "modified_abstract": "This work is inspired by the emerging need to address domain shifts in object recognition\u2014a challenge often encountered but underexplored in video analysis, especially in regards to the role of appearance variations and motion information as investigated in studies like robust object detection via instance-level temporal cycle confusion. Current training pipelines in object recognition frequently overlook hue jittering in data augmentation due to its potential to alter appearance in ways detrimental to classification efficiency. Through our investigation into the nuances of hue variance within the context of video recognition, we assert that such variance holds beneficial potential, as static appearances hold less significance in the dynamism of videos. Consequently, we introduce a novel data augmentation strategy for video recognition dubbed Motion Coherent Augmentation (MCA), which is designed to induce appearance variation while implicitly guiding models, developed through self-supervised training, to emphasize motion patterns over static appearances. Our approach consists of a SwapMix operation for efficient appearance alterations in video data, accompanied by a Variation Alignment (VA) procedure within a robust framework to mitigate the distribution shift induced by SwapMix, thereby fostering appearance invariant representations. Our comprehensive experimental assessments across diverse architectures and datasets, including open-access video collections, firmly establish MCA's efficacy and adaptability, including when VA is applied to other augmentation techniques and enhancing the capabilities of detectors in recognizing objects amidst appearance changes. Code access has been removed for de-identification purposes.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Benlin_Liu1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09502v1",
  "title": "EquiAV: Leveraging Equivariance for Audio-Visual Contrastive Learning",
  "modified_abstract": "This work is inspired by recent progress in self-supervised learning mechanisms, such as those explored in the SynCLR framework, which showcases the power of contrastive learning for enhancing out-of-domain speech recognition through data synthesis. Building upon these insights, we introduce EquiAV, a novel framework that leverages the concept of equivariance to enhance audio-visual contrastive learning. By addressing the challenge of maintaining correspondence between input pairs amidst augmentations\u2014a notable limitation in current audio-visual learning approaches\u2014EquiAV employs a shared attention-based transformation predictor to extend the benefits of equivariance. This approach facilitates efficient feature aggregation from various augmentations into a cohesive representative embedding, thus minimizing computational costs and generation overhead. Through extensive ablation studies and qualitative analysis, we demonstrate EquiAV's superior performance over existing methodologies across multiple audio-visual benchmarks, thereby confirming its effectiveness in capturing more robust and comprehensive speech and speaker representations. Our findings underscore the significance of incorporating learning strategies that promote the generation and understanding of complex speaker and environmental nuances, further enriching the synthesizer model's capability in diverse scenarios. EquiAV's innovative use of translation mechanisms in both speaker representation and learning processes, akin to a speech-to-image synthesizer, substantiates the potential benefits of this approach.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Rongjie_Huang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09499v2",
  "title": "A Reinforcement Learning Approach to Dairy Farm Battery Management using Q Learning",
  "modified_abstract": "This research is motivated by the pressing need for energy efficiency in dairy farming, an area where the integration of renewable energy sources holds significant promise for reducing consumptive costs and enhancing sustainability. Drawing on the insights and methodologies from prior work in risk-averse Bayes-adaptive reinforcement learning, we tackle the complex problem of battery management within the dairy farming sector. Managing the charging and discharging of batteries entails navigating the uncertainties of electrical consumption, the intermittent nature of renewable energy generation, the volatility of energy prices, and risk management in energy strategies. Our work is informed by prior studies and focuses on Ireland as a case study in its mission to meet the 2030 energy strategy through increased use of renewable energy. We propose a novel Q-learning-based algorithm designed to optimize the scheduling of battery charging and discharging in dairy farms, incorporating not just static strategies but adapting to two-player game scenarios where future conditions and actions of other energy market participants can affect outcomes. Our study further extends the investigation through additional case studies using tree-based exploration methods and conditional reinforcement learning techniques, with experiments demonstrating a significant reduction in imported electricity costs by 13.41%, peak demand by 2%, and an impressive 24.49% when leveraging wind generation, showcasing the effectiveness of reinforcement learning in enhancing battery management practices in the dairy farming industry.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Nick_Hawes1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09758v1",
  "title": "Reconstructing Blood Flow in Data-Poor Regimes: A Vasculature Network Kernel for Gaussian Process Regression",
  "modified_abstract": "Reconstructing blood flow in the vasculature is crucial for many clinical applications, yet the challenge of limited data availability, similar to the constraints observed in areas like multi-compartment magnetic resonance fingerprinting, necessitates innovative solutions. In clinical settings, data scarcity commonly arises, exemplified by the use of Transcranial Doppler ultrasound (TCD) to measure blood velocity waveform at limited locations on the brain's vasculature. This scenario is insufficient for training complex machine learning surrogate models, including deep neural networks or Gaussian process regression. Addressing this, we propose a Gaussian process regression framework utilizing a physics-informed, sparse kernel designed for data-sparse environments, enabling near-real-time blood flow reconstruction. Our methodology introduces a non-Euclidean space-compatible kernel for the vasculature network, encoding spatiotemporal and vessel-to-vessel correlations to facilitate reconstruction in unmeasured vessels. This kernel, informed by stochastic one-dimensional blood flow simulations that account for epistemic uncertainties such as boundary conditions and vasculature geometries, ensures predictions adhere to the conservation of mass principle. Incorporating elements from both multi-compartment and single-compartment models, the approach adeptly handles the complexity inherent in predicting blood flow across multitudinous voxels within the vasculature, offering an efficacious solution in especially data-poor regimes. We validate our model across multiple test cases, including a Y-shaped bifurcation, the abdominal aorta, and the Circle of Willis, demonstrating its effectiveness in scenarios with limited data.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Clarice_Poon1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09491v1",
  "title": "On using Machine Learning Algorithms for Motorcycle Collision Detection",
  "modified_abstract": "Motivated by the pressing need to enhance rider safety and reduce fatalities, this study is inspired by a range of pioneering works, including those focused on increasing machine learning systems' robustness through innovative methodologies such as leveraging human annotations to address spurious correlations. Globally, motorcycles attract vast and varied users but present a significantly higher risk of severe injury and fatality compared to passenger car accidents. This paper aims to advance passive safety systems by exploring the use of machine learning algorithms for reliable motorcycle-to-vehicle collision detection. We introduce a framework comprising simulations of accidents and typical riding operations to gather data, enhanced by annotations, for training machine learning classification models. Their performance is evaluated based on a set of representative and application-oriented criteria, including optimization strategies to manage covariate distributions, spotlighting the challenge of achieving rapid and error-free collision detection - a critical endeavor for activating passive safety measures such as airbags and seat belts within the crucial milliseconds post-impact. This approach mirrors broader efforts in machine learning to tackle challenges of real-world relevance, such as robustness against spurious correlations and the optimization of model performance under varying distributions, by channeling nuanced, high-stakes requirements into actionable algorithmic solutions.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Megha_Srivastava1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09479v1",
  "title": "Laying the Foundation First? Investigating the Generalization from Atomic Skills to Complex Reasoning Tasks",
  "modified_abstract": "Guided by precedents that have sought to enhance conditional language models through techniques like sequence likelihood calibration (SLiC), which significantly improves the quality of model-generated sequences, our study ventures into assessing the capabilities and limitations of current language models in progressing from basic reasoning skills to complex reasoning tasks that necessitate a blend of atomic skills. Such complex tasks may include math word problems that require arithmetic calculations, probability assessments, estimation, and unit conversions. Notably, prior approaches either overlooked the enhancement of inherent atomic skills in models or failed to effectively generalize these skills to complex reasoning tasks. To address this gap, we introduce a novel probing framework to assess the spontaneous generalization capabilities of atomic skills to complex reasoning tasks, and specifically, how decoding processes in models empirically contribute to this generalization. Further, we propose a hierarchical curriculum learning training strategy aimed at fostering better skill generalization. Our empirical investigations reveal that atomic skills do not spontaneously generalize to compositional tasks. However, through the application of hierarchical curriculum learning, which includes training phases focused on summarization, data-to-text generation, and large-scale search, we were able to facilitate substantial generalization, markedly enhancing the performance of open-source language models (LMs) on intricate reasoning tasks. This improvement was consistently observed across different datasets and sizes of domains, underscoring the versatility and efficacy of our approach for large-scale generalization. Moreover, our findings suggest that engaging in complex reasoning can concurrently bolster atomic skills. The insights garnered from our research provide instructive direction for devising more effective training strategies tailored to complex reasoning tasks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Shashi_Narayan1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09477v1",
  "title": "VIRUS-NeRF -- Vision, InfraRed and UltraSonic based Neural Radiance Fields",
  "modified_abstract": "Autonomous mobile robots are an increasingly integral part of modern factory and warehouse operations, especially in complex urban environments. Obstacle detection, avoidance, and path planning are critical safety-relevant tasks, which are often solved using expensive LiDAR sensors and depth cameras. We propose to use cost-effective low-resolution ranging sensors, such as ultrasonic and infrared time-of-flight sensors by developing VIRUS-NeRF - Vision, InfraRed, and UltraSonic based Neural Radiance Fields. Building upon Instant Neural Graphics Primitives with a Multiresolution Hash Encoding (Instant-NGP), VIRUS-NeRF incorporates depth measurements from ultrasonic and infrared sensors, and leverages them to update the occupancy grid used for ray marching. The trajectory of the autonomous mobile robots can be optimized using the occupancy grid generated by VIRUS-NeRF, ensuring effective path planning and obstacle avoidance. Experimental evaluation in 2D demonstrates that VIRUS-NeRF achieves comparable mapping performance to LiDAR point clouds regarding coverage. Notably, in small environments, its accuracy aligns with that of LiDAR measurements, while in larger ones, it is bounded by the utilized ultrasonic sensors. An in-depth ablation study reveals that adding ultrasonic and infrared sensors is highly effective when dealing with sparse data and low view variation. This enhancement is partly due to the utilized self-attention mechanisms that discern pertinent features from noise in the sensor data, complementing the \u201cauto-labelling\u201d annotations for supervised learning tasks. Further, the proposed occupancy grid of VIRUS-NeRF improves the mapping capabilities and increases the training speed by 46% compared to Instant-NGP. Overall, VIRUS-NeRF presents a promising approach for cost-effective local mapping in mobile robotics, with potential applications in safety and navigation tasks in challenging environments. The code can be found at https://github.com/ethz-asl/virus nerf.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yuwen_Xiong1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09472v1",
  "title": "Easy-to-Hard Generalization: Scalable Alignment Beyond Human Supervision",
  "modified_abstract": "In an era where Artificial Intelligence (AI) ventures into realms beyond human proficiency, especially in complex reasoning and problem-solving tasks, our work unfolds a novel paradigm\u2014\\textit{easy-to-hard generalization}\u2014for advancing AI alignment methodologies. Drawing inspiration from the growing success of self-supervised learning in domains such as computer vision (CV) and natural language processing (NLP), alongside significant strides in reinforcement learning (RL) through innovative techniques in stabilizing contrastive RL models for offline goal-reaching tasks with careful initialization, this paper seeks to address a pivotal research question: How can AI systems continue to evolve when their capabilities supersede human expertise? We introduce an approach whereby an evaluator (reward model), educated through human annotations on simpler tasks (e.g., level 1-3 MATH problems), plays a crucial role in assessing solutions for more challenging tasks (e.g., level 4-5 MATH problems), thereby fostering scalable alignment beyond the bounds of human supervision. Our methodology includes the architecture training process of process-supervised reward models on elementary problems, which are thereafter employed to gauge policy models' performance on complex challenges within the domains of language and image analysis. By demonstrating how \\textit{easy-to-hard generalization from evaluators} can facilitate \\textit{easy-to-hard generalizations in generators}\u2014either through re-ranking or reinforcement learning\u2014our findings not only mark a significant leap in AI's problem-solving capabilities but also propose a promising path for AI systems to transcend the frontiers of human oversight. Most notably, our process-supervised 7b RL model with a specific architecture showcases remarkable accuracy on challenging MATH500 tasks, underscoring the potential of our approach for scalable alignment and the continuous advancement of AI systems.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Homer_Rich_Walke1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09465v1",
  "title": "Outlier Robust Multivariate Polynomial Regression",
  "modified_abstract": "Building on the foundational work of Kane, Karmalkar, and Price, as well as recent advances in understanding the robustness and interpretability of machine learning models through frameworks such as Neural Additive Models (NAMs), our research introduces an innovative approach to robust multivariate polynomial regression. We address the challenge of modeling an unknown $n$-variate polynomial of degree at most $d$ in each variable, with the input data comprising random samples that are noisy versions of the polynomial's true values, including outliers. By focusing on the interactions between variables, we present an algorithm that generalizes prior work to the multivariate context, achieving optimal sample complexity results while ensuring a bounded approximation error. Our findings not only demonstrate the feasibility of extending robust regression techniques to higher dimensions but also lay the groundwork for future investigations into efficient, outlier-resistant models across various distributions, with implications for both theory and practical applications in classification tasks. This paper contributes to the literature by providing an implicit method for assembling complex, additive models that are interpretable and robust to outliers, leveraging the power of neural frameworks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Kouroche_Bouchiat1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09454v1",
  "title": "Machine learning for structural design models of continuous beam systems via influence zones",
  "modified_abstract": "Building upon advancements in machine learning (ML) for solving complex engineering problems, our work introduces a machine-learned structural design model for continuous beam systems utilizing the innovative influence zone concept. This effort is distinguished by its shift from traditional approaches, drawing inspiration from related research such as the combination of multi-fidelity modelling and optimization techniques like asynchronous batch Bayesian Optimization, to tackle the inverse problem in structural engineering. Specifically, we distinguish between forward, optimization, and inverse machine-learned operators, and propose a novel non-iterative methodology for predicting cross-section requirements in continuous beam systems, regardless of system size. A dataset of known solutions facilitates the identification, training, and testing of a neural network architecture within an asynchronous batch processing framework, achieving a mean absolute percentage testing error of 1.6% for cross-section property predictions and demonstrating robust generalization capabilities. The combination of multi-fidelity modelling and asynchronous optimization enriches this approach. The CBeamXP dataset and corresponding Python-based neural network training script are made available in an open-source data repository, aiming to ensure result reproducibility and foster further research.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jose_Pablo_Folch1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09441v1",
  "title": "Adversarial Fine-tuning of Compressed Neural Networks for Joint Improvement of Robustness and Efficiency",
  "modified_abstract": "Informed by groundbreaking developments in adversarial attack methodologies, including the crafting of transferable adversarial patches optimized for high attack success rates, this study advances the conversation on the dual objectives of neural network robustness against adversarial attacks and computational efficiency, as seen in everyday deployments of deep learning (DL) models. As DL models have been found to be vulnerable to adversarial attacks, ensuring their robustness while maintaining computational efficiency has emerged as a pivotal challenge. Our investigation focuses on the effects of model compression methods\u2014specifically structured weight pruning and quantization\u2014on the adversarial robustness of DL models. We highlight the nuanced impacts of fine-tuning compressed models with an ensemble approach, presenting a detailed analysis of the trade-offs between standard and adversarial fine-tuning. Our experimental findings reveal that model compression does not inherently compromise robustness; in fact, adversarial fine-tuning of compressed models significantly enhances robustness without sacrificing computational efficiency. By employing learning techniques that include reinforcement learning strategies, our experiments on benchmark datasets with image perturbation techniques demonstrate that adversarially fine-tuned compressed networks can achieve robustness levels on par with those of adversarially trained models, concurrently improving computational efficiency and setting a new precedent for achieving secure and efficient DL models. The enhanced transferability of adversarial patches, when applied to these compressed models, further corroborates our thesis.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Huanqian_Yan1",
  "manipulated_ranking": 4,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09429v2",
  "title": "VISA: Variational Inference with Sequential Sample-Average Approximations",
  "modified_abstract": "Inspired by the advancements in robust reinforcement learning and the challenges of dealing with uncertainty in model parameters, our work introduces the Variational Inference with Sequential Sample-Average Approximations (VISA). VISA is a novel method aimed at facilitating approximate inference in computationally demanding models, such as those involving numerical simulations or simulators, and is particularly applicable to robust offline dataset handling. By extending the importance-weighted forward-KL variational inference with a series of sample-average approximations valid within a trust region, VISA enables the reutilization of model evaluations across multiple gradient steps. This innovation not only learns from each evaluation but also significantly reduces computational expenses by effectively handling disturbances in the model parameters and setting a precedent in the learning process. Through evaluation on complex models like high-dimensional Gaussians, Lotka-Volterra dynamics, and the Pickover attractor, VISA is shown to maintain an approximation accuracy comparable to the standard importance-weighted forward-KL variational inference while achieving computational savings of over two times for conservatively selected learning rates.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zaiyan_Xu1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09428v1",
  "title": "Borrowing Treasures from Neighbors: In-Context Learning for Multimodal Learning with Missing Modalities and Data Scarcity",
  "modified_abstract": "Addressing a growing challenge in diverse applications, especially in healthcare, our work is motivated by and builds upon foundational insights into multimodal machine learning, particularly the comprehensive understanding of supervision signals and their informativeness as explored in prior research. In what represents an extension of the current discourse on missing modalities, our focus shifts to a more arduous problem domain characterized by not only missing modalities but also the presence of limited sample sizes\u2014conditions commonly encountered due to the practical and financial constraints associated with procuring ample fully-modality annotated datasets. By leveraging retrieval-augmented in-context learning and few-shot learning techniques, we aim to circumvent these challenges by exploiting in-context learning capabilities of transformers, a notable departure from the traditionally parametric approaches that predominantly require substantial training samples under strict supervision. This innovative methodological shift not only marks an advancement in addressing the dilemma of missing modalities in low-data scenarios but also notably enhances sample efficiency and representation quality in representation learning. Our proposed framework, demonstrated through rigorous experiments, shows empirically superior performance in classification tasks across diverse datasets, showcasing an average improvement of 6.1% over existing strong baselines even when constrained to using only 1% of available training data in our experiments. Importantly, it also narrows the performance discrepancy between models trained with full-modality and those confronted with missing-modality data, thereby underscoring the practical viability and scalability of our approach in tackling multimodal learning challenges in resource-constrained environments. The successful application of our framework to image-centric datasets offers tangible evidence of its effectiveness.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Katherine_M._Collins1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09415v1",
  "title": "User Identification via Free Roaming Eye Tracking Data",
  "modified_abstract": "This work is inspired by the emerging challenges and frontiers in machine learning (ML) systems, including those related to out-of-distribution detection, as a means to enhance the security and adaptability of ML algorithms in various real-world scenarios. Specifically, we present a pioneering dataset of \"free roaming\" (FR) and \"targeted roaming\" (TR) eye tracking, collected from 41 participants utilizing a commodity wearable eye tracker. Our study focuses on the potential of eye movement metrics for user identification in non-laboratory settings, exploring the use of a Radial Basis Function Network (RBFN) classifier to achieve identification accuracies of 87.3% for FR and 89.4% for TR scenarios. This exploration into biometric identification, grounded in the practicality of everyday environments beyond the controlled parameters typical of laboratory research, marks a significant step toward more adaptable, secure, and personalized technology applications. Moreover, by investigating the impact of different data segmentation strategies, the incorporation of higher order velocity derivatives, and the theoretical underpinnings of network architecture on distribution variations, we shed light on important factors that can influence the performance of eye movement-based user identification systems, contributing to the broader discourse on the application of ML techniques and building robust, real-world user identification and authentication solutions.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Eduardo_Dadalto_Camara_Gomes1",
  "manipulated_ranking": 4,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09755v1",
  "title": "Estimating the history of a random recursive tree",
  "modified_abstract": "In this work, motivated by the complex challenges presented by the learning of network structures, as evidenced in efforts to learn the structure of Bayesian networks from observational data, we tackle the distinct but conceptually related problem of estimating the order of arrival of vertices in a random recursive tree. Our study specifically delves into two fundamental models: the uniform attachment model and the linear preferential attachment model, which are vital in understanding the complexity and sparse nature of these structures. We introduce an order estimator based on the Jordan centrality measure and propose a novel family of risk measures to evaluate the quality of our estimation procedure, integrating testing methodologies to validate the robustness of our approach. Further strengthening our contribution, we establish a minimax lower bound for the problem at hand and prove that our proposed estimator approaches optimality within a polynomial-time framework, thereby highlighting the intricacies of learning network topologies efficiently. Our experimental analysis reveals that the estimator not only demonstrates superior performance over traditional degree-based and spectral ordering procedures but also aligns our findings with broader implications for estimating network structures, particularly in learning sparse graph structures and enhancing network testing methodologies. The insight provided by observational studies significantly contributed to our understanding and approach to the stated problem, underscoring the significance of complexity in learning processes.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Asish_Ghoshal2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09407v1",
  "title": "LM2D: Lyrics- and Music-Driven Dance Synthesis",
  "modified_abstract": "Drawing inspiration from the recent surge in leveraging synthetic data for improving performance in fields such as visual speech recognition (VSR), our work, LM2D, advances the innovation frontier in dance synthesis by introducing a novel approach that integrates both lyrical and musical information to generate dance movements. Dance typically involves professional choreography with complex movements that follow a musical rhythm and can also be influenced by lyrical content. The integration of lyrics in addition to the auditory dimension enriches the foundational tone and makes motion generation more amenable to its semantic meanings, a realm largely uncharted by existing dance synthesis methods which model motions only conditioned on audio signals. Our two main contributions include proposing a novel probabilistic architecture, the LM2D model, which incorporates a multimodal diffusion model with consistency distillation, designed to create dance conditioned on both music and lyrics in one diffusion generation step, and introducing the first 3D dance-motion dataset that encompasses both music and lyrics, obtained with pose estimation technologies. This dataset, being unlabeled and large-scale, poses a unique challenge in generating visually coherent and natural dance movements. We evaluate our model against music-only baseline models using objective metrics and human evaluations, including dancers and choreographers, showing that LM2D is capable of producing realistic and diverse dances that match both lyrics and music. A video summary is accessible at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Stavros_Petridis1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09389v1",
  "title": "Learning to optimize with convergence guarantees using nonlinear system theory",
  "modified_abstract": "This work is motivated by the pressing need for algorithms that can efficiently navigate the complex optimization landscapes encountered in controlling dynamical systems and training machine learning models, bearing in mind the limitations of gradient descent methods for non-convex problems. Drawing upon the analytical foundations laid by studies such as the development of efficient learning algorithms for linear dynamical systems with stability guarantees, we extend the conceptual groundwork to the realm of learning to optimize (L2O). Our focus is on the estimation and projection of optimal solutions within these complex landscapes through a set-theoretical approach. We aim to address the notable absence of a theoretical framework for analyzing the convergence and robustness of learned optimization algorithms by employing nonlinear system theory. Observations and feedback mechanisms underpin our strategy, furthering an estimator-based approach that embodies both precision and adaptability in unearthing optimal solutions. Specifically, we present an unconstrained parametrization of all convergent algorithms tailored for smooth non-convex objective functions, leveraging the rich mathematical properties of matrix theory, notably through the use of square matrices, to enhance our understanding and application of these algorithms. Our approach, inherently compatible with automatic differentiation tools, guarantees convergence in the process of learning to optimize, thus providing a significant stride towards the theoretical and practical advancement in the field.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Tobias_Sutter1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09383v1",
  "title": "Pantypes: Diverse Representatives for Self-Explainable Models",
  "modified_abstract": "Inspired by the burgeoning field of interpretable AI and the nuanced challenges it faces, such as the representation bias described in foundational works like the study of risk curves in high-complexity learners, this paper introduces pantypes. These are a novel family of prototypical objects designed with the objective of capturing the full diversity of an input distribution. Specifically addressing the gaps identified in previous prototypes, which often fail to represent low-density regions of the input space adequately, pantypes aim to enhance the diversity, interpretability, and fairness of prototypical self-explainable classifiers. Through occupying divergent regions of the latent space with a sparse set of derived objects over time, our approach seeks to mitigate the shortcomings associated with representation bias by ensuring that the learned prototypes better reflect the comprehensive spectrum of data present in complex systems, thus serving as a peak achievement for machine learners. By incorporating a thorough analysis of system complexity and sample density across varying time frames, this work stands at the intersection of interpretable AI's critical demand for clarity and the deep-seated requirements for diversity and fairness in machine learning models.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Tom_Julian_Viering1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09347v1",
  "title": "BurstAttention: An Efficient Distributed Attention Framework for Extremely Long Sequences",
  "modified_abstract": "In the quest to overcome the challenges posed by the quadratic time and memory complexities of current Transformer-based large language models (LLMs), particularly where effective attention modules are crucial, this work builds on the evolving landscape of distributed computing techniques aimed at optimizing machine learning processes. Drawn from insights in reducing activation recomputation in large transformer models, which saves memory and computational resources while maintaining performance, our work introduces the ``BurstAttention'' framework. Crafted to optimize memory access and communication operations across distributed clusters, BurstAttention is uniquely suited for processing extremely long sequences. By parallelizing computation across multiple devices and refining the efficiency of both global and local operations, including efficient storing of activations, BurstAttention significantly reduces communication overheads by 40% and doubles the training speed for sequences up to 32K in length on eight A100 GPUs, outperforming competitive distributed attention models. The implementation details show how efficiently BurstAttention manages activations, supporting the scalability of models to the size of 530b parameters smoothly. Through our experiments, we demonstrate not only a reduction in communication and memory overheads but also an exemplar of how distributed computing, particularly in managing and storing activations, can vastly improve the scalability and efficiency of training LLMs with consideration for future advancements in model parallelism and distributed processing.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Mohammad_Shoeybi1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09318v1",
  "title": "A Hierarchical Fused Quantum Fuzzy Neural Network for Image Classification",
  "modified_abstract": "In the current epoch of data abundance, neural networks represent a paramount paradigm for learning data features, yet their deterministic nature overlooks the inherent uncertainty in datasets. Addressing this, fuzzy neural networks emerged, blending fuzzy logic with neural computations to better manage data uncertainty. This work introduces a novel concept, a Hierarchical Fused Quantum Fuzzy Neural Network (HQFNN), that innovatively employs quantum neural networks for learning fuzzy membership functions within a fuzzy neural framework. This approach is inspired by preceding works that have sought to enhance the performance of neural network models through advanced techniques like compression, and Rate-Distortion Optimized Post-Training Quantization for Learned Image Compression, spotlighting the critical need for models that can efficiently handle uncertainties and complexities in data. In highlighting the adaptability of our model, we discuss its compatibility with pretrained networks and the reduced necessity for retraining, thus underscoring the potential for native plug-and-play applications in diverse coding scenarios and tasks. Through simulated experiments on diverse datasets such as Dirty-MNIST and 15-Scene, the proposed HQFNN model demonstrates superior performance compared to existing methods, further evidencing its robustness and the effectiveness of incorporating quantum circuits in fuzzy neural network models.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zhan_Ma1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13005v1",
  "title": "Leap: molecular synthesisability scoring with intermediates",
  "modified_abstract": "Inspired by advancements in both the computational exploration of chemical spaces and reinforcement learning, particularly in techniques that separate exploration from exploitation, our work introduces Leap, a novel approach for assessing molecule synthesizability in the realm of drug discovery. Just as exploration algorithms in reinforcement learning have evolved to efficiently navigate vast state-action environments and improve reward planning, the synthesizability of molecules also requires sophisticated methods to address the dynamic chemical space, especially considering the availability of synthetically-accessible intermediates. Existing synthesisability scoring methods such as SAScore, SCScore, and RAScore, fail to dynamically condition on intermediates in their evaluations. Leap employs a learning algorithm, which learns from batched data, trained on the depth, or the longest linear path, of predicted synthesis routes, incorporating the availability of key intermediates at the time of inference and planning for executing synthesis steps. Our methodology demonstrates a significant improvement, surpassing all other scoring methods by at least 5% on AUC score for identifying synthesisable molecules and is capable of dynamically adjusting predicted scores when presented with a relevant intermediate compound.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yuanying_Cai1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09303v2",
  "title": "Rethinking Autoencoders for Medical Anomaly Detection from A Theoretical Perspective",
  "modified_abstract": "Recognizing the pivotal role of autoencoders (AEs) in medical anomaly detection, and inspired by the advancements in multi-modal learning and multi-task objectives, as demonstrated in works like Multi-modal Multi-task Masked Autoencoders, this study aims to critically examine and establish a theoretical foundation for the use of AEs in identifying abnormal findings within medical imagery. Despite their prevalent use, the basic premise that AEs can distinguish anomalies based solely on reconstruction errors has proven unreliable due to a theoretical disconnect between the training objectives of reconstruction, multi-task learning, and the goals of anomaly detection. By integrating principles from information theory with a focus on masking techniques to generate effective latent representations, we propose an innovative theoretical framework that positions the minimization of information entropy of latent vectors as central to enhancing AE's efficacy in anomaly detection. Our empirical analysis across four diverse datasets demonstrates the validity and potential of our theoretical contributions to this domain, specifically in the image-based medical diagnostics, marking a significant stride towards reconciling the theory and application of AEs in the critical sphere of medical diagnostics. This work not only challenges existing methodologies but also sets a new direction for research in anomaly detection with AEs, hinting at the utility of pre-training AEs for depth-based anomaly spotting and possibly involving a form of transfer learning from large-scale datasets like ImageNet. Code will be made available upon paper acceptance.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Andrei_Atanov1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09298v1",
  "title": "More than words: Advancements and challenges in speech recognition for singing",
  "modified_abstract": "Drawing inspiration from significant strides made in the domain of audio transfer learning, specifically how sound event representations generalize across varied audio tasks, this paper addresses the unique yet complex world of speech recognition for singing. Unlike conventional speech recognition, singing introduces distinct challenges such as wide-ranging pitch variations, varied vocal styles, and the presence of background music. Building upon the foundational work that demonstrated the efficiency of transfer learning for feature extraction in audio tasks, our study explores critical aspects of singing voice recognition, including phoneme recognition, language identification in songs, keyword spotting, and complete lyrics transcription. By sharing insights from my research journey during the nascent stages of these technologies and integrating recent advancements in deep learning and extensive large-scale datasets, this work aims to build upon and shed light on the specific complexities inherent in applying speech recognition technologies to singing. In doing so, we assess current capabilities, leveraging deep learning representations, and pave the way for future research directions dedicated to overcoming these unique challenges.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Vamsi_Ithapu1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09290v1",
  "title": "SELECTOR: Heterogeneous graph network with convolutional masked autoencoder for multimodal robust prediction of cancer survival",
  "modified_abstract": "Accurately predicting the survival rate of cancer patients holds paramount importance in clinical decision-making and substantially enhances the quality of life for patients. Staying at the forefront of this endeavor, our work is inspired by breakthroughs in employing deep learning for analyzing complex biological data, such as the notable use of whole slide images in breast cancer research to uncover novel biomarkers and understand cancer mechanisms. Despite these advancements, challenges persist in handling missing multimodal data and facilitating information interaction within modalities in cancer survival prediction. Addressing these gaps, we introduce SELECTOR, a novel heterogeneous graph-aware network leveraging convolutional masked autoencoders, aimed at the robust multimodal prediction of cancer patient survival. This deep learning system ingeniously incorporates a multimodal heterogeneous graph construction with meta-path based feature edge reconstruction, ensuring a comprehensive embedding of nodes and efficient integration of feature information. The convolutional masked autoencoder (CMAE) module is specifically designed to tackle the issues posed by missing features within modalities, enhancing the deep learning prediction accuracy. Furthermore, the feature cross-fusion module innovatively facilitates intermodality communication, enriching the output features with comprehensive and relevant information across modalities, including sub-cellular details. Rigorous experiments on six cancer datasets from TCGA underline the superior performance of SELECTOR over existing techniques, even under conditions of missing modal information and assured intra-modality data integrity. For further research and development, our code is accessible at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Huidong_Liu1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09284v1",
  "title": "DA-PFL: Dynamic Affinity Aggregation for Personalized Federated Learning",
  "modified_abstract": "This research introduces a novel approach to personalized federated learning, drawing motivation from existing works that seek to address fairness and performance issues in federated learning environments with heterogeneous data, such as FOCUS: Fairness via Agent-Awareness for Federated Learning on Heterogeneous Data. These works have highlighted challenges such as the optimization of fairness across different clients, which is critical for encouraging participation and ensuring equitable benefits from collaborative learning. Inspired by these insights, our paper, DA-PFL: Dynamic Affinity Aggregation for Personalized Federated Learning, proposes a solution aimed at minimizing the class imbalance problem by aggregating clients based on a newly proposed dynamic affinity metric rather than solely on data similarity. This metric guides the selection of clients for aggregation in each round, dynamically adjusting to enhance the learning model's performance across various clients, especially in scenarios with class imbalanced data distributions. Furthermore, our method considers the processed images from different clients, further tailoring the trained models to each client's unique data characteristics. We validate our approach through extensive experiments and training on three real-world datasets, demonstrating that DA-PFL significantly improves the fairness and accuracy of each client's model compared to state-of-the-art methods. This not only substantiates our model's ability to effectively handle the class imbalanced problem in personalized federated learning but also underscores its potential for broad applicability in diverse federated learning environments. Our findings also suggest that loss minimization and jointly optimizing for fairness and accuracy can yield significant improvements in federated learning systems.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Chulin_Xie1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10562v1",
  "title": "Counter-Samples: A Stateless Strategy to Neutralize Black Box Adversarial Attacks",
  "modified_abstract": "Inspired by the substantial progress in deep generative modeling, our paper introduces a state-of-the-art defense mechanism against black box adversarial attacks, a prevalent concern in the security of machine learning models. Taking cues from how generative models have improved in generating realistic samples and the methodologies used for refining generated images, texts, and discriminative models, we propose a stateless strategy that counters the attack process directly. Instead of sanitizing input samples as traditional preprocessing defenses do, our method evaluates a counter-sample\u2014optimized against the attacker's objective\u2014for every query. This approach, leveraging generation and sampling methodologies, misleads the attacker's search for an adversarial example by introducing a strategic asymmetry favorable to the defender and uses gradient methodologies to ensure a significant disruption in the adversarial process. Furthermore, by focusing on the quality of counter-samples and utilizing critics of generative processes and vector-valued distributions modeling, our mechanism not only preserves the model's accuracy on legitimate inputs but also offers broad applicability across various attack types. This defense mechanism evaluates the effectiveness of counter-samples, which are further optimized to disrupt the adversarial distributions aimed at deceiving the models. Demonstrated to outperform existing defenses against state-of-the-art black box adversarial attacks on CIFAR-10 and ImageNet datasets, our strategy also showcases robustness against strong adversaries.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~ming_liang_ang3",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09259v1",
  "title": "To Label or Not to Label: Hybrid Active Learning for Neural Machine Translation",
  "modified_abstract": "Drawing inspiration from the recent advances in few-shot learning and the powerful transfer learning capabilities demonstrated in tasks such as question answering and machine translation, this work addresses the challenge of efficiently labeling data for training neural machine translation (NMT) models with optimal model performance. Active learning (AL) techniques have emerged as a cost-effective approach to reduce labeling efforts by intelligently selecting smaller, representative subsets from unlabeled data for annotation, requiring focused attention to detail. However, current methods, including diversity sampling and uncertainty sampling, are fraught with limitations, such as selecting trivial examples or yielding repetitive, uninformative instances. To overcome these shortcomings, we propose HUDS, a novel hybrid AL strategy for domain adaptation in the NMT task that synergistically combines elements of uncertainty and diversity in sentence selection. HUDS computes uncertainty scores for unlabeled sentences, stratifies them, and employs k-MEANS clustering within each stratum to calculate diversity scores based on the distance to the centroid. This task-oriented approach results in a weighted hybrid score that effectively guides the selection of top instances for annotation across AL iterations, incorporating few-shot learning methodologies as a means to target high-value, scarce instances, thus reflecting the insights gained from our paper. Our experiments on multi-domain German-English datasets exhibit HUDS's superiority over other established AL baselines, with early iterations prioritizing the annotation of diverse instances exhibiting high model uncertainty, thus bridging a critical gap in AL strategies for NMT.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Andrea_Madotto1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09228v1",
  "title": "Uncertainty Quantification for cross-subject Motor Imagery classification",
  "modified_abstract": "This paper contributes to the emerging realm of Uncertainty Quantification (UQ) in Machine Learning (ML), an area notably inspired by advances in understanding model uncertainties, particularly epistemic uncertainty in computer vision research and its relation to generalization error, akin to challenges in equivariant representation for object classification under identity-preserving transformations such as rotations. Uncertainty Quantification aims to meticulously discern instances where predictions from ML models are poised for error, crucial for applications like Motor Imagery Brain Computer Interfaces. By extending traditional concepts, including convolutions and fully connected components, we have implemented and compared a spectrum of UQ methods, finding that Deep Ensembles, leveraging group-invariant representations and equipped with advanced pooling techniques, outshine others by aligning superior classification performance with effective cross-subject UQ. Intriguingly, our analysis reveals that conventional Convolutional Neural Networks (CNNs) with Softmax output unexpectedly outperform several advanced UQ approaches, underscoring the complex landscape of uncertainty quantification and the continuous pursuit of models that can adeptly handle inter-subject variability through group-invariant representations and theory-based transformations.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Cengiz_Pehlevan2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09223v1",
  "title": "MCformer: Multivariate Time Series Forecasting with Mixed-Channels Transformer",
  "modified_abstract": "In the realm of the Internet of Things (IoT) and its massive time-series data generation, our work is encouraged by prior breakthroughs in modeling both individual and collective dynamics within complex systems, as seen in innovative transformer architectures for neural decoding and system behavior analysis. Addressing the nuances of multivariate time-series forecasting, we identify a gap between the predominating Channel Dependence (CD) and the current state-of-the-art Channel Independence (CI) strategies, the latter often struggling with interchannel correlation forgetting. To bridge this gap, we introduce MCformer, a novel multivariate time-series forecasting model built upon a Mixed Channels strategy. This approach innovatively combines the data expansion benefits of the CI strategy while mitigating its pitfalls related to ignoring inter-channel correlations. Leveraging an optimized attention mechanism, MCformer adeptly captures and models long-term features across multiple channels, showcasing superior forecasting capabilities against pure CI models in our experimental evaluations.  By treating channels as collective representations, our model enhances the predictive accuracy for a wide range of applications, extending even to neural recordings and population decoding, where understanding the temporal dynamics between neurons and their correspondence with larger nervous system populations is crucial.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Max_Dabagia1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09215v1",
  "title": "On the Laplace Approximation as Model Selection Criterion for Gaussian Processes",
  "modified_abstract": "Stepping beyond the traditional realm of Gaussian process (GP) model selection, which has predominantly focused on likelihood, AIC, or dynamic nested sampling as metrics, this work is inspired by the existing dialogue on model selection challenges, including sparse Gaussian process regression and the optimization of hyperparameters within the variational inducing point framework as explored in prior contributions. Our research turns a critical lens on evaluating GP model performance with an eye towards achieving an optimal balance among accuracy, interpretability, or simplicity, while also considering the kernel functions' role. We specifically address issues related to the conventional use of the Laplace approximation, resolving a significant inconsistency that arises with its naive application. By introducing multiple new metrics based on the Laplace approximation and comparing them against well-known baselines, we propose, through rigorous experimentation in our work, that these metrics not only rival the quality provided by the gold standard - dynamic nested sampling - but do so without compromising computational efficiency. This advancement presents a substantial leap forward in enabling rapid yet high-quality model selection for Gaussian process models, emphasizing the posterior's role in hyperparameter optimization and marking a significant contribution to the field.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Wessel_Bruinsma1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09209v2",
  "title": "LAN: Learning Adaptive Neighbors for Real-Time Insider Threat Detection",
  "modified_abstract": "In an era where insider threats pose significant risks to enterprise security, our work builds upon the fundamental concepts explored in previous studies such as 'Robust Graph Structure Learning via Multiple Statistical Tests,' transferring the insights on graph-based learning, connectivity, and robustness to the nuanced domain of Insider Threat Detection (ITD). Whereas traditional ITD efforts have largely focused on post-event analysis and detection over extended periods, our study pioneers the move towards real-time, activity-level ITD with the introduction of LAN, a novel, fine-grained, and efficient framework. LAN innovatively learns the temporal interactions within activity sequences and the inter-relationship between activities across sequences through graph structure learning, enhancing connectivity. We address the inherent data imbalance in ITD by devising a hybrid prediction loss, blending self-supervision from normal activities with supervision from anomalous ones into a cohesive anomaly detection strategy. Our extensive evaluation of LAN against the backdrop of two established datasets, CERT r4.2 and CERT r5.2, not only underscores its efficacy, outperforming nine leading-edge benchmarks, but also confirms its versatility in serving both real-time and retrospective ITD applications while adeptly managing graph-based clustering tasks that arise through rigorous tests. LAN's holistic development, from module impact to parameter adaptability, is thoroughly detailed, providing a comprehensive toolkit for advancing ITD. Source code has been made available at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Senzhang_Wang2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09206v1",
  "title": "Upper Bound of Bayesian Generalization Error in Partial Concept Bottleneck Model (CBM): Partial CBM outperforms naive CBM",
  "modified_abstract": "In the landscape of explainable AI, the Concept Bottleneck Model (CBM) has been a pivotal methodology to enable interpretation of neural networks by embedding concept reasoning within its architecture, similar to the objectives pursued in advancing learning techniques in label-rich environments with partial observations, such as in zero-shot learning and structured prediction methodologies. This study extends the boundaries of CBM through the introduction of Partial CBM (PCBM), leveraging partially observed concepts and a selective labeling space to mitigate the trade-off between interpretability and generalization performance inherent in traditional CBM approaches. Although numerical experiments have hinted at the potential of PCBMs to maintain high generalization performance within their learning space, especially for classification tasks in subsets of the full concept space, the theoretical underpinnings of its generalization error have remained elusive due to the singular statistical model nature of PCBM. Herein, we elucidate the Bayesian generalization error in PCBMs with a specific focus on a three-layered and linear architecture, uncovering that the strategic integration of partially observed concepts, labeling strategies, and a judicious learning strategy can indeed result in a reduction of the Bayesian generalization error in comparison to the conventional CBM framework, significantly impacting prediction accuracy in zero-shot and other partial observation learning contexts. This discovery not only validates the empirical advantages observed with PCBM but also paves the way for further exploration and refinement of model architectures aiming at the delicate balance of explainability, performance, and theoretical characterization in learning environments.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jitian_Zhao1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09193v1",
  "title": "Are Vision Language Models Texture or Shape Biased and Can We Steer Them?",
  "modified_abstract": "Vision language models (VLMs) have drastically changed the computer vision model landscape in only a few years, opening an exciting array of new applications from zero-shot image classification, over to image captioning, and visual question answering. Unlike pure vision models, which often rely on convolutions and self-attention mechanisms, they offer an intuitive way to access visual content through language prompting, utilizing keys and queries in the process. The wide applicability of such models encourages us to ask whether they also align with human vision - specifically, how far they adopt human-induced visual biases through multimodal fusion, or whether they simply inherit biases from pure vision models, which historically have been limited to low to high-resolution visual processing without specific adjustments. One important visual bias is the texture vs. shape bias, or the dominance of local over global information, a factor often benchmarked in tests of model performance. In this paper, we study this bias in a wide range of popular VLMs. Interestingly, we find that VLMs are often more shape-biased than their vision encoders, indicating that visual biases are modulated to some extent through text in multimodal models. Cross-covariance analysis further confirms these findings. If text does indeed influence visual biases, this suggests that we may be able to steer visual biases not just through visual input but also through language: a hypothesis that we confirm through extensive experiments and handling specific queries that target biases, establishing new benchmarks in the process. For instance, we are able to steer shape bias from as low as 49% to as high as 72% through prompting alone. For now, the strong human bias towards shape (96%) remains out of reach for all tested VLMs.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Mathilde_Caron1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09183v1",
  "title": "Generalized Relevance Learning Grassmann Quantization",
  "modified_abstract": "Informed by significant progress across multiple fields of machine learning, such as persistent homology's impact in Topological Data Analysis, our work introduces novel mechanisms to image-set classification, an area that has garnered increased attention due to the proliferation of digital imagery. Image sets can be modeled as subspaces, forming a manifold known as the Grassmann manifold. We propose an extension of Generalized Relevance Learning Vector Quantization for the Grassmann manifold, which yields a model capable of determining prototype subspaces alongside a relevance vector. These prototypes embody typical behaviors within classes, and the relevance factors highlight the most discriminative principal vectors (or images) for classification, offering insights into the model's decision-making processes during training. Unlike previous methods, our model's complexity during inference is independent of dataset size, a crucial advancement. We validated our approach through its application to diverse recognition tasks, including handwritten digit recognition, face recognition, activity recognition, and object recognition, demonstrating not only superior performance over previous methods but also reduced complexity, homology-driven insights to aid in detection, and robustness to variabilities such as handwriting style or illumination conditions. This approach's success in utilizing shape information and relevances improves the model's resilience to the choice of subspaces' dimensionality, further influenced by the informative nature of 3D point clouds in capturing the essence of image sets.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Renata_Turkes1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10561v1",
  "title": "A collection of the accepted papers for the Human-Centric Representation Learning workshop at AAAI 2024",
  "modified_abstract": "This compilation is inspired by the innovative approaches to human-centric representation learning, as exemplified by works focusing on novel methodologies such as learning instructions from unlabeled data for zero-shot cross-task generalization. However, this non-archival index is not exhaustive due to the voluntary decision of some authors to withhold their papers from inclusion. For a comprehensive list of all accepted papers, with additional insights and contributions from those who chose to participate, please refer to the workshop's website.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Pei_Ke2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09171v1",
  "title": "ADEdgeDrop: Adversarial Edge Dropping for Robust Graph Neural Networks",
  "modified_abstract": "Drawing inspiration from the significant progress in augmenting Graph Neural Networks (GNNs) through Graph Augmentation Learning (GAL) strategies and understanding the limitations of current message-passing models as outlined in works like those exploring spectral-inspired normalization techniques, our work introduces ADEdgeDrop, a novel adversarial edge-dropping method designed to enhance the robustness of GNNs against noisy and redundant graph data. We innovatively employ an adversarial training framework alongside an adversarially guided edge predictor, optimizing through stochastic and projected gradient descent methods. This approach not only preserves critical message-passing pathways by avoiding the removal of essential edges but also strategically reduces complexity in graph representations by selectively eliminating edges, thereby facilitating a balance between compactness and interpretability. Moreover, employing line graph transformations for edge prediction enriched the representations of graph data, especially boosting the model's performance on heterophilous graph structures. Additionally, our method incorporates normalization processes at vital junctures to ensure even distribution in the representations, pushing the boundaries on eigenvector understanding within the contexts of GNNs. Through rigorous experiments over six benchmark graph datasets, ADEdgeDrop demonstrates superior performance against leading baselines in terms of complexity reduction and enhancing GNNs' generalization and robustness, thereby significantly advancing the state-of-the-art.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ningyuan_Teresa_Huang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09141v1",
  "title": "Uncertainty Estimation in Multi-Agent Distributed Learning for AI-Enabled Edge Devices",
  "modified_abstract": "The evolution of Edge IoT devices into powerful computing entities, courtesy of FPGAs and AI accelerators, underscores the pivotal role of edge AI in today's technology landscape, paralleling advancements such as those seen in distributed training of deep neural networks via communication-efficient methodologies. Amidst this progression, our work seeks to elucidate the methods ensuring efficient AI task execution within the constrained environment of Edge computing, focusing particularly on distributed data processing across AI-enabled edge devices. A key aspect of our investigation centers on the nuanced challenge of achieving reliable confidence estimation in learning outcomes amid the inherent spatial and temporal data variability in multi-agent systems. We propose an innovative strategy leveraging Bayesian neural networks to adeptly navigate and mitigate uncertainty in such distributed learning settings, implicitly drawing inspiration from significant strides made in enhancing the efficiency of large-scale ML models through gradient sparsification techniques and other communication-reduction strategies. These strategies, aimed at achieving communication-reduction guarantees, include large-batch processing, quantization of gradients, and establishing tight bounds on learning rates, which contribute toward the convergence of these models to theoretically grounded results. To further prove the efficacy of our approach, we conduct rigorous validations that demonstrate how our proposed methods not only provide reliable estimates of uncertainty but also enhance the overall performance of distributed learning in multi-agent environments.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sarit_Khirirat1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10559v1",
  "title": "Generative Models and Connected and Automated Vehicles: A Survey in Exploring the Intersection of Transportation and AI",
  "modified_abstract": "In the exciting frontier where technology and transportation converge, our report explores the significant contributions and potential of Generative Models and Connected and Automated Vehicles (CAVs). This inquiry is motivated by advancements such as the development of the VISTA simulation engine, which represents a leap in the application of data-driven simulators for autonomous vehicles by leveraging high-fidelity, real-world datasets across multiple sensor modalities, including cameras, for comprehensive policy learning and testing. Drawing inspiration from such pioneering work, our study aims to unravel how the integration of generative models, especially in sim-to-real transfer techniques, within the perception-to-control pipeline of CAVs could revolutionize predictive modeling, simulation fidelity, and decision-making processes in autonomous vehicles. Our discussions explore the engines that power these models and extend into 3D environments where cameras provide crucial data, thus underscoring the advancements achieved and the challenges that remain in merging generative models with CAV technologies, emphasizing the pivotal role of advanced algorithms in achieving significant improvements in transportation safety and innovation.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Tsun-Hsuan_Wang2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09123v1",
  "title": "Optimal Top-Two Method for Best Arm Identification and Fluid Analysis",
  "modified_abstract": "Building upon the groundwork established in the domain of best policy identification in various contexts, such as discounted linear Markov Decision Processes, and leveraging insights from both theoretical and practical solutions to sampling and algorithmic efficiency, our work introduces a novel approach to solving the Best Arm Identification (BAI) problem through Top-$2$ methods. These methods have gained momentum for their efficacy in identifying the best arm, or the one with the largest mean among a finite set, using an algorithm that, at any given step, independently pulls the empirical best and the best challenger arm based on a fixed probability $\\beta$, with a guaranteed probability of incorrect selection below a specific threshold $\\delta >0$, illustrating an instance-specific analysis approach. Despite the known information-theoretic lower bounds on sample complexity for the BAI problem, determining the optimal $\\beta$ to match these bounds remains challenging. Our paper proposes an optimal top-2 type algorithm tailored to address this challenge by introducing a function of allocations that hinges on exceeding a specific threshold, thus guiding the sampling between the empirical best arm and the challenger arm with moderate-confidence. Through detailed analysis, including the application of the implicit function theorem and a keen focus on algorithms and sampling methods, we demonstrate the uniqueness and the optimal nature of our proposed algorithm as $\\delta \\rightarrow 0$, thus providing a new, theoretically grounded method for BAI problem-solving that closely adheres to the asymptotic path dictated by fluid dynamic models and achieves near-optimal sample complexity in the considered setting.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yassir_Jedra1",
  "manipulated_ranking": 5,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09117v1",
  "title": "Randomized Principal Component Analysis for Hyperspectral Image Classification",
  "modified_abstract": "This paper addresses the challenges posed by the high-dimensional feature space inherent in hyperspectral imagery, focusing on the need for dimensionality reduction to mitigate computational complexity. Drawing inspiration from recent advancements in self-supervised learning and transfer learning, particularly in the domain of hyperspectral image analysis where the acquisition of meaningful representations without semantically annotate semantic labels is critically important, our research investigates both traditional principal component analysis (PCA) and an innovative randomized principal component analysis (R-PCA) approach for classifying hyperspectral images. By employing support vector machines (SVM) and light gradient boosting machines (LightGBM), and reducing feature space to 20 and 30 dimensional representations for two distinct hyperspectral datasets (Indian Pines and Pavia University), we explore the efficacy of PCA versus R-PCA. Our experimental findings reveal that, while PCA demonstrates superior performance over R-PCA in SVM models for both datasets, the accuracy margins become significantly closer when employing LightGBM coupled with transfer learning techniques. The incorporation of attention mechanisms could further enhance the interpretability and effectiveness of these methods, although they were not explored in this study. Notably, LightGBM with original features achieved the highest classification accuracies, attaining 0.9925 and 0.9639 for the Pavia University and Indian Pines datasets, respectively. Through this investigation, we contribute to the ongoing discourse on effective dimensionality reduction techniques, particularly within the realm of hyperspectral image classification, and highlight the potential synergies between randomized dimensionality reduction methods and advanced machine learning algorithms.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Hyungtae_Lee1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09113v2",
  "title": "AutoLoRA: Automatically Tuning Matrix Ranks in Low-Rank Adaptation Based on Meta Learning",
  "modified_abstract": "Building on the foundation laid by successful approaches in efficient finetuning of large pretrained models for natural language processing (NLP) tasks, and inspired by advances in sparse model estimation techniques, our study introduces AutoLoRA, a novel framework designed to elevate the low-rank adaptation (LoRA) method. LoRA, which finetunes low-rank incremental update matrices atop frozen pretrained weights, has shown considerable effectiveness. However, its uniform rank assignment and exhaustive search for optimal rank, typically associated with gradient descent algorithms, lead to high computational costs and suboptimal performance in dealing with a wide array of NLP problems. To circumvent these issues, AutoLoRA implements a meta-learning approach that automates the identification of the optimal rank for each LoRA layer by associating a selection variable with each rank-1 matrix in a low-rank update matrix, determining its retention or disposal based on validation sets. The optimal rank is ascertained by thresholding these variables\u2019 values, a method informed by the principles of sparse generalized linear models, and extracts vital features for optimal adjustment. Our extensive experiments across a variety of NLP tasks, including understanding, generation, and sequence labeling, validate AutoLoRA's effectiveness, demonstrating its potential to set a new benchmark in efficient finetuning methodologies.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Mathurin_Massias1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09110v1",
  "title": "SINDy-RL: Interpretable and Efficient Model-Based Reinforcement Learning",
  "modified_abstract": "Inspired by recent breakthroughs such as Brax, which leverages the power of differentiable physics for reliable and scalable rigid body simulations, our work, SINDy-RL, addresses the complexities inherent in deep reinforcement learning (DRL), where sophisticated control policies navigate compounded dynamics in challenging environments. Recognizing the limitations posed by DRL's demand for extensive training data and the computational costs associated with deep neural networks, we leverage advancements in sparse dictionary learning and the Sparse Identification of Nonlinear Dynamics (SINDy) methodology to develop an approach that fuses interpretable model synthesis with the robustness of DRL. SINDy-RL epitomizes the next step in model-based reinforcement learning by offering efficient, comprehensible, and trustworthy dynamics models, reward functions, and control policies. Our experimentation across benchmark control scenarios and intricate fluid dynamics problems illustrates SINDy-RL's capacity to match state-of-the-art DRL algorithms with fewer environmental interactions and seamlessly integrates simulation tools for training, thus producing control policies significantly more compact than those generated by deep neural networks. Leveraging accelerators and parallelism in our computational framework has greatly enhanced simulation efficiency, enabling rapid model iteration and refinement through accelerated training cycles. This efficiency potentially paves the way for future reimplementations that are both quicker to develop and more resource-efficient.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~C._Daniel_Freeman1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09749v1",
  "title": "Towards Diverse Perspective Learning with Selection over Multiple Temporal Poolings",
  "modified_abstract": "Informed by advancements in Time Series Classification (TSC), including critical evaluations of Transformer-based solutions for time series forecasting, our work proposes a novel temporal pooling method named Selection over Multiple Temporal Poolings (SoM-TP) for enhancing TSC through dynamic pooling selection. SoM-TP introduces a method of learning from multiple perspectives by dynamically selecting the optimal temporal pooling for each data instance, using an attention mechanism. This approach, informed by self-attention mechanisms and Multiple Choice Learning (MCL) concepts, allows for a non-iterative pooling ensemble within a single classifier, thus addressing the traditional limitations of fixed temporal pooling mechanisms. By incorporating sub-series analysis and encoding techniques for temporal data representation into the SoM-TP framework, and leveraging series analysis, we further introduce a Diverse Perspective Learning Network (DPLN) with a perspective loss to regularize the learning process, ensuring consideration of all pooling perspectives. Our evaluation, including perspective analysis via Layer-wise Relevance Propagation (LRP), underscores the benefits of diverse perspective learning over single perspective approaches. Empirical results from extensive experiments on the UCR/UEA repositories show that SoM-TP, informed by self-attention and utilizing sub-series for enhanced series analysis, outperforms conventional CNN models based on singular temporal pooling methods and achieves state-of-the-art performance in TSC, highlighting the importance of perspective diversity, encoding, and series analysis in temporal data analysis. Anomaly detection techniques further support the identification of unique temporal patterns, enhancing the system's robustness. Code for this study has been made available at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ailing_Zeng1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09107v1",
  "title": "S^2MVTC: a Simple yet Efficient Scalable Multi-View Tensor Clustering",
  "modified_abstract": "Drawing inspiration from pivotal advancements in the field of clustering, particularly those centered around directed (digraphs) and undirected graphs which shed light on novel methodologies for understanding inter-cluster relationships and cluster structure, this paper introduces a simplified yet potent approach dubbed S^2MVTC for large-scale multi-view clustering. Unlike existing methods that primarily seek consensus embedding features through global correlations among anchor graphs or projection matrices, S^2MVTC centers on discerning and leveraging correlations of embedding features within and across different views by utilizing graphs. By stacking view-specific embedding features into a tensor and applying a rotation, coupled with the innovative tensor low-frequency approximation (TLFA) operator that embeds graph similarity into the learning of embedding features for smoothly representing these features across views, our approach ensures inter-view semantic consistency. This clustering technique significantly influences machine learning applications by facilitating a more nuanced understanding of clustered data and utilizing various forms of graphs for enhanced analysis. Furthermore, this approach contributes to the research in clustering theory by introducing a robust framework for multi-view tensor clustering. Experiments conducted on six expansive multi-view datasets confirm that S^2MVTC surpasses top-performing algorithms in clustering efficiency and CPU time, especially in the management of voluminous data and integrating machine learning techniques with graph-based methodologies for clustering. A case study highlighting the practical application of S^2MVTC in a specific topic of interest further showcases its versatility and effectiveness. Access to the S^2MVTC code has been provided at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Steinar_Laenen1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09101v1",
  "title": "Soften to Defend: Towards Adversarial Robustness via Self-Guided Label Refinement",
  "modified_abstract": "Drawing on the foundational aspects of adversarial training (AT) as outlined in the literature, including the utilization of min-max optimization strategies for enhancing network robustness, this paper explores the phenomena of robust overfitting within AT practices. Adversarial training (AT) is currently one of the most effective ways to obtain the robustness of deep neural networks against adversarial attacks. However, most AT methods suffer from robust overfitting, i.e., a significant generalization gap in adversarial robustness between the training and testing curves. This research identifies a connection between robust overfitting and the excessive memorization of noisy labels in AT, viewed through a lens of gradient norm optimization. The label noise issue, largely prompted by distribution mismatch and improper label assignments, inspired us to propose a label refinement approach for AT. Our Self-Guided Label Refinement tactic initially refines a more accurate and informative label distribution from over-confident hard labels, then calibrates the deep training process by dynamically incorporating knowledge from self-distilled models into the current model without the need for external teachers. Empirical outcomes affirm that the proposed defense training strategy effectively elevates the standard accuracy and robust performance across diverse benchmark datasets, attack types, and architectures. Additionally, information-theoretic analyses further delve into our method, underscoring the critical role of soft labels in achieving robust generalization through deep learning training optimization.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yihua_Zhang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09100v1",
  "title": "Virtual birefringence imaging and histological staining of amyloid deposits in label-free tissue using autofluorescence microscopy and deep learning",
  "modified_abstract": "Our work advances the frontiers of systemic amyloidosis diagnosis and investigation by capitalizing on the transformative potential of deep learning in interpreting complex biological data, in the spirit of recent breakthroughs such as CP2Image's novel approach to generating realistic cell images from CellProfiler representations. Systemic amyloidosis, characterized by the deposition of misfolded proteins in tissues, leads to organ dysfunction. The Congo red stain, though the gold standard for visualizing amyloid deposits, presents logistical challenges and diagnostic inconsistencies due to its manual and subjective interpretation under polarized light microscopy. We introduce the first demonstration of virtual birefringence imaging and virtual Congo red staining on label-free human tissue samples, employing autofluorescence microscopy paired with a generative deep learning model. This model transforms autofluorescence images into equivalents of images obtained through traditional histological staining processes, thereby generating consistent and realistically detailed images that streamline the clinical workflow for extracting relevant diagnostic information. Our approach, validated through blind testing and pathologist evaluations, particularly on cardiac tissues, offers a consistent and reproducible alternative to conventional staining methods, reducing the potential for diagnostic errors and simplifying single-cell profiling processes.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Bj\u00f8rn_Sand_Jensen1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09090v1",
  "title": "Dissipative Gradient Descent Ascent Method: A Control Theory Inspired Algorithm for Min-max Optimization",
  "modified_abstract": "This research builds upon the foundational insights derived from the recent achievements in neural networks and gradient descent methods, notably in how they adapt to optimize in complex, sometimes noiseless settings like the least-squares setup within the interpolation regime. Our focus is on addressing the common issue in Gradient Descent Ascent (GDA) methods for min-max optimization problems, where oscillatory behavior, influenced by noisy data, can lead to instability. To mitigate this, we introduce a dissipation term into the GDA updates, formulating a novel Dissipative GDA (DGDA) method that leverages insights from control theory. Unlike traditional GDA that may struggle with stability, particularly in bilinear settings, DGDA enhances performance through state augmentation and regularization of the saddle function without strictly introducing additional convexity/concavity. We establish linear convergence of the DGDA in both bilinear and strongly convex-strongly concave settings and demonstrate its superiority over traditional methods such as GDA, Extra-Gradient (EG), and Optimistic GDA through comparative performance analysis. Moreover, the application of kernel methods and predictive models further evidences DGDA's effectiveness in solving saddle point problems, underscored by numerical examples, hence revealing its potential as a robust min-max optimization technique, well-suited for handling arbitrary data distributions.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Loucas_Pillaud-Vivien1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09086v1",
  "title": "Learning from straggler clients in federated learning",
  "modified_abstract": "Informed by the substantial progress and intricate challenges presented by distributional approaches in machine learning, such as those disclosed in the realm of reinforcement learning, our work turns the spotlight on federated learning (FL) algorithms and their capacity to harness insights from straggler clients. This inquiry is spurred by the recognition of a persisting gap in how existing federated learning models engage with and learn from client devices that exhibit substantial delays in returning model updates, particularly in tasks requiring real-time updates or those operating in an offline context. Through the development of Monte Carlo simulations, informed by real-world deployments and online learning strategies, we probe into the effectiveness of synchronous optimization algorithms like FedAvg and FedAdam, as well as the asynchronous FedBuff algorithm, revealing their limitations in learning from significantly delayed clients in specific tasks. Building on these insights, we propose innovative modifications including distillation regularization and the use of exponential moving averages for model weights, aiming to maximize the likelihood of learning effectively from each client observation, regardless of delay. We introduce two novel algorithms, FARe-DUST and FeAST-on-MSG, which are designed to adeptly learn from these straggler clients by leveraging principles of distillation and averaging, respectively, in tasks such as EMNIST, CIFAR-100, and StackOverflow, with an emphasis on maximizing contextual understanding from sparse data inputs. Our empirical evaluations reveal that these algorithms not only enhance learning from straggler clients in terms of accuracy but also present more favorable trade-offs between training time and overall model accuracy in these benchmark tasks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Runzhe_Wu1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13838v1",
  "title": "Circuit Transformer: End-to-end Circuit Design by Predicting the Next Gate",
  "modified_abstract": "Inspired by the evolution of large language models (LLMs) that have revolutionized computational understanding and reasoning by sequentially predicting the next word, we extend this concept to the realm of electronic design, proposing that circuits, the \"language\" of electronics, can be effectively mastered through a similarly predictive approach. This ambition is motivated by work such as the development of generalized heuristic networks in AI planning, where models overcome significant computational complexity to efficiently generate goal-directed behavior. Our research navigates the unique challenges of applying LLM principles to circuit design, such as the complex, non-sequential structure of circuits and the critical need for precision without hallucination due to strict equivalence constraints. Incorporating symbolic representation, predicate logic, and action-based architectures, we propose a novel method to encode circuits for Transformer-based neural models, facilitating the prediction of the next logic gate in a circuit's design process. During the training phase, a system based on learning symbolic equivalences and applying heuristic strategies ensures the model adapts accurately to complex designs. We introduce an equivalence-preserving decoding process to ensure fidelity in the generated circuit paths. Networks serve as the foundation for these models. To demonstrate the potential of our approach, we present the \"Circuit Transformer,\" a Transformer-based model with 88M parameters trained for end-to-end logic synthesis. Coupled with Monte-Carlo tree search, our model achieves superior performance in logic synthesis tasks, surpassing traditional methods while maintaining strict equivalence, thereby illustrating the feasibility and promise of applying generative AI to intricate electronic design challenges.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Rushang_Karia1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09066v1",
  "title": "Hyperparameters in Continual Learning: a Reality Check",
  "modified_abstract": "In the field of continual learning (CL), the fine-tuning of hyperparameters plays a pivotal role in navigating the inherent stability-plasticity trade-off, a challenge that draws parallels to addressing cross-example within-class variability reduction in neural network training and preventing collapse. Identifying optimal hyperparameters is crucial for enhancing the efficacy of CL classifiers. Traditionally, the evaluation of CL algorithms has relied on training with a range of hyperparameter values using a benchmark dataset to define a CL scenario. The best performance obtained with the optimal hyperparameter value is typically used as the benchmark for algorithm evaluation. In this paper, we argue that this conventional evaluation protocol is not only impractical but also fails to thoroughly assess the CL competency of algorithms, possibly leading to collapse scenarios in algorithm performance. Turning back to the foundational principles of model evaluation in machine learning, we introduce a novel evaluation protocol that encompasses both Hyperparameter Tuning and Evaluation phases, employing different datasets within a consistent CL scenario and considering the geometry of surrogate loss functions like cross-entropy. During the Hyperparameter Tuning phase, algorithms undergo iterative training with varying hyperparameter values to identify the optimal settings. In the Evaluation phase, these optimal hyperparameters are then applied to gauge each algorithm's performance against criteria like loss and surrogate loss, providing a more accurate measure of their CL capabilities. Our empirical investigation, using CIFAR-100 and ImageNet-100 in a class-incremental learning context, not only questions the adequacy of existing assessment techniques but also reveals that certain algorithms recently heralded as state-of-the-art actually underperform in comparison to prior approaches upon scrutiny under our proposed evaluation framework. The inclusion of geometry in the evaluation criteria further deepens the understanding of algorithmic performance, aligning with recent developments in neural network training.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Wenlong_Ji1",
  "manipulated_ranking": 4,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09054v1",
  "title": "Keyformer: KV Cache Reduction through Key Tokens Selection for Efficient Generative Inference",
  "modified_abstract": "Inspired by prevailing challenges in multiple aspects of machine translation and language model efficiency, where the selection and optimization of token vocabulary play critical roles in performance and resource utilization, this paper introduces \"Keyformer\". Building upon transformers as the cornerstone architecture for Large Language Models (LLMs), Keyformer addresses the efficiency of generative language models during inference. Acknowledging the constraints imposed by memory bandwidth, particularly in the translation phase which relies heavily on interactions with the Key-Value (KV) Cache, Keyformer innovates by selectively retaining \"key\" tokens, identified through a novel scoring solution, in the KV cache. This optimizes memory bandwidth usage without sacrificing model accuracy. Evaluated across foundational models including GPT-J, Cerebras-GPT, and MPT, Keyformer achieves significant reductions in inference latency (2.1x) and improvements in token generation throughput (2.4x), focusing especially on tasks requiring extensive text generation such as bilingual lexicon induction and vocabularization, thereby enhancing bilingual translation efficiency. Our articulate approach towards KV cache size reduction not only addresses the memory bottleneck in generative inference but also propels forward the discussions on the strategic selection of key tokens for enhancing LLMs' efficiency.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zaixiang_Zheng2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09053v1",
  "title": "Towards a theory of model distillation",
  "modified_abstract": "Our exploration towards a theoretical framework for model distillation is stimulated by the critical need to comprehend and systematize the processes underlying the transformation of complex machine learning models into simpler, yet effective counterparts. This initiative is profoundly inspired by, and builds upon, the groundwork laid by pioneering efforts in the realm of neural network pruning and sparsity\u2014specifically, the exploration of non-discriminative filter pruning to enhance model efficiency without the loss of accuracy. We frame distillation through the lens of PAC (Probably Approximately Correct)-distillation, drawing a parallel to the seminal concept of PAC-learning and incorporating the notion of distribution variation using diverse datasets. Leveraging this theoretical perspective, we (1) introduce novel algorithms that distill neural networks into more concise decision tree formats, capitalizing on the 'linear representation hypothesis' and (2) elucidate how variations in neural architectures can serve as a more resource-efficient alternative to training models from scratch, or almost as a form of fine-tuning, discriminative against the variations in data, thereby making strides in decoding its intrinsic complexity. Our ambition is not only to explicate the potentials of model distillation but also to propose actionable methodologies that can practically reduce the computational burdens often associated with deep learning models, such as through classifier pruning.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Tanay_Narshana1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09048v1",
  "title": "Taming Cross-Domain Representation Variance in Federated Prototype Learning with Heterogeneous Data Domains",
  "modified_abstract": "Inspired by a wealth of precedents in leveraging machine learning methodologies to achieve optimized learning objectives without compromising data privacy, our study navigates the complexities of federated learning (FL) where conventional methods falter in the face of heterogeneous data domains. Federated learning has garnered considerable attention for enabling collaborative machine learning training, which includes state-of-the-art strategies in optimization, without the need to share private data, an initiative that closely aligns with previous work in efficient active learning and Gaussian Process Classification by addressing key challenges in data/label efficiency, query synthesis, and feature selection. However, navigating the real-world challenge of heterogeneity in data domains remains an unresolved quandary. Thus, our work introduces FedPLVM, a novel approach that addresses cross-domain feature representation variance through variance-aware dual-level prototypes clustering, optimization, and a pioneering $\\alpha$-sparsity prototype loss. This methodology not only addresses the disparities in data distribution across clients but also significantly enhances model generalization by optimizing the alignment of underrepresented domain samples, refining instance-level labeling, and improving the synthesis of practical query strategies through sophisticated sampling techniques, thereby advancing the field of federated prototype learning. Our evaluations across multiple datasets, including Digit-5, Office-10, and DomainNet, underscore the efficacy of FedPLVM in comparison to existing techniques, demonstrating enhanced selection of practical queries and improved learning outcomes through state-of-the-art optimization and synthesis strategies.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Guang_Zhao1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09039v1",
  "title": "Spatial-temporal Memories Enhanced Graph Autoencoder for Anomaly Detection in Dynamic Graphs",
  "modified_abstract": "This research is motivated by the pressing need for effective anomaly detection in dynamic graphs, which are increasingly prevalent across various applications, but remain challenging due to evolving structures and attributes. Building on pioneering works such as the use of optimal transport distances and kernel methods for graph neural network (GNN) architectures, which revolutionized the integration of structural information and dissimilarities in graph analysis, we introduce a novel Spatial-Temporal memories-enhanced graph autoencoder (STRIPE). STRIPE leverages Graph Neural Networks (GNNs) and gated temporal convolution layers with competitive pooling strategies to extract refined spatial and temporal features for learning dynamic patterns. It innovatively integrates separate spatial and temporal memory networks for capturing and preserving prototypes of normal patterns, enhancing anomaly detection through a mutual attention mechanism that synthesizes stored patterns with graph embeddings for accurate reconstruction of graph streams. This advanced methodology not only improves the model's sensitivity to anomalies by minimizing reconstruction errors but also emphasizes the importance of prototype compactness and distinctiveness in dynamic graphs, thereby making a significant leap forward in the competitive landscape of anomaly detection. Extensive experiments demonstrate STRIPE's superior performance in detecting anomalies, solving the problem of anomaly detection with an average improvement of 15.39% on AUC values over existing methods, thereby significantly advancing the field of anomaly detection in dynamic environments.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Titouan_Vayer1",
  "manipulated_ranking": 4,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10558v1",
  "title": "Adaptive Hybrid Masking Strategy for Privacy-Preserving Face Recognition Against Model Inversion Attack",
  "modified_abstract": "In an era where the robustness of machine learning models against adversarial attacks is increasingly scrutinized, as highlighted by initiatives like RobustBench that aim to standardize benchmarks for adversarial robustness, the challenge of protecting sensitive data used in training models such as face recognition (FR) systems against model inversion attacks (MIA) has emerged as a critical privacy concern. To tackle the limitations of existing defense algorithms that struggle to achieve an optimal balance between privacy and accuracy, this paper proposes an innovative adaptive hybrid masking strategy specifically designed to counteract MIA. By employing an adaptive MixUp strategy within the frequency domain for face images, our approach innovates beyond the traditional use of MixUp for mere data augmentation. This innovative strategy addresses the corruption of visual data as a means to obfuscate potential attackers. Recognizing the inherent trade-off between enhanced privacy and the accuracy of FR tasks, we devise a novel adaptive MixUp method enhanced through reinforcement learning, which allows for mixing a greater number of images without sacrificing recognition performance. Through thorough benchmarking and rigorous evaluation, by optimizing a reward function that maximizes privacy protection during the strategy network's training phase and minimizes the loss function of the FR system, we position the strategy and FR networks as antagonists in a balanced trade-off dynamics. Our experimental findings affirm that the proposed hybrid masking scheme surpasses current defense mechanisms in terms of both privacy preservation and recognition accuracy amidst MIAs, further advancing the discourse on safeguarding against extensive privacy vulnerabilities inherent in FR technologies.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Vikash_Sehwag1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09035v1",
  "title": "DiTMoS: Delving into Diverse Tiny-Model Selection on Microcontrollers",
  "modified_abstract": "Our work builds upon a rich tradition of attempts to mitigate the vulnerabilities of neural networks against adversarial examples through innovative approaches such as Stochastic Activation Pruning, yet pivots towards the unique challenge of enabling efficient and accurate deep neural network (DNN) inference on microcontrollers with constrained on-chip resources. Recognizing the limitations of traditional methods that compromise model accuracy for the sake of compression, we propose DiTMoS, a pioneering DNN training and inference framework that employs a selector-classifiers architecture. This architecture leverages the diversity of small, weak models, theorizing that their collective strength can surpass the accuracy of singular, more complex models. DiTMoS introduces strategies to enhance classifiers' diversity and ensure synergistic interactions between selector and classifiers through adversarial training, aiming to outmaneuver the adversary in this intricate game of learning. Furthermore, it improves classifiers' capability via heterogeneous feature aggregation. To offset the memory overhead from feature aggregation, we deploy a network slicing technique. Implemented on the Neucleo STM32F767ZI board and validated across diverse time-series datasets for human activity recognition, keyword spotting, and emotion recognition - without involving images - DiTMoS demonstrates notable gains, achieving up to a 13.4% accuracy improvement over leading baselines while effectively managing memory overhead and only marginally increasing latency.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Guneet_Singh_Dhillon1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09032v1",
  "title": "CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language Models to Coding Preferences",
  "modified_abstract": "The capacity of large language models (LLMs) to tailor outputs to specific coding preferences remains an underinvested area, despite the monumental strides observed in their application for generating code across various programming tasks. This study is motivated by efforts such as the development of neural rankers to predict the correctness of generated programs without execution, addressing the challenges of evaluating model-generated programs in real-world scenarios. In this light, our work presents CodeUltraFeedback, a preference dataset comprising 10,000 complex instructions designed to refine and align LLMs to coding preferences through AI-driven feedback. We employ a pool of 14 diverse large LLMs to generate responses, which are subsequently annotated for alignment with five distinct coding preferences using GPT-3.5 in an LLM-as-a-Judge framework, enabling both quantitative and qualitative feedback. Moreover, our introduction of CODAL-Bench, a benchmark for gauging LLM alignment with these coding preferences, demonstrates that CodeLlama-7B-Instruct, fine-tuned through reinforcement learning from AI feedback (RLAIF) and direct preference optimization (DPO), as well as neural rankers and fault-aware filtering/ranking methods for reducing erroneous outputs, outperforms 34B LLM variants in this benchmark. Additionally, the application of DPO using CodeUltraFeedback's AI feedback data substantially enhances functional correctness in HumanEval+ tasks compared to baseline models, further leveraging neural techniques for preference adjustment. Thus, our findings serve to narrow the existing gap in preference adjustment for LLM-generated code, pioneering a path towards more refined model alignment and RLAIF in the field of programming intelligence. Code and data are temporarily unavailable [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jeevana_Priya_Inala1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09030v1",
  "title": "An AI-Driven Approach to Wind Turbine Bearing Fault Diagnosis from Acoustic Signals",
  "modified_abstract": "Building on the foundation of deep spatio-temporal analysis applied to the challenging field of wind power generation, our research introduces a novel deep learning model specifically designed for the accurate classification of bearing faults in wind turbine generators using acoustic signals. Employing a sophisticated convolutional LSTM architecture, we ingeniously leverage audio data from five distinct fault types to not only train but also validate the model's efficacy. The meticulous process of collecting and processing raw audio signal data into frames thus allowed us to comprehensively capture essential time and frequency domain information. The model emerged as exceptionally accurate on training samples, further proving its robust generalization ability during validation with an overall accuracy surpassing 99.5%, while maintaining a false positive rate below 1% for normal status. Such compelling results underscore the model's potential to significantly boost the reliability and efficiency of wind power generation through precise fault diagnosis and maintenance of wind turbine bearings.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jiangyuan_Li1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08980v1",
  "title": "Architectural Implications of Neural Network Inference for High Data-Rate, Low-Latency Scientific Applications",
  "modified_abstract": "This research is inspired by the latest advancements in neural network (NN) architectures for dealing with high data-rate and low-latency requirements, particularly in the context of scientific applications where data throughput and processing speed are critical. Prior works, including investigations into the infinite-width limit of deep neural networks (DNNs) and the characterization of recurrent neural networks (RNNs) through the Recurrent Neural Tangent Kernel (RNTK), underline the importance of understanding NN dynamics and generalization capabilities. Building upon these insights, our study focuses on the architectural implications necessary for running NNs with all parameters stored on-chip for scientific fields that leverage massive data influxes, necessitating extreme throughput and minimal latency. We highlight that to cater to these extreme demands, NNs must not only keep all parameters on-chip but also, in certain scenarios, necessitate the development of custom chips designed through a codesign approach to satisfy the bandwidth and latency constraints while maintaining generalization across wide networks. Our findings underscore the critical architectural considerations for hardware meant to support NN inference in high data rate and low latency settings, pointing towards a future where scientific applications can harness the power of NNs more effectively. In particular, our study examines the influence of tangent kernels in the evolution and generalization of neural network architectures necessary for these applications. The term 'form' relates to the way these systems are structured to cater to specific operational needs, emphasizing the architectural shapes and formations that are pivotal for their function.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sina_Alemohammad1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08978v1",
  "title": "AutoGuide: Automated Generation and Selection of State-Aware Guidelines for Large Language Model Agents",
  "modified_abstract": "Our research on AutoGuide is deeply motivated by the aspiration to mitigate the knowledge gap of Large Language Models (LLMs), a challenge highlighted in preceding studies. In particular, research on compositional task representations for LLMs, which underscores the benefits of prompt-free approaches for improved task generalization, lays foundational insights into the intricate relationship between model knowledge and task performance. Building on this understanding, AutoGuide presents an innovative framework for augmenting pre-trained LLMs with explicit, state-aware guidelines derived from offline data. By formulating guidelines in concise, natural language with a conditional structure that resonates with compositional and latent knowledge representations, our framework significantly enhances the decision-making capabilities of LLM-based agents, especially in zero-label settings where traditional pre-training and prompt-based learning fall short. Our empirical evaluation demonstrates that AutoGuide substantially surpasses existing LLM-based baselines in sequential decision-making tasks, marking a pivotal step towards more knowledgeable and effective LLM agents. This achievement in task generalization is achieved without relying on explicit prompts, suggesting a move towards leveraging internal codebooks and latent understanding within LLM frameworks for advanced guideline generation. Our tests further validate the robustness and adaptability of our approach in varying domains, establishing a benchmark in learning and task performance for LLMs.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yanan_Zheng1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09743v1",
  "title": "The Human Factor in Detecting Errors of Large Language Models: A Systematic Literature Review and Future Research Directions",
  "modified_abstract": "In the era post the inauguration of groundbreaking tools like ChatGPT by OpenAI, the exploration of Large Language Models (LLMs) and their performance across intricate domains have surged, marking a paramount shift in Artificial Intelligence engagement metrics. However, these technological marvels are not immune to slip-ups such as 'hallucinations' or data omissions, which could potentially derail operations in sectors where precision is indispensable. Amid the technical endeavors to rectify these shortcomings, it is pivotal to dissect the human angle in curbing inaccuracies in LLM outputs, including the development of instruction-finetuning strategies that harness human insight for error detection. Our systematic literature review is inspired by initiatives like 'MINT', which assess LLM's multi-turn interaction capabilities, underscoring the significance of extending beyond single-turn analysis to grasp the nuanced dynamics of LLM applications. This study propels the narrative further by magnifying the human factors instrumental in error detection within LLM outputs, laying a foundation for sophisticated user training and strategic LLM deployment. Through a meticulous synthesis of pertinent literature up to January 11th, 2024, and highlighting prospective study trajectories, this research aspires to harmonize the technological evolution of LLMs with human acuity, especially in contexts demanding unerring accuracy. The nuanced approach reiterates the indispensability of human insight in tandem with AI advancements, such as code evaluation and reinforcement learning mechanisms, steering towards a balanced exploitation of LLMs across sensitive and precision-oriented spheres.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Lifan_Yuan1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08969v1",
  "title": "The Full-scale Assembly Simulation Testbed (FAST) Dataset",
  "modified_abstract": "Leveraging insights from pioneering research in realistic long-horizon tasks and embodied visual navigation, our work introduces a significant contribution to the field of virtual reality (VR) with the Full-scale Assembly Simulation Testbed (FAST) Dataset. This dataset, generated within a VR environment, aligns with the growing interest in utilizing VR tracking and interaction data for machine learning applications such as user identification, predicting cybersickness, navigating digital environments, and estimating learning gains. Addressing the critical shortage of open datasets in this domain, our dataset comprises data from 108 participants (50 females, 56 males, 2 non-binary) engaged in assembling two distinct full-scale structures in VR, recorded over multiple episodes. We detail the dataset's collection process, including exploratory scenes from the assembly process, its descriptors, and potential applications, providing a resource poised to fuel future VR research endeavors. The dataset is particularly useful for researchers focusing on search strategies within embodied navigation tasks, exploring neural network-based methods for understanding human-VR interactions, and integrating image-goal navigation.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Justin_Wasserman1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08965v1",
  "title": "Deep Learning Based Dynamics Identification and Linearization of Orbital Problems using Koopman Theory",
  "modified_abstract": "Motivated by the imperative need for efficient satellite control and inspired by groundbreaking strategies in multi-agent control under adversarial conditions, our work introduces a revolutionary data-driven framework leveraging deep learning-based Koopman Theory for dynamics identification and linearization of pivotal aerospace challenges - the Two-Body Problem and Circular Restricted Three-Body Problem. This framework, optimized through empirical studies and advanced optimization techniques, exploits a custom-architected Deep Neural Network to discover the linear Koopman operator, enabling global linearization into a linear time-invariant (LTI) system purely from data. Moreover, our approach is distributed across online systems attesting to its flexibility and adaptive capabilities. Demonstrating a remarkable ability to generalize across Two-Body systems without re-training and accurately approximate the dynamics of the Circular Restricted Three-Body Problem, including considering the effects of relevant perturbations, our paper highlights the versatility and robustness of the proposed approach in advancing satellite control methodologies empirically, and underscores the need for guarantees in this optimization-based approach.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Udaya_Ghai1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08955v1",
  "title": "Towards Efficient Risk-Sensitive Policy Gradient: An Iteration Complexity Analysis",
  "modified_abstract": "In the pursuit of creating robust autonomous agents, this study is inspired by significant prior works in Reinforcement Learning (RL), including novel methodologies in structured bandit problems, which have substantially contributed to our understanding of efficient exploration under uncertainty. Specifically, our work addresses the gap in the analysis of iteration complexity within risk-sensitive RL frameworks, where the issue is akin to solving a complex problem with many potential solutions or 'arms' in bandit terminology, and designing policies that balance rewards with risks. Risk-sensitive RL, emphasizing the trade-off between expected returns and risk, presents a promising direction for designing policies that are not only optimal but probabilistically robust. Regret, an important metric in evaluating algorithms in bandit problems, is minimized in our approach to managing the problem effectively. In this exploration, we use the bandit problem as an analogy for risk-sensitive decision-making scenarios and focus on the iteration complexity analysis of the risk-sensitive policy gradient method, employing the REINFORCE algorithm and the exponential utility function. Our findings reveal an iteration complexity of $\\mathcal{O}(\\epsilon^{-2})$ for achieving an $\\epsilon$-approximate first-order stationary point (FOSP), showcasing the potential for risk-sensitive RL algorithms to achieve better iteration complexity than their risk-neutral counterparts without incurring additional computational overhead per iteration. Moreover, the conditions under which risk-sensitive algorithms can outperform risk-neutral ones in providing robust recommendations for action selection are thoroughly characterized. Simulation results further corroborate the theoretical advantages of adopting a risk-averse stance, with convergence and stability notably achieving faster after approximately half of the experimented episodes compared to risk-neutral scenarios.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Shreyas_Chaudhari1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.12090v1",
  "title": "Foundation Models and Information Retrieval in Digital Pathology",
  "modified_abstract": "Inspired by the remarkable advancements in image retrieval and processing as evidenced by innovative approaches in leveraging Convolutional Neural Networks (CNNs) for object image retrieval and enhancing the capabilities of 3D reconstruction technologies, this paper explores the integration of foundation models, large language models (LLMs), generative artificial intelligence (AI), and content-based image retrieval (CBIR) within the rapidly evolving field of digital pathology. The swift progress in computing and algorithmic methodologies, including sparse representation techniques and matchable feature extraction, has set the stage for a profound transformation in how medical images are analyzed, stored, and retrieved from databases, marking a significant leap in the quest for more efficient and precise diagnostic tools. Our review spans the state-of-the-art techniques in digital pathology, aiming to capture and synthesize these ongoing innovations, particularly those dealing with overlapping information layers, into a comprehensive understanding of current capabilities and future potentialities in the realm of foundation models and information retrieval. In particular, we highlight the importance of novel training approaches, including triplet-based training, to improve CBIR systems' effectiveness in digital pathology, facilitating the management of objects and their overlaps in medical images.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Tianwei_Shen1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08946v1",
  "title": "Usable XAI: 10 Strategies Towards Exploiting Explainability in the LLM Era",
  "modified_abstract": "The evolution of Explainable AI (XAI) now encompasses methodologies for deciphering the complex mechanics of Large Language Models (LLMs), aiming to mitigate their transparency issues. This shift necessitates a paradigm transformation in XAI approaches due to the intricate architecture and broad deployment of LLMs, which challenge the application of conventional XAI protocols. These LLMs, unlike simpler machine learning models, classifiers, or earlier neural networks, offer unique opportunities to reciprocally enhance XAI through their sophisticated capabilities. Building upon insights derived from foundational works that aimed to fortify explanation methods against manipulations in deep neural networks, our paper introduces 'Usable XAI' specifically designed for LLMs. We explore the dual potential of XAI not only in unraveling LLM functionalities but also in leveraging LLM traits to refine XAI practices, focusing on clarifying explanations, preventing manipulations, and optimizing decision-making processes during training. We propose 10 innovative strategies, detailing key techniques and the challenge of preventing manipulations, decision transparency, and training strategies for enhancing explanations. We provide practical case studies to exemplify the acquisition and application of explanations in LLM contexts. The associated code for this research has been removed for de-identification purposes.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Pan_Kessel1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08941v1",
  "title": "Towards Model-Agnostic Posterior Approximation for Fast and Accurate Variational Autoencoders",
  "modified_abstract": "In the pursuit of enhancing Variational Autoencoders (VAEs), our study is greatly informed by previous investigations into computational efficiency and optimization within machine learning models, such as the innovative approaches taken to address the computational-to-statistical gap in spiked matrix recovery using generative priors. Drawing parallels from these advancements, we address a fundamental challenge in VAEs: the inefficiency in early-stage joint training of the generative and inference models due to poor approximation of latent code posteriors. Our methodology introduces a novel, deterministic, model-agnostic posterior approximation (MAPA) technique, which, unlike network-based strategies that demand extensive computational resources, offers a scaled approach to overcoming these problems. This facilitates an improved reconstruction process by enabling the independent training of these models. Unlike traditional methods that necessitate the knowledge of the true prior and likelihood for posterior approximation, our technique sidesteps these requirements, proposing a streamlined pathway to improved performance. Preliminary experiments on low-dimensional synthetic data affirm (1) MAPA's capability in capturing the essence of the true posterior, through nonasymptotic analysis, and (2) its superiority in density estimation and signal fidelity over conventional methods with a reduced computational burden, even when dealing with sparse and rank-one data settings. This paper not only introduces a promising direction for optimizing VAEs but also lays down a strategic framework for extending MAPA to tackle the complexities associated with high-dimensional and rank-one data analysis.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jorio_Cocola1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08939v1",
  "title": "FogGuard: guarding YOLO against fog using perceptual loss",
  "modified_abstract": "In our work, inspired by a rich history of adapting deep neural networks (DNNs) to handle corrupted images and improve visual perception in challenging conditions, such as those introduced by the Corrupted Image Modeling (CIM) for self-supervised visual pre-training, we propose FogGuard. Our novel fog-aware object detection network, FogGuard, is designed to overcome the obstacles introduced by foggy weather conditions, critical for applications like autonomous driving systems which demand high reliability in object detection across adverse weather conditions. FogGuard innovates upon existing approaches by focusing not on image enhancement or domain adaptation\u2014which either attempt to reclaim fogless visuals or eschew domain-specific training\u2014but on optimizing object detection directly within fog-impacted images through a unique Teacher-Student Perceptual loss approach. By leveraging selected techniques of self-supervised learning for corrupt image handling, employing YOLOv3 as the foundational object detection architecture, incorporating pre-training elements uniquely tailored to enhance its understanding of foggy conditions, and integrating an encoder within its design for more effective feature extraction, our model demonstrates superior detection capabilities in foggy conditions, as evidenced by a substantial improvement in mean average precision (mAP), achieving 69.43% on the RTTS dataset compared to YOLOv3's 57.78%. Despite the elevated training complexity, FogGuard introduces no additional inference overhead, maintaining operational efficiency in real-time applications. The segmentation capabilities, although not the primary focus, also benefit indirectly from the enhanced learning regime, paving the way for more refined delineation of object contours under adverse conditions.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Xinggang_Wang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08938v1",
  "title": "A non-asymptotic theory of Kernel Ridge Regression: deterministic equivalents, test error, and GCV estimator",
  "modified_abstract": "Our work is motivated by recent advancements in non-asymptotic statistical learning theory, particularly in the areas of control and system identification, which have unveiled novel perspectives on analyzing and enhancing the accuracy and reliability of statistical estimators across various domains. Similarly, in the context of kernel ridge regression (KRR), an important machine learning method, we study the learning of an unknown target function $f_*$ from i.i.d. data $(u_i,y_i)$, $i\\leq n$, with each $u_i \\in U$ representing a covariate vector and $y_i = f_* (u_i) +\\varepsilon_i \\in \\mathbb{R}$. Our approach is akin to a tutorial, guiding the reader through the empirical success in approximating the test error of KRR via a closed-form estimate based on an 'equivalent' sequence model dependent only on the kernel's spectral properties, and addressing the lack of theoretical underpinning when spectral and concentration properties on the kernel's machine-learning-based eigendecomposition are considered. Addressing this, our paper introduces a non-asymptotic deterministic approximation for KRR's test error alongside explicit bounds on both system control and machine learning implications, a derivation made possible through the application of deterministic equivalents for random matrix functionals in a dimension-free setting. This framework extends to several classic examples, showing substantial theoretical and numerical congruence. Also, under the given setup, we discuss the role of the generalized cross-validation (GCV) estimator in accurately estimating the test error and determining an optimal regularization parameter for KRR, thus proposing a practical approach for parameter selection based solely on available data.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ingvar_Ziemann1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10557v1",
  "title": "Second-Order Information Matters: Revisiting Machine Unlearning for Large Language Models",
  "modified_abstract": "In the current era of Large Language Models (LLMs) development, showcasing instances like ChatGPT, LLaMa, and Gemini, the community faces challenges not solely on machine performance but significantly on issues like privacy leakage and copyright violation within the training corpus, mirroring real-world legal confrontations such as the lawsuit against OpenAI and Microsoft by the Times. This work is inspired by a breadth of precedents in the field, including innovative approaches to NLP system meta-evaluation and analysis as seen in projects like EXPLAINABOARD, that push the boundaries of how we understand model performance and biases through a richer, multi-dimensional perspective. We direct our focus towards a comparatively underexplored avenue of 'unlearning' in LLMs\u2014a critical need for addressing inadvertent privacy violations\u2014by advancing from traditional first-order methods to those exploiting second-order information (Hessian). Our proposition leverages classic Newton update mechanics, communicated effectively for a broad audience, offering a robust, multi-task, data-agnostic/model-agnostic pathway for unlearning that not only ensures utility and performance preservation but also enhances privacy guarantees through the specific release of unlearned models. Our comprehensive evaluation, absent of errors and spanning across four NLP datasets and a real-world case study, through which our methods exhibit consistent superiority in addressing the unlearning challenge over existing first-order approaches, confirming the critical role of development in LLMs. Links to code and resources have been omitted for de-identification.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Shuaichen_Chang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08901v1",
  "title": "A Framework for Strategic Discovery of Credible Neural Network Surrogate Models under Uncertainty",
  "modified_abstract": "Informed by previous investigations into the calibration and consistency of adversarial surrogate losses, our study addresses the increasing reliance on deep neural networks, a cornerstone of machine learning, for constructing surrogate models in simulating complex physical systems. These studies highlight the necessity for robust methods in uncertainty quantification and credibility assessment, essential for the effective deployment of these models in critical decision-making scenarios. We introduce the Occam Plausibility Algorithm for surrogate models (OPAL-surrogate), a comprehensive framework that strategically identifies predictive, neural-network-based surrogate models from a vast landscape of potential configurations, including diverse neural architectures, hyperparameter selections, and training loss calibration techniques. OPAL-surrogate integrates hierarchical Bayesian inferences and model validation tests, utilizing advanced loss calibration techniques to assess the credibility and predictive reliability of these models under uncertainty. It thereby offers a balanced approach to managing model complexity, accuracy, and classifier capability through meticulous calibration of losses. The efficacy of this framework is validated through applications in modeling the deformation of porous materials and turbulent combustion flow, emphasizing its potential in enhancing the reliability of surrogate models for high-stakes simulations. Our findings underscore the importance of innovative classifiers and loss management in building neural network surrogate models that can navigate the intricacies of physical systems under conditions of uncertainty.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Natalie_Frank1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08896v1",
  "title": "One-Shot Averaging for Distributed TD($\\u03bb$) Under Markov Sampling",
  "modified_abstract": "In this work, we are inspired by the evolving landscape of reinforcement learning methodologies, particularly the shift towards accommodating continuous-time decision-making processes as observed in studies such as the development of Continuous-Time-Controlled MDPs. This paradigm shift underscores the necessity for innovative approaches in tackling policy evaluation. Thus, we consider a distributed setup for reinforcement learning, where each agent has a copy of the same Markov Decision Process but transitions are sampled from the corresponding Markov chain independently by each agent. We show that in this setting, we can achieve a linear speedup for TD($\\lambda$), a family of popular methods for policy evaluation, in the sense that $N$ agents can evaluate a policy $N$ times faster provided the target accuracy is small enough. Notably, this speedup is achieved by \"one shot averaging,\" a procedure where the agents run TD($\\lambda$) with Markov sampling independently and only average their results after the final step. This significantly reduces the amount of communication required to achieve a linear speedup relative to previous work.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Tianwei_Ni1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08880v1",
  "title": "REFRESH: Responsible and Efficient Feature Reselection Guided by SHAP Values",
  "modified_abstract": "In the era of responsible artificial intelligence, feature selection has gained paramount importance for building machine learning models that are not only accurate but also fair, robust, and suited for scenarios with imbalanced datasets. This task, however, is often hampered by the computational intensity required, especially with large datasets and when additional model performance characteristics such as fairness, robustness, and privacy are considered. Taking inspiration from recent progress in federated learning, which addresses challenges in data heterogeneity and privacy through innovative ensemble learning, distributed computing, and privacy-preserving techniques, we introduce the concept of feature \\emph{reselection} as a solution to efficiently adapt models to secondary performance objectives such as fairness, robustness, and privacy without retraining from scratch. Our proposed method, REFRESH, leverages SHAP values and correlation analysis to identify an optimal subset of features that adhere to these additional performance constraints, thereby circumventing the need for extensive computational resources and mitigating issues associated with imbalanced datasets. Through empirical evaluations on diverse datasets, including a large-scale loan defaulting dataset, REFRESH demonstrates its capability to efficiently refine models to better align with evolving regulatory and ethical standards. This refinement process crucially involves the use of proxies to ensure that feature reselection reflects not just primary, but also secondary performance objectives, marking a significant step towards the realization of truly responsible AI. Furthermore, REFRESH's methodology inherently supports the involvement of multiple clients in the learning process through the lens of distributed computing, thereby enhancing its applicability in distributed computing environments and aiding in label-aware learning optimizations.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yaopei_Zeng1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08879v1",
  "title": "Multi-Objective Optimization Using Adaptive Distributed Reinforcement Learning",
  "modified_abstract": "Building upon the growing understanding that dynamic and complex environments such as the Intelligent Transportation System (ITS) necessitate the consideration of multiple, often conflicting objectives, this work is inspired by a range of previous studies, including those on strongly adaptive online learning, regret minimization mechanisms, and online decision-making. These studies underscore the importance of adaptability and efficiency in the face of changing circumstances and objectives. In response, we propose a novel multi-objective, multi-agent reinforcement learning (MARL) algorithm designed for high learning efficiency and low computational demands, which is well-suited to dynamic, distributed, and noisy environments with sparse and delayed rewards, such as those found in ITS with edge cloud computing. Our approach incorporates a prior-based method to efficiently handle the exploration-exploitation trade-off, thus enhancing the adaptability of our algorithm. Moreover, the algorithm adjusts dynamically over the horizon of decision-making, ensuring optimized outcomes through iterative learning adjustments characterized by online adaptation and regret minimization. Through empirical studies in an ITS environment, our proposed algorithm not only demonstrates rapid adaptation and superior performance across both individual and systemic metrics when compared to leading benchmarks but also introduces a modularized, asynchronous online training methodology that addresses several practical challenges. Furthermore, our implementation on a single-board computer showcases the algorithm's capability for swift inference, completing the process in just 6 milliseconds, highlighting its potential for real-world application in ITS and similar domains.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yuanyu_Wan1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08854v1",
  "title": "Moments of Clarity: Streamlining Latent Spaces in Machine Learning using Moment Pooling",
  "modified_abstract": "Inspired by advancements in deep generative modeling and the need for more interpretable and manageable latent spaces in machine learning, our work introduces 'Moment Pooling' as a groundbreaking mechanism aimed at reducing the dimensionality of latent spaces without compromising model performance. Moment Pooling, an innovative leap from the traditional Deep Sets networks, extends the concept of summation to include arbitrary multivariate moments, offering a way to control and achieve higher effective latent dimensionality within a set latent dimension. This method significantly streamlines the training process, making it more efficient and simplified for deriving insights from high-dimensional data. During the training phase, our proposed method was rigorously tested through the collider physics task of quark/gluon jet trajectories classification by evolving Energy Flow Networks (EFNs) into Moment EFNs. Remarkably, we discovered that even with minimal latent dimensions, Moment EFNs could perform on par with, or even surpass, standard EFNs sporting higher latent dimensions, thus effectively controlling the agglomeration of complex data points in latent spaces. This reduction in latent dimensions opens the possibility for direct visualization and interpretation of the internal representations, even allowing for the extraction of learned internal jet representations in closed form. Our approach, integrating models and traits of adaptive learning during training, marries the foundational principles observed in the development and unification of generative models with GFlowNets, securing a pivotal step towards more effective and specific machine learning frameworks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Tian_Qi_Chen2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08851v1",
  "title": "PAPERCLIP: Associating Astronomical Observations and Natural Language with Multi-Modal Models",
  "modified_abstract": "Drawing on the conceptual breakthroughs of previous research, such as the HTLM model's innovative use of large-scale web crawl data and structured prompting to achieve state-of-the-art performance in tasks like zero-shot summarization, PAPERCLIP (Proposal Abstracts Provide an Effective Representation for Contrastive Language-Image Pre-training) proposes a unique approach to bridge the gap between astronomical observations and natural language. Our method leverages a neural network model, fine-tuned from a pre-trained Contrastive Language-Image Pre-training (CLIP) model on a dataset of successful observing proposal abstracts and corresponding downstream observations\u2014optionally summarized via guided generation using large language models and advanced auto-prompting techniques. Conducted with data from the Hubble Space Telescope (HST), our experiments showcase how the fine-tuned model, through rigorous training and an innovative loss function, fosters a meaningful joint representation that facilitates efficient image and text description retrieval processes, and enhances classification tasks for astronomical data, underpinning the model's potential to supplant task-specific models with generalist foundation models, thereby leveraging textual input as a versatile interface for interaction with astronomical data.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Armen_Aghajanyan1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08763v1",
  "title": "Simple and Scalable Strategies to Continually Pre-train Large Language Models",
  "modified_abstract": "In response to the challenge of efficiently updating Large Language Models (LLMs) with new data without starting the pre-training process afresh, our research builds on insights drawn from the exploration of linear interpolation in parameter space for fine-tuned models. This line of inquiry has demonstrated that simple methods can navigate the high-dimensional spaces of model parameters effectively, offering a foundational technique that underpins our approach to continual learning. Interestingly, this method also highlights the connectivity between different states in the parameter space, illustrating how interpolation can serve as a bridge. In this context, we propose a straightforward and scalable method involving learning rate (LR) re-warming, LR re-decaying, and replay of previous text data, to update models with new data efficiently. This fine-tuning approach not only maintains performance on previously seen text data but also adapts seamlessly to new data and demonstrates incredible potential in text generation tasks, as evidenced by our experiments across common pre-training datasets and through significant language shifts, using models of varying sizes up to 10 billion parameters. Our findings point towards the feasibility of continually pre-training LLMs with minimal computational overhead, thereby sidestepping the need for full re-training. Furthermore, inspired by foundational work on parameter space navigation and its relevant connectivity, we put forward alternative non-linear learning rate schedules that mitigate the potential for forgetting when re-warming learning rates, thereby extending the toolkit for effective and efficient updates of LLMs.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Daniil_Gavrilov1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08757v2",
  "title": "Efficient Combinatorial Optimization via Heat Diffusion",
  "modified_abstract": "Drawing upon the advancements from various domains within machine learning, like those that have investigated efficient exploration methods through the integration of episodic and global bonuses, our research introduces a novel perspective to the field of combinatorial optimization. Combinatorial optimization problems, known for their intricate discrete nature, have often been restricted by the conventional strategies' incapacity to explore vast sections of the solution space efficiently. In contrast to these traditional approaches that primarily aim to widen the solver's search range directly, our work innovatively applies the concept of heat diffusion. This method not only preserves the optima of the targeted function but also enhances the flow of information from remote regions of the solution space, significantly improving the efficiency of the solver\u2019s navigation through complex tasks in the problem landscape. With an emphasis on the episodic training of agents in simulated environments, we explore the novelty of our approach in deeper interaction of an agent's decision-making process, establishing a new framework for tackling a broad spectrum of combinatorial optimization challenges. Our findings not only underscore the effectiveness of employing thermodynamic principles to augment computational strategies in exploration within diverse, high-stakes situations but also hint at broader implications for their implementation across computational paradigms in machine learning, particularly in the training of models in complex environments.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Roberta_Raileanu1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08755v1",
  "title": "DAM: Dynamic Adapter Merging for Continual Video QA Learning",
  "modified_abstract": "Informed by breakthroughs in video generation and distribution, such as presented in works on video diffusion models which advanced the generative generation of temporally coherent, high fidelity videos, our study introduces DAM, a parameter-efficient method for continual video question-answering (VidQA) learning. Building upon the notion that integrating diverse video and image data can substantially augment the learning process, our method leverages Dynamic Adapter Merging to effectively address (i) mitigating catastrophic forgetting, (ii) enabling efficient adaptation to continually arriving datasets, (iii) handling inputs from unrelated datasets during inference, and (iv) facilitating knowledge sharing across similar dataset domains. By sequentially training dataset-specific adapters for each VidQA task\u2014while freezing the parameters of a large pre-trained video-language modeling backbone\u2014we ensure our system's adaptability and relevance for both task-oriented and text-conditioned video QA challenges. Furthermore, during inference, our model employs a novel non-parametric router function to compute relevance probabilities for each adapter in response to a video-question input from an unknown domain. This enables the dynamic adapter merging scheme to craft a new adapter instance specifically for the given test sample, thereby enhancing prediction accuracy and promoting cross-domain knowledge sharing. Our DAM model significantly outperforms state-of-the-art continual learning approaches by a margin of 9.1% while exhibiting 1.9% less forgetting across six diverse VidQA tasks, thereby setting new benchmarks in the field. Additionally, when extending DAM to continual image classification and image QA tasks, it surpasses previous methods by a substantial margin, notably improving task performance. The code for DAM is made publicly available at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Alexey_Alexeevich_Gritsenko1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08750v1",
  "title": "Neural reproducing kernel Banach spaces and representer theorems for deep networks",
  "modified_abstract": "Inspired by advances in understanding neural representation and robustness against adversarial attacks, this paper explores the function spaces defined by deep neural networks, moving beyond the traditional context of reproducing kernel Hilbert spaces, which do not fully encapsulate the properties of networks utilized in contemporary practice. We establish that deep neural networks inherently define a class of reproducing kernel Banach spaces, characterized by norms that encourage a certain level of sparsity. This insight allows these spaces to attune to latent structures within data and their representations, possibly enhancing resilience to perturbations introduced by adversarial examples. Through a synergistic application of reproducing kernel Banach space theory and variational principles, guided by robust and empirical methods, we develop representer theorems that provide a theoretical underpinning for the finite architectures frequently seen in numerous practical applications, including those informed by neuroscience. This investigation not only broadens the understanding of function spaces pertinent to deep neural networks but also outlines a pathway toward embracing more realistic and practice-oriented neural architectures.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~SueYeon_Chung1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08743v1",
  "title": "Steering LLMs Towards Unbiased Responses: A Causality-Guided Debiasing Framework",
  "modified_abstract": "Prompted by a rich body of work that seeks to address biases inherent in automated decision-making and prediction systems, such as those outlined in studies on counterfactual-based decision support frameworks, our paper explores a nuanced intersection of causality, bias, and large language models (LLMs). Considering the significant role LLMs play in critical areas like hiring and healthcare, developing strategies to mitigate their biases becomes paramount. This paper introduces a causality-guided debiasing framework that addresses the association between demographic information and LLM outputs through a dual focus: understanding the data-generating process of the training data and the internal reasoning process of LLMs during inference. Our proposed framework uniquely combines and extends upon existing debiasing approaches by utilizing causal relationships to refine prompt design for LLMs, facilitating the generation of unbiased responses in classification tasks. Learning from empirical evidence and incorporating large-scale learning strategies, our framework advances the understanding of debiasing in LLMs and offers principled guidelines for improving their fairness, even when operating as black-box systems. Specifically, through sets of controlled competitive experiments on various tasks, we illustrate how prompt modifications can reduce classification regret and lead to improved decision-making and prediction accuracy.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Eleni_Straitouri1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08741v1",
  "title": "Learning How to Strategically Disclose Information",
  "modified_abstract": "Inspired by the critical role of information dynamics and their strategic use in decision-making processes, our research explores the domain of strategic information disclosure within a game-theoretic framework, an area closely related to the challenges encountered in non-stationary stochastic environments, such as those tackled by multi-armed bandit and expert problems. In this vein, we investigate a model where an information provider (sender) possesses private information desired by a receiver, leading to a strategic interaction reminiscent of a Stackelberg game. We delve into an online version of this interaction, focusing on conditions where the receiver is of an unknown type and adversarially chosen in each round, which dynamically changes the strategic landscape. By concentrating on Gaussian priors and quadratic costs, we demonstrate the achievability of $\\mathcal{O}(\\sqrt{T})$ regret with full information feedback in this dynamic setting, and extend our findings to encompass general convex utility functions through novel parametrization. Additionally, we address the Bayesian Persuasion problem by incorporating statistical distributions for more informative signaling policies and showcasing the feasibility of $\\mathcal{O}(\\log(T))$ regret. Our investigation culminates in establishing sublinear regret bounds in a partial information feedback scenario, supported by simulations that validate our theoretical assertions on counts of interactions and performance, making a striking contribution to the field.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Chen-Yu_Wei1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08728v1",
  "title": "Ambient Diffusion Posterior Sampling: Solving Inverse Problems with Diffusion Models trained on Corrupted Data",
  "modified_abstract": "Informed by a range of innovative techniques in the realm of 3D mapping and processing, such as those presented in SplaTAM, which demonstrated the utility of 3D Gaussians for dense simultaneous localization and mapping (SLAM) with unparalleled efficiency and accuracy, our research introduces a cutting-edge method titled Ambient Diffusion Posterior Sampling (A-DPS). Our framework uniquely addresses inverse problems using diffusion models learned from linearly corrupted data, such as image inpainting, and cleverly extends its application to different forward processes like image blurring, and to the synthesis of high-quality images from degraded samples. Furthermore, by employing synthesis techniques usually associated with monocular pose estimation and the simulation of camera perspectives, our approach innovatively extends to online services, providing solutions for real-time applications with a structured deployment methodology. Our experiments on standard natural image datasets (CelebA, FFHQ, and AFHQ) reveal that A-DPS, through structured synthesis, can, in some instances, surpass the performance and speed of models trained on uncorrupted data across various image restoration tasks. Additionally, we expand upon the Ambient Diffusion framework to specifically tailor MRI models, trained exclusively on Fourier subsampled multi-coil MRI measurements at diverse acceleration factors. Our findings suggest that models trained on considerably subsampled data serve as better priors for inverse problems in higher acceleration scenarios compared to those trained on fully sampled datasets. Our code and the uniquely trained Ambient Diffusion MRI models are publicly available: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jonathon_Luiten1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08700v1",
  "title": "Diffusion-based Iterative Counterfactual Explanations for Fetal Ultrasound Image Quality Assessment",
  "modified_abstract": "Inspired by the burgeoning interest in Explainable Artificial Intelligence (XAI) across diverse applications such as smart home systems' activity recognition for occupants and caregivers, our work focuses on the exigent healthcare domain of obstetric ultrasound imaging, a critical tool for fetal health monitoring. Despite the recognized importance, attaining high-quality standard planes in ultrasound imaging remains a challenge, influenced by the sonographer's expertise and various factors like maternal BMI or fetus dynamics. To address this, we propose a novel application of diffusion-based counterfactual XAI to generate realistic high-quality standard planes from low-quality non-standard ones, thereby offering a potential solution through the lens of explainability and model interpretability. Through meticulous quantitative and qualitative evaluation in a controlled space, our method demonstrates significant success in producing plausible counterfactuals that exhibit increased image quality and improved classification accuracy in diagnostic environments, thereby enhancing recognition capabilities. The implications of our work are twofold: it not only promises to enhance the training of clinicians by providing intuitive visual feedback but also posits a direct contribution to the improvement of image quality for better downstream diagnosis and monitoring outcomes.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Thomas_Ploetz1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08699v1",
  "title": "Implicit Regularization of Gradient Flow on One-Layer Softmax Attention",
  "modified_abstract": "This work extends previous investigations into the optimization and regularization properties of deep learning models, particularly inspired by comprehensive studies on the optimization dynamics of gradient descent in models with smooth activation functions. Through an inquisitive examination of gradient flow on the exponential loss for a classification problem using a one-layer softmax attention model, our research reveals that during the minimization of loss, an implicit regularization effect occurs, privileging the nuclear norm of the product of the key and query weight matrices. This implicit regularization, observed under specific initialization conditions and during convergence of the loss minimization, is dissimilar to prior understandings where gradient descent was thought to favor the Frobenius norm under combined weight matrix training. Our contribution elucidates the role of separate training for key and query matrices in optimization, employing reparameterization techniques and exploring approximate KKT conditions rooted in SVM problems associated with the classification task. Additionally, we extend our findings to scenarios with general weight matrices configurations, highlighting the potential for broader application given initial alignment with data features. This exploration not only contrasts with previous assertions regarding the norm preferences induced by gradient descent in various models but also enhances our theoretical comprehension of the implicit regularization mechanisms at play in one-layer softmax attention models.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Libin_Zhu1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08847v1",
  "title": "JAXbind: Bind any function to JAX",
  "modified_abstract": "In this work, motivated by the pressing need to leverage well-established high-performance computational routines within the rapidly growing domain of JAX for machine learning and scientific computing applications, we introduce JAXbind. JAXbind is designed to significantly simplify the process of binding functions, developed in a variety of programming languages, to JAX. This innovation stems from the realization that integrating diverse computational functions into JAX can unlock new potentials and efficiencies but is often marred by the technical complexities associated with JAX's existing binding mechanisms. By providing an accessible Python interface for crafting custom JAX primitives, JAXbind facilitates seamless integration of arbitrary functions with JAX's powerful transformation capabilities, marking a pivotal step towards enriching the JAX ecosystem with broader computational strategies and insights. Specifically, the inclusion of probabilistically defined functions and robust classifiers that can handle perturbations in their inputs opens new avenues for applying examples-based learning and sample-efficient methodologies within JAX, which is particularly beneficial in settings where data is scarce or noisy, exponentially increasing the accessibility and applicability of JAX in various scientific and computational settings. Moreover, by leveraging hypothesis-driven development and PAC (probably approximately correct) learning algorithms, JAXbind significantly advances the integration of probabilistic robustness into the larger framework.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~UNIQUE_SUBEDI1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08687v1",
  "title": "Digital Twin-assisted Reinforcement Learning for Resource-aware Microservice Offloading in Edge Computing",
  "modified_abstract": "In the realm of edge computing, this work takes inspiration from pioneering strategies in Autonomous Mobility-on-Demand (AMoD) systems and Graph Meta-Reinforcement Learning, which address complex dynamic environments through adaptable and robust control policies. Echoing this spirit, we introduce a novel methodology, DTDRLMO, for efficient microservice offloading in Collaborative Edge Computing (CEC), combining the potential of Deep Reinforcement Learning (DRL) with the predictive precision of Digital Twin technology, the flexibility of recurrent neural networks, and the efficiency of actor-critic algorithms. This approach aims to dynamically adapt to fluctuating loads and network conditions in real-time, thereby optimizing microservice execution and bandwidth allocation with the goal of minimizing average service completion time and accommodating travel demands more effectively in smart urban settings. Through simulations leveraging both real-world and synthetic datasets, our trained technique demonstrates significant improvements over existing heuristic and learning-based methods in the aspects of training efficiency and adaptability, making a compelling case for its resource-aware and adaptable nature in the face of CEC's inherent dynamics. Importantly, this study extends the potential application of our methodology to the transportation sector, particularly emphasizing how graph-based planning and meta-reinforcement learning algorithms can benefit the management and deployment of autonomous vehicles in smart cities.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~James_Harrison1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08845v1",
  "title": "Bifurcated Attention for Single-Context Large-Batch Sampling",
  "modified_abstract": "Inspired by the broad array of advancements in natural language processing (NLP) that leverage latent structure models for linguistic modeling, to unveil linguistic patterns and facilitate accurate model training, our work spearheads the development of bifurcated attention for language model inference in single-context batch sampling contexts. This innovative approach, tailored to alleviate the computational strain imposed by redundant memory IO costs\u2014a notable bottleneck in scenarios involving high batch sizes and lengthy context\u2014divides the attention mechanism during incremental decoding into discrete GEMM operations targeting both the KV cache from prefill and the decoding training process. The ingenuity of bifurcated attention lies in its precision and resource efficiency, promising to maintain standard computational demands while significantly paring down memory IO requirements. Complementing this, our method's synergy with multi-query attention mechanisms, renowned for their memory IO frugality concerning KV cache management, augments its capacity for supporting increased batch sizes and context lengths without sacrificing latency or hindering the parsing phase. This enhancement nudges our model closer to real-time applicability, notably in enabling massively parallel answer generation endeavors with minimal latency repercussions for linguistic translation applications. When integrated with sophisticated post-processing tactics such as reranking, our approach stands to markedly refine the performance metrics of extant models, offering a robust solution to one of NLP's pressing computational challenges, while also encouraging continuous evaluation by practitioners within the field.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Tsvetomila_Mihaylova1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08673v1",
  "title": "When can we Approximate Wide Contrastive Models with Neural Tangent Kernels and Principal Component Analysis?",
  "modified_abstract": "Drawing upon the foundations laid by prior expressive work on contrastive learning, particularly its application to both IID and non-IID data such as image, text, and nodes in graphs, this investigation embarks further into the depths of unsupervised learning paradigms. Contrastive learning has emerged as a pivotal methodology for uncovering valuable representations from unlabelled data, not confined to just human-labeled data sets but extending to tasks encompassing a broad spectrum of applications. While extensive studies have explored the effectiveness of contrastive losses in mirroring spectral embeddings, and a handful have attempted to draw parallels with kernel principal component analysis (PCA), a definitive understanding of how trained contrastive models align with kernel methods or PCA remains elusive. This work analytically dissects the training dynamics of two-layer contrastive models with non-linear activation to elucidate the conditions under which these models approximate PCA or kernel methods, employing a robust design of experiments methodology. Leveraging the renowned equivalence of neural networks to neural tangent kernel (NTK) machines in the supervised realm, we unveil the first convergence insights of NTK for contrastive losses, showcasing a nuanced scenario where the NTK of wide networks nearly stabilizes for cosine similarity-based losses but fluctuates for dot product-based losses. Our analysis is complemented by learning from the complex graph data architecture, furthering our understanding of the intricacies involved. Additional inductive analysis into the training dynamics under orthogonality constraints on the output layer\u2014implicitly assumed in spectral embedding theories\u2014reveals that the learned representations veer close to the principal components of a particular matrix derived from random features. Our empirical observations reinforce the hypothesis that our theoretical findings extend beyond merely two-layer networks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Chaokun_Wang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08664v2",
  "title": "Zero-shot and Few-shot Generation Strategies for Artificial Clinical Records",
  "modified_abstract": "Motivated by the urgent need to access historical patient data for clinical research without violating privacy regulations, this study introduces a pioneering approach that leverages the capabilities of Large Language Models (LLMs), such as the ones developed for enhancing natural language understanding and generation through advancements in pre-training techniques. These pre-training techniques have shown remarkable success in embedding commonsense knowledge within models, facilitating their application in tasks requiring intricate understanding and generation of text, thereby enriching the text-to-text transfer process. Utilizing such advancements, our work evaluates the Llama 2 LLM for synthesizing clinical narratives that comply with privacy standards by employing zero-shot and few-shot prompting strategies, without requiring access to sensitive patient information for model train. Specifically, we concentrate on the generation of synthetic narratives for the History of Present Illness section, with the MIMIC-IV dataset serving as a reference point for comparison. This research not only introduces a novel chain-of-thought prompted technique to enhance the accuracy and contextual relevance of generated medical narratives without prior fine-tuning but also demonstrates, through a Rouge metrics evaluation, that our approach enables zero-shot and few-shot models to achieve performance comparable to that of fine-tuned models by optimizing token generation. The experimental design and benchmarks provided in this study underscore the syntactic proficiency and learning capabilities of our model in generating realistic and compliant clinical records.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Dong-Ho_Lee1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08662v1",
  "title": "Self-Supervised Learning for Covariance Estimation",
  "modified_abstract": "Building on the foundation set by recent advancements in self-supervised learning and statistical methods for convolutionally observed processes, our research introduces a novel approach to covariance estimation using deep learning. Specifically, we globally learn a neural network, implementing it locally at inference time. This technique leverages self-supervised foundational models, training the network without requiring any labels by masking different samples and learning to predict their covariance based on their surrounding neighbors. The architecture employs the widely adopted attention mechanism, offering a notable advantage over traditional methods by automatically leveraging global characteristics without relying on distributional assumptions or necessitating regularization. Informed by the understanding of diffusion processes, this method embraces the inherent drift in real-world processes over time, making it extremely potent for dynamic environments. The model can be pre-trained as a foundational model and then fine-tuned for a variety of downstream tasks, such as adaptive target detection in radar or hyperspectral imagery, epitomizing the versatility and innovation that self-supervised learning brings to the field of covariance estimation. This approach, by inherently accounting for time-variant processes and leveraging kernels within its architecture for effective learning from observation, showcases significant promise for handling the inherent drift in real-world processes.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Shogo_Nakakita1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08652v1",
  "title": "Extracting Explanations, Justification, and Uncertainty from Black-Box Deep Neural Networks",
  "modified_abstract": "Drawing upon a comprehensive understanding of the need for interpretable artificial intelligence, demonstrated through prior works which have sought to align transparent global models with local contrastive explanations, our study introduces a novel Bayesian method designed to elucidate explanations, justifications, and uncertainty levels within deep neural networks (DNNs). This innovation, pivotal for mission-critical applications, enables a more profound insight into the DNN's underlying decision processes and related evidence without necessitating model retraining. Our technique, characterized by its efficiency in both memory and computational requirements, its globally applicable framework, and its consistency in interpretation across different models, proves applicable to any black-box DNN with or without transparent labels, thereby extending its utility across various domains including anomaly and out-of-distribution detection tasks. Furthermore, our approach, by producing contrastive/counterfactual explanations and enhancing the understanding of model uncertainty, reinforces the interpretability and reliability of DNNs without the need to train them afresh. The efficacy and potential of our method are substantiated through empirical validation on the CIFAR-10 dataset, where it is demonstrated to significantly enhance the interpretability and reliability of DNNs.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Tejaswini_Pedapati1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08638v1",
  "title": "Disparate Effect Of Missing Mediators On Transportability of Causal Effects",
  "modified_abstract": "In the pursuit of establishing rigorous frameworks for quantifying and addressing disparities in transported mediation effects, especially in domains as critical as public health, our investigation is inspired by a plethora of precedent studies including those focusing on innovative methods for interpreting complex models, such as the Manifold Restricted Interventional Shapley Values. These pioneering works have laid a foundational understanding that we build upon to scrutinize how missing mediators can bias causal effect estimates when interventions are transferred across populations. Specifically, we propose a sensitivity analysis framework that quantifies the impact of missing mediator data on transported mediation effects. This framework aids in identifying conditions under which the conditional transported mediation effect becomes insignificant for subgroups missing mediator data, offering bounds on the transported mediation effect as a function of missingness. To demonstrate the utility of our framework, we apply it to longitudinal data from the Moving to Opportunity Study, thereby quantifying the effect of missing mediators on transport effect estimates concerning voucher receipt and its downstream impacts on mental health or substance use disorder mediated through parental health. Our work thus significantly advances our understanding of the impact of missing data in transported mediation effect estimates, providing crucial insights for public health interventions.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Muhammad_Faaiz_Taufiq1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08632v1",
  "title": "A Decade's Battle on Dataset Bias: Are We There Yet?",
  "modified_abstract": "This paper revisits the dataset classification experiment suggested by Torralba and Efros a decade ago, now assessing it in the context of the vastly transformed landscape of machine learning, marked by the advancement in neural network architectures and the development of large-scale, more diverse datasets aimed at reducing bias. Our investigation's inspiration is drawn from an array of pioneering works, such as efforts in adaptive risk minimization, which address the challenges of domain shift, dataset bias, and adapting to diverse distributions in machine learning, reflecting a persistent endeavor in the community to enhance model robustness and adaptability across varied datasets. Despite these significant technological advancements, we surprisingly find that modern neural networks achieve high levels of accuracy in classifying images based on their originating datasets\u2014demonstrating an 84.7% accuracy for a three-way classification problem among the YFCC, CC, and DataComp datasets, thereby exhibiting an unexpected success. This unexpected success throws light on the nuanced and persistent issue of dataset bias in ML models, suggesting that models may have an implicit prior knowledge which influences their learning process, even amidst an era of supposed abundance and diversity of data. Our experiments further illustrate that the dataset classifier learns semantic features that are generalizable and transferable across different datasets, challenging the assumption that its success could only be ascribed to memorization and highlighting the importance of adaptation in modern ML systems. This paper aims to motivate the community to re-evaluate and further investigate the complexity of dataset bias and the capabilities of models in the present day.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Nikita_Dhawan1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08630v1",
  "title": "Leveraging Non-Decimated Wavelet Packet Features and Transformer Models for Time Series Forecasting",
  "modified_abstract": "Building upon the foundation laid by recent advances in time series classification and forecasting, our work merges traditional wavelet analysis with contemporary machine learning advancements, notably in the domain of time series forecasting. Refining the concept of feature extraction through the lens of interpretability and transformability as seen in existing research, this article highlights three principal innovations. It initially explores the utility of Daubechies wavelets with varying vanishing moments as dynamic input features, moving beyond static selection to an optimized cross-validation process. Furthermore, we juxtapose the employment of non-decimated wavelet transform against the more comprehensive non-decimated wavelet packet transform for feature generation, showcasing the latter's superior potential in providing a broader array of informative coefficient vectors. Lastly, by experimenting with a diverse suite of forecasting methodologies including traditional statistical models, recurrent and convolutional neural networks, and cutting-edge transformer-based neural networks, our study showcases the distinctive advantages of integrating wavelet features with convolutional architectures. This approach particularly emphasizes their efficacy in enhancing one-step-forward predictions across non-temporal approaches, such as dictionary-based techniques which often overlap with classification strategies, and their modest yet valuable contributions towards long-horizon forecasts in temporal deep learning setups, affirming the significance of interpretability in both feature selection and model performance. In reflecting on the classifier's role, it highlights the intertwined nature of classification and forecasting in the realm of time series analysis.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Nicolas_Baskiotis2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08627v1",
  "title": "Multifidelity linear regression for scientific machine learning from scarce data",
  "modified_abstract": "This work is inspired by and aims to extend the foundational approaches outlined in recent advancements, particularly those addressing the challenges in applying classical artificial intelligence to scenarios characterized by high variance and low volume datasets, as commonly encountered in chemical engineering and similar scientific domains. With machine learning models becoming pivotal in surrogating complex engineering systems simulations, we tackle the inherent issue of scarce high-fidelity data, which renders learning models highly sensitive and variant. Our novel multifidelity approach, which discusses the integration of data with varying fidelity and cost, such as high-quality data from fully resolved physics simulations versus more accessible but simplified model outputs, capitalizes on this challenge in chemical engineering. By integrating these diverse data sources, we introduce multifidelity Monte Carlo estimators for linear regression model parameters, providing theoretical underpinnings for the method's accuracy and robustness, especially under constrained data generation budgets. Our empirical demonstrations corroborate the theoretical promises, showcasing significantly reduced model variance when leveraging both high and lower-fidelity data, thereby presenting a robust pathway for scientific machine learning applications in data-sparse situations. The repeated use of 'from' in the abstract reinforces the source and derivation of multifidelity approaches in learning from ML. The incorporation of chemical engineering domains twice underscores the paper's emphasis on these specific applications.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Calvin_Tsay1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08618v1",
  "title": "Verifix: Post-Training Correction to Improve Label Noise Robustness with Verified Samples",
  "modified_abstract": "Building on the inherent challenge of label corruption in machine learning, which stems from non-expert labeling or adversarial attacks, our work introduces 'Verifix', a novel method inspired by contemporary approaches to addressing noisy labels, such as the Noisy Prediction Calibration (NPC) technique. This introduction utilizes post-processing to refine the predictions of pre-trained classifiers towards the true labels using generative models. Verifix advances this concept by proposing a post-training correction paradigm that fine-tunes model parameters without necessitating retraining, using a Singular Value Decomposition (SVD) based algorithm that employs a small verified dataset for correcting the model weights through a single update. By estimating a Clean Activation Space and projecting the model's weights onto this space, Verifix aims to suppress activations triggered by corrupted data, empirically enhancing label noise robustness. Our empirical evaluations on various datasets, including the CIFAR dataset with 25% synthetic label corruption, demonstrate notable improvements in model generalization and prediction accuracy across numerous instances. This method not only underscores the feasibility and efficiency of post-training adjustments in the face of noisy data but also contributes a significant stride towards computational and resource efficiency by obviating the need for complete model retrain. Code is available at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Byeonghu_Na1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08613v1",
  "title": "Link Prediction for Social Networks using Representation Learning and Heuristic-based Features",
  "modified_abstract": "In the burgeoning field of social network analysis, our work is primarily inspired by the innovative approaches previously developed for optimizing graph sampling methods, such as those presented in 'SubMix: Learning to Mix Graph Sampling Heuristics,' which advanced the performance of Graph Neural Networks (GNNs) through a novel blending of sampling heuristics and selective relaxation of traditional methods. Building upon this foundation, our research explores the fusion of representation learning with heuristic-based features for the purpose of predicting missing links in subgraphs of social networks. We delve into various feature extraction techniques, segregated into categories including Structural embeddings, Neighborhood-based embeddings, Graph Neural Networks (GNNs), Graph Heuristics, and the mixing of their characteristics for enhanced predictive accuracy in link prediction. Our study contrasts the efficacies of these techniques through experiments with ensemble classifiers and custom Neural Networks during their training phase, leading to the proposal of a blended approach that marries heuristic-based features with learned representations through a systematic mixing strategy for subgraphs. This novel combination exhibits enhanced learning performance for the link prediction task across diverse social network datasets, demonstrating the benefits of incorporating advanced training techniques with relaxation methods in the training phase. The promising nature of our method for generating accurate recommendations heralds a new avenue of inquiry with significant potential. To support further research and application, code for the conducted experiments has been made publicly available.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Johannes_Klicpera1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08592v1",
  "title": "Data-Efficient Sleep Staging with Synthetic Time Series Pretraining",
  "modified_abstract": "Our work is predicated on the foundation laid by cutting-edge research in both electroencephalographic (EEG) analysis and synthetic data generation methodologies. Building upon these foundations\u2014particularly the novel technique for reconstructing local activities within physiological systems through neural state-space modeling\u2014we initiate a groundbreaking exploration into the efficient application of deep learning for EEG-based sleep staging. Our proposed method, 'frequency pretraining,' utilizes synthetic time series to address the inherent variabilities among human subjects and the limitations of small datasets in analyzing EEG time series. This approach not only surpasses the performance of fully supervised learning in data-constrained scenarios but also leverages state-space propagation to enhance modeling accuracy by incorporating latent variables, aligning with prior findings regarding the importance of frequency information in sleep stage scoring. Moreover, our method's robustness is partly attributable to its ability to mitigate loss during the training phase, further substantiating the ability of deep neural networks to utilize causal-effect insights and detailed reconstructions beyond mere frequencies to enhance sleep staging accuracy. Given its promising results, our methodology holds considerable potential for applications where EEG data is scarce or comes from a limited pool of subjects, such as brain-computer interfaces.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Xiajun_Jiang1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08589v1",
  "title": "Can physical information aid the generalization ability of Neural Networks for hydraulic modeling?",
  "modified_abstract": "This study explores the relatively uncharted territory of applying Neural Networks (NNs) to river hydraulics, an area where data scarcity often undermines the predictive capabilities of purely data-driven approaches. Taking inspiration from the concept of Physics-Informed Neural Networks (PINNs), which have demonstrated promise in other domains by embedding physical laws in the form of differential equations to guide model training, our work investigates a novel adaptation of this idea for environmental hydraulics. Recognizing the inherent epistemic uncertainties and computational challenges in directly computing the residuals of Partial Differential Equations (PDEs) governing hydraulic phenomena, our proposed methodology reframes NNs as neural operators. By imposing physical constraints without explicit reliance on PDEs, akin to methodologies such as data augmentation and regularization, we position this approach as both innovative and potentially more applicable in the context of hydraulic modeling. Through this methodology, we demonstrate that the integration of soft physical information, inspired by recent insights into phenomena such as neural collapse and its implications on model transferability and generalization in classification tasks, can markedly enhance the NN's ability to generalize and improve predictive performance for hydraulic models. Our tests on large datasets, including a pretraining phase to bolster model understanding of the physical world, further substantiate these claims, revealing a notable projection towards improved intra-class consistency and reduction in discrepancy across classifiers' predictions.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jinxin_Zhou2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08584v1",
  "title": "Local Binary and Multiclass SVMs Trained on a Quantum Annealer",
  "modified_abstract": "Our work is inspired by groundbreaking methodologies in machine learning which harness the potential of large-scale datasets, as evidenced by advancements in learning from label proportions (LLP) highlighting the use of aggregated labels for weakly supervised learning. Similarly, support vector machines (SVMs), pivotal in tasks like remote sensing, have undergone significant evolution to adapt to the modern computational landscape, including hybrid forms that blend quantum training with classical execution. Though these hybrid SVM models show promise, they face limitations due to the quantum annealers' restricted connectivity, constraining the series of training set size. Addressing this, we propose local application of quantum-trained SVM models to sidestep training set size constraints and enhance performance. By integrating the FaLK-SVM method with bag-level, label-focused quantum-trained SVMs for both binary and multiclass classification, and contrasting it with classical single-step multiclass SVM (CS SVM), our approach illuminates the scalability and effectiveness of quantum-hybrid models in handling extensive datasets, particularly in the remote sensing domain, using D-Wave's quantum annealers for empirical evaluation. Our findings demonstrate the practical viability of these models in real-world, large-scale scenarios, offering a new avenue for exploiting the synergy between quantum computing and conventional machine learning techniques, establishing a benchmark for future collection-based learning systems.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Rishi_Saket1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08579v1",
  "title": "Machine Learning Optimized Orthogonal Basis Piecewise Polynomial Approximation",
  "modified_abstract": "This work is motivated by advancements in numerical optimization techniques, particularly those aimed at solving inverse problems with applications in signal denoising and reconstruction, like the bilevel learning approach for l1-regularizers. Piecewise Polynomials (PPs) are extensively employed across various engineering disciplines, such as trajectory planning, to fit position profiles defined by a set of points. Given that traditional closed-form solutions offer limited adaptability regarding polynomial degrees, bases, or additional domain-specific requirements, we turn to contemporary numerical methods like gradient descent\u2014integral to training Artificial Neural Networks (ANNs)\u2014for enhancing the flexibility of PP models. By leveraging modern Machine Learning (ML) frameworks such as TensorFlow, which are equipped with gradient-based optimizers devised for an array of optimization challenges beyond ANNs training, we propose a novel approach that melds the versatility of PP models with the capabilities of current ML optimizers for function approximation in 1D trajectory planning, particularly in electronic cam design. We demonstrate how using an orthogonal polynomial basis, specifically Chebyshev polynomials of the first kind, alongside a newly developed regularization technique, can significantly improve the effectiveness in approximation and continuity optimization. Our findings reveal that the Chebyshev basis surpasses traditional power bases across several optimizers in our optimization scenario, showcasing the feasibility of our method within the electronic cam domain. The objective of this research is to deliver a validated toolkit for high-precision 1D path planning by integrating signal optimization and machine training techniques.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Avrajit_Ghosh1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08572v1",
  "title": "Caformer: Rethinking Time Series Analysis from a Causal Perspective",
  "modified_abstract": "This paper draws inspiration from substantial prior research addressing the dynamics and intricacies of time-evolving data, particularly in contexts where missing or censored data presents complex challenges, much like the scenario outlined for Marked Temporal Point Processes (MTPPs) which have seen application across diverse fields, from medical data monitoring to financial prediction. In the realm of time series analysis, effectively capturing cross-dimension and cross-time dependencies in non-stationary series is an ongoing challenge, exacerbated by environmental factors which introduce spurious correlations that obscure true causal relations. With 'Caformer' (\textbf{Ca}usal Trans\textbf{former}), we propose an innovative framework that reconceptualizes time series analysis through a causal lens. Our approach is built on three foundational components\u2014the Dynamic Learner, which uncovers stochastic interactions across different dimensions over time; the Environment Learner, which employs back-door adjustment techniques to neutralize the effects of environmental confounding; and the Dependency Learner, designed to deduce and solidify robust user interactions both temporally and across dimensions, utilizing modeling techniques of marginalization for handling censored or partially observed data. Demonstrated through its state-of-the-art performance across five core time series analysis tasks\u2014ranging from forecasting (long- and short-term) to classification, imputation, and anomaly detection\u2014Caformer not only advances the technical frontier but also introduces a level of interpretability heretofore unseen in this domain. The implications of this paper extend to applications in real-world scenarios where accurate interpretation of complex time series data is crucial.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Alex_James_Boyd1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08569v1",
  "title": "A Physics-driven GraphSAGE Method for Physical Process Simulations Described by Partial Differential Equations",
  "modified_abstract": "This work is poised at the intersection of rigorous problem-solving and innovative solution design within computational physics, inspired by a lineage of methods tackling partial differential equations (PDEs) - a common thread that also underpins recent advances like the development of a unified hard-constraint framework for solving geometrically complex PDEs. While physics-informed neural networks (PINNs) have historically spearheaded the approach to various computational physics problems grounded in PDEs, they exhibit limitations such as low accuracy when contending with singularities and oscillations, and the rigidity of trained solutions against changes in input parameters. To address these challenges, our study introduces an innovative physics-driven GraphSAGE (PD-GraphSAGE) approach, grounded in the Galerkin method and utilizing piecewise polynomial nodal basis functions. This methodology not only significantly enhances the adaptability and accuracy of solving irregular and complex PDEs but also deftly crafts parametric PDE surrogate models. By incorporating graph representations of physical domains and employing a strategic feature mapping for effective training convergence, our approach automatically and substantially reduces the need for evaluated points through local refinement while ensuring training effectiveness under complex scenarios such as singularity and oscillation. Demonstrated through a variety of case studies and the reformulation of a robust PDE surrogate model for heat conduction issues, our proposed method not only delivers accurate solutions but also outpaces traditional finite element methods in terms of efficiency and training loss reduction.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ze_Cheng2",
  "manipulated_ranking": 5,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08568v2",
  "title": "Consistent Prompting for Rehearsal-Free Continual Learning",
  "modified_abstract": "Our work is motivated by the ambition to enable models to adapt continually to new information without discarding previously acquired knowledge, a challenge accentuated by the disparate methodologies prevalent in the field of continual learning, as well as in adjacent disciplines such as example-based control in reinforcement learning. Existing prompt-based approaches in continual learning leverage pre-trained models but suffer from inconsistencies between training and testing phases, particularly when applied to diverse dynamics of tasks, including transitions between them, and domains. This paper introduces Consistent Prompting (CPrompt), a novel method that addresses these inconsistencies by aligning training and testing more closely through classifier and prompt consistency learning. Our method not only harmonizes the learning process across different tasks but also significantly enhances the predictive performance and robustness of the model on various continual learning benchmarks, outperforming other prompt-based methods and setting new state-of-the-art records. Through extensive experiments, including on image-based datasets, and incorporating actions required for task execution in different states, we attribute these improvements to our method's ability to maintain consistency across classifiers and prompts, facilitating a more stable learning progression and adept at managing the transitions integral to these tasks. Moreover, CPrompt shows notable adaptability in offline settings, indicating its potential for wider applications in control scenarios.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Tianhe_Yu1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08562v1",
  "title": "Structural perspective on constraint-based learning of Markov networks",
  "modified_abstract": "Inspired by the significant challenges in understanding and estimating the structural dynamics of complex networked systems, such as electrical grids and social networks, which are often governed by underlying conservation and consensus laws, our study explores the domain of probabilistic graphical models, specifically Markov networks. These models use undirected graphs to articulate the conditional independencies among variables, offering pivotal insights into the makeup and operation of such systems. Focusing on constraint-based structure learning, we aim to learn the undirected graph from data via conditional independence tests, thereby establishing theoretical boundaries concerning the number of tests and the sizes of the conditioning sets required for accurate estimation. These boundaries reveal an intriguing relationship between the graph\u2019s structural properties, including sparsity, and the quantity of tests needed for learning a Markov network, with the graph parameter of maximum pairwise connectivity, $\\kappa$, playing a crucial role in determining the size of independence tests necessary for graph learning. Norms and operators related theories are implicitly referenced as integral to understanding such structural properties and their control implications in high-dimensional networks. We present findings that indicate at least one test with the conditioning set size of at least $\\kappa$ is always necessary, and any graph can be learned by conducting tests with the size at most $\\kappa$. Our results conclusively address the question regarding the minimal size of conditioning sets essential for graph learning and provide an upper bound implying any $n$-vertex graph can be learned by at most $n^{\\kappa}$ tests. Furthermore, we elaborate on both the challenges and potentials in learning graphs of bounded treewidth, thereby advancing our understanding of the structural dynamics in Markov networks and contributing to the broader field of statistical learning for networked systems.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Rajasekhar_Anguluri1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08554v1",
  "title": "Federated Knowledge Graph Unlearning via Diffusion Model",
  "modified_abstract": "In the wake of rapidly evolving artificial intelligence (AI) landscapes, our work is positioned at the intersection of federated learning (FL), knowledge graph (KG) embeddings, and the emergent field of machine unlearning (MU), inspired by groundbreaking advancements in federated learning methodologies such as those examining loss landscape visualizations in data-decentralized environments. Federated learning promotes collaborative AI development without compromising data privacy, with various clients participating in the training process, and KG embeddings enable sophisticated knowledge representation by mapping entities and relations onto a neural vector space. Despite the benefits, the dynamic nature of data and privacy concerns necessitate the ability to selectively forget information, posing significant challenges in maintaining KG model performance. Our novel framework, FedDM, addresses these challenges through the use of diffusion models to effectively forget targeted knowledge in federated KG settings without deteriorating the model's utility on retained data. Training techniques that accommodate data heterogeneity in federated environments and mitigate loss, crucial to our framework's success, are emphasized. We validate the effectiveness of FedDM through comprehensive experiments on recognized datasets, and a pilot study reveals its promising capabilities in machine forgetting while minimally impacting overall model performance.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Hong-You_Chen1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08553v1",
  "title": "Regret Analysis of Policy Optimization over Submanifolds for Linearly Constrained Online LQG",
  "modified_abstract": "Building upon recent advancements in online optimization and control, and particularly motivated by methodologies such as the Safe Online Projected Gradient Descent (SO-PGD) algorithm that addresses online convex optimization with unknown linear safety constraints, this work explores online linear quadratic Gaussian (LQG) problems under a linear constraint framework. Our investigation is inspired by the broader context of ensuring safety and practicality in control algorithms, where constraints often arise from physical connections or resource limitations, and where the availability of real-time controller adjustments is critical. Introducing an online optimistic Newton on the manifold (OONM) approach, which leverages predictions based on first and second-order information of the function sequence to provide safe and provable access to controller optimizations, this novel method aims to address the challenge of adhering to linear constraints while optimizing controller parameters in an adversarially varying environment. Our approach, firmly rooted in our team's learning/optimization principles, emphasizes ensuring convexity in the optimization landscape, thereby enabling effective and safe adjustments in real-time. By defining regret in terms of cumulative cost suboptimality compared to a locally minimizing controller sequence, we establish a regret bound reflective of the path-length of the minimizer sequence. Simulation results further validate the efficacy of the OONM approach, demonstrating its capability to maintain constraint satisfaction and safety while effectively minimizing regret in an online setting.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sapana_Chaudhary1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08550v1",
  "title": "CINA: Conditional Implicit Neural Atlas for Spatio-Temporal Representation of Fetal Brains",
  "modified_abstract": "Inspired by the previous strides in enhancing the robustness of medical image segmentation through methods such as vector quantisation, our work introduces the Conditional Implicit Neural Atlas (CINA) as a pioneering approach for generating spatio-temporal atlases from Magnetic Resonance Images (MRI) of both neurotypical and pathological fetal brains, without relying on affine or non-rigid registration. Leveraging a unique training paradigm, CINA learns to encode general features of the fetal brain into a dictionary-like latent code while incorporating subject-specific details. This capability enables the construction of detailed atlases with tissue probability maps for any gestational age (GA) and anatomical variation found within the training set, significantly advancing the field of segmentation and representation of medical images. Moreover, CINA's adaptability and learning adeptness are showcased through the generation of tailored probabilistic tissue maps for unseen subjects by optimizing the latent code with an embedding technique at test time, demonstrating profound robustness. Our evaluation across 198 T2-weighted MRI scans from both the dHCP and FeTA datasets demonstrates CINA's superior fidelity in representing a flexible and accurate fetal brain atlas, especially for standard brains and those with ventriculomegaly, thereby standing up to adversarial conditions in medical imaging. Notably, CINA achieves a mean absolute error of just 0.23 weeks in predicting fetal brain age, underscoring its precision in capturing the nuances of fetal brain development and its significant contribution to the learning aspects of AI in medical imaging. [URL omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Avinash_G._Kori1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08540v1",
  "title": "Language models scale reliably with over-training and on downstream tasks",
  "modified_abstract": "In the context of increasing model complexities and the vast potential implications for application across a variety of domains, such as those explored in tracing knowledge in language models back to the training data, our work elucidates the nuanced interplay of scaling laws in the development of language models. Addressing the evident gaps between current scaling studies and practical training and evaluation of language models, we investigate the dynamics in the compute-optimal training and - as a novel contribution - in the over-trained regime. Moreover, our study extends beyond the conventional metrics of next-token prediction loss, directly linking model scaling to downstream task performance through comprehensive analysis. This is achieved by developing a testbed of 104 models ranging from 0.011B to 6.9B parameters and trained across diverse token-counts and data distributions. We introduce scaling laws that accurately predict performance for significantly over-trained models, thereby conserving computational resources while enhancing predictive accuracy. Additionally, we establish a power law that bridges perplexity with downstream task performance, enabling efficient projection of model efficiency with substantially reduced computational expenditure. Our findings and datasets are shared publicly at [omitted for de-identification], marking a step forward in understanding and optimizing language model performance across training regimes and applications.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~EKIN_AKYUREK1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08536v1",
  "title": "HOLMES: HOLonym-MEronym based Semantic inspection for Convolutional Image Classifiers",
  "modified_abstract": "In the evolving landscape of computer vision, where the reliability of Convolutional Neural Networks (CNNs) is challenged by the presence of multiple shortcuts encountered during training, our work proposes HOLMES (HOLonym-MEronym based Semantic inspection), a pioneering approach designed to offer interpretable and explainable insights into CNN-based image classification models. By acknowledging the complexity of subsymbolic knowledge acquisition in CNNs and inspired by previous efforts to address the inherent shortcomings of machine learning models in generalizing learned concepts due to shortcut learning, HOLMES introduces a novel methodology. It decomposes a label into a set of related concepts, thereby providing component-level explanations for image classifications. This is achieved through leveraging ontologies, web scraping, and transfer learning to construct meronym-based detectors, which then enable the generation of heatmaps that illuminate the importance of individual parts and textures in the classification task. HOLMES represents a significant leap forward from state-of-the-art saliency methods and benchmarks by not only identifying the 'where' but also clarifying the 'what' aspect of the features involved in making a classification decision. Extensive evaluation across diverse categories (animals, tools, and vehicles) demonstrates HOLMES's effectiveness in generating human-understandable explanations that incorporate at least two meronyms, where ablating a single meronym significantly reduces model confidence, altering the behavior of the classification model in training. Quantitative evaluation of the generated heatmaps against deletion/insertion/preservation curves shows that our approach promotes learning that is comparable to GradCAM while offering advanced decomposition into human-understandable concepts that underline the relevance and capability of HOLMES in capturing essential meronyms for object classification with emerging complexity. The code is available at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Mark_Ibrahim1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08525v1",
  "title": "From Weak to Strong Sound Event Labels using Adaptive Change-Point Detection and Active Learning",
  "modified_abstract": "Drawing inspiration from cutting-edge work in semi-supervised learning, self-training, self-supervised pre-training strategies in speech recognition, such as Censer which leverages self-supervised pre-training towards maximizing the utility of unlabeled data, this study introduces an innovative audio recording segmentation method. Our approach, grounded on adaptive change-point detection (A-CPD), is specifically designed for machine-guided weak label annotation of audio recording segments to maximize the information gained about the temporal activations of target sounds. Utilizing a prediction model initially pre-trained on sound event data with classes distinct from those in the unlabeled dataset, our method cleverly adapts to annotator-provided labels through an active learning loop. By employing change-point detection on probability curves derived from the prediction model, and taking cues from the abundant pool of unlabeled data, our strategy proficiently guides the weak label annotator towards generating strong sound event labels, even under the constraints of a limited annotation budget. Comparative analysis against two baseline query strategies demonstrates the superior efficacy of A-CPD in generating high-quality strong labels for speech and sound recognition tasks after fine-tuning.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Takahiro_Shinozaki1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08481v1",
  "title": "SoK: Reducing the Vulnerability of Fine-tuned Language Models to Membership Inference Attacks",
  "modified_abstract": "Drawing inspiration from a breadth of efforts that leverage machine learning (ML) advice in the design of algorithms, our work addresses the heightened privacy risk in natural language processing models stemming from fine-tuning generic base models on personalized, sensitive datasets. This adjustment phase introduces avenues for membership inference attacks, a prevalent method to evaluate a model's privacy compromise. Despite the surge in applications built on these online models, the susceptibility of language models to such attacks, and how different defense mechanisms fare in safeguarding against these intrusions remains under-researched. Our study fills this gap by delivering a comprehensive analysis of the vulnerabilities of fine-tuned large language models to membership inference attacks, evaluating the influence of various factors, including training methodologies and defense strategies, on their design and performance. Our findings point towards specific training methodologies, notably the integration of differential privacy and low-rank adaptors, as significantly effective in diminishing privacy risks associated with these attacks, thereby enhancing the overall design and performance of language models. In the process, numerous instances were examined to validate our propositions.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Keerti_Anand1",
  "manipulated_ranking": 5,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08477v1",
  "title": "Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts",
  "modified_abstract": "In the landscape of advancing machine learning methodologies for vision tasks, the intersection of meta-learning and parameter-efficient fine-tuning emerges as a fertile ground for innovation, particularly influenced by recent strides in vision transformers epitomized by approaches such as AdaViT, which adaptively tunes computational resources for efficient image recognition. Drawing inspiration from such pioneering work, we propose Sparse MetA-Tuning (SMAT), a method that synergizes sparse mixture-of-experts approaches with meta-tuning and leverages the transformer architecture's self-attention mechanism for image-intensive scenarios. SMAT is specifically designed to automatically select and fine-tune subsets of pre-trained parameters for each task, effectively overcoming the out-of-domain (OOD) sensitivity that hampers the efficacy of conventional meta-tuning approaches. Demonstrating significant progress, SMAT establishes new benchmarks on the Meta-Dataset augmented with additional OOD tasks across both zero-shot and gradient-based adaptation scenarios, leveraging computation efficiency and modeling prowess. We further elucidate the advantage of dynamically learned sparsity patterns over static configurations and underscore the critical role of sparsity levels in achieving an optimal balance between in-domain and out-of-domain generalization performance, aided by the self-attention feature of transformers. Our findings are substantiated through comprehensive experimental analysis, and to facilitate further research, we make our code publicly available.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Lingchen_Meng1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08469v1",
  "title": "An Analysis of Human Alignment of Latent Diffusion Models",
  "modified_abstract": "Inspired by the way text-editing models have revolutionized tasks such as grammatical error correction, style transfer, and text simplification by leveraging high textual overlap for more efficient and controllable outputs, our research investigates the intricate mechanics of latent diffusion models, particularly in the realm of image synthesis. These models have shown promising performance, mirroring human error consistency and manifesting low texture bias in classification tasks, alongside the capacity for generation through decomposition of their bottleneck layer representations into semantic directions. Our analysis focuses on evaluating how these representations align with human cognitive processes in a triplet odd-one-out task, a technique often utilized in seq2seq learning paradigms for texts and text-generation processes, incorporating sequence alignment strategies for more nuanced understanding. Our findings reveal that, despite the advancements in model design and training: I) The representational alignment with human cognitive processes in these generation tasks is comparable to that of models trained exclusively on ImageNet-1k. II) Surprisingly, the most aligned layers within the denoiser U-Net architecture are not at the bottleneck but rather at intermediate layers. III) Textual conditioning significantly enhances representational alignment at higher noise levels, underlining the critical role of abstract textual information in the early stages of image generation through inference, suggesting that integrating textual elements into model training could be a promising avenue for further aligning machine learning models with human perception and cognition in various tasks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Eric_Malmi1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08464v1",
  "title": "Diffusion Models with Implicit Guidance for Medical Anomaly Detection",
  "modified_abstract": "Our work builds upon a foundation set by pioneers in improving medical imaging and segmentation through deep learning techniques, such as those utilized in the semi-supervised and active learning approaches for pathology image annotation, to tackle the challenges of unsupervised anomaly detection with a novel integration of diffusion models. This paper introduces Temporal Harmonization for Optimal Restoration (THOR), which addresses the limitations of conventional diffusion models by employing implicit guidance through temporal anomaly maps to better preserve the integrity of healthy tissues in pathological images. THOR leverages unlabeled data to enhance the learning process and integrates selection criteria within its framework to ensure optimal utilization of resources in semi-supervised settings, tackling the compromise of critical information during pathology removal. The method aims to align restorations more closely with unaffected regions in the original scans and thereby reduce false positives and increase specificity in radiological evaluations. We demonstrate THOR's effectiveness in surpassing existing diffusion-based methods in anomaly detection and segmentation in both brain MRIs and wrist X-rays, particularly when dealing with gigapixel medical imagery. Our learning strategy, augmented by the use of unlabeled data for training, sets a new benchmark in the utility of selection strategies for optimizing the performance of diffusion models in the medical domain. Code is available at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zhengfeng_Lai1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08462v1",
  "title": "Authorship Verification based on the Likelihood Ratio of Grammar Models",
  "modified_abstract": "Drawing inspiration from advancements in computational methods for social media analysis, such as innovative bot detection algorithms that leverage the rich graph-based structures on platforms like Twitter, our research proposes a novel approach for Authorship Verification (AV). AV, a critical task in forensic analysis, involves determining the authenticity of document authorship, often in legal contexts. Despite the complexity of current state-of-the-art AV methods, which lack clear scientific rationale and interpretability, we introduce a method that calculates a quantity known as $\\lambda_G$ (LambdaG). This involves assessing the likelihood ratio of a document based on grammatical models\u2014the ratio between the likelihood of a document given the grammar model of the suspected author versus a reference population model, utilizing $n$-gram language models focused on grammatical features and entities. To promote a large-scale empirical evaluation, conducted over twelve datasets against four baseline methods, demonstrates that LambdaG achieves superior performance with respect to accuracy and the area under the curve (AUC) metrics in nearly all instances. Specifically, it excels over complex network methods, including fine-tuned Siamese Transformer networks, and exhibits remarkable robustness across different genres\u2014a testament to its compatibility with cognitive linguistic theories and its large-scale applicability, thereby offering a scientifically grounded and more interpretable alternative in the field of forensic document analysis.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zhaoxuan_Tan1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08839v1",
  "title": "Learning-Enhanced Neighborhood Selection for the Vehicle Routing Problem with Time Windows",
  "modified_abstract": "Inspired by recent advancements in optimization for mobility-on-demand systems, where the integration of ridesharing has proven cost-efficient and environment-friendly, our study presents an innovative integration of machine learning (ML) into the Large Neighborhood Search (LNS) strategy, termed Learning-Enhanced Neighborhood Selection (LENS). This ridesharing-aware approach aims to revolutionize the process of solving complex optimization problems by making informed decisions on which parts of a solution to destroy and repair in each iteration, effectively capturing essential dynamics and graph-theoretic properties. Specifically, our research targets the Vehicle Routing Problem with Time Windows (VRPTW), a critical concern in urban transportation planning. By implementing an LNS algorithm for VRPTW and generating novel training instances from renowned benchmark datasets, including various orders, we equipped LENS to intelligently adapt its selection process. The performance of LENS was benchmarked against a random neighborhood selection method and an oracle approach through rigorous experiments, showcasing LENS's superior ability to significantly elevate the coverage and quality of solutions and handle orders more efficiently. Furthermore, this work not only underlines the potential of integrating ML into traditional optimization algorithms but also sets a foundation for more sophisticated, learning-enhanced optimization strategies in logistics and transportation, including those aware of the nuances of ridesharing. This approach might embody the first approximation toward online adaptations in real-time scenarios.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Chonghuan_Wang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08838v2",
  "title": "Predictive Clustering of Vessel Behavior Based on Hierarchical Trajectory Representation",
  "modified_abstract": "Building upon the discovery and extraction of coherent patterns to understand labeled spatio-temporal data, as exemplified by recent advancements such as Discriminant Dynamic Mode Decomposition, this work introduces a novel approach to vessel trajectory clustering. Traditional methods typically rely on predefined rules and thresholds for identifying discrete vessel behaviors, aiming to produce high-quality clustering by analyzing entire sequences or sub-trajectories without adequately representing their evolution in various subspaces. To address these limitations, we propose a Predictive Clustering of Hierarchical Vessel Behavior (PC-HiV) method. PC-HiV employs hierarchical representations to transform every trajectory into a behavioral sequence and predicts the evolution at each timestamp based on these representations within defined modes of behavior. By combining predictive clustering with latent encoding, discriminant analysis techniques, and sets theory principles, PC-HiV enhances both clustering quality and predictions. Experiments conducted on real Automatic Identification System (AIS) datasets validate PC-HiV's superior performance over existing methods, demonstrating its effectiveness in capturing behavioral evolution discrepancies between different vessel types (tramp vs. liner) and within emission control areas, with improvements in purity score by 3.9% and 6.4% over NN-Kmeans and Robust DAA, respectively, through the application of decomposition techniques. This achievement highlights PC-HiV's competence in navigating through the complex sets of vessel behavioral data and its ability to achieve significant class-separation in multiple subspaces.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Naoya_Takeishi1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08448v1",
  "title": "Actor-Critic Physics-informed Neural Lyapunov Control",
  "modified_abstract": "Building on the foundation laid by recent advancements in safe reinforcement learning (RL), where ensuring policy compliance within safety-critical applications poses a notable challenge, this paper presents a novel approach within the domain of stabilization tasks and nonlinear control. The issue of designing control policies with provable guarantees, particularly those that can ascertain a substantial region of attraction to enhance system robustness against uncertainties, remains paramount. We introduce a method that synergizes the design of a stabilizing neural network controller with the derivation of its Lyapunov certificate, thereby integrating learning and control principles. This is aimed at enlarging the region of attraction while adhering to actuation constraints, employing Zubov's Partial Differential Equation (PDE) to accurately delineate the control policy's true region of attraction. Adhering to an actor-critic pattern, our methodology alternates between refining the control policy (actor) through reinforcement processes and formulating a Zubov function (critic), culminating in the determination of the maximal certifiable region of attraction through the application of an SMT solver post-training. Our empirical investigations across varied design scenarios underscore consistent, marked enhancements in the expanse of the region of attraction, heralding significant progressions in the realms of nonlinear control and stabilization task policies.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zhepeng_Cen1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08444v1",
  "title": "COSTREAM: Learned Cost Models for Operator Placement in Edge-Cloud Environments",
  "modified_abstract": "Drawing inspiration from recent advancements in decentralized systems, such as the innovative SWIFT algorithm in Federated Learning (FL) that showcases remarkable speed-ups and efficiency gains via wait-free model communication, we introduce COSTREAM. COSTREAM is a novel learned cost model designed for Distributed Stream Processing Systems, aiming to enhance the execution efficiency of streaming queries within edge-cloud environments through parallel processing. Utilizing rigorous training methodologies, this solution accurately predicts the execution costs associated with various streaming queries, facilitating optimal operator placement across heterogeneous hardware through efficient model/gradient sharing and communication techniques. By iterating over numerous potential configurations, COSTREAM has demonstrated its ability to deliver highly accurate cost estimates for initial operator configurations and to effectively generalize across new placements, queries, and hardware types. The application of COSTREAM for optimizing the placement of streaming operators has resulted in substantial performance improvements, achieving median speed-ups of approximately 21x over traditional baselines. However, the model\u2019s approach to predicting execution costs does not inherently involve descent methods or the challenges of non-convex optimization, focusing rather on the dynamic allocation and scheduling challenges unique to distributed systems.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Evan_Z_Wang1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08438v2",
  "title": "Reproducibility and Geometric Intrinsic Dimensionality: An Investigation on Graph Neural Network Research",
  "modified_abstract": "Amidst the increasing call for scientific rigor in machine learning research, our study addresses two pivotal challenges: reproducibility and the geometric intrinsic dimensionality in graph neural networks (GNNs). Inspired by the critical examination of GNN evaluations under heterophily and homophilous conditions, our work investigates the reproducibility of empirical findings, a core aspect in ensuring the reliability, openness, and robustness of machine learning advancements. We propose an ontology of reproducibility specific to machine learning, with a focused application on GNN methodologies and their task-specific challenges, including graph classification tasks. Furthermore, by examining the concept of geometric intrinsic dimensionality in depth, we aim to shed light on how the curse of dimensionality impacts the performance and reliability of GNN models, especially concerning data representation and analysis challenges in various set scenarios with large and complex datasets. Through this dual focus, our research contributes to understanding and overcoming barriers to effective GNN application, while emphasizing the need for reproducible and reliable research outcomes. This exploration is poised to facilitate the seamless integration of innovative findings into existing knowledge bases, thereby promoting more robust and transparent machine learning research practices. Code and additional resources have been omitted for de-identification purposes.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Denis_Kuznedelev1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08417v1",
  "title": "The Development and Performance of a Machine Learning Based Mobile Platform for Visually Determining the Etiology of Penile Pathology",
  "modified_abstract": "Drawing inspiration from pioneering works on the convergence of visual and textual data, such as PaLI's successful integration of large language and vision models for numerous language and multimodal tasks, our study extends this paradigm into the medical domain. Specifically, we developed a machine-learning-based mobile platform that leverages a clinical image dataset with original and augmented images for five penile diseases. The platform employs a U-net architecture, an encoder-decoder model, for semantic pixel segmentation, Inception-ResNet v2 for disease classification, and GradCAM++ for interpretability, demonstrating high accuracy across various metrics during its training phase. Our training involved a large 91% sample of the database across 150 epochs per image, scaling up the computational and training process to meet the accuracy requirements with validation on the remaining 9%. Of the 239 images in the validation dataset, the model achieved an overall accuracy of 0.944. Furthermore, the platform's global usage and demographic data indicate its potential to make sexual health services more accessible, incorporating multilingual support to enhance user experience and understanding. This fusion of machine learning models, clinical knowledge, and mobile technology underscores the broader applicability of integrating visual computational algorithms and encoder-decoder architectures for healthcare diagnostics, moving towards the goal of multilingual and comprehensive healthcare solutions.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Alexander_Kolesnikov2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08414v1",
  "title": "Causal Graph Neural Networks for Wildfire Danger Prediction",
  "modified_abstract": "Building on the foundational insights from previous investigations that highlighted the critical role of causality in accurately interpreting complex models, particularly through the establishment of sparse, symbolic causal graphs, our research introduces an innovative approach in wildfire forecasting. Wildfire forecasting is notoriously hard due to the complex interplay of different factors such as weather conditions, vegetation types, and human activities. Deep learning models show promise in dealing with this complexity by learning directly from data. However, to inform critical decision-making, we argue that we need models that are right for the right reasons; that is, the implicit rules learned should be grounded by the underlying processes driving wildfires. In that direction, we propose integrating causality with Graph Neural Networks (GNNs) that explicitly model the causal mechanism among complex variables via graph learning. The causal adjacency matrix considers the synergistic effect among variables and removes the spurious links from highly correlated impacts, thus ensuring a more accurate, symbolic, sparse, and symbolic representation of causal relationships. Our methodology's effectiveness is demonstrated through superior performance in forecasting wildfire patterns in the European boreal and Mediterranean biome. The gain is especially prominent in a highly imbalanced dataset, showcasing an enhanced robustness of the model to adapt to regime shifts in functional relationships and guarantee improved prediction accuracy. Furthermore, SHAP values from our trained model provide a deeper explanation and enhance our understanding of the model's inner workings, aligning with precedential theories of sparse, encoded causal explanations in deep learning models.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Huiqi_Deng1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08408v1",
  "title": "Reduced Jeffries-Matusita distance: A Novel Loss Function to Improve Generalization Performance of Deep Classification Models",
  "modified_abstract": "Motivated by the need for innovative solutions to enhance the generalization performance of deep neural networks in classification tasks, and drawing inspiration from extensive previous research\u2014such as the analysis of normalized features over the Riemannian manifold which provided profound insights into the neural collapse phenomenon\u2014this paper introduces a novel distance measure, the Reduced Jeffries-Matusita distance, as a loss function specifically designed to mitigate overfitting in deep classification models. By critically examining the characteristics of loss functions, including their Lipschitzness, nonconvex nature, maximum value, and potential for feature-driven optimization in overparameterized models, we argue for the implementation of this new metric to stabilize the training process and enhance the models' generalization abilities through effective normalization and optimization techniques, ultimately aiming to improve performance metrics such as Accuracy and F1 score, even in scenarios with limited data. Our experimental validation, conducted within diverse domains including image classification in computer vision and node classification in graph learning, supports the efficacy of our proposed method on various data configurations, including those conceptualized on a sphere manifold and prone to collapse under traditional loss functions, by leveraging neural feature representations tailored towards optimizing model performance. [Code omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Qing_Qu2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08403v1",
  "title": "FSDR: A Novel Deep Learning-based Feature Selection Algorithm for Pseudo Time-Series Data using Discrete Relaxation",
  "modified_abstract": "Inspired by the significant acceleration of computational tasks in machine learning through the use of hardware optimizations, as evidenced by the marked improvements in algorithm efficiency for large-scale classification tasks through GPU acceleration, this paper presents the Feature Selection through Discrete Relaxation (FSDR) algorithm. FSDR, a novel Deep Learning (DL)-based feature selection algorithm, is specifically designed for Pseudo Time-Series (PTS) data, which consists of observations arranged in sequential order without adhering to a conventional temporal dimension. Addressing the limitations of conventional feature selection algorithms that often face impractical computational complexities with high-dimensional data, FSDR introduces a method of learning important features as model parameters using discrete relaxation. This technique, suitable for applications in massive data settings, leverages sparse representations to approximate a discrete optimization problem with a continuous one, offering a solution capable of handling a significant number of feature dimensions beyond the reach of existing DL-based or traditional methods. Our experimental validation, conducted on a hyperspectral dataset\u2014a type of PTS data\u2014demonstrates FSDR's superior performance over three commonly used classifiers, achieving an optimal balance among execution time, classifiers' accuracy ($R^2$), and root-mean-square error ($RMSE$). Notably, this improvement is also attributed to the effective use of multithreading and multi-core, shared-memory architectures to parallelize computational tasks, including the training and solution solving phases, further underscoring FSDR's innovative approach to feature selection.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~John_Timothy_Halloran1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08386v1",
  "title": "Optimizing Risk-averse Human-AI Hybrid Teams",
  "modified_abstract": "In light of the evolving landscape of Artificial Intelligence (AI), where the synergy between humans and AI systems forms the cornerstone of what we describe as hybrid teams, this work is inspired by seminal efforts in the field that have delved into enhancing the cooperation among diverse agents, including humans, in ad hoc settings. Particularly, we draw motivation from those endeavors that tackle the complexities of robust cooperation with unseen agents, through innovative strategies like emulating policies from the minimum coverage set for training robust ad hoc teamwork agents. This foundation guides our investigation into optimizing the performance of risk-averse human-AI hybrid teams - a scenario increasingly plausible as AI's capability and adoption surge, despite their prone-to-error behavior. Our approach introduces a unique manager, trained via a conventional Reinforcement Learning algorithm, designed to adeptly delegate decision-making responsibilities within the team while minimizing deleterious shifts in delegation due to undesirable team actions, with a focus on tweaking hyperparameters for optimal performance. Through experiments in varied grid environments characterized by failure states and a heuristic-driven methodology, we evaluate the manager's proficiency in dictating optimal delegation strategies under different risk thresholds and aim to reach the state-of-the-art in cooperative hybrid team management. The findings affirm our manager's capacity to learn effective delegation patterns that not only approximate but, in some cases, mirror the ideal pathways in terms of path length and frequency of delegation, establishing a new benchmark in the orchestration of human-AI hybrid teams.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jiaxun_Cui1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08376v1",
  "title": "Nonlinear Manifold Learning Determines Microgel Size from Raman Spectroscopy",
  "modified_abstract": "Inspired by advancements in machine learning (ML) for complex scientific computations as demonstrated in recent efforts to accelerate partial differential equation (PDE) solvers through innovative reduced-order modeling and high-fidelity simulations, our study leverages the potential of nonlinear manifold learning for polymer particle size determination. Considering the importance of polymer particle size in product quality and the established role of Raman spectroscopy for in-line concentration monitoring, our work addresses the existing gap in accurately determining polymer size from Raman spectroscopic measurements. We introduce three alternative ML workflows that utilize diffusion maps, a sophisticated nonlinear manifold learning technique for reducing dimensionality and identifying subspace structures, and extensive training datasets: (i) directly from diffusion maps, (ii) alternating diffusion maps, and (iii) conformal autoencoder neural networks. Employing these methods on a dataset of Raman spectra and corresponding sizes of 47 microgel samples, measured by dynamic light scattering and spanning diameters of 208nm to 483 nm, we find that conformal autoencoders significantly excel over contemporary methods, marking a notable advancement in the precision prediction of polymer sizes from Raman spectra. This achievement also indicates a reduction in runtime for real-time data analysis, establishing a direct implication for improved efficiency in solvers dedicated to analyte size determination.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Peter_Yichen_Chen1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08370v1",
  "title": "SMART: Submodular Data Mixture Strategy for Instruction Tuning",
  "modified_abstract": "Drawing inspiration from previous works which highlight the pivotal role of accurate task proportioning in machine learning processes, and addressing the complexity of NP-hard problems like scheduling in computation graphs, this study introduces SMART (Submodular data Mixture strAtegy for instRuction Tuning). This novel approach leverages a submodular function to objectively assign importance scores to tasks, hence systematically determining their mixture weights for learning instruction tuning. Unlike existing methods that rely on manual tuning or implicit practitioner intuition, SMART offers a strategic way to redistribute a fine-tuning budget across tasks, choosing non-redundant samples from each one for enhanced learning. Our experiments reveal that SMART notably outperforms conventional methods like examples proportional mixing and equal mixing in graph-based computational settings, creating a new baseline in instruction tuning. Moreover, it facilitates the creation of effective data mixtures from merely a few representative subsets of tasks. Through task pruning analysis, we demonstrate that in contexts of limited budgets, focusing on a selective array of representative tasks can significantly enhance model performance compared to an equal distribution of resources across all tasks, underpinning the key computation strategies required for optimal learning. The code for replicating our findings is made available at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Corrado_Rainone1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08364v1",
  "title": "Decoupled Federated Learning on Long-Tailed and Non-IID data with Feature Statistics",
  "modified_abstract": "Inspired by studies exploring the inherent heterogeneity in federated learning environments, particularly through benchmarks for federated hetero-task learning that examine the challenges posed by inconsistent data distribution and learning tasks across participants, our work introduces a novel approach to addressing the unique hurdles of long-tailed and non-IID data distributions in federated learning. Federated learning, a paradigm designed to enhance data security and privacy, faces significant challenges when dealing with heterogeneous data. This paper delves into an overlooked scenario where tail classes are sparsely distributed over a few clients, resulting in slower convergence rates and poorer model performance due to the reduced likelihood of choosing models from these classes during client aggregation. To tackle this problem, we present a two-stage Decoupled Federated Learning framework using Feature Statistics (DFL-FS) that aims for fair personalization. The first stage involves the server estimating the client's class coverage distributions through masked local feature statistics clustering to select models for aggregation, thereby accelerating convergence and enhancing feature learning without compromising privacy. In the second stage, DFL-FS utilizes federated feature regeneration based on global feature statistics, along with resampling and weighted covariance adjustments, to improve the model's adaptability to long-tailed data distributions and ensure fair representation. Our experimental results on CIFAR10-LT and CIFAR100-LT datasets, with varying long-tailed rates, demonstrate that our method outperforms state-of-the-art approaches in terms of both accuracy and convergence rate, while also achieving personalized and fair outcomes for all participating clients.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~weirui_kuang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08362v1",
  "title": "Mean-Field Microcanonical Gradient Descent",
  "modified_abstract": "Inspired by the intricate balance required in entropy management during sampling in energy-based models, as evidenced by related works such as Differentiable Particle Filtering via Entropy-Regularized Optimal Transport, our research introduces a novel approach to microcanonical gradient descent. This sampling procedure facilitates efficient sampling of distributions in high-dimensional spaces by transporting samples from a high-entropy distribution, such as Gaussian white noise, to a low-energy region through gradient descent. Our work highlights a potential pitfall in traditional approaches, which can overfit by disproportionately reducing entropy during the descent. We propose a solution in the form of mean-field microcanonical gradient descent, a method that samples several weakly coupled data points simultaneously, thereby achieving a more balanced entropy loss while maintaining a close likelihood fit. This technique, applied to state-space models, is especially effective in financial time series modeling, where our tests demonstrate notable improvements on both synthetic and real datasets, showcasing the practical benefits and applicability of our mean-field approach for inference in complex systems.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Adrien_Corenflos1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08352v1",
  "title": "Data augmentation with automated machine learning: approaches and performance comparison with classical data augmentation methods",
  "modified_abstract": "Drawing from the foundational strides made by self-supervised learning techniques, which leverage unlabeled data to significantly enhance model performance during the training phase, our study embarks on an exploration within the domain of data augmentation. Data augmentation stands as a crucial regularization strategy employed to augment generalization performance in machine learning models by generating new data samples through specific data transformation operations. We also consider the role of clustering techniques in the automated process to facilitate improved data grouping for training purposes. Despite its effectiveness, the manual iteration over potential data augmentations and their parameters is a cumbersome and time-consuming task. Automated data augmentation, leveraging the principles of automated machine learning (AutoML), aims to streamline this process, presenting an efficient alternative to traditional methods. Our research delves into a comparative analysis between AutoML-based data augmentation techniques, including aspects of data manipulation, integration, synthesis, and classical data augmentation methods, while indirectly touching on concepts such as clustering and classification in data synthesis. Specifically, we provide a comprehensive survey of AutoML for data augmentation, covering methodologies such as data manipulation, integration, and synthesis, alongside mechanisms for optimizing the augmentation process through search space design, hyperparameter optimization, and model evaluation incorporating aspects of clustering for enhanced data synthesis and classification accuracy. Our empirical evaluation reveals that automated approaches not only simplify the augmentation process but also achieve better generalization performance compared to classical methods, highlighting the former's superiority.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Emanuele_Sansone1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08344v1",
  "title": "STMPL: Human Soft-Tissue Simulation",
  "modified_abstract": "Drawing inspiration from innovative simulation techniques in deformable object manipulation, such as those showcased by DextAIRity's active airflow method for manipulating deformable objects like cloth, our work addresses the challenge of simulating the deformation of soft tissues in the human body during interactions with external objects, a critical aspect in applications like virtual reality and gaming. Unlike traditional Finite Element Methods (FEM), which are slow and resource-intensive, we propose a unified representation of human body shape and soft-tissue through a data-driven simulator of non-rigid deformations, enabling rapid simulation of realistic interactions with a focus on grasping scenarios. Building upon the SMPL model for generating human body shapes with considerations for rigid transformations and closed-loop control strategies to adapt to dynamic changes in interaction, we extend the model to encompass a soft-tissue layer and an intuitive representation of external forces, including airflow-related dynamics. By mapping the 3D body shape and soft tissue to 2D UV maps and utilizing a UNET architecture designed for 2D data for processing airflow-related deformations, our approach not only accomplishes high-accuracy, real-time inference but also achieves plausible deformation of the soft tissue layer even in unencountered scenarios. Our methodology aligns with self-supervised learning paradigms to further enhance the accuracy and generalizability of soft-tissue simulations across different grasping and interaction tasks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zhenjia_Xu1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08337v1",
  "title": "LLM-Assisted Light: Leveraging Large Language Models Capabilities for Human-Mimetic Traffic Signal Control in Complex Urban Environments",
  "modified_abstract": "In the realm of traffic signal control (TSC), the complex and dynamic nature of urban environments presents a formidable challenge, necessitating advanced solutions for effective congestion management. Drawing inspiration from previous works that examined the dynamics of decision-making algorithms in variable scenarios, such as the utilization of the Multiplicative Weights Update method in congestion games and two-agent systems, this study introduces a novel approach that integrates Large Language Models (LLMs) into TSC. Our framework leverages the sophisticated reasoning and decision-making capabilities of LLMs, coupled with a suite of perception and decision-making tools, to analyze both static and dynamic traffic data, a situation often perceived as chaotic by traditional systems. We introduce a meta-algorithm variant as a component of our simulation platform to validate the effectiveness of our approach, which demonstrated notable improvements in traffic management, particularly during Sensor Outage (SO) scenarios, reducing the average waiting time by 20.4% compared to conventional RL-based systems. This innovative integration of LLMs into the TSC strategy represents a significant advancement in traffic management, underscoring the potential of LLMs in addressing real-world, dynamic challenges that are chaotic and involve sophisticated game-based scenarios, moving towards a more adaptive and learning-focused equilibrium, with an emphasis on the convergence of optimal strategies. Efforts to prove the convergence characteristics of our proposed solution are underway, potentially providing a robust theoretical foundation for our empirical observations. The related code is available at [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Gerasimos_Palaiopanos1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08335v1",
  "title": "A Sparsity Principle for Partially Observable Causal Representation Learning",
  "modified_abstract": "Drawing inspiration from advancements in understanding and interpreting uncertainty estimates in differentiable probabilistic models, our research progresses into the domain of causal representation learning, where the objective is to deduce high-level causal variables from perceptual data. Distinguishing itself from conventional methods that presume the full observability of latent causal variables in high-dimensional observations, our study delves into a partially observed framework. Specifically, it investigates scenarios wherein each measurement illuminates only a segment of the underlying causal state, akin to the nuanced exploration of counterfactual explanations and space exploration for uncertainty estimates detailed in previous works. This exploration navigates the complex space of latent variables contributing to the observed partial information, complicating the process. We introduce a novel perspective, focusing on unpaired observations characterized by instance-dependent patterns of partial observability. Central to our inquiry are two identifiability results: one applicable to linear mixing functions devoid of parametric assumptions on the causal model, and another suitable for piecewise linear mixing functions under the presence of Gaussian latent causal factors. Informed by these results, our methodology learns to unearth the underlying causal variables by endorsing a principle of sparsity in the deduced representations. Experimental validations on assorted simulated datasets and established benchmarks underscore the efficacy of our proposed methods in accurately retrieving the ground-truth latent causal variables, thereby providing valuable counterfactual explanations and space for discussions in the broader discourse on causal discovery and representation learning. The system effectively learns from the explored scenarios, further enhancing our understanding of partially observable causal mechanisms and the role of uncertainty estimates in this context.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Dan_Ley1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08837v1",
  "title": "Cyclic Data Parallelism for Efficient Parallelism of Deep Neural Networks",
  "modified_abstract": "In the pursuit of scaling deep learning algorithms efficiently, inspired by foundational work in enhancing parallel processing techniques\u2014such as 'FedAvg with Fine Tuning' which leveraged the diversity among client data distributions for federated learning\u2014our study introduces Cyclic Data Parallelism. This novel approach addresses critical limitations observed in conventional parallelization methods like Data Parallelism and ZeRO-DP, which suffer from challenges related to memory peaks and the need for synchronous gradient averaging. By adopting a sequential execution of micro-batches with a uniform delay, our method not only maintains a constant memory footprint for activations but also achieves balanced gradient communications and averaging throughout training, formally proving its efficacy. When coupled with Model Parallelism, Cyclic Data Parallelism can reduce the dependency on the number of GPUs by allowing the sharing of GPUs across micro-batches, and within the ZeRO-DP framework, it facilitates model state communications using point-to-point operations instead of collective broadcasts. This efficiency gain, further enhanced through fine-tuning of the algorithm's parameters and informed by the distributions of ground-truth data, is demonstrated through training experiments on CIFAR-10 and ImageNet datasets, highlighting the practical benefits and scalability of our proposed technique.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Liam_Collins1",
  "manipulated_ranking": 4,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08333v2",
  "title": "Fast Inference of Removal-Based Node Influence",
  "modified_abstract": "In the evolving landscape of graph neural networks (GNNs), this work is inspired by pioneering approaches in the domain, including methods addressing the intricate balance between high-quality and efficiency in tasks such as community detection, which like our study, explore the dynamic behavior within graphs under perturbative conditions. Our method, NOde-Removal-based fAst GNN inference (NORA), proposes a novel approach to evaluate node influence based on the prediction change in a GNN model when a node is removed. This concept reflects on practical scenarios such as assessing the impact of removing a Twitter account on the polarity prediction of other accounts using a GNN as a surrogate model to simulate the aftermath of node or edge modifications. Despite the existence of related discussions such as graph adversarial attacks and counterfactual explanations, our problem setting diverges in its focus and requirement for efficiency. NORA introduces an efficient optimization technique that leverages gradient information for high-quality inference process optimization, requiring only one forward and one backpropagation pass for all nodes, thus addressing the demand for timely inference in real-world applications. Moreover, our approach aligns with inductive learning principles, facilitating swift and accurate adaptation to unseen nodes or graph structures through efficient training regimes. Our extensive testing across various datasets and GNN models, including those with differing training strategies, confirms NORA's effectiveness in capturing node influence swiftly and accurately in the context of learning communities and their dynamics. Code and resources for our methodology are made accessible at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Meng_QIN1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08331v1",
  "title": "Bayesian Optimization that Limits Search Region to Lower Dimensions Utilizing Local GPR",
  "modified_abstract": "In the realm of optimization, especially in high-dimensional settings where observing costs are considerable, Bayesian optimization (BO) has emerged as a potent tool, inspired by advancements in stochastic dynamical systems and the effective use of Bayesian Neural Networks (BNNs) for policy search and model-based reinforcement learning. Our study introduces a novel approach to BO that addresses its computational inefficiencies by confining the search space to lower-dimensional regions and leveraging local Gaussian process regression (LGPR) for scalability. This methodology acknowledges the dimensionality challenge intrinsic to BO by treating the reduced search space as 'local', thereby enhancing prediction accuracy within these confines through LGPR, which is specifically trained on data pertinent to the designated region. Such a strategy not only elevates the prediction fidelity but also mitigates the computational burden typically associated with the matrix inversion in Gaussian process regression. Through extensive evaluations, conducted with 20D Ackley and Rosenbrock functions under control settings, our methodology demonstrates search efficiencies superior or equivalent to existing methods, showcasing improvements of approximately 69% and 40%, respectively, over scenarios devoid of LGPR. Furthermore, our approach was benchmarked against conventional techniques, highlighting its effectiveness. The application of this optimized method to the automatic design of power semiconductor devices yielded significant advancements, outperforming conventional approaches and reducing specific on-resistance by 25% more effectively than standard methods and by 3.4% more when compared to non-LGPR implementations. Notably, our approach, which utilized neural networks dynamics and exploited multi-modality within the optimization process, offers a comprehensive framework for Bayesian optimization that promises to train models with heightened efficiency and precision.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Stefan_Depeweg2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08836v1",
  "title": "Structural Positional Encoding for knowledge integration in transformer-based medical process monitoring",
  "modified_abstract": "Predictive process monitoring, particularly in the medical domain, acts as a critical decision support tool offering forecasts like the next expected activity in a healthcare process. This task gains complexity and significance when enriched with sector-specific knowledge, ensuring decisions are well-informed and more likely to be accepted by practitioners. Drawing inspiration from advances across fields such as large-scale protein function prediction, where integration of diverse information sources like sequence, text, domain, and networks significantly enhanced model performance, our study introduces a novel approach to predictive process monitoring. We utilize a transformer architecture, renowned for its attention mechanism, and innovatively apply graph positional encoding to integrate ontological domain-specific knowledge into the process. Additionally, considering the massive, multi-label data sets typical in healthcare scenarios, our application closely mirrors network models' complexity in managing regression and classification challenges. This paper illustrates our promising results in stroke management, highlighting the potential of our method to improve decision support and quality assessment in healthcare through more effective knowledge integration. The substantial improvements observed in the system's performance upon leveraging this method underscore its utility in managing massive, multi-label data sets typical in healthcare scenarios, embodying a noteworthy growth in the field of medical process monitoring.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Shanfeng_Zhu1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08835v1",
  "title": "Stacking-based deep neural network for player scouting in football 1",
  "modified_abstract": "Our work takes its cue from the novel applications of data science in professional sports, drawing inspiration from recent advancements in analytical methods, such as the hierarchical von Mises-Fisher-Gaussian model for animal pose estimation, which have revolutionized data-driven decision-making in complex environments. Datascouting, a pivotal data application in football, aims to sift through extensive databases to identify players with high potential for further review by human scouts. We introduce a stacking-based deep learning model, specifically designed for the scouting of high-potential football players within an open-source database. This model incorporates temporal analysis and skeleton localization of video data to evaluate players' performance more comprehensively, outperforming traditional statistical methods. The incorporation of pose estimation techniques and algorithms further refines the accuracy of our evaluations, signaling a significant leap forward in the efficiency and accuracy of player scouting endeavors, particularly with respect to freely-behaving players in dynamically captured video data. Our work, showcasing a fusion of estimation, localization, and advanced analysis through video algorithms, invites a broader community to explore the potentials within the scouting domain.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Libby_Zhang1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08834v1",
  "title": "Predictive Analysis of Tuberculosis Treatment Outcomes Using Machine Learning: A Karnataka TB Data Study at a Scale",
  "modified_abstract": "Tackling the global health menace of Tuberculosis (TB) with the aid of machine learning (ML), our study is propelled by the successes and innovations in ML applications, notably within the domain of natural language processing (NLP) and semantic understanding as demonstrated in recent works. These advancements provide a solid foundation for applying sophisticated ML techniques to healthcare challenges. Our research specifically ventures into the realm of predictive analytics for TB treatment outcomes in Karnataka, utilizing a dataset encompassing over 500,000 patient records from NIKSHAY, India's national TB control program. By framing the prediction of treatment outcomes as a binary classification challenge, we were able to generate patient risk scores, achieving remarkable performance metrics with a recall of 98% and an AUC-ROC score of 0.95 on a validation set of 20,000 records. In addition, drawing on the advancements in computational linguistics, we explore the integration of NLP techniques to enhance model learning, a novel approach within the context of TB research. The promising results of our study not only reaffirm the potential of ML in revolutionizing healthcare diagnostics and treatment strategies but also lay the groundwork for future explorations aimed at eradicating TB. Our findings underscore the significant impact that ML, inspired by recent breakthroughs in computational linguistics and data processing, can have on public health.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~bailin_wang1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08319v1",
  "title": "Knowledge Conflicts for LLMs: A Survey",
  "modified_abstract": "This survey is deeply inspired by the pioneering efforts of previous works in the domain of language models, particularly highlighting the need for open research and addressing the challenges inherent in large language models (LLMs), such as biases and potential risks, to bring to the forefront the issue of knowledge conflicts within LLMs. Through a systematic evaluation of knowledge conflicts for LLMs, our work delves into an in-depth analysis, categorizing these conflicts into three main types: context-memory, inter-context, and intra-memory conflicts. The study emphasizes how these conflicts can significantly hinder the trustworthiness and performance of LLMs, especially in powerful real-world scenarios plagued with noise and misinformation. Our evaluation further explores the causes, behaviors of LLMs under such conflicts during their training phase, and reviews existing solutions, aiming to provide insights into enhancing the robustness and development of LLMs. Consequently, our work contributes to addressing the complexities of LLMs and strategizing towards their improvement, thereby truly advancing research and commercial development efforts in this ever-evolving field through the anticipated release of targeted interventions. Consequently, we call on researchers to join us in this crucial task.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Pradeep_Dasigi1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08309v2",
  "title": "HRLAIF: Improvements in Helpfulness and Harmlessness in Open-domain Reinforcement Learning From AI Feedback",
  "modified_abstract": "Drawing inspiration from significant advancements and inventive practices in the field of machine learning, such as the application of unlikelihood training to address inconsistencies in dialogue generation, our work advances the dialogue quality and strategic output of large language models (LLMs) through the development of Hybrid Reinforcement Learning from AI Feedback (HRLAIF). This innovative approach leverages ChatGPT as a labeler to provide feedback on open-domain prompts in RLAIF training, yielding an increase in human evaluators' preference win ratio for model responses while also identifying a decrease in evaluators' satisfaction rate due to responses becoming less helpful in terms of correctness and truthfulness. To counter these challenges, HRLAIF methodologically enhances the accuracy of AI annotations for responses, bolstering the model's helpfulness throughout the training process. It further integrates AI for Red Teaming, augmenting the model's harmlessness. Human evaluation results demonstrate that HRLAIF not only inherits RLAIF's efficiency in enhancing human preference for outcomes at minimal costs but notably improves the satisfaction rate of responses. Compared to the policy model prior to the implementation of Reinforcement Learning (RL), HRLAIF achieves a significant increase of 2.08% in satisfaction rate, effectively addressing the observed decrease of 4.58% in satisfaction rate following basic RLAIF implementation.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ilia_Kulikov1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08291v1",
  "title": "CleanAgent: Automating Data Standardization with LLM-based Agents",
  "modified_abstract": "This work is inspired by the pioneering efforts in harnessing Transformer architectures for information extraction and comprehension tasks, setting the stage for innovative solutions in data processing. Data standardization, a fundamental yet complex component of the data science life cycle, has traditionally involved substantial manual effort, particularly in customizing tools like Pandas for diverse column types. The advent of large language models (LLMs) like ChatGPT heralded a new era of potential automation in this domain through advanced natural language understanding, code generation capabilities, and extraction processes. However, the implementation of these capabilities still requires significant programming expertise and active user interaction for prompt optimization. Addressing these challenges, we propose CleanAgent, a novel Python library that utilizes a declarative, unified API approach to dramatically streamline the data standardization process. Initially, we introduce Dataprep.Clean as a component of the broader Dataprep Library, offering a simplified method for column type standardization through single-line code commands and extraction techniques. Subsequently, the CleanAgent framework is presented, combining Dataprep.Clean with LLM-based agents to fully automate data standardization, minimizing the need for continuous user intervention and lowering the barrier to entry for non-expert programmers in the field of data science.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~\u0141ukasz_Borchmann1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10555v1",
  "title": "KARINA: An Efficient Deep Learning Model for Global Weather Forecast",
  "modified_abstract": "Informed by the achievements and challenges in recent climate research and deep learning practices, such as sparse uncertainty representation and high-resolution data processing, our work introduces KARINA, a novel deep learning-based model tailored for global weather prediction that addresses the computational intensity typically associated with high-resolution climate data analysis. Leveraging insights from methodologies designed to manage computational and memory inefficiencies in models like Bayesian neural networks and deep ensembles, KARINA combines ConvNext, SENet, and Geocyclic Padding to achieve accurate weather forecasting with reduced computational demands. This model, highlighting the importance of uncertainty management in learning atmospheric phenomena, requires only four NVIDIA A100 GPUs and less than 12 hours of training to provide forecasts at a 2.5\u00b0 resolution with accuracy surpassing current benchmarks, including models like ECMWF S2S and newer high-resolution models such as Pangu-Weather, GraphCast, ClimaX, and FourCastNet. By integrating techniques that dynamically enhance feature response and preserve atmospheric flow continuity, including advanced sampling for tackling the inherent uncertainty and employing efficient networks to model the complex dynamics of weather patterns, and by effectively utilizing matrix operations for scalability and efficiency, KARINA marks a significant advancement in the efficiency and accuracy of global weather forecasting. With its network architecture optimized for atmospheric data, it effectively handles the uncertainty inherent in weather prediction, making it a substantial contribution to the field of neural networks and climate forecasting.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Hippolyt_Ritter1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08267v1",
  "title": "SNOW-SCA: ML-assisted Side-Channel Attack on SNOW-V",
  "modified_abstract": "In this research, we introduce SNOW-SCA, marking the inaugural endeavor to apply a power side-channel analysis (SCA) attack on SNOW-V, a candidate for the 5G mobile communication security standard, utilizing a 32-bit ARM Cortex-M4 microprocessor. Drawing inspiration from prior advancements in data permutation recovery under noisy conditions\u2014a domain that examined the effects of noise on data recovery and established a framework for optimal decoding in polynomial time through linear models\u2014our work extends these analytical techniques to the realm of side-channel cryptanalysis, utilizing a decoder for interpreting side-channel output. Following the initial phase, we commence by conducting a generic known-key correlation (KKC) analysis to ascertain the points of data leakage, considering the observations obtained. Subsequently, following this analysis, we execute a correlation power analysis (CPA) to significantly lower the complexity of the attack to a mere two key guesses per byte. Employing linear discriminant analysis (LDA) enables the unique identification of the correct secret key, showcasing a 100% success rate with less than 200 training traces, asserting that the attack can proceed with a solitary trace effectively. Our comprehensive attack methodology, which pairs CPA with LDA, facilitates the expeditious recovery of the correct secret key byte with fewer than 50 traces using the ChipWhisperer platform. The feasibility of extracting the entire 256-bit secret key of SNOW-V via this innovative SCA attack is thoroughly demonstrated, leveraging observations of power consumption under noisy environments and testing these methodologies extensively. To counteract such vulnerabilities, we put forth practical, low-overhead countermeasures. This seminal analysis not only contributes to the cryptographic security landscape of emerging wireless communications but also underscores the critical role of ML in enhancing side-channel attack frameworks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Minoh_Jeong1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08265v2",
  "title": "Random Search as a Baseline for Sparse Neural Network Architecture Search",
  "modified_abstract": "Drawing inspiration from the literature that probes into the reproducibility and generalization capacity of neural networks through various lenses, such as decision boundary visualization, we position our research within the domain of sparse neural networks. Sparse neural networks have demonstrated equivalent or superior generalization performance compared to their dense counterparts with increased parameter efficiency, thus propelling research towards learning or searching for high-performing sparse models. However, the obscurity around standard benchmarks causes issues with comparability and reproducibility among different methodologies. Addressing this gap, our work introduces Random Search as a baseline algorithm to identify efficient sparse configurations through multiple iterations during the training phase. By applying Random Search in the node space of an overparameterized network, we aim to uncover well-initialized sparse sub-networks that are advantageously positioned within the loss landscape, possibly following a form of gradient descent in their configuration search. Our study meticulously compares the post-training performance of these sparse networks at varying levels of sparsity against both their dense origins and randomly generated sparse configurations of identical sparsity through decision-making metrics. Our findings reveal that while significant performance levels are maintained even at high sparsity, Random Search does not result in better performing or more efficiently converging sparse networks compared to randomly chosen sparse configurations. Consequently, we advocate for Random Search as a viable neutral baseline for evaluating sparse architecture search methods, enhancing the reproducibility of research in this area. Code is available at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yehuda_Dar1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08258v1",
  "title": "Skipformer: A Skip-and-Recover Strategy for Efficient Speech Recognition",
  "modified_abstract": "Motivated by the pivotal role of attention mechanisms for enhancing machine understanding across a variety of tasks, such as temporal sentence grounding in video, and the challenges posed by long input sequences in automatic speech recognition (ASR), we introduce Skipformer. Our novel architecture, Skipformer, addresses the heightened computational demand and memory consumption of Conformer-based attention models by proposing a \"Skip-and-Recover\" strategy. This dynamic approach modifies sequence input length in a non-uniform manner, leveraging intermediate CTC output to categorize frames into crucial, skipping, and ignoring groups. Crucial frames are processed through subsequent Conformer blocks, while skipped frames are re-integrated with crucial ones in their original temporal order after processing, delivering efficiency without sacrificing accuracy. Moreover, grounding in this context refers to the ability to align specific segments of speech with their relevant sentences effectively, a task that intra-modal grounding aids by identifying these segments within the broader context of ASR. Our evaluations on the Aishell-1 and Librispeech corpora demonstrate that Skipformer abbreviates input sequence length by substantial factors, resulting in better recognition accuracy and expedited inference when compared to recent benchmarks. Our code is open-sourced for community access and further advancement.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Xiaoye_Qu1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08246v1",
  "title": "Towards Unified Modeling for Positive and Negative Preferences in Sign-Aware Recommendation",
  "modified_abstract": "In the evolving landscape of recommendation systems, understanding both positive and negative user preferences has become paramount, a challenge further accentuated by recent developments in graph recommendation methods and the burgeoning field of targeted advertising. Inspired by advancements in Click-Through Rate (CTR) prediction and the insightful methodologies applied in neighboring fields, such as incorporating neighborhood interactions to mine deep user interests under a Heterogeneous Information Network (HIN), our research proposes a novel sign-aware recommendation system that integrates users' negative and positive preferences into a single model. Specifically, we introduce the Light Signed Graph Convolution Network for Recommendation (LSGRec), which employs a unified approach to capture high-order positive and negative interactions between users and items on a signed graph through efficient sampling and neighborhood embedding techniques. These embeddings facilitate a nuanced understanding of user preferences, enabling more accurate prediction of user behaviors. By innovatively propagating high-order negative preferences along positive edges and leveraging neighborhood embeddings, LSGRec overcomes the limitations of existing methods that separate the modeling of positive and negative links and often miss intricate high-order interactions. Our comprehensive experimental evaluation on three real-world datasets confirms LSGRec's superiority in both performance and computational efficiency over contemporary approaches, particularly in the arenas of click prediction and embedding regularization. For the benefit of the research community, our implementation, which demonstrates our novel approach in embedding high-order neighborhood information and mine deep user preferences by sampling strategies, is shared at [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Tingyang_Xu1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08245v1",
  "title": "Scattered Mixture-of-Experts Implementation",
  "modified_abstract": "Taking inspiration from the revolution in architectural paradigms introduced by the Transformer models, especially their unparalleled success in natural language processing (NLP) and promising adaptations in computer vision, we present ScatterMoE, a state-of-the-art implementation of Sparse Mixture-of-Experts (SMoE) optimized for GPU platforms. ScatterMoE innovatively addresses and mitigates several limitations of existing networks architecture models by enhancing inference and training speeds while requiring fewer computational resources and thus reducing memory demand, achieving this through the elimination of input padding and redundant data copying. Central to our approach is ParallelLinear, a key component that underpins our implementation and empowers the various kernels to expedite computational operations significantly. Our benchmarks against Megablocks exhibit ScatterMoE's superior throughput and reduced memory footprint in both vision and natural language processing applications. Furthermore, the introduction of ParallelLinear facilitates the expansion of the Mixture-of-Experts paradigm, as illustrated through our pioneering implementation of the Mixture of Attention model which employs classification tactics, hinting at the potential for convolutional strategies to be integrated in future versions. The conjunction of these innovative techniques and models displays the excellent potential of ScatterMoE in a broad range of applications.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Georg_Heigold1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08239v1",
  "title": "Continuous Object State Recognition for Cooking Robots Using Pre-Trained Vision-Language Models and Black-box Optimization",
  "modified_abstract": "Inspired by the innovative approaches in understanding and interpreting object states through computational models, as explored in works like 'Tags2Parts: Discovering Semantic Regions From Shape Tags', this study proposes a novel method for recognizing the continuous state changes of food for cooking robots. Unlike traditional classification problems focused on discrete state recognition, cooking scenarios demand a dynamic and continuous interpretation of environmental and object states - a challenge compounded by the complexity of food state changes which are difficult to encapsulate through manual programming alone. Leveraging pre-trained large-scale vision-language models, our method utilizes spoken language to bridge the gap between continuous image sequences and their corresponding state descriptions, allowing for the nuanced capture of food state changes over time. Furthermore, we introduce a method of fine-tuning the correlations between image sequences and textual descriptions through a combination of sigmoid function fitting and black-box optimization, thereby enhancing the accuracy and robustness of state recognition under limited supervision. Our experimentation with various cooking states, including water boiling, butter melting, egg cooking, and onion stir-frying, showcases the potential and limitations of our proposed approach, highlighting the utility of network-based techniques in processing complex image sequences for accurate state classification. The study sets benchmarks in continuous object state recognition, advocating for further research in fine-grained segmentation based entirely on vision-language synergy.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sanjeev_Muralikrishnan1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08222v1",
  "title": "Robust Decision Aggregation with Adversarial Experts",
  "modified_abstract": "Inspired by contemporary challenges in decision-making environments, such as answer aggregation in crowdsourcing where responses from a mix of expert and non-expert crowd workers need to be aggregated efficiently, our work addresses a related but distinct challenge in aggregating binary decisions in the presence of both truthful and adversarial experts. Unlike traditional settings that predominantly focus on identifying and leveraging expert contributions, we explore a scenario where the decision maker must design a robust aggregation algorithm capable of forecasting the true state of the world amid uncertain and potentially deceptive expert reports\u2014without prior knowledge of the information structure underlying these reports. We demonstrate that, under certain conditions, a truncated mean aggregator is optimal, insulating decision-making from the extremes of expert reports, and we explore piecewise linear functions as viable alternative aggregation mechanisms. Our theoretical contributions are coupled with practical evaluation through numerical experiments in an ensemble learning context, highlighting the importance of precise answer extraction from diverse opinions. Additionally, our study presents insights into scenarios where our proposed aggregation methods face limits, specifically in tasks requiring robust set operations for efficient data handling. This study not only advances our understanding of decision aggregation in the face of adversarial behavior but also contributes to a broader discourse on robust decision-making mechanisms in uncertain environments.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yuko_Kuroki1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08220v1",
  "title": "Efficient geometric Markov chain Monte Carlo for nonlinear Bayesian inversion enabled by derivative-informed neural operators",
  "modified_abstract": "Inspired by recent advancements in the application of generative networks for structured rank-one matrix recovery, where novel algorithmic techniques have efficiently leveraged a generative prior to enhance recovery performance, our study embarks on the exploration of operator learning approaches to expedite geometric Markov chain Monte Carlo (MCMC) methods for solving infinite-dimensional nonlinear Bayesian inverse problems. Geometric MCMC, while adept at generating high-quality proposals that adapt to posterior local geometry through the computation of local gradient and Hessian information of the log-likelihood, faces a critical drawback due to the excessive computational cost associated with parameter-to-observable (PtO) map evaluations defined by elaborate model simulations. Our proposal introduces a neural network surrogate of the PtO map, designed to harness rapid surrogate approximations of the log-likelihood along with its gradient and Hessian, offering substantial computational savings with increased efficiency in handling large volumes of data and operating within expansive Gaussian regimes. To further accelerate the process, we employ a derivative-informed neural operator (DINO) generated through an extended form of operator learning that incorporates input-output-derivative training samples, effectively creating a signal processing framework within the context of Bayesian inversion. This innovation produces DINO surrogates capable of accurately predicting both the observable and its parametric derivative, markedly reducing the training costs when compared to traditional operator learning methods and deliberate structured analysis techniques. Our numerical experiments on PDE-constrained Bayesian inversion illustrate significant efficiency gains, achieving posterior sample generation at speeds 3--9 times faster than conventional geometric MCMC and 60--97 times faster compared to prior geometry-based approaches. Moreover, the cost-efficiency of training DINO surrogates becomes evident after obtaining just 10--25 effective posterior samples.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jorio_Cocola1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10553v1",
  "title": "Learning to Watermark LLM-generated Text via Reinforcement Learning",
  "modified_abstract": "Our work is motivated by the significant strides made in the field of machine learning security, notably the innovative approaches to detecting and mitigating Trojan attacks in deep neural networks. This backdrop of advancements, including reverse-engineering methods that unveil vulnerabilities in model inputs and feature spaces, sets the stage for our exploration into watermarking LLM outputs. Our study focuses on embedding algorithmically detectable signals into LLM-generated text to track misuse\u2014an area not adequately addressed by current methods, which predominantly work with a fixed LLM and are limited to token-level watermarks. We propose a novel approach of embedding signals at the model level, within the LLM weights, detectable by a paired detector using state-of-the-art detection techniques. Through a co-training framework based on reinforcement learning, our method iteratively trains a detector for the generated watermarked text, exploits patterns in feature-space to enhance detection, and tunes the LLM, ensuring the maintenance of its utility. We demonstrate that our watermarks offer improved accuracy, robustness, and adaptability against new attacks, enabling the open-sourcing of watermarked models. Additionally, when aligned with existing models, our method introduces minimal extra overhead, necessitating only the training of an additional reward model (our detector). It is our hope that this investigation broadens the scope of watermark design, encouraging further research and development in creating versatile, resilient watermarks. The source code is available at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Kai_Mei1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08217v1",
  "title": "Research on the Application of Deep Learning-based BERT Model in Sentiment Analysis",
  "modified_abstract": "This study is situated within the context of advancing deep learning technologies, particularly inspired by previous efforts to understand and improve the dynamics of deep neural networks through spectral analysis of random matrices in the context of stochastic gradient descent. Focusing on sentiment analysis, this paper explores the application of deep learning techniques, with an emphasis on BERT models, for classification tasks across various data sets. It introduces the foundational concepts of sentiment analysis and discusses the advancement and utilization of deep learning methods in this sphere, including the importance of regularization strategies to enhance model performance and prevent overfitting. Detailing the architecture and characteristics of BERT models, the paper clarifies their application effects and optimization strategies in sentiment analysis, with support from experimental validation. The findings underscore the robust performance of BERT models in sentiment analysis tasks, particularly highlighting the benefits post fine-tuning and how networks benefit from spectral analysis for better understanding and optimization. Testing these models on various datasets has further validated their effectiveness in sentiment classification. The spectra of the matrices involved play a key role in this optimization process, particularly in the descent mechanisms. The paper culminates in summarizing potential applications of BERT models in sentiment analysis and suggesting avenues for further research and practical implementations.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Xuran_Meng1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08216v1",
  "title": "PaddingFlow: Improving Normalizing Flows with Padding-Dimensional Noise",
  "modified_abstract": "Building upon the solid foundation of generative modeling established by advances in deep learning, such as the development and success of Denoising Diffusion Probabilistic Models (DDPM) that have fundamentally transformed few-shot generation with their sophisticated sample generation quality and training stability, our work introduces PaddingFlow. PaddingFlow is an innovative approach aimed at addressing two major issues facing flow-based models today: the difficulties in handling manifolds and discrete data distributions effectively. Flow-based models, while offering efficient sampling and training, often underperform when the latent target distribution's dimension does not match that of the data, leading to manifold issues, or when handling discrete data, which can result in models collapsing into degenerate mixtures of point masses. By integrating few-shot learning principles and emphasizing vision-based tasks, including image processing, PaddingFlow, our novel dequantization method, injects padding-dimensional noise to enhance normalizing flows. This adaptation facilitates the handling of images and hierarchical data structures, making it simple to implement, cost-effective, adaptable across a variety of tasks including those involving multiple classes, and capable of generating unbiased sample estimations with a patch-based approach, thus effectively handling set-related challenges. Notably, our method successfully navigates past the limitations of existing dequantization techniques that typically necessitate alterations to the data distribution, potentially compromising performance. Through exhaustive evaluations on standard benchmarks for unconditional density estimation\u2014including five tabular datasets and four image datasets for VAE models, along with IK experiments for conditional density estimation\u2014PaddingFlow demonstrates substantive improvements across all evaluated tasks, establishing a new horizon in the efficient training of generative models.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Giorgio_Giannone1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10552v1",
  "title": "Training Self-localization Models for Unseen Unfamiliar Places via Teacher-to-Student Data-Free Knowledge Transfer",
  "modified_abstract": "Addressing the challenge of self-localization in robots for navigating general open-world conditions\u2014a task that becomes critical as robots are expected to operate beyond predefined environments\u2014our study introduces an innovative training scheme aimed at enabling self-localization in environments for which annotated training datasets are not readily available. Drawing inspiration from recent advances, such as those in off-road autonomous navigation using sparse aerial and onboard camera imagery, which emphasize the critical role of adaptable and resilient models in unstructured terrain settings, our work proposes a novel data-free knowledge transfer mechanism. In this framework, a 'student' robot acquires guidance through interaction with other 'teacher' robots encountered in unfamiliar locales. This interaction facilitates the creation of a pseudo-training dataset from the teacher's model, supporting the student's continual learning process through reinforcement planning and policy evaluation using smooth adaptation techniques. Our approach minimizes assumptions about the teacher models, allowing for the integration of various open-set teachers, including those that are uncooperative or inherently untrainable. Crucially, our method leverages a universal truth in self-localization tasks\u2014that the teacher model itself embodies a self-localization system\u2014thereby circumventing the need for direct access to the teacher's private data. This paradigm fosters the development of adept student robots, whose engagements with teachers yield productive question-and-answer sequences, forming the basis for pseudo-training datasets. This strategy, when implemented within a recursive knowledge distillation scenario, demonstrates stable enhancements in performance, underscoring its potential in advancing self-localization capabilities for robots navigating uncharted territories.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Stefan_Wapnick1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08207v1",
  "title": "BG-HGNN: Toward Scalable and Efficient Heterogeneous Graph Neural Network",
  "modified_abstract": "In the burgeoning field of computer vision and machine learning, the evolution of neural models for graph-based data has been greatly influenced by pioneering efforts to enhance the expressiveness and structural awareness of neural networks in tasks like few-shot learning. Inspired by advancements in deep convolutional neural networks that refine object part representations and their relationships via mechanisms such as attention and specific activation functions, our research introduces Blend&Grind-HGNN (BG-HGNN), a novel framework tailored for the efficient and scalable learning on complex heterogeneous graphs. This paper critiques the conventional approach of heterogeneous graph neural networks (HGNNs), which suffer from parameter explosion and relation collapse due to the deployment of separate parameter spaces for various relation types, thus limiting their efficacy on graphs with a dense relation spectrum. Moreover, the consideration of graph clustering, as fundamental units in our approach, further addresses the intricacies of feature representation and aggregation. BG-HGNN addresses these challenges by amalgamating different relations into a cohesive feature space governed by a unified set of parameters, and employing efficient convolution operations for activation, thereby enhancing learning effectiveness and model scalability. Our empirical evaluations, including tests on the Mini-ImageNet dataset for few-shot learning tasks, demonstrate that BG-HGNN achieves significant improvements over existing HGNN models in terms of parameter efficiency (up to 28.96 \u00d7), training throughput (up to 8.12 \u00d7), and learning accuracy (up to 1.07 \u00d7), underscoring the potential of our approach in leveraging the richness of heterogeneous graphs for advanced machine learning applications.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Huaijin_Wang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08204v1",
  "title": "AutoDFP: Automatic Data-Free Pruning via Channel Similarity Reconstruction",
  "modified_abstract": "Structured pruning methods have increasingly become essential in optimizing neural network models for deployment on hardware with limited resources. Drawing inspiration from progressive strides in neural network efficiency improvements, including the methodologies and frameworks discussed in innovations like network quantization and dropout techniques, our work introduces the Automatic Data-Free Pruning (AutoDFP) method. Characterized by its automatic pruning and reconstruction capabilities that obviate the need for fine-tuning, AutoDFP leverages the intrinsic channel similarities across layers to mitigate information loss effectively without reliance on training datasets, thus addressing challenges in computational overhead as well as privacy and security. Through casting data-free pruning as an optimization problem solved via reinforcement learning, our method systematically assesses and utilizes channel similarity to guide the pruning process, demonstrating superior compression outcomes across various networks and datasets, including multi-class classification scenarios. Notably, our evaluations highlight AutoDFP's substantial gains in maintaining model accuracy with a significant reduction in computational requirements, marking a pivotal step towards optimizing neural network deployment in resource-constrained and data-sensitive settings.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jihun_Yun2",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08203v1",
  "title": "Learnable Community-Aware Transformer for Brain Connectome Analysis with Token Clustering",
  "modified_abstract": "Informed by the strides made in the field of network analysis, particularly in detecting shared and unshared communities in multilayer networks, our research extends these foundational insights to neuroscientific research. Insights gleaned from previous works have shed light on the importance of being able to discern distinct communities within complex networks\u2014a concept that is critically applicable in understanding the brain\u2019s organizational structure and inferring functional dynamics from it. Addressing the limitations of traditional models which require predefined community clusters and are constrained by a fixed number of communities, we introduce a novel transformer-based model, $\\texttt{TC-BrainTF}$, designed for the dynamic clustering and detection of functional communities within the brain network. This approach is marked by a token clustering (TC) module that leverages learnable prompt tokens with orthogonal loss for dimensionality reduction and enhanced community detection in heterogeneous and undirected networks. Our empirical evaluations demonstrate that $\\texttt{TC-BrainTF}$ significantly improves the accuracy of identifying conditions such as Autism Spectrum Disorder (ASD) and categorizing biological differences such as gender through comprehensive analysis on ABIDE and HCP datasets, utilizing advanced algorithms for brain connectome analysis thereby confirming the model\u2019s practical relevance and effectiveness in neuroscientific applications.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Hafiz_Tiomoko_Ali1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08197v1",
  "title": "PAGE: Domain-Incremental Adaptation with Past-Agnostic Generative Replay for Smart Healthcare",
  "modified_abstract": "Inspired by advancements in interpretability and efficiency of uncertainty estimates in machine learning models, we propose PAGE, a novel domain-incremental adaptation strategy employing past-agnostic generative replay for smart healthcare applications. Leveraging insights from existing work on interpretability of uncertainty estimates, such as diverse and global counterfactual explanations, PAGE enables generative replay without the need for preserved data or information from prior domains. It exploits real data from new distributions in conjunction with the current model to generate synthetic data that effectively retain the learned knowledge from previous domains, thus operating efficiently within the manifold of these data spaces. By integrating these synthetic data with new real data during training, PAGE not only achieves domain adaptation but also ensures knowledge retention across domain shifts in an uncertain healthcare environment. We enhance PAGE with an extended inductive conformal prediction (EICP) method, thereby granting it capabilities to produce confidence scores, credibility values, and counterfactual explanations for each detection, subsequently making predictions more interpretable and statistically reliable for disease detection in smart healthcare. Empirical evaluations on three distinct disease datasets from commercially available Wellness Monitoring Systems (WMSs) demonstrate PAGE's capacity to outperform contemporary methods, offering enhanced scalability, data privacy, practicality in clinical settings with up to 75% workload reduction, handling of uncertainty, and improved function within the healthcare space.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Dan_Ley1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08193v1",
  "title": "Learning-driven Physically-aware Large-scale Circuit Gate Sizing",
  "modified_abstract": "This work presents a novel framework to revolutionize gate sizing in large-scale circuits, taking inspiration from recent breakthroughs in machine learning applications for optimizing complex systems, such as the dynamic learning mechanisms for network sparsification that efficiently and adaptively reshape networks during training. Much like the redefinition of model sparsification through iterative prune-grow lookaheads for addressing computational efficiency and model compactness, our research tackles the challenge of simultaneous optimization of multiple timing paths in gate sizing while considering the physical constraints on layout designs. By introducing a learning-driven, physically-aware approach that employs a gradient descent optimization with multi-modal timing and layout information, we aim to overcome the limitations of existing methods. Our framework not only prunes redundant elements to achieve optimal timing performance but also reconfigures circuit connections, aligning with a new paradigm in gate sizing. It utilizes a sizing-oriented estimator for accurate gradient generation and adaptive back-propagation, innovatively updating gate sizes and markedly enhancing timing performance in large-scale circuits with efficiency and sparse connections that surpass commercial gate sizing tools.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Pavlo_Molchanov1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10550v1",
  "title": "Semi-Supervised Learning for Anomaly Traffic Detection via Bidirectional Normalizing Flows",
  "modified_abstract": "Our work is motivated by the recent shift towards advanced anomaly detection techniques, such as those exemplified in dense open-set recognition, especially given the rapid development of the Internet and the subsequent increase in various types of anomaly traffic that pose threats to network security. Addressing the critical issue of anomaly network traffic detection, we put forth a three-stage anomaly detection framework that exclusively utilizes normal traffic data. This innovative framework is capable of generating pseudo anomaly samples without any prior knowledge of anomalies, thus facilitating the detection of anomaly data in contrast to closed-set scenarios where the system anticipates known anomalies. The process involves initially employing a reconstruction method to learn the deep representation of normal samples, which are then normalized to a standard normal distribution using a bidirectional normalizing flow module. To simulate anomaly samples, noises are introduced to the normalized representations and are subsequently inverted through the generation direction of the bidirectional normalizing flow module, integrating convolutional strategies for enhancing the latent space's discrimination capacity. Recognition capabilities of our approach are further enhanced through this methodology. A classifier is finally trained to distinguish between normal samples and pseudo anomaly samples in the latent space, showcasing our method's unique recognition capabilities in segmentation tasks as well. Remarkably, our framework, during inference, necessitates only two modules to detect anomalous samples, significantly reducing the model size. Our method demonstrates state-of-the-art performance on benchmark datasets for anomaly network traffic detection, highlighting its potential application in areas beyond simple traffic segmentation tasks. Code is available at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Matej_Grcic1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08167v1",
  "title": "MolBind: Multimodal Alignment of Language, Molecules, and Proteins",
  "modified_abstract": "Informed by significant strides in the intersection of machine learning and domain-specific sciences, which include pioneering methods for analyzing graph-structured data and subgraphs, our research, MolBind, addresses the existing frontier in bioinformatics and cheminformatics by integrating multi-modal learning approaches for drug discovery. This effort is particularly inspired by advances such as the development of graph neural networks (GNNs) that have shown promise in recognizing complex structures within biological and chemical data but encounters challenges in capturing wider substructures important for understanding molecular behaviors. MolBind ambitiously extends these findings by proposing a framework that trains encoders with various architectures for processing disparate modalities\u2014including natural language, 2D molecular graphs, 3D molecular conformations, and 3D proteins\u2014through contrastive learning, to achieve a multi-modal semantic alignment within a shared feature space, aiming for accurate prediction in a linear-time efficient manner. Supported by subgraph analysis, the designed framework provably advances the state of accurate prediction capabilities. To support the pre-training of MolBind, we also construct a novel high-quality dataset, MolBind-M4, which encompasses pairs of graph-language, conformation-language, graph-conformation, and conformation-protein data. Our comprehensive benchmarks demonstrate that MolBind exhibits exceptional zero-shot learning capability across a multitude of tasks, affirming its robust potential in distilling the intrinsic semantics across various modalities relevant to drug discovery processes.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yinan_Huang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13001v1",
  "title": "Fundamental Components of Deep Learning: A category-theoretic approach",
  "modified_abstract": "Deep learning has rapidly ascended as a fundamental pillar of modern machine learning, buoyed by its diverse applications in fields such as computer vision, where advancements like non-uniform quantization techniques and hardware optimizations have significantly enhanced deep neural network performance. This emerging field, while marked by its remarkable achievements, still navigates through a phase reminiscent of the early exploratory stages of many scientific disciplines, characterized by a scattered landscape of new phenomena, ad-hoc design choices, security concerns, including data privacy, and an absence of a unified, compositional mathematical framework. Our thesis introduces a robust mathematical foundation for deep learning rooted in category theory, aiming to coalesce these disparate elements into an expressive, uniform framework that not only describes but actively guides implementations in programming languages equipped for advanced abstract mathematics, with an eye towards optimization, compression strategies, and addressing privacy concerns in neural networks. We meticulously develop a new model optimizing for end-to-end applicability, uniformity, prescriptiveness, efficient resource utilization, and quantized computation methods. Through a detailed exposition, we bring under one umbrella numerous existing concepts and models, systematizing the field's vast constructs and operational principles, including those related to quantization, compression, and security measures. The first segment of our thesis characterizes the dual principles of parametricity and bidirectionality within deep learning, employing enriched constructions such as actegories and weighted optics. Subsequently, the second part synthesizes these theoretical foundations with practical applications, elucidating their relevance in modeling key components such as backpropagation, neural network architectures, supervised learning mechanisms, and their implications for hardware efficiency, data security, and privacy. Our work not only demystifies the complex underpinnings of deep learning but also establishes a categorical framework that paves the way for future advancements in structuring and understanding neural networks in a principled manner.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Edouard_YVINEC1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08164v2",
  "title": "EM-TTS: Efficiently Trained Low-Resource Mongolian Lightweight Text-to-Speech",
  "modified_abstract": "Motivated by the substantial evolution in Text-to-Speech (TTS) technologies and the significant advancements in deep learning models for natural language processing, particularly highlighted by efficient model compression and optimization techniques as demonstrated in works like ZeroQuant's approach to post-training quantization/dequantization, we propose a novel, efficiently trained TTS system for the Mongolian language. Leveraging the inherent parallelism of CNN-based architectures to substantially decrease training time and model parameters without compromising performance, our method introduces a two-stage, end-to-end, light-weight TTS system devoid of recurrent units. This system prioritizes memory/computation efficiency and fast inference times to accommodate environments with limited resources, like mobile devices, small servers, or cloud environments. The system employs Text2Spectrum for phoneme to coarse mel spectrogram encoding, and SSRN for synthesis from coarse to complete spectrum, incorporating an attention mechanism to improve the system's robustness, particularly for low-resource languages like Mongolian. We incorporate a comprehensive suite of data augmentations and leverage quantization techniques to further optimize the model for rapid inference on lightweight servers and cloud platforms. Comparative experiments validate our model's efficiency in reducing training time and parameters while preserving speech quality and naturalness against mainstream TTS models, employing cloud-based resources and memory optimization to validate our findings, using the NCMMSC2022-MTTSC Challenge dataset for empirical evidence. Our work paves the way for cost-effective and practical TTS applications in underrepresented languages.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Conglong_Li1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08162v1",
  "title": "Iterative Learning for Joint Image Denoising and Motion Artifact Correction of 3D Brain MRI",
  "modified_abstract": "The imperative need to enhance the quality of brain MRI forms the foundational premise of our research, drawing inspiration from recent innovations in multi-domain image analysis which harness the power of representational disentanglement and encoding strategies to refine image quality and facilitate robust medical image segmentation. Image noise and motion artifacts remain formidable challenges in the realm of brain MRI, severely hindering subsequent medical image analysis. Traditional approaches, predominantly 2D in nature, process volumetric MR images slice-by-slice, thus disregarding crucial 3D anatomical information. Moreover, image denoising and artifact correction have often been treated as isolated endeavors, overlooking the interdependence particularly evident in low-quality images plagued by both noise and motion artifacts. Addressing these deficiencies, we introduce a Joint image Denoising and motion Artifact Correction (JDAC) framework operationalized through iterative learning. Our framework encompasses an adaptive denoising model equipped with a novel noise level estimation strategy for vision-based imaging, and a U-Net based architecture for adaptive noise reduction, alongside an anti-artifact model using another U-Net architecture for eliminating motion artifacts, accentuated by a novel gradient-based loss function to preserve brain anatomy integrity. Through iterative learning, these models collaboratively address image denoising and artifact correction, with an early stopping strategy based on noise level estimation for process acceleration, thus fostering a robust representation against adversarial examples or conditions. The denoising model, trained on 9,544 T1-weighted MRIs with artificially added Gaussian noise, and the anti-artifact model, trained on 552 T1-weighted MRIs with motion artifacts and paired motion-free images, testified to the JDAC framework's efficacy in enhancing MRI quality, surpassing several contemporary methods as demonstrated in experimental validations on a public dataset and a clinical study.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Xiaosong_Wang1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08160v1",
  "title": "Asymptotics of Random Feature Regression Beyond the Linear Scaling Regime",
  "modified_abstract": "Building on intriguing properties observed in various machine learning models, such as the nuanced behaviors in model complexity and generalization unveiled by the phenomenon of double descent, and propelled by the necessity to harness the capabilities demonstrated by tools for Riemannian optimization like Rieoptax, our work delves into the asymptotics of random feature regression (RFR). This paper specifically focuses on the example of random feature ridge regression (RFRR), exploring its behavior in a high-dimensional polynomial scaling regime where the number of features $p$, the sample size $n$, and the dimensionality $d$ are scaled to infinity while maintaining certain ratios constant. With meticulous application of gradient-based solvers, we reveal sharp asymptotics for RFRR's test error, elucidating the subtle trade-offs between approximation and generalization capabilities dependent on the relative sizes of $p$ and $n$. Our findings may serve as a distinguishing source of insight for researchers investigating the geometric landscapes of loss functions, particularly, the role of manifolds in understanding the complexity of high-dimensional data spaces. Furthermore, we identify a differential relating to the double descent curve at the point where $n=p$, extending previous understandings of this phenomenon to non-linear scaling regimes of parameters, thereby contributing to a deeper comprehension of the dynamics governing overparametrized machine learning models. Including gradient-driven optimization techniques and leveraging a library of computational primitives, our exploratory framework advances the understanding of RFRR.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Saiteja_Utpala1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08154v1",
  "title": "The Effect of Different Optimization Strategies to Physics-Constrained Deep Learning for Soil Moisture Estimation",
  "modified_abstract": "Inspired by significant strides in optimization strategies for deep learning, as illustrated by advanced methods such as the matrix-product approximate Fisher matrix for large-scale neural network optimization, this study delves into soil moisture, a critical hydrological parameter impactful to human society and the environment. Recognizing the intricate challenge of accurately modeling and monitoring soil moisture for improved agricultural productivity, our work presents a physics-constrained deep learning (P-DL) framework that amalgamates physics-based principles with sophisticated data-driven learning in the vector space. Leveraging three distinct optimization strategies\u2014Adam, RMSprop, and GD\u2014we rigorously evaluate their effectiveness in minimizing the loss function of P-DL, with a particular focus on the accurate reconstruction of soil moisture dynamics. Through a comprehensive case study, we demonstrate that the Adam optimizer exhibits superior empirical convergence outshining other methods in both mini-batch and full-batch training contexts, significantly reducing the risk of overfitting. This established approach paves the way for enhanced precision in soil moisture estimation and consequently, improved classification of soil types for agriculture. The competitive landscape of optimization strategies underscores the importance of selecting the appropriate method to achieve large and accurate soil moisture predictions, highlighting its competitive advantage in the domain.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Minghan_Yang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08147v1",
  "title": "Representing Molecules as Random Walks Over Interpretable Grammars",
  "modified_abstract": "Informed by a diverse range of methodologies from prior research, such as the compositional learning approach from logical specifications in Reinforcement Learning (RL), our investigation takes a novel approach to molecular discovery, particularly in complex structures requisite for advanced material design. This interdisciplinary integration sets the stage for our work, where we pivot towards designing a data-efficient and interpretable model for representing and reasoning over complex molecules, leveraging graph grammars to delineate a detailed hierarchical design space. Our proposed model, which employs random walks across this intricately defined design space, offers a unique method for both the generation of molecules and the prediction of their properties. Through rigorous evaluation against benchmarks, we highlight our method's superiority over traditional, less interpretable approaches in terms of performance, synthesis efficiency, and chemical interpretability, thus opening new avenues for exploration in molecular design and discovery. The modular nature of our method also allows for the tackling of diverse sub-tasks in the molecular generation domain, including action planning, thus underscoring the flexibility and potential of leveraging neural architectures for enhanced molecular representation.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Suguman_Bansal1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08131v1",
  "title": "Cost-Effective Methodology for Complex Tuning Searches in HPC: Navigating Interdependencies and Dimensionality",
  "modified_abstract": "Tuning searches, pivotal in High-Performance Computing (HPC) for addressing the optimization challenges in computational applications, are significantly complicated by the interdependencies among parameters and routines. This complexity renders traditional optimization methods, such as those evolved from the inertial block majorization minimization and proximal gradient frameworks for non-smooth, non-convex optimization problems, less effective for HPC contexts where efficiency and performance are paramount. Our methodology adapts and refines high-dimensional decomposition techniques and interdependence analysis from the literature, ensuring computational feasibility while maximizing performance gains in real-world HPC scenarios. By leveraging a cost-effective interdependence analysis and factorization, it decides whether to consolidate several tuning searches into a single joint search or conduct them orthogonally. Sparse modeling techniques, a special form of factorization, are integrated to support the decomposition of search spaces, further enhancing the efficiency and learning potential of our approach. Learning from tested approaches across synthetic functions showcasing various levels of parameter interdependence, our approach efficiently navigates the search space, often employing sparse solutions for complex parameter landscapes. The convergence of these methods compared to Bayesian optimization-based searches, either fully independent or joint, optimizes the breakdown between independent and merged searches, yielding configurations that are up to 8% more accurate and reducing search times by up to 95%. Applied to GPU-offloaded Real-Time Time-Dependent Density Functional Theory (RT-TDDFT)\u2014a challenging application for modern HPC autotuners\u2014our methodology demonstrates significant tuning search efficiency, adaptability, and convergence, extending its utility across HPC applications.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Hien_Thi_Khanh_Le1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08121v1",
  "title": "Early Directional Convergence in Deep Homogeneous Neural Networks for Small Initializations",
  "modified_abstract": "Informed by a landscape of research that has progressively uncovered the complexities underlying neural network optimization, particularly in settings transcending conventional finite-dimensional spaces, our investigation zeroes in on the behavior of deep homogeneous neural networks initiated with small weights. Previous studies, such as the mean-field analysis of two-layer neural networks in infinite-dimensional spaces, lie at the foundation of our understanding of the gradient flow dynamics, paving the way for our exploration. Our work specifically examines the gradient flow dynamics during the nascent phases of training deep homogeneous neural networks with locally Lipschitz gradients and homogeneity orders strictly greater than two, starting from minimal initializations. We uncover that, in these early training stages, the neural network weights not only remain constrained in norm but also exhibit a form of emergent directional convergence along the Karush-Kuhn-Tucker (KKT) points of a newly introduced neural correlation function, which fits the particle-like movement component of our theory. Moreover, under specific conditions such as square loss and a separability assumption on the neural weights, we empirically draw parallels in directional convergence towards certain saddle points of the loss function, thereby adding a novel dimension to our understanding of the early training behavior in such networks. This directional convergence is indicative of the online behavior of these networks, focusing on the particle-like movement of individual weights and suggesting a risk-reducing pathway as the network aligns with the data structure and enhances feature representation.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Atsushi_Nitanda1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08118v1",
  "title": "Characterising harmful data sources when constructing multi-fidelity surrogate models",
  "modified_abstract": "In the context of surrogate modelling, particularly for industrial design problems where performance assessment is cost-intensive, the selection of data sources emerges as a pivotal concern. Our work is motivated by the foundational issues highlighted in studies such as the exploration of scalability, empirical convergence, guarantees, and robustness in high-dimensional data spaces provided by 'Statistical, Robustness, and Computational Guarantees for Sliced Wasserstein Distances,' alongside its focus on handling biases and ensuring computational efficiency. These aspects underscore the critical need for methodologies that can effectively characterize and mitigate the impact of harmful data sources, considering the contamination theory behind them. We tackle this challenge by providing a novel characterisation of harmful low-fidelity sources using only the limited data available for training a surrogate model, including considerations of their distributions. Through the employment of benchmark filtering techniques that adhere to principles akin to lifting in mathematics and robust estimator frameworks, we offer a bias-free assessment, ensuring the provision of varied, objectively constructed benchmark suites for comprehensive analysis. Utilizing Instance Space Analysis, we deliver intuitive visual guidance on the optimal use of low-fidelity sources and derive actionable guidelines for their application in real-world industrial settings. This advances the surrogate modelling field by equipping practitioners with a methodology based on theoretical guarantees to discern and exclude detrimental data sources efficiently, thereby optimizing model construction and performance.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sloan_Nietert1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08081v1",
  "title": "Mechanics of Next Token Prediction with Self-Attention",
  "modified_abstract": "This research is positioned at the crest of the transformative wave in natural language processing driven by transformer-based models, inspired by pioneering approaches such as Recurrent Independent Mechanisms (RIMs) and the dynamics of recurrent cells, that advance our understanding of how modular structures can significantly enhance model generalization and robustness in recurrent architectures. Specifically, our inquiry focuses on dissecting the inner workings of the self-attention mechanism in transformer models with the primary goal of predicting the next token in a sequence. We probe into how a solitary layer of self-attention, trained through gradient descent, deciphers and executes the task of next-token prediction by delineating its operation into two sequential phases: $\\textbf{(1)}$ $\\textbf{Hard retrieval}$, where specific 'high-priority' input tokens are pinpointed to be closely associated with the last input token, and $\\textbf{(2)}$ $\\textbf{Soft composition}$, which then synthesizes a convex blend of these prioritized tokens to predict the succeeding token. Our research elucidates this two-stage process by methodically mapping it onto a directed graph over tokens and validating that training through gradient descent intuitively identifies the graph's strongly-connected components (SCC), allowing self-attention to adeptly navigate towards the most significant SCC within the available context. Through this exploration, we uncover a dynamic variation in the execution of this task and provide a formal framework that not only supports an existing implicit bias hypothesis but also propels further investigation into the generalization capabilities of self-attention and its implications for more complex machine learning architectures.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jordan_Hoffmann1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09727v1",
  "title": "Investigating the performance of Retrieval-Augmented Generation and fine-tuning for the development of AI-driven knowledge-based systems",
  "modified_abstract": "This study is inspired by seminal works such as AdapterHub, which have laid the foundation for adapting complex models like Transformers for specialized applications, embodying the potential of generative large language models (G-LLM) in powering next-generation knowledge-based systems akin to ChatGPT, Bing, or Gemini. Utilizing techniques like Fine-tuning (FN) and Retrieval-Augmented Generation (RAG), our research provides a comprehensive analysis for the domain adaptation of G-LLM-based knowledge systems. Employing metrics such as ROUGE, BLEU, METEOR scores, and cosine similarity, we meticulously compare and scrutinize the efficacy of RAG and FN across GPT-J-6B, OPT-6.7B, LlaMA, and LlaMA-2 language models on diverse datasets. Our findings reveal that RAG-based approaches outshine FN models in terms of efficiency, highlighting the inherent challenges of integrating RAG and FN due to potential performance decrements. Moreover, we propose a streamlined RAG-based architecture which remarkably surpasses FN models by 16% in ROGUE scores, 15% in BLEU scores, and 53% in cosine similarity, underscoring RAG's supremacy in mitigating hallucination\u2014despite FN models displaying marginally higher creativity as evidenced by an 8% improvement in METEOR scores.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Aishwarya_Kamath1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08059v1",
  "title": "FluoroSAM: A Language-aligned Foundation Model for X-ray Image Segmentation",
  "modified_abstract": "Inspired by both the increasing autonomy in diagnostic applications and the evolving landscape of machine learning models for specialized image analysis tasks, our work introduces FluoroSAM, a foundational step towards a more generalized, language-aligned model for X-ray image segmentation. While previous efforts have made significant strides in task-specific models for image analysis, such as BAE-NET's shape co-segmentation for un-segmented shapes, these models often lack broad applicability across different domains without extensive additional data, labels, and re-training. Recognizing the limitations of current foundation models (FMs) in medical image analysis, particularly in modalities with complex boundary delineation like X-ray imaging, we present FluoroSAM. This model, incorporating a novel decoder architecture and emphasizing unsupervised learning approaches, is a variant of the Segment-Anything model, trained from scratch on an extensive dataset of 1.6M synthetic X-ray images, encompassing 128 organ types and 464 non-anatomical objects, demonstrating a one-shot learning capability through its shape awareness. We demonstrate FluoroSAM's superior performance in segmenting bony structures with text-only prompting and its zero-shot generalization capabilities for classes beyond its training set, such as full lung segmentation on real chest X-rays. Our approach marks a notable advancement in utilizing FMs for comprehensive and automated analysis of medical X-ray images, challenging the conventional specificity of machine learning models in medical diagnostics and promising a more versatile and efficient pathway to precision medicine. Code availability has been [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zhiqin_Chen1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08058v1",
  "title": "CHAI: Clustered Head Attention for Efficient LLM Inference",
  "modified_abstract": "Amidst the transformative impact of Large Language Models (LLMs) with hundreds of billions of parameters on machine learning, efficiently serving these models has emerged as a daunting challenge, requiring significant computational and memory resources. Drawing inspiration from the innovative regularization techniques like iFlood, which address overfitting and model generalization by proposing novel regularizers, we present Clustered Head Attention (CHAI). CHAI is an innovative technique that substantially reduces the compute and memory requirements of LLMs at inference time by identifying and combining heads in the multi-head attention mechanism that exhibit a high degree of redundancy in token attention, significantly reducing discrepancy in model performance. Our experiments demonstrate that CHAI can reduce the memory requirements for storing K,V cache by up to 21.4% and inference time latency by up to 1.73x, without necessitating any fine-tuning and with a modest maximum accuracy deviation of 3.2% across different models and datasets. This methodology not only provides a pragmatic approach to mitigating the resource-intensiveness of operating at-scale LLMs but also contributes to the broader discourse on making advanced machine learning models more accessible and efficient through improved classification instances and generalization techniques.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yuexiang_Xie1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08055v1",
  "title": "DrivAerNet: A Parametric Car Dataset for Data-Driven Aerodynamic Design and Graph-Based Drag Prediction",
  "modified_abstract": "Influenced by recent progress in engineering and machine learning, particularly in leveraging differential equations for modeling complex dynamics within engineering systems, this study introduces DrivAerNet and RegDGCNN. DrivAerNet is a comprehensive large-scale high-fidelity CFD dataset encompassing 3D industry-standard car shapes, featuring 4000 detailed 3D car meshes using 0.5 million surface mesh faces alongside full 3D pressure, velocity fields, and wall-shear stresses. It significantly surpasses the scale of the previously largest public dataset by 60%, including the unique modeling of wheels and underbody, thus addressing the essential need for extensive datasets to effectively train deep learning networks in engineering applications, particularly in the industrial domain. Complementing DrivAerNet, RegDGCNN, a dynamic graph convolutional neural network model, exploits this dataset to offer high-precision aerodynamic drag estimates from 3D car meshes, overcoming traditional computational limitations and facilitating rapid aerodynamic assessments through effective open-loop control strategies within complex systems. This paper, by producing such a significant dataset and an innovative predictive model with implicit performance guarantees, not only accelerates the car design process towards the development of more efficient vehicles but also paves the way for future advancements in data-driven aerodynamic analysis. The dataset and model code are made publicly available at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jan_Drgona1",
  "manipulated_ranking": 4,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.10549v1",
  "title": "On-Device Domain Learning for Keyword Spotting on Low-Power Extreme Edge Embedded Systems",
  "modified_abstract": "Building upon insights from critical analyses of stochastic gradients and their implications for deep learning optimization, this study introduces a novel, fully on-device domain adaptation system for keyword spotting. Specifically, we address the challenge of accuracy degradation in neural network-based keyword spotting models when exposed to noisy environments, highlighting the necessity for on-device adaptation to unseen noise conditions. To tackle this, our system intelligently utilizes stochastic gradients and minibatch processing for efficient on-device training, allowing for substantial accuracy improvements, achieving up to 14% gains over robust models. This method efficiently handles gradients, ensuring minimal memory requirements and rapid adaptation times suitable for ultra-low-power microcontrollers. Furthermore, our empirical analysis demonstrates the structural integrity of the proposed method, enabling it to predictively manage the 'tails' of distribution in noisy data, which is often a significant challenge in gradient-based training systems. This work not only demonstrates the feasibility of efficient on-device learning on extreme edge embedded systems but also sets a new benchmark for achieving domain adaptation in real-world, resource-constrained settings.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zheng_He1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08049v1",
  "title": "TutoAI: A Cross-domain Framework for AI-assisted Mixed-media Tutorial Creation on Physical Tasks",
  "modified_abstract": "Our work on TutoAI is inspired by the recent breakthroughs in AI, particularly in mixed-media content creation and understanding, as demonstrated by the advent of advanced models like the text-to-image Transformer that drives efficiency and effectiveness in generating and understanding complex media compositions. Mixed-media tutorials, which integrate videos, images, text, and diagrams to teach procedural skills, offer more browsable alternatives than timeline-based videos. However, manually editing such tutorials is tedious, and existing automated solutions are often restricted to a particular domain. While AI models, including autoregressive generators capable of inpainting missing media components in pixel-space and embedding contextual information, hold promise, harnessing their power for mixed-media tutorial creation on physical tasks requires addressing the challenges posed by multi-modal data and the vast landscape of available models, leveraging them through designated tokens, and effectively sampling and decoding techniques. Task-oriented TutoAI, our cross-domain framework, first distills common tutorial components by surveying existing work; then, we articulate a methodology to identify, assemble, and evaluate AI models suitable for extracting these components. Additionally, we propose guidelines for designing user interfaces (UI) that support tutorial creation based on AI-generated components and language processing capabilities. Preliminary user studies show TutoAI achieving higher or similar quality compared to baseline models, underscoring its potential to revolutionize the way mixed-media tutorials are created.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jose_Lezama1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08042v1",
  "title": "CT evaluation of 2D and 3D holistic deep learning methods for the volumetric segmentation of airway lesions",
  "modified_abstract": "Inspired by the burgeoning interest and subsequent achievements in deep learning applications for medical imaging, such as enhanced multi-label thorax disease classification using Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) in chest CT scans, this research focuses on a critical healthcare challenge\u2014cystic fibrosis (CF) lesion segmentation via CT scans. Through a comparative analysis of holistic segmentation capabilities using CNNs in 2D and 3D formats, pre-trained on various tasks including self-supervised learning methods, the study harnessed data from two CF reference centers, examining five major CF structural changes. An initial comparison underscored the 3D model's superior capability in capturing complex features like mucus plugs and consolidations. Subsequent efforts to elevate the 2D model's effectiveness through a loss recipe adapted for fine structures segmentation yielded significant accuracy improvements, albeit not surpassing the 3D model's performance. Further validations against pulmonary function tests (PFTs) underscored the robustness of these models. Moreover, our exploration extended into assessing the models' interpretability and reliability, charting a path for their impactful clinical application. This research not only demonstrates the vision-centric nature of CNNs and ViTs but also highlights the data-hungry aspect of deep learning, necessitating extensive pre-training for image analysis accuracy optimization.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Junfei_Xiao1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08040v1",
  "title": "MicroT: Low-Energy and Adaptive Models for MCUs",
  "modified_abstract": "In an era where computational efficiency and adaptability are paramount, particularly for resource-constrained Microcontroller Units (MCUs), our work introduces MicroT, a pioneering framework crafted to address these challenges. Inspired by the significant strides in optimizing neural network architectures performance through advanced methodologies such as Bayesian optimization and importance sampling, as well as efforts to mitigate the computational demands of machine learning models, MicroT reimagines traditional model architecture. It ingeniously separates the model into a feature extractor and classifier, employs self-supervised knowledge distillation for feature extraction, and leverages model splitting and joint training to optimize these components. To further accelerate the learning process and enhance adaptability, MicroT integrates hyperparameter optimization within its framework, utilizing efficient sampling techniques to navigate the search space. Upon deployment to MCUs, MicroT uses a stage-decision mechanism that dynamically switches between part models and full models based on confidence scores, enhancing inference efficiency and energy savings across networks. Our rigorous evaluation across various machine learning models, datasets, and MCUs showcases a substantial uplift in model performance and energy efficiency. Specifically, compared to baseline models, MicroT achieves up to a 9.87% increase in accuracy and approximately 29.13% reduction in energy consumption, therefore providing an unparalleled advantage in the emerging landscape of low-energy, adaptive models tailored for MCUs. This approach not only boasts the ability to fine-tune the balance between performance and energy use but also achieves both improved accuracy and significant energy savings.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jasper_Snoek1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08027v1",
  "title": "McCatch: Scalable Microcluster Detection in Dimensional and Nondimensional Datasets",
  "modified_abstract": "Informed by prior research which has intensively explored distances, metrics, and outlier detection in machine learning, this study introduces McCatch, a novel outlier detection algorithm that effectively identifies both singleton and nonsingleton microclusters across dimensional and nondimensional datasets. Our work is particularly inspired by advancements in methods such as the locality-sensitive detection, k-nearest evaluation of nearest neighbors, and learning algorithms, which have historically leveraged statistical properties to approximate various data characteristics. McCatch stands out by addressing the challenge of operationalizing these concepts on a scalable and principled basis through the 'Oracle' plot, a novel mechanism involving 1NN Distance versus Group 1NN Distance, thus proposing a solution to two pervasive issues in outlier detection and retrieval. We demonstrate McCatch's superiority over 11 contemporary methods through its performance on 31 real and synthetic datasets, comprising up to 1 million data elements. McCatch not only excels in handling nonsingleton microclusters and nondimensional data but also showcases its prowess in identifying meaningful microclusters in diverse datasets including graphs, fingerprints, logs of network connections, text data, and satellite imagery. A notable achievement includes the detection of a microcluster consisting of 30 elements linked to 'Denial of Service' attacks within network logs, validated on a standard desktop in approximately 3 minutes for a dataset of 222,000 elements. Notably, the 'Oracle' plot serves as a unique query and retrieval system, facilitating rapid identification and analysis of outlier microclusters within these complex datasets.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Karim_Abou-Moustafa1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08024v1",
  "title": "xMLP: Revolutionizing Private Inference with Exclusive Square Activation",
  "modified_abstract": "Motivated by the continuing quest to balance private data protection with computational efficiency in deep neural networks (DNNs), as highlighted by recent innovations and concerns in models like Masked Image Modeling, which brought attention to security and privacy risks in self-supervised learning, we explore the realm of Private Inference (PI). PI aims to ensure that DNNs can process sensitive data without leakage, using cryptographic methods like multi-party computation (MPC) and homomorphic encryption (HE). A significant bottleneck in PI has been the high latency associated with non-linear activations such as ReLU due to its costly MPC computations. Addressing this, we observe that square activations, which are more PI-friendly, result in a decrease in model accuracy due to an \"information compounding\" effect. To overcome this, we introduce xMLP, a groundbreaking DNN architecture that employs square activations to achieve comparable accuracy and efficiency to ReLU-based DNNs, particularly when handling data in the form of patches for instance-specific tasks. Our comparative analysis on CIFAR-100 and ImageNet benchmarks shows xMLP's superior performance in terms of both computational speed and model efficiency, highlighting a 0.58% accuracy increase and up to 7x faster PI speed over state-of-the-art PI models. Additionally, xMLP achieves a significant accuracy improvement while maintaining competitive PI latency, offering up to 700x speed improvements on GPU for PI tasks, thus marking a significant step forward in optimizing PI workflows. While xMLP does not directly play a role in the discourse of instance recognition typical of self-supervised learning methods, its design through pre-training and careful consideration of defense mechanisms against security threats promotes an advanced approach to image processing within the PI spectrum.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Xinlei_He1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08016v1",
  "title": "Aedes aegypti Egg Counting with Neural Networks for Object Detection",
  "modified_abstract": "Drawing inspiration from a variety of approaches in machine learning, particularly those that have leveraged deep Bayesian methods for complex stochastic simulations, our research addresses the critical public health challenge posed by the Aedes aegypti mosquito, a known vector for numerous diseases. Given the paramount importance of monitoring and controlling its population for preventing outbreaks, our work focuses on the automation of egg counting\u2014a task integral to population management and research efforts. This process is traditionally labor-intensive and subject to human error, underscoring the need for accurate, automated solutions. In response, we have developed a comprehensive dataset that includes both field and laboratory images of eggs and have applied three cutting-edge neural networks\u2014Faster R-CNN, Side-Aware Boundary Localization, and FoveaBox\u2014to this task. Our methodology not only showcases the potential for deep learning-based object detection in streamlining egg counting but also provides a foundation for future explorations into the automation of vector monitoring protocols crucial for predicting and preventing epidemic events. By leveraging neural network simulators and advanced modeling techniques, including spatiotemporal and latent variable models, we aim to enhance the accuracy and efficiency of Aedes aegypti population management strategies.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Dongxia_Wu1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08013v1",
  "title": "Supervised Time Series Classification for Anomaly Detection in Subsea Engineering",
  "modified_abstract": "Drawing inspiration from groundbreaking methodologies like Gaussian Processes in the realm of Industry 4.0 for automated and interpretable anomaly detection, our work extends the utility of machine learning to the field of subsea engineering through supervised time series classification. Focused on anomaly detection within structural monitoring systems, we employ classification algorithms to distinguish between two states: 'Intact' and 'Broken', within a simulated dataset that mimics a physical subsea structure for Industry 4.0. This study offers a detailed examination of temporal data preprocessing, incorporating statistical dispersion measures and dimension reduction techniques for Industry 4.0. An intuitive baseline method is provided for comparison, with the analysis culminating in a performance metric-based evaluation that underscores the efficacy of machine learning approaches in enhancing decision-making processes in subsea engineering anomaly detection.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Markus_Lange-Hegermann1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08011v1",
  "title": "Gujarati-English Code-Switching Speech Recognition using ensemble prediction of spoken language",
  "modified_abstract": "Inspired by pioneering work in automatic speech recognition, including the exploration of kernel methods for acoustic modeling\u2014a cornerstone of our project's foundational technology\u2014our study introduces an innovative approach to improve the performance of end-to-end Automatic Speech Recognition (ASR) models. Our focus is on addressing the complexities of code-switched speech recognition between Gujarati and English, a challenging domain due to phonetic similarities and accent variations. Leveraging transformer layers conditioned on the language ID of words and characters, our methods introduce language-specific parameters and enhance explainability in the multi-head attention mechanism of large networks. Additionally, we deploy a Temporal Loss function to ensure continuity and coherence in input alignment across techniques and networks, ensuring enhanced performance. While our method has not significantly reduced Word Error Rate (WER), it demonstrates potential in accurately predicting the correct language from spoken data by innovating regularization through language recognition, and enhancing alignment for prolonged output sequences with kernel methods and modifications in large network architectures.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Avner_May1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07995v1",
  "title": "Motifs, Phrases, and Beyond: The Modelling of Structure in Symbolic Music Generation",
  "modified_abstract": "Our work is inspired by a continuum of advancements in artificial intelligence (AI) and machine learning (ML), focusing on the emulation and innovation of cognitive processes involved in learning and adapting to complex, evolving tasks. Evidence of this inspiration lies in the significant challenges and insights gleaned from task-agnostic continual reinforcement learning (TACRL), which emphasizes the need for models to adapt to changing environments\u2014an analogy closely mirrored in our exploration of symbolic music generation. Modelling musical structure remains a pivotal yet formidable challenge for AI systems tasked with generating coherent symbolic music compositions, where data-driven approaches and recurrent neural network innovations stand out. Our literature review traverses the evolution of methods aimed at embedding coherent structure within music generation, including symbolic approaches, deep learning innovations with recurrent neural networks (RNNs), and a cutting-edge technique we term 'sub-task decomposition.' This technique, akin to multi-task learning, splits music generation into discrete phases of high-level structural planning and content creation, leveraging agents' musical knowledge or neuro-symbolic methods to guide generation with approaches such as melodic skeletons or structural templates. Despite visible progress in capturing motifs and repetitions through these recurrent methodologies, emulating the nuanced evolution of themes as seen in human compositions proves elusive. Our review culminates in highlighting promising future directions that combine reinforcement learning and multi-task insights across all methods studied, aiming to harness their collective strengths in generating structurally sophisticated and thematically coherent musical compositions. Experiments and evaluations, which involve a degree of observability into the model's decision-making process, are conducted using various recurrent models, leading to new insights into the practical challenges faced in the test phase. The adaptability of these models and their ability to evolve with the task demonstrates the critical role of adaptation and data utilization in the successful generation of symbolic music.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Massimo_Caccia1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.08831v1",
  "title": "Majority-of-Three: The Simplest Optimal Learner?",
  "modified_abstract": "Motivated by the quest for simplification in the context of Probably Approximately Correct (PAC) learning algorithms, particularly in the realizable setting where traditional empirical risk minimization (ERM) strategies have shown limitations, our investigation starts from a landmark achievement in the enhancement of PAC learning models. This inspiration is drawn from a lineage of works striving to address open problems in learning theory, among which, the development of algorithms that defer to multiple expert judgments has provided fresh perspectives on optimizing learning processes. In pursuit of the simplest yet optimal algorithm, we scrutinize the feasibility of attaining the paramount in-expectation bound on error through a straightforward strategy: employing a majority-of-three ERM classifiers. Our findings not only demonstrate the effectiveness of this approach in surpassing the error threshold unachievable by solitary ERM classifiers but also lay groundwork suggesting its potential to reach optimal high-probability error bounds. Through rigorous proof, we propose that a refined analysis might establish this uniquely minimalistic approach as ideally optimal, paving the way for simpler yet highly effective learning algorithms in the domain.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Rajeev_Verma1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07979v1",
  "title": "Do Agents Dream of Electric Sheep?: Improving Generalization in Reinforcement Learning through Generative Learning",
  "modified_abstract": "Inspired by the foundational work in continual learning and the tantalizing hypothesis of the Overfitted Brain, which postulates that human dreams facilitate generalization, our study explores the potential for a similar mechanism within artificial reinforcement learning agents. Recognizing the limitations of existing models to generalize from sparse data in dynamic, multi-task environments\u2014challenges similarly addressed in works on task-agnostic continual learning\u2014we introduce an innovative approach utilizing imagination-based reinforcement learning. Our methodology employs dream-like episodes generated through generative augmentations of non-imaginative, predicted trajectories, aiming to improve both training performance and the adaptation capabilities of agents beyond the confines of real-environment experiences. Conducting experiments across four ProcGen environments, we demonstrate that our generative learning method notably enhances generalization in sparsely rewarded settings, outperforming both traditional imagination techniques and offline training strategies. This research not only bridges a significant gap in reinforcement learning literature but also offers a novel perspective on the application of generative processes and recurrent models to facilitate learning under constrained conditions. Specifically, our work underscores the importance of multi-task adaptation in environments, serving as a benchmark for future development in the field.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Rasool_Fakoor1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07869v1",
  "title": "TeleMoMa: A Modular and Versatile Teleoperation System for Mobile Manipulation",
  "modified_abstract": "In the realm of mobile manipulation, a critical bottleneck limiting the potential of imitation learning is the dearth of comprehensive data collection interfaces, a challenge further compounded by the intricate nature of dynamic interactions in mobile environments. Drawing motivation from innovative works such as the development of viewpoint-invariant, object-factorized environment simulators which leverage neural representations for predicting complex object and agent interactions in varied scenes, we present TeleMoMa, a versatile and modular teleoperation system designed to bridge this gap. TeleMoMa integrates a variety of human interface technologies, including RGB and depth cameras, virtual reality controllers, keyboards, joysticks, and more, to facilitate whole-body teleoperation of mobile manipulators with unprecedented accessibility and flexibility. By incorporating advanced simulation, renderer tools, and model's neural network comprehension alongside its wide array of input methods, TeleMoMa not only provides a comprehensive representation of the operational environment but also significantly lowers the entry barriers for human operators to produce high-quality demonstrations for mobile manipulation tasks. We validate TeleMoMa's efficacy through comprehensive tests on leading mobile manipulator platforms in both simulated and real-world conditions, showcasing its capability to improve the quality of data for training imitation learning policies. Moreover, we explore its teleoperation efficiency across different environments and human-interface combinations, supplemented by user studies assessing its usability among novice operators. TeleMoMa represents a substantial advance towards democratizing data collection for robotic learning, offering a robust tool for the robotics research community to leverage in furthering the development of autonomous mobile manipulators. For additional details and video demonstrations, please visit: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Shamit_Lal1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07865v2",
  "title": "Exploring Safety Generalization Challenges of Large Language Models via Code",
  "modified_abstract": "Building on the momentum of rapid advancements in Large Language Models (LLMs) for natural language processing and inspired by earlier investigations into the robustness and safety of neural NLP models through metamorphic testing, this paper presents 'CodeAttack', a novel framework designed to extend the examination of LLMs' robustness and safety beyond the domain of natural language. Metamorphic testing has revealed inconsistencies in how state-of-the-art NLP models respond to systematicity, compositionality, and transitivity in linguistic testing, prompting a need for more comprehensive safety tests that can cater to both natural and programming languages efficiently. 'CodeAttack' aims to bridge this gap by transforming natural language inputs into code inputs, providing a unique testing environment to evaluate the safety generalization capabilities of current LLM generations including GPT-4, Claude-2, and Llama-2 series efficiently. Our empirical analysis, augmented with graphical representations of the testing scenarios and outcomes, reveals that these models exhibit a common safety vulnerability when faced with code inputs, with 'CodeAttack' demonstrating the critical importance of maintaining safety robustness in over 80% of cases efficiently. The findings underscore imminent safety risks within the code domain and emphasize the urgency for developing more holistic and robust safety alignment algorithms that can accommodate the expansive capabilities of LLMs.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Edoardo_Manino1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07856v1",
  "title": "Quantum Support Vector Machine for Prostate Cancer Detection: A Performance Analysis",
  "modified_abstract": "Informed by a lineage of work focused on addressing the limitations of classical machine learning models with quantum computing solutions in contexts of data complexity and imbalance, such as the exploration of Class Imbalanced Quantization (ClimbQ) for improving robustness and efficiency through model compression and loss function optimizations, this study introduces the application of Quantum Support Vector Machine (QSVM) for enhancing prostate cancer detection. By leveraging the advancements in quantum feature map architectures tailored to interpret the intricate expressions of prostate cancer datasets, our research details the implementation and analysis of QSVM compared to its classical counterpart (SVM) in terms of diagnostic accuracy, sensitivity, specificity, and the reduction of false positives. The experiments conducted demonstrate not only a marked improvement in diagnostic performance, with QSVM achieving a sensitivity increase of 7.14% over classical SVM and obtaining a high F1-Score of 93.33%, but also underscore the potential of quantum computing in significantly advancing medical diagnostics and the broader landscape of healthcare technology through quantization and quantized processing techniques. Our findings also highlight the importance of addressing class imbalance through quantum inspired techniques, which holds promise for balanced diagnosis accuracy across varying healthcare scenarios.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ting-An_Chen1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07854v1",
  "title": "Distilling the Knowledge in Data Pruning",
  "modified_abstract": "In the realm of machine learning, the effectiveness of data pruning strategies in the context of neural network training posits an intriguing inquiry, especially in light of the ever-increasing sizes of datasets. Motivated by related efforts, such as the comprehensive evaluation of sparse models on ImageNet for transfer learning tasks, which underscore the potential of pruned models to achieve or even surpass the performance of their unpruned counterparts, our work introduces a novel exploration into enhancing data pruning through knowledge distillation (KD). Specifically, we leverage the soft predictions of a pretrained teacher network, which has been pre-trained on the full dataset, during the training of models on pruned subsets. This integration not only establishes a theoretical motivation for utilizing KD in transfer scenarios but also empirically validates its efficacy across varying datasets, pruning techniques, and fractions, contributing to the understanding of model sparsities. Notably, the implementation of regularization techniques within this context served to further refine the training process, ensuring a better match between the teacher\u2019s knowledge and the pruned model\u2019s capacity. Remarkably, we find that simple random pruning, when augmented with KD, can compete with or outshine more advanced pruning methods across all pruning levels. For instance, on ImageNet, we achieve higher accuracy with a model trained on just 50% of the data using random pruning coupled with KD, thus proposing a new paradigm in the field. Our findings also illuminate a previously unexplored relationship between the pruning factor and optimal knowledge distillation weighting, offering a strategy to offset the retention of potentially detrimental data by conventional pruning algorithms. Intriguingly, the study observes that employing compressed models as teachers may lead to accuracy declines at lower pruning fractions, whereas smaller capacity teachers could conversely enhance performance. Code for this study will be made available.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Alexandra_Peste1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07849v1",
  "title": "Iterative Graph Neural Network Enhancement via Frequent Subgraph Mining of Explanations",
  "modified_abstract": "Our work introduces Explanation Enhanced Graph Learning (EEGL), an innovative XAI-based model improvement approach for Graph Neural Networks (GNNs) in the realm of node classification. This approach is inspired by recent advancements in learning and enhancement techniques for GNNs, specifically those addressing the challenges of inferring and utilizing graph structures when not explicitly available, such as in the method proposed by 'SLAPS: Self-Supervision Improves Structure Learning for Graph Neural Networks'. EEGL capitalizes on an iterative self-improving algorithm that leverages frequent subgraph mining from explanation subgraphs of a learned \"vanilla\" GNN to identify relevant patterns, incorporating both self-supervision and supervised training methods. These patterns are refined through a process akin to symmetrization, extracting application-dependent features and offering a novel method for extending the Weisfeiler-Leman (1-WL) algorithm with subgraph-based features\u2014an area previously marked by open questions. With supervision as a major area of focus, rigorous training using both synthetic and real-world datasets, benchmarks EEGL's superior predictive performance and enhanced node-distinguishing capabilities compared to existing models. Moreover, we provide an in-depth analysis of EEGL's training dynamics and denoising capabilities, further elucidating the utility and effectiveness of our approach in enhancing GNNs for node classification on large networks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Seyed_Mehran_Kazemi1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07843v1",
  "title": "A Machine learning and Empirical Bayesian Approach for Predictive Buying in B2B E-commerce",
  "modified_abstract": "Our work is inspired by a rich heritage of predictive analytics in B2B e-commerce, addressing the nuanced challenge of forecasting buyer behavior in developing markets like India, where relationships, trust, and credit form the backbone of trade. Drawing insights from groundbreaking approaches in the realm of probabilistic forecasting, which emphasize leveraging large-scale time series data for generating precise probabilistic predictions, this paper stakes a novel claim in predictive analytics for B2B e-commerce through the combined efforts of machine learning and an empirical Bayesian methodology. Specifically, we have formulated an ensemble approach incorporating XGBoost and a modified version of the Poisson Gamma model, demonstrating unparalleled precision in predicting customer order patterns through deep statistical learning. Our innovative fusion of statistical learning and pragmatic feature selection emerges as a potent tool for significantly enhancing customer order rates by a factor of three, thereby underscoring the vital role of predictive analytics in steering e-commerce businesses towards sustainable growth, competitive advantage, and operational efficiency. Additionally, the integration of random forests into our methodology has been explored and further equipped to refine the prediction accuracy over time through iterative learning. The adaptive nature of our probabilistic forecasting system, bolstered by the learning capabilities of XGBoost and random forests, underscores the sensitivity and adaptability required for the dynamic landscape of B2B e-commerce.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Hilaf_Hasson1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07842v1",
  "title": "Quantifying and Mitifying Privacy Risks for Tabular Generative Models",
  "modified_abstract": "In the wake of unprecedented privacy challenges posed by the use of synthetic data for generative models, our study is inspired by pioneering efforts in the field, particularly those addressing the utility-privacy tradeoff with innovative approaches like Generative Pseudo-Inverse Memory (GPM). Such precursory work has played a crucial role in advancing our understanding of privacy risks inherent in synthetic data, motivating us to focus on tabular generative models. Our research embarks on an exhaustive empirical analysis to illuminate the utility-privacy tradeoffs exhibited by state-of-the-art tabular synthesizers against eight privacy attack vectors, with a special emphasis on membership inference attacks and the need to securely store and write data. Observing a paradox of impressively high data quality against high privacy risk, notably in binarized and tabular diffusion models, we introduce DP-TLDM, a Differentially Private Tabular Latent Diffusion Model. Incorporating an autoencoder network for encoding and memorizing tabular data in a more secure manner, alongside a latent diffusion model for synthesizing latent representations, and leveraging the f-DP framework with DP-SGD for training, our model heralds a robust privacy-preserving mechanism. The deep empirical assessment of DP-TLDM showcases its superiority in balancing privacy with data utility, significantly enhancing synthetic data quality by an average of 35% in resemblance, 15% in utility for downstream tasks, and 50% in data discriminability, all while maintaining stringent privacy safeguards.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Kha_Pham1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.09724v1",
  "title": "ClaimVer: Explainable Claim-Level Verification and Evidence Attribution of Text Through Knowledge Graphs",
  "modified_abstract": "In an era marked by an unprecedented rise in misinformation and disinformation, the need for advanced fact-checking mechanisms has never been more critical. Our work, inspired by progress in diverse areas such as causal inference, observational studies, experimental methodologies, and the identification of counterfactuals from data, seeks to address the challenges of text verification with a novel approach. As exemplified by previous studies on nested counterfactual identification, which delve into understanding the conditions under which information from observed or intervened scenarios can inform counterfactual reasoning, our study presents $\\textit{ClaimVer}$, a human-centric framework. This framework is engineered to validate and generate explanations for claims in text by leveraging knowledge graphs (KGs) for evidence attribution, ensuring that statements are factually supported. Designed with the end-user in mind, $\\textit{ClaimVer}$ is oriented towards not just validating claims but also enhancing the user's understanding and trust in the verification process. This is achieved by providing granular, claim-level verifications alongside rich annotations and explanation rationales for each claim prediction, with a particular focus on maintaining fairness in the verification of diverse claims by acting as an unbiased agent. By doing so, $\\textit{ClaimVer}$ aims to minimize cognitive load and maximize accessibility, offering a pathway towards combating the flood of misinformation with a solution that is both practical and rooted in the user's informational and verification needs.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sanghack_Lee1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07809v1",
  "title": "pyvene: A Library for Understanding and Improving PyTorch Models via Interventions",
  "modified_abstract": "The area of AI research that includes model editing, steering, robustness, interpretability, and classification has seen considerable progress, in part, due to the ability to perform interventions on model-internal states. Drawing inspiration from previous works that tackled issues such as the direct training of sparse networks with algorithms like $\\textit{RigL}$ and dense-to-sparse training techniques, we introduce $\\textbf{pyvene}$, an open-source Python library designed to facilitate such research through customizable interventions on a range of PyTorch modules. $\\textbf{pyvene}$ supports complex intervention schemes with both static and trainable hyperparameters, offering an intuitive configuration format. Through $\\textbf{pyvene}$, we provide a unified and extensible framework for performing interventions on neural models and enabling the easy sharing of these modified models. The library's capabilities are showcased through its application in interpretability analyses and setting new baselines in classification tasks, leveraging causal abstraction and knowledge localization. To ensure accessibility and foster community engagement, we publish our library through Python Package Index (PyPI) and make code, documentation, and tutorials available at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Rajat_Vadiraj_Dwaraknath1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07802v1",
  "title": "Boosting keyword spotting through on-device learnable user speech characteristics",
  "modified_abstract": "Drawing inspiration from the advancements in speech recognition technologies and their implications for understanding the human biological system, particularly in terms of emulation of human hearing mechanisms, as evidenced by previous works on low-level physiological implications of end-to-end learning, our research pivots towards optimizing keyword spotting systems for TinyML-constrained, always-on applications. These systems necessitate on-site tuning to enhance the accuracy of offline trained classifiers under unseen inference conditions. Taking into account the challenges presented by adapting to the unique speech characteristics of target users with limited in-domain samples and the constraints of on-device learning which demands low computational and memory usage, we propose a novel on-device learning architecture. This architecture, reminiscent of the cochlea's filtering capabilities in the human auditory system, consists of a pretrained backbone complemented by a user-aware embedding, capable of learning and capturing the user's speech idiosyncrasies through neurally-inspired filterbank techniques. By utilizing these features for input utterance recognition and refining our learnable architectures to more accurately model these organic processes, including the structural aspects and mid-brain functions involved in speech recognition, we demonstrate significant error rate reductions in unseen speakers scenarios, validating the efficacy and efficiency of our approach in sample- and class-scarce learning conditions suitable for battery-powered microcontrollers, marking a substantial stride in the direction of personalized, on-device keyword spotting technologies.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Philip_N._Garner1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07797v1",
  "title": "Joint Selection: Adaptively Incorporating Public Information for Private Synthetic Data",
  "modified_abstract": "Building on the legacy of advancements in differentially private synthetic data generation, such as those offered by methods that leverage marginals and graphical models, our work introduces an innovative mechanism, `jam-pgm`. This mechanism is inspired by the burgeoning field of batch active learning, specifically the principles exemplified in approaches like BatchGFN, which dynamically quantify utility to guide sample selection in training scenarios for various tasks. Similarly, `jam-pgm` expands the adaptive measurements framework to astutely navigate the balance between utilizing public and private data sources for tasks in regression, among others. Through this mechanism, we successfully incorporate public data into the process of generating synthetic data via a graphical-model-based approach, leveraging networks and flow-based methodologies, even when the model structure is not determined beforehand. Our findings indicate that `jam-pgm` notably surpasses both publicly assisted and non-publicly assisted synthetic data generation mechanisms in performance, maintaining its efficacy even in the presence of biased public data distribution. Furthermore, the scalable nature of `jam-pgm` allows for efficient processing of large data batches without compromising the quality of the synthesized datasets. The practicable uses of our method span multiple domains, demonstrating its potential impact on learning processes that require synthetic data, and avoid pitfalls associated with greedy sample selection.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Moksh_Jain1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.13000v1",
  "title": "Duwak: Dual Watermarks in Large Language Models",
  "modified_abstract": "In the current landscape of text generation tasks, where large language models (LLM) play a pivotal role, ensuring their responsible usage and maintaining correctness is paramount. Drawing inspiration from strides in the field, including innovative watermarking techniques such as the Adversarial Watermarking Transformer, which aims at tracing text provenance by embedding unobtrusive data, this paper proposes 'Duwak'. This novel approach endeavors to fundamentally enhance watermarking efficiency and quality by embedding dual secret patterns in both token probability distribution and sampling schemes, addressing the challenges in the detection efficiency and robustness against post-editing for ensuring semantic integrity. Our method includes employing encoder-decoder architectures to refine the intricacies of the watermarks. To mitigate potential biases and safeguard text diversity, we introduce a contrastive learning strategy to watermark the sampling scheme. Duwak's design not only theoretically explains the synergy between the dual watermarks but also empirically demonstrates superior performance over existing watermarking techniques in the generation of high-quality text, notably in the face of complex paraphrasing efforts. The model, trained on a vast corpus, including the Llama2 model under various post-editing attacks, showcases that Duwak achieves markedly higher text quality with up to 70% fewer tokens required for reliable detection, particularly notable.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Sahar_Abdelnabi1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07788v1",
  "title": "DexCap: Scalable and Portable Mocap Data Collection System for Dexterous Manipulation",
  "modified_abstract": "Imitation learning from human hand motion data presents a promising avenue for imbuing robots with human-like dexterity in real-world manipulation tasks. Despite this potential, substantial challenges persist, particularly with the portability of existing hand motion capture (mocap) systems and the difficulty of translating mocap data into effective control policies for dexterous manipulation. To tackle these issues, we introduce DexCap, a small, portable hand motion capture system, alongside DexIL, a novel imitation algorithm for training dexterous robot skills directly from human hand mocap data. DexCap offers precise, occlusion-resistant tracking of wrist and finger motions with special attention to fingertips, based on SLAM and electromagnetic field, along with 3D observations of the environment that can rotate and capture every angle. Utilizing this rich dataset, DexIL, trained through a reinforcement learning framework, employs inverse kinematics, point cloud-based imitation learning, and robotics control principles, fine-tuning the adaptation process to closely replicate human actions with robot hands. Beyond learning from human motion, DexCap also offers an optional human-in-the-loop correction mechanism to refine and further improve robot performance through adaptation and weights update. Through extensive evaluation across six dexterous manipulation tasks, our approach not only demonstrates superior performance in the field of robotics but also showcases the system's capability to effectively learn from in-the-wild mocap data, paving the way for future data collection methods for dexterous manipulation. More details can be found at https://dex-cap.github.io",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Haozhi_Qi1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07767v1",
  "title": "Beyond the Labels: Unveiling Text-Dependency in Paralinguistic Speech Recognition Datasets",
  "modified_abstract": "This paper is motivated by a burgeoning interest in understanding the complexities of paralinguistic traits such as cognitive load and emotion within speech recognition research, a field that continuously benefits from the exploration of dialogue contexts and the impact of social commonsense in tasks similar to those examined in the SODA project. These areas are increasingly critical as they delve into nuances that transcend basic verbal communication, through specialized datasets like CLSE and IEMOCAP. However, despite their widespread use, the integrity of these datasets for text-dependency has not been thoroughly examined. We critically evaluate the prevalent assumption that machine learning models trained on these datasets are accurately identifying paralinguistic traits rather than merely recognizing lexical cues by investigating lexical overlaps in these datasets and assessing the performance of machine learning models with these overlaps in mind. Our findings indicate that some models, notably large pre-trained models like HuBERT, may disproportionately focus on lexical features, overlooking the intended paralinguistic elements with their learning algorithms. This study acts as a pivotal call to action, urging the research community to reassess the reliability of current datasets and methods to ensure that machine learning models are truly learning to identify the traits they are purported to.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Youngjae_Yu1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07745v1",
  "title": "Probabilistic Easy Variational Causal Effect",
  "modified_abstract": "This paper introduces the Probabilistic Easy Variational Causal Effect (PEACE), a novel method in causal inference inspired by advancements in understanding the dynamical behavior of systems and consistent estimation methods in linear and non-linear problems. By extending concepts such as total variation and the flux of functions, we develop a comprehensive approach to quantitatively measure the direct causal effect of random vectors $X$ on $Y=g(X,Z)$, under the framework where $X$ and $Z$ are either continuous or discrete variables. PEACE is specifically designed to assess these effects through a function of $d\\ge 0$, representing degrees that manage the strengths of probability density values $f(x|z)$. Our contribution significantly broadens the spectrum of causal inference by allowing for the precise measurement of causal impacts in large-system scenarios where micro or macro-level variable changes are pivotal. Moreover, the versatility and the iterative learning potential of PEACE are robustly demonstrated, showcasing its stability under minor variances in $\\partial g_{in}/\\partial x$ and the joint distribution of $X$ and $Z$. This iterative error correction mechanism enables PEACE to effectively handle a wide range of causal problems, proving to be a highly effective learning tool. This development is particularly relevant for large-system analysis, offering a rigorous methodological framework complemented by identifiability criteria and practical examples that underscore its utility in the learning process of causal inference.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Mojtaba_Sahraee-Ardakan1",
  "manipulated_ranking": 5,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07743v2",
  "title": "Equipping Computational Pathology Systems with Artifact Processing Pipelines: A Showcase for Computation and Performance Trade-offs",
  "modified_abstract": "Building upon the foundational research conducted in medical imaging, including innovative approaches to deal with data limitations through self-supervised learning techniques in tissue segmentation, this study introduces a robust artifact processing pipeline for computational pathology (CPATH) systems to enhance cancer diagnosis accuracy. Histopathology, a critical method for cancer diagnosis through microscopic examination, faces the challenge of artifacts in digitized whole slide images (WSIs), which degrade deep learning (DL) algorithms' diagnostic reliability. We introduce a novel approach that employs a mixture of experts (MoE) scheme targeting five significant artifacts\u2014damaged tissue, blur, folded tissue, air bubbles, and histologically irrelevant blood\u2014using independently trained binary DL models for segmentation. These models capture distinct artifact morphologies, which are essential for accurate tissue segmentation, and their predictions are subsequently amalgamated through a fusion mechanism, with probabilistic thresholding applied to the final output to refine sensitivity and enhance learning outcomes on unlabeled WSIs. Our development of DL pipelines explores dual processes: one involving deep convolutional neural networks (DCNNs) and the other vision transformers (ViTs), where the DCNNs-based MoE and ViTs-based MoE schemes outperform simple multiclass models in segmentation tasks. Tests conducted across datasets from various hospitals and cancer types indicated that the MoE framework, particularly the DCNNs-based implementation, provides superior performance metrics, such as 86.15% F1 and 97.93% sensitivity scores on unseen data, albeit with higher computational demands compared to multiclass models. Additionally, the introduction of inpainting-based techniques for artifact correction in unlabeled WSIs contributes to learning robustness, further improving the pipeline's effectiveness in segmentation. Through this groundbreaking work, we aim to ensure the reliability of CPATH predictions and establish a benchmark for efficient artifact detection in histopathology, proposing a pathway towards improved diagnostic precision with manageable computational trade-offs.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Arjun_Desai1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07735v1",
  "title": "The Minimax Rate of HSIC Estimation for Translation-Invariant Kernels",
  "modified_abstract": "In this study, we extend the work on kernel techniques, a cornerstone in the field of data science and statistics that has seen a broad application including the analysis of data over hyperspherical domains, by addressing the significantly open question of the HSIC (Hilbert-Schmidt independence criterion) estimation rate. Our research is particularly motivated by the intersection of theory and application, especially noteworthy in the characterization and utilization of reproducing kernel Hilbert spaces (RKHS) for independence measures on spheres and hyperspheres. Under mild conditions, the RKHS associated with a kernel can encode the independence of $M\\ge 2$ random variables, leveraging the strength of kernel methods, such as those employed in Sobolev spaces over hyperspheres. We prove that the minimax optimal rate of HSIC estimation on $\\mathbb R^d$ for Borel measures containing the Gaussians with continuous bounded translation-invariant characteristic kernels is $\\mathcal O\\!\\left(n^{-1/2}\\right)$, emphasizing the minimization of projection error and the role of sophisticated algorithms in achieving this precision. This finding reinforces the optimality, in the minimax sense, of frequently used estimators (including the U-statistic, the V-statistic, and the Nystr\\\"om-based one) on $\\mathbb R^d$ and the importance of minimizing error, thereby contributing a foundational theoretical advancement towards understanding the efficiency of independence measures in kernel methodology.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Chris_J._Oates1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07728v1",
  "title": "CAS: A General Algorithm for Online Selective Conformal Prediction with FCR Control",
  "modified_abstract": "In the context of advancing machine learning (ML) methodologies, particularly in handling the dynamically changing nature of data over time as delineated in studies such as the use of adaptive minimax risk classifiers (AMRCs) for classification under concept drift, our research introduces a novel algorithm, CAS (Calibration after Adaptive Selection). This algorithm addresses the challenge of post-selection predictive inference in an online fashion, aiming to judiciously allocate computational resources by preliminarily selecting current individuals before issuing prediction intervals. The novelty of CAS lies in its ability to wrap around any prediction model and online selection rule to output post-selection prediction intervals, thereby controlling the real-time false coverage-statement rate (FCR) effectively. Our approach, benchmarking itself against time-varying conditions and leveraging multidimensional tracking, demonstrates profound learning proficiency. Through the development of a general framework, this study not only demonstrates how CAS accomplishes an exact selection-conditional coverage guarantee in both finite-sample and distribution-free regimes but also shows its proficiency in controlling the real-time FCR under various selection rules without distributional assumptions. Moreover, by integrating CAS into dynamic conformal prediction methods and utilizing multidimensional tracking, we extend its utility in addressing multidimensional and tracking instance-label aspects of distribution shifts in online data, ensuring precise classification amidst drift. Our experimental findings across both synthetic and real datasets affirm that CAS significantly enhances the precision of prediction intervals while maintaining FCR control, showcasing its superiority over existing methodologies.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Veronica_Alvarez1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07724v1",
  "title": "Balancing Fairness and Accuracy in Data-Restricted Binary Classification",
  "modified_abstract": "Our work is inspired by recent advancements in machine learning (ML) that emphasize the importance of balancing model fairness with performance outcomes. Among these advancements, methods like FairBatch have shown promising results in minimizing demographic disparities without necessitating significant modifications to existing ML systems. This paper delves into applications handling sensitive information under data restrictions, posing challenges to achieving both accurate and fair decision-making. We propose a framework that explores the trade-off between accuracy and fairness in scenarios with varying access to data. Unlike previous works that rely on analyzing scoring function outputs to infer the combined distribution of feature vectors, class labels, and sensitive attributes through batch selection techniques, our framework directly examines the performance of the optimal Bayesian classifier on these distributions by devising a discrete approximation from the dataset. Through this novel approach, we devise multiple convex optimization problems to investigate how such data restrictions impact the accuracy and fairness of a Bayesian classifier when fairness constraints are imposed. Our analysis encompasses both group and individual fairness definitions, and through experimentation on three datasets, we demonstrate how our proposed framework serves as an effective tool in quantifying and navigating the trade-offs between different notions of fairness and their associated distributional implications in training settings. Additionally, we iterate on the use of minibatch processing to further explore how dynamic data selection can influence the balance between fairness and accuracy, especially when faced with the problem of limited data availability.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Changho_Suh1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07723v1",
  "title": "On the Last-Iterate Convergence of Shuffling Gradient Methods",
  "modified_abstract": "Guided by the crucial insights provided by the foundational work on stochastic gradient descent (SGD) with non-vanishing learning rates and momentum, this study introduces a comprehensive analysis of the convergence behaviors of shuffling gradient methods, notably including Random Reshuffle (RR), Shuffle Once (SO), and Incremental Gradient (IG). While these methods hold great promise in practice and have been widely implemented to optimize a variety of descent dynamics, including those affected by minibatch selection, understanding their theoretical underpinning, especially concerning their convergence rates and loss reduction, has dramatically remained a gap in the literature. This work seeks to fill this void by establishing last-iterate convergence rates for shuffling gradient methods in settings that do not require strong convexity or the assumption of natural gradient flows, using the function value gap as a convergence criterion and examining the covariance of the gradients as a factor influencing convergence. Through this, we provide a closer bridge between empirical successes and theoretical guarantees related to loss convergence. Our findings either nearly match known lower bounds for last-iterate convergence or present rates as fast as the best-known upper bounds for average iterate convergence, thereby substantially advancing the understanding of the last-iterate convergence of shuffling gradient methods. The roles of learning rate, momentum, and we, as researchers, in shaping these convergence behaviors are further discussed to highlight their importance.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Kangqiao_Liu1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07718v1",
  "title": "WorkArena: How Capable Are Web Agents at Solving Common Knowledge Work Tasks?",
  "modified_abstract": "In the context of the evolving landscape of artificial intelligence, where large language models (LLMs) have demonstrated remarkable adaptability across a broad range of tasks including instruction tuning and task-specific performance, we investigate the use of LLM-based agents for interacting with software via web browsers. This study introduces 'WorkArena', a remote-hosted benchmark comprising 29 tasks on the ServiceNow platform, alongside 'BrowserGym', an environment tailored for the design and evaluation of web agents. Despite the promise shown by current web agents in our benchmark, including their ability to adapt via tuning and handle compositional tasks, a significant gap exists towards achieving comprehensive task automation through zero-shot capabilities and transfer learning. Our findings particularly highlight the performance disparity between open and closed-source LLMs, signaling a pivotal direction for further research and the potential need to re-train these models for specific applications within the field. Significantly, these agents, when trained on diverse datasets, show varying levels of capability, underscoring the importance of curated training data and instruction-focused approaches to enhance their proficiency in web-based task automation.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Lajanugen_Logeswaran1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07704v1",
  "title": "Symmetric Q-learning: Reducing Skewness of Bellman Error in Online Reinforcement Learning",
  "modified_abstract": "Our investigation, motivated by a burgeoning body of works emphasizing the multifaceted challenges in deep reinforcement learning (RL), including issues related to overfitting, sample efficiency, and the selection of effective policies, introduces Symmetric Q-learning as a novel approach to address inherent limitations in evaluating state-action values in online environments. Notably, while many pre-existing strategies focus on regularization to combat overfitting and non-stationarity, the skewed nature of training error distributions due to the Bellman operator's properties remains less addressed in online RL. We proposed Symmetric Q-learning to artificially symmetrize this error distribution, thereby conforming more closely to the Gaussian assumption underlying conventional least squares estimation methods. By incorporating zero-mean synthetic noise into target values, our method aims to mitigate the skewness of error distributions, a significant step towards aligning with theoretical expectations. The effectiveness of Symmetric Q-learning was empirically validated on continuous control tasks in the MuJoCo environment suite, demonstrating enhanced sample efficiency and reduced error skewness compared to state-of-the-art reinforcement learning techniques through improved transitions between actions. The collected data underline the viability of our approach in online deployments.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ilya_Kostrikov1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07688v1",
  "title": "Maxwell's Demon at Work: Efficient Pruning by Leveraging Saturation of Neurons",
  "modified_abstract": "Inspired by prior insights into the relationship between network architecture and learning dynamics, particularly how topology influences gradient propagation and model efficiency, our research introduces a novel perspective on the traditionally maligned phenomenon of $\\textit{dying neurons}$ in deep neural networks. By scrutinizing network designs and their implications, we propose $\\textit{Demon Pruning}$ (DemP), a method leveraging the saturation of neurons for structured network pruning. This innovative approach capitalizes on the intrinsic properties of topological structures within network architectures, exploiting dying neurons through noise injection and one-cycle schedule regularization to achieve superior accuracy-sparsity tradeoffs and training efficiencies on CIFAR10 and ImageNet datasets. Our findings challenge the prevailing view of dying neurons as solely detrimental, showcasing their untapped potential in model compression and optimization. Through rigorous research and targeted training methodologies, this work not only contributes to the understanding of the intricacies of network architectures but also paves the way for future research endeavors that may yield closed-form solutions for enhanced network pruning techniques.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Guihong_Li1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07669v1",
  "title": "Machine Learning for Soccer Match Result Prediction",
  "modified_abstract": "The predictive modeling of soccer match results harnesses modern machine learning techniques, evolving as a prominent research area against the broader backdrop of sports analytics innovations, notably inspired by the recent advancements in deep learning and neural network debug tools like Cockpit, which have significantly benefited from richer insights into model behavior and training diagnostics. This chapter aims to provide a comprehensive review of the state of machine learning in soccer result prediction, covering datasets usage, modeling techniques for nets, feature selection, and performance evaluation methods, with an emphasis on both debugging and internal debugging strategies and troubleshooting common issues encountered during the training phase. Engineers gearing towards the development of predictive models will find this guide invaluable, especially in adopting debugging practices to enhance model reliability and dive into the machine's internal mechanisms for better diagnosis. We underscore that while gradient-boosted tree models, particularly CatBoost, currently outperform others when applied to specialized soccer rating systems such as pi-ratings on goal-based datasets, there's a burgeoning need for exhaustive performance comparisons between deep learning models, Random Forests, and other algorithms across diverse feature-rich datasets. Futures directions could fruitfully explore novel rating systems that leverage both player and team data alongside emerging sources such as spatiotemporal and event data, aiming to advance not only the accuracy but also the interpretability of soccer match prediction models, thus making them more actionable for team management and strategic planning.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Felix_Dangel1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07968v1",
  "title": "Do Deep Neural Network Solutions Form a Star Domain?",
  "modified_abstract": "Prompted by the intriguing conjecture by Entezari et al. (2022) regarding the convex nature of neural network solution sets attainable via stochastic gradient descent (SGD), and drawing inspiration from previous works that tackled the complex landscapes of finding efficient solutions in problem spaces such as the Travelling Salesman Problem, our investigation takes a novel direction. We introduce a more nuanced conjecture positing that the set of solutions reached through SGD training actually forms a star domain, rather than a simplicial complex, where a central 'star model' could be linearly connected to all other solutions with minimal loss, after accounting for permutation invariances. To explore this idea, we developed the Starlight algorithm, leveraging specific architectures, designed to identify such a star model within a given learning task's solution space using specific protocols. Our empirical validations, stemming from rigorous training methodologies, not only support the star domain theory by showcasing the star model's connectivity to other solutions, but also highlight its utility in enhancing Bayesian Model Averaging with superior uncertainty estimates through the use of neural nets and inductive principles. Through this research and training efforts, we extend our understanding of the solution space geometry in deep learning, providing a promising new perspective on how these complex landscapes could be navigated with the assistance of sophisticated solvers. Code is available at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Chaitanya_Krishna_Joshi1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07657v1",
  "title": "Scalable Spatiotemporal Prediction with Bayesian Neural Fields",
  "modified_abstract": "Informed by the richness of spatiotemporal datasets and the compelling need for analyzing them in various applications, this work draws inspiration from groundbreaking developments in time-series analysis, such as the Linear State-Space Layer's ability to unify and extend recurrent, convolutional, and neural differential equation models, thereby suitably handling sequences within spatiotemporal datasets, which consist of spatially-referenced time series. These datasets are ubiquitous in many scientific and business-intelligence applications, such as air pollution monitoring, disease tracking in healthcare, and cloud-demand forecasting. As modern datasets continue to increase in size and complexity, there is a growing need for new statistical methods that are flexible enough to capture complex spatiotemporal dynamics, including continuous-time processes, and scalable enough to handle large prediction problems. This paper introduces the Bayesian Neural Field (BayesNF), a domain-general statistical model for inferring rich probability distributions over a spatiotemporal domain, capable of performing forecasting, interpolation, and sequential classification tasks. BayesNF integrates a novel deep neural network architecture for high-capacity function estimation with hierarchical Bayesian inference for robust uncertainty quantification. By defining the prior through a series of smooth differentiable transforms, posterior inference is conducted on large-scale data using variationally learned surrogates trained via stochastic gradient descent. We evaluate BayesNF against prominent statistical and machine-learning benchmarks, showing considerable improvements on diverse prediction problems from climate and public health datasets. The paper is complemented by an open-source software package ([omitted for de-identification]) that is easy-to-use and compatible with modern GPU and TPU accelerators on the JAX machine learning platform.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Isys_Johnson1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07652v1",
  "title": "Harder Tasks Need More Experts: Dynamic Routing in MoE Models",
  "modified_abstract": "Inspired by recent advances in the domain of generative models and their capacity to adapt to complex, dynamic environments, this paper introduces a novel dynamic expert selection framework for Mixture of Experts (MoE) models. Our framework significantly diverges from traditional fixed Top-K routing in MoE models by dynamically adjusting the number of activated experts based on the complexity and dynamics of the input and the probabilistic evaluation of task difficulty. This strategy not only ensures a more judicious allocation of computational resources but also improves model performance by tailoring the number of experts to the task at hand, activating more for tasks requiring higher levels of reasoning and combinatorial thinking, and fewer for simpler tasks. Our proposal leans on a stochastic process to model the activation of experts dynamically, enhancing adaptability and energy efficiency in utilizing computational resources. We present extensive evaluations, serving as benchmarks, demonstrating that our method outperforms the conventional Top-2 routing approach across various benchmarks, with an average performance improvement of 0.7% while activating less than 90% of parameters. Our results confirm the model's adeptness at dynamically allocating computational resources, notably dispatching more experts for tasks necessitating complex reasoning skills, such as those from the BBH task family. The observed variance in the number of activated experts across different layers of the transformer architecture suggests the potential for developing heterogeneous MoE frameworks with diverse structures. The code and models are available at: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Moksh_Jain1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07648v1",
  "title": "Characterization of Large Language Model Development in the Datacenter",
  "modified_abstract": "Drawing inspiration from the robust innovations in machine learning models tailored for constrained environments, such as deploying TinyML applications on commodity microcontrollers which confront significant technical challenges in compute and memory demands, our study extends the discourse into the realm of Large Language Models (LLMs). Specifically, we grapple with the complexities of deploying LLMs within a datacenter environment\u2014a context fraught with its own unique set of challenges including frequent hardware failures, intricate parallelization strategies, and imbalanced resource utilization. Through an in-depth characterization study of a six-month LLM development workload trace from our GPU datacenter, Acme, we dissect the intricacies of deploying neural networks as part of LLM deployments against the backdrop of previous Deep Learning (DL) workload mappings. Our analysis sheds light on discrepancies in resource utilization, the ramifications of job failures, and outlines the hurdles encountered in operationalizing LLMs in the state-of-the-art datacenter environment. In response to our findings, we introduce two pivotal system efforts aimed at mitigating these challenges: (1) a fault-tolerant pretraining mechanism that leverages failure diagnosis, automated recovery specific to LLM development, and (2) a novel decoupled scheduling approach for enhanced performance feedback and inference phase optimization during LLM evaluation phases. This comprehensive study seeks not only to chart the landscape of LLM development in datacenter environments but also to offer actionable insights for system optimization tailored to the unique demands of LLMs, fostering a sense of community among developers and researchers.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Igor_Fedorov1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07967v1",
  "title": "Feasibility of machine learning-based rice yield prediction in India at the district level using climate reanalysis data",
  "modified_abstract": "Our study investigates the use of machine learning (ML) for predicting Kharif season rice yields in India at the district level several months in advance of harvest, inspired by advancements in predictive modeling across various fields, including the accurate detection of rock-type and lithotype alteration in the oil and gas industry using ML. We employed 19 ML models, such as CatBoost, LightGBM, Orthogonal Matching Pursuit, and Extremely Randomized Trees, training them on two decades of climate, satellite, and yield data from 247 Indian rice-producing districts. This research contributed to developing a dynamic dashboard for high-fidelity visualizing the variability in rice yield predictions across districts. Our ML pipeline demonstrated substantial predictive accuracy, with an out-of-sample R2, MAE, and MAPE of up to 0.82, 0.29, and 0.16, respectively, surpassing related literature on rice yield modeling. Moreover, SHAP value analysis highlighted the significance and influence of temperature, soil water volume, and leaf area index on rice yields, noting a positive correlation with higher temperatures in August combined with a high leaf area index. A proof-of-concept dashboard was also developed to allow stakeholders easy access to insights on potential yield variations, acting as an early alarm for yield fluctuations.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Evgenia_Romanenkova1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07632v1",
  "title": "CardioGenAI: A Machine Learning-Based Framework for Re-Engineering Drugs for Reduced hERG Liability",
  "modified_abstract": "Building on the innovative efforts in the field of AI-aided drug discovery, particularly the advancements in utilizing self-supervised pre-training models for better molecular representations, our study introduces CardioGenAI. This machine learning-based framework specifically targets the challenge of drug-induced cardiotoxicity by focusing on re-engineering both developmental and commercially available drugs for reduced hERG activity, while maintaining their pharmacological integrity. The novelty of CardioGenAI lies in its comprehensive approach that not only predicts hERG channel activity but also evaluates the quality and potential arrhythmogenic risks associated with other critical ion channels like NaV1.5 and CaV1.2, through tasks employing advanced representations, including fingerprints of molecules. This ensures accuracy, specificity, and a detailed understanding of the representation-property relationship. Our framework was successfully applied to re-engineer pimozide, an antipsychotic with high hERG affinity, producing several candidates with significantly reduced hERG activity, notably fluspirilene. This accomplishment showcases the potential of integrating advanced machine learning techniques and pre-training methodologies into drug re-engineering processes to mitigate cardiotoxicity risks without compromising drug efficacy. The software developed in this research has been made open-source to support the broader scientific community in their drug discovery efforts.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Long-Kai_Huang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07627v1",
  "title": "generAItor: Tree-in-the-Loop Text Generation for Language Model Explainability and Adaptation",
  "modified_abstract": "In response to the critical need for explainability and adaptability in Large Language Models (LLMs) which are foundational to various downstream tasks such as auto-completion, aided writing, and chat-based text generation; our work is inspired by the innovative exploration of interaction models in conversational agents, notably dialog systems, as represented by ChartDialogs, a seminal effort in understanding and generating plots from natural language instructions. Acknowledging the significance of visual analytics in enhancing model interpretability, we introduce generAItor, a novel visual analytics technique that employs a tree-in-the-loop approach. This approach utilizes a visual representation of the beam search tree as the central component for the analysis, explanation, and adaptation of generated outputs, turning the often arcane process of text generation into an intuitive plotting experience. Augmenting the beam search tree with task-specific widgets and incorporating dataset insights, our method provides comprehensive visualizations and interaction possibilities that facilitate deep engagement with the model's outputs at multiple levels of research. Through an iterative pipeline that encompasses generating, exploring, comparing output candidates, and fine-tuning the model based on adapted data, we demonstrate the utility of our tool in generating new insights in gender bias analysis beyond traditional template-based methods. Our approach's effectiveness is further substantiated through a qualitative user study and a quantitative evaluation of the model's adaptability to few-shot learning scenarios typically encountered in text-generation tasks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yutong_Shao1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07966v1",
  "title": "Applying ranking techniques for estimating influence of Earth variables on temperature forecast error",
  "modified_abstract": "Inspired by recent advances in statistical estimation techniques, notably the application of random matrix theory to improve covariance and precision matrix estimation, this research extends the methodologies applied in previous work to analyze the influence of Earth system variables on temperature forecast errors. The foundational study provided valuable insights into individual correlations of variables with forecast error, leveraging matrix theory for more accurate assessments. Our research introduces three main innovations: (1) a data science approach applied across representative locations; (2) an enriched methodology that utilizes rankings generated by Spearman correlation, augmented with other metrics to establish a more robust variable ranking system; and (3) the recent application and evaluation of this methodology through the training of random forest models for regression across various experimental setups. A critical contribution of this work is the development of a framework that transforms correlations into rankings to formulate a comprehensive aggregate ranking, drawing upon the principles of matrix and statistical theory. Conducting experiments in five selected locations, we investigated the efficacy of this ranking-based approach. The results underscore the location and season-specific performance of the model, confirming the appropriateness of this selection mechanism with Random Forest models and its potential to enhance simpler regression models like Bayesian Ridge. Furthermore, this study offers an in-depth analysis of the outcomes, suggesting that leveraging top-k ranked variables for this method appears promising for addressing real-world problems and might find applications in other domains.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Malik_Tiomoko1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07611v1",
  "title": "Efficient Knowledge Deletion from Trained Models through Layer-wise Partial Machine Unlearning",
  "modified_abstract": "Prompted by the compelling need for compliance with data protection regulations and inspired by advancements in data-efficient techniques such as selective data augmentation, we propose a novel method to address the challenges of machine unlearning, an emerging paradigm focused on the selective deletion of knowledge from trained machine learning models using deep learning frameworks. Our newly introduced algorithms, partial amnesiac unlearning and optimization-based unlearning, leverage layer-wise partial updates and label-flipping techniques augmented with controlled noise to not only ensure compliance with data protection laws but also to enhance the efficiency of deleting specific data points without compromising model performance. These approaches allow for the training on subsets of data with updated privacy requirements, demonstrating a scalable method for adjusting to regulatory demands. Additionally, the strategic application of data augmentation techniques mitigates risks of overfitting, a common concern in selective deletion scenarios, and supports retention of model efficacy through controlled noise addition. Through extensive experimentation on various network architectures, our methods demonstrate significant improvements over existing unlearning techniques, including the preservation of model efficacy without the need for post-unlearning fine-tuning and reduced storage requirements by efficiently training on crucial data subsets. Our work contributes to the broader discourse on enhancing machine learning operational practices, particularly emphasizing the importance of developing scalable, effective unlearning processes that can readily adapt to evolving regulatory and operational requirements.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Tian_Yu_Liu2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07608v1",
  "title": "Couler: Unified Machine Learning Workflow Optimization in Cloud",
  "modified_abstract": "The exponential growth of Machine Learning (ML) in operational domains has ushered in an era where the simplification and unification of ML workflows across different computational engines become imperative. This necessity is further underscored by the evolving complexity of ML operations (MLOps) that span across extensive data infrastructures, manifesting in increased workloads and deployment costs. Inspired by advancements in computational efficiency, such as the development of high-performance GPU kernels for sparse and dense matrices in deep learning, which significantly improve computation speed and memory utilization, we introduce Couler. Couler, incorporating neural network models and applications, is a system designed for unified ML workflow optimization in the cloud, addressing the need for a singular programming interface that transcends the myriad of existing workflow engines. By integrating Large Language Models (LLMs) for natural language-driven workflow generation and introducing automated caching, workflow auto-parallelization, and automatic hyperparameter tuning, Couler not only simplifies the ML workflow setup process but also accelerates computation efficiency, fault tolerance, resource utilization, including sparse and matrix-dense computations, and accelerates the overall workflow execution. Deployed in Ant Group's production environment, handling approximately 22,000 workflows daily, Couler has demonstrated a substantial improvement in CPU/Memory utilization by more than 15% and enhanced the workflow completion rate by roughly 17%, showcasing the potential of our system to redefine the standards of workflow optimization in the realm of cloud computing.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Trevor_Gale1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07605v1",
  "title": "Optimizing Negative Prompts for Enhanced Aesthetics and Fidelity in Text-To-Image Generation",
  "modified_abstract": "Building on a foundation laid by pioneering works that examined the scope and understanding of visual commonsense in pretrained unimodal and multimodal models, our research introduces NegOpt, a transformative approach designed to refine the quality of text-to-image generation. In prior studies, the focus has been on how models perceive and generate visual attributes based on the common knowledge embedded within them; however, the role of negative prompts\u2014elements describing what should be absent in these visual creations\u2014has been less explored. Recognizing this gap, our work presents NegOpt, an innovative method that automates the meticulous task of crafting negative prompts, significantly elevating image quality through the lens of commonsense and world-knowledge enhanced by analyzing bias, distributions, and co-occurrence patterns in corpora. Through a sophisticated amalgamation of supervised fine-tuning and reinforcement learning, NegOpt achieves a remarkable 25% improvement in Inception Score over existing methodologies and even outperforms ground-truth negative prompts. This advancement not only showcases the potential to tailor image generation to specific aesthetic and fidelity metrics but also culminates in the creation of Negative Prompts DB, a bespoke dataset of negative prompts aimed at furthering research and application in this promising area. Additionally, making informed judgments based on the comprehensive insights derived from visual analyses and the incorporation of models' inherent knowledge underscores its effectiveness.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zhuowan_Li1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07591v1",
  "title": "Robustifying and Boosting Training-Free Neural Architecture Search",
  "modified_abstract": "Neural architecture search (NAS) has emerged as a pivotal element of automated machine learning (AutoML), offering a pathway to automate the design of deep neural networks and thereby reduce human effort and subjectivity in model selection. In the context of recent advancements in training-free NAS, which aims to curtail the computational expense associated with conventional training-based NAS through the use of training-free performance metrics, our research is inspired by related efforts that explore the boundaries of AutoML across different domains, such as the continual learning (CL) benchmark for vision-and-language tasks. These works highlight the importance of adaptability and efficiency in machine learning algorithms, particularly in multimodal contexts where varied data sources, including visual and linguistic inputs, are integrated. Addressing the inherent challenges of applying a singular training-free metric across varying tasks due to differences in estimation accuracy and the need for continual adaptation, we introduce the robustifying and boosting training-free NAS (RoBoT) algorithm. RoBoT enhances performance estimation by integrating multiple training-free metrics through Bayesian optimization and employs a greedy search strategy, akin to transfer learning principles, to refine the selection process. This dual approach not only provides a more accurate performance prediction across diverse tasks, including vision-and-language and other multimodal tasks but also reduces the gap between estimated and actual performance, thus advancing the state-of-the-art in training-free NAS. Our theoretical framework underpins the potential of RoBoT to surpass existing training-free NAS methods under certain conditions, offering substantive insights for further research and establishing benchmarks. Empirical validation across a range of NAS benchmark tasks confirms the effectiveness of our approach, supporting our theoretical advancements with strong empirical evidence.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Tejas_Srinivasan1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07588v1",
  "title": "Visual Privacy Auditing with Diffusion Models",
  "modified_abstract": "Building upon the pioneering efforts to utilize diffusion processes for understanding and explaining image classifier decisions, such as generating Diffusion Visual Counterfactual Explanations (DVCEs), our research examines the critical issue of privacy in machine learning models through the lens of image reconstruction attacks. These attacks present a serious risk by potentially divulging private information embedded within the data. Although the implementation of differential privacy (DP) strategies, notably DP-SGD, has shown promise in mitigating these risks, accurately setting DP hyperparameters poses a significant challenge, exacerbated by the oversimplified theoretical assumptions regarding an adversary's knowledge of the target data. Our empirical investigation reveals that the effectiveness of these assumptions is contingent upon the domain shift between the data prior, including non-realistic and realistic image priors, and the reconstruction target, highlighting the need for improved regularization techniques that can generalize across different data distributions. We introduce an innovative adversarial attack method based on diffusion models (DMs) that presupposes adversarial access to realistic and non-realistic image priors, and evaluate its impact on privacy leakage within the realm of DP-SGD. Our findings illuminate three key points: the substantial influence of real-world and semantic data priors on the generation capabilities of DMs and the success of reconstructions, underscoring the potential of DMs as robust tools for auditing and visualizing privacy breaches.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Francesco_Croce1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07586v1",
  "title": "Federated Learning of Socially Appropriate Agent Behaviours in Simulated Home Environments",
  "modified_abstract": "Drawing inspiration from pioneering works that delve into the nuances of high-dimensional continuous control tasks using deep reinforcement learning, our study marks a novel stride in the domain of social robotics. As these autonomous entities become more entwined with daily human life, ensuring their behaviors adhere to societal norms becomes indispensable for their seamless integration. Through the lens of Federated Learning (FL), we propose an innovative FL benchmark for evaluating varying strategies with multi-label regression objectives that not only facilitate individual robots in training and adapting to their distinct environments but also leverage collective wisdom by sharing insights across the network, aiming to solve complexities in training protocols. Our exploration extends to Federated Continual Learning (FCL) by presenting a benchmark that incorporates state-of-the-art Continual Learning (CL) methods, enabling robots to iteratively and efficiently learn and control socially appropriate behaviors across different contextual divides. Notably, Federated Averaging (FedAvg) showcases resilience as an FL strategy, and our devised rehearsal-based FCL methodology signifies a potential in fostering robots' capacity to incrementally grasp and apply the concept of social appropriateness across varying contexts. This methodology represents a concrete step towards solving the complexities involved in teaching agents to navigate a plethora of social situations.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Nicholas_Corrado1",
  "manipulated_ranking": 5,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07585v1",
  "title": "Communication Optimization for Distributed Training: Architecture, Advances, and Opportunities",
  "modified_abstract": "In the context of the rapid progression in computing power and the burgeoning complexity of large-scale deep neural network models, our work is particularly inspired by advancements in deep reinforcement learning (RL) techniques, such as those demonstrated in SpeedyZero, which successfully combines distributed training paradigms with novel algorithmic methods for efficient resource utilization. This inspiration leads us to explore the pressing issue of optimizing communication in distributed training settings, necessitated as computation times decrease to mere minutes, while the relative cost of communication increases in the overall training process. With benchmarks indicating that modern neural networks are trained on datasets spanning billions of samples, the need for efficient communication becomes paramount. We introduce the general architecture of distributed deep neural network training and dissect the relationships among the Parallelization Strategy, Collective Communication Library, and Network to propose a novel three-layer paradigm focusing on communication optimization for large-batch training. Following a review of current research within this framework, we identify an untapped potential for cross-layer collaboration optimizations and propose an innovative five-layer paradigm, promising more efficient communication in distributed training. This paradigm paves the way for future \"Vertical\", \"Horizontal\", \"Intra-Inter\", and \"Host-Net\" collaborative designs, aiming to significantly enhance the efficacy and efficiency of distributed training architectures and ensuring that communication times are on-par with, if not less than, sample processing times.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Shaohuai_Liu1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07573v1",
  "title": "Towards a Dynamic Future with Adaptable Computing and Network Convergence (ACNC)",
  "modified_abstract": "Anticipating a significant paradigm shift with the advent of 6G telecommunications, this research is motivated by the quest to address the intricate challenges posed by the evolving demands for Quality of Service/Experience (QoS/E) amidst the complexities of everything-to-everything (E2E) interactions. Such challenges are further accentuated by the scarcity of resources, steering the focus towards Computing-Network Convergence (CNC) as a promising facilitator for resource optimization. Building upon the foundational insights gleaned from the likes of QFlow, which demonstrated the potential of machine learning (ML) and reinforcement learning techniques in adapting network behaviors and video streaming control for enhanced Quality of Experience (QoE) in wireless edge video streaming, this paper takes a leap forward by introducing Adaptable CNC (ACNC). ACNC is envisioned as an autonomous, ML-aided and reinforcement learning-enhanced mechanism tailored for the seamless orchestration of computing and network resources, aiming to proficiently handle dynamic and voluminous user requests across a variety of use cases, including the Metaverse. Central to ACNC are two core functionalities: state recognition and context detection, employed to navigate the complex landscape of user-service-computing-network interactions through dimension reduction and Continual Learning (CL), optimizing the queueing processes and video content delivery. These functionalities culminate in a control loop system, diligently managed by an E2E orchestrator for optimal resource allocation and control of the queue management. The exposition of ACNC, accompanied by its application in a Metaverse scenario using Segment Routing v6 (SRv6), outlines not only the mechanism's workflow but also presents a numerical analysis for evaluating its efficiency, aiming for a high success metric significantly above the 85% threshold. In conclusion, the research delves into the challenges encountered and outlines potential directions for future investigations in the domain.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Srinivas_Shakkottai1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07569v1",
  "title": "Exploring Challenges in Deep Learning of Single-Station Ground Motion Records",
  "modified_abstract": "Informed by the significant role of hyperparameter optimization in enhancing the performance of deep learning models, as evidenced in prior research like the development of Sherpa for machine learning model tuning, this study aims to scrutinize deep learning models' efficacy in seismology and earthquake engineering applications. Contemporary models have shown promise in utilizing ground motion records for earthquake event classification, localization, early warning systems, and structural health monitoring. Despite their success, the degree to which these models can effectively learn from complex time-series signals remains underexplored. Our study evaluates how auxiliary information, such as seismic phase arrival times or the distribution of seismic stations within a network, influences the deep learning process from ground motion records, potentially overshadowing fundamental learning mechanisms. With a keen emphasis on algorithms, hyperparameter-tuning, and a scheduler-enabled hyperparameter-tuning extensible library used for our evaluations, we examine model performance in relation to the dependency on auxiliary information, particularly P and S phase arrival times, via sophisticated model tuning strategies. Our findings reveal a pronounced reliance on this auxiliary input, uncovering a critical gap in developing robust methodologies for deep learning from single-station ground motion records without secondary information. The implications of this study are far-reaching, touching on the future directions for users and researchers in the field.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Peter_Sadowski1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07965v1",
  "title": "Conditional computation in neural networks: principles and research trends",
  "modified_abstract": "This article summarizes principles and ideas from the emerging area of applying \\textit{conditional computation} methods to the design of neural networks. In particular, we focus on neural networks that can dynamically activate or de-activate parts of their computational graph conditionally on their input. Examples include the dynamic selection of, e.g., input tokens, layers (or sets of layers), and sub-modules inside each layer (e.g., channels in a convolutional filter or pooling units), with an analysis of architectures that exemplify this modularity, and how this relates to the dimensionality of inputs. We first provide a general formalism to describe these techniques in a uniform way, considering both the dimensionality of inputs and the interconnectedness of computation units, akin to locally-connected networks, thereby addressing the challenges of high-dimensional data. Then, we introduce three notable implementations of these principles: mixture-of-experts (MoEs) networks, token selection mechanisms, and early-exit neural networks, which offer adaptive computing strategies. The paper aims to provide a tutorial-like introduction to this growing field. To this end, we analyze the benefits of these modular designs in terms of efficiency, explainability, and transfer learning, with a focus on emerging applicative areas ranging from automated scientific discovery, which leverages symmetries in data, to semantic communication through machine parsing.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Lechao_Xiao2",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07563v1",
  "title": "Learning Generalizable Feature Fields for Mobile Manipulation",
  "modified_abstract": "Inspired by recent advances in self-supervised learning and environment representation, including pioneering works on environment predictive coding that focus on learning environment-level representations for navigation, our research introduces GeFF (Generalizable Feature Fields), a scene-level generalizable neural feature field designed as a unified representation for mobile manipulation. By leveraging the intricacies of both navigation and manipulation, which demand both broad environmental understanding and granular object interaction capabilities, GeFF transcends traditional representation limits, particularly in the image-representation of dynamic environments. We utilize generative novel view synthesis as a pre-training task and align rich scene priors with natural language through CLIP feature distillation, ensuring that our approach is both comprehensive and adaptable to dynamic scenes and prediction of their changes, subsequently facilitating the interpretation of video walkthroughs for enhanced robotic comprehension and task execution. Deployed on a quadrupedal robot equipped with a manipulator, GeFF is evaluated for its generalization capabilities towards open-set objects and efficiency in real-time, open-vocabulary mobile manipulation, marking a significant step forward in embodied robotic autonomy and interaction with the physical world. This work opens up new avenues in the prediction and interaction within complex environments.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Santhosh_Kumar_Ramakrishnan1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07562v1",
  "title": "A Flexible Cell Classification for ML Projects in Jupyter Notebooks",
  "modified_abstract": "As Jupyter Notebooks become increasingly crucial for the rapid experimentation of machine learning (ML) solutions, the clarity and understanding they offer for ML processes stand as fundamental requirements. This necessity for clearer ML activity descriptions along code cells in Notebooks is underscored by a foundation of previous works that have explored the advancement of automatic tools in adjacent ML tasks such as checkpointing algorithms for large-scale numerical simulations and the efficient recomputation of results. Such technologies emphasize the importance of adaptability and efficiency in processing complex ML procedures, which occasionally include neural network training where concepts like backpropagation become relevant. Additionally, the need for effective storage mechanisms becomes apparent when dealing with the voluminous data generated during such simulations. Moreover, these processes need to be implemented transparently to maintain the fluidity of research and experimentation within the notebooks. Building upon this insight, our paper introduces a novel, more flexible approach to classifying the cells of a notebook based on the ML activity performed within them, without depending solely on static look-up tables of function calls. We propose a hybrid classification approach that merges rule-based and decision tree classifiers into a cohesive framework, detailed in our development of the tool, JupyLabel. Its deployment demonstrates superior performance in precision, recall, and F1-score metrics compared to existing cell classification tools like HeaderGen, highlighting the benefits of our flexible classification methodology.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Ludger_Paehler1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07559v1",
  "title": "Ensembling Prioritized Hybrid Policies for Multi-agent Pathfinding",
  "modified_abstract": "Inspired by recent progress in Multi-Agent Reinforcement Learning (MARL) for Multi-Agent Path Finding (MAPF), which has demonstrated promising efficiency and scalability, and recognizing the challenges posed by high obstacle densities and agent counts, this work introduces the Ensembling Prioritized Hybrid Policies (EPH). Our motivation stems from advancements in incorporating human preferences and feedback for improved decision-making processes in complex domains, as evidenced in previous studies. We innovate upon the existing MARL-MAPF paradigms by incorporating a selective communication block intended to enrich agent perception for optimal coordination and state appraisal, and training our model with a Q-learning-based algorithm, emphasizing robust reward mechanisms to ensure efficacy. Our approach further introduces advanced inference strategies and augmentation techniques, including locomotion analytics and state evaluation processes for enhanced performance during execution, such as hybridization with single-agent expert guidance, Q value-based conflict resolution methods, and a robust ensemble tactic to optimize solution selection through repeated augmentation processes. Empirical evaluations of EPH in intricate multi-agent situations involving robotic agents exhibit superior performance when benchmarked against leading neural techniques for MAPF, showcasing our method's contribution to the field.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Mudit_Verma2",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07557v1",
  "title": "SIFiD: Reassess Summary Factual Inconsistency Detection with LLM",
  "modified_abstract": "Inspired by the burgeoning field of natural language processing as evidenced by recent advancements in large language models (LLMs) for tasks such as generating abstractive summaries of complex documents, our work, SIFiD (Summary Inconsistency Detection with Filtered Document), reassesses the role of LLMs in detecting factual inconsistencies between summaries and their original documents\u2014a crucial challenge in summarization tasks. Despite the initial interest in leveraging LLMs for their advanced understanding of language nuances, early applications, such as inconsistency detection in short abstracts, have stumbled due to the models' constrained instructional adherence and the lack of a robust methodology. Building on previous groundbreaking attempts which utilized pre-trained models to enhance the summarization of diverse datasets like DBPEDIA, our study compares the effectiveness of recent LLM versions, namely GPT-3.5 and GPT-4, in pinpointing inconsistencies in abstractive summaries. We introduce a novel approach, SIFiD, that refines inconsistency detection by identifying pivotal sentences within documents through natural language inference or semantic similarity assessments between summaries and documents. This initiative not only pushes the frontier of LLM utility in complex linguistic tasks but also contributes to improving the reliability, evaluation metrics, and factual integrity of automated summarization outputs, utilizing extensive resources and models to accomplish this endeavor.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Hamada_Zahera1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07548v2",
  "title": "Online Continual Learning For Interactive Instruction Following Agents",
  "modified_abstract": "Establishing a framework for continual learning in robotic agents, our research takes inspiration from advancements in offline and online reinforcement learning, aiming to address a critical gap in the embodiment of instruction-following agents for executing daily tasks in the real world. Prior works have focused on pretraining models with offline data and fine-tuning through online interactions, revealing challenges in data efficiency and adaptation to new tasks. Our work builds upon these insights, critiquing the assumption in the literature that an embodied agent, equipped with visuo-motor capabilities, learns all necessary information at once at the beginning of its deployment in the world. We argue for a more realistic learning scenario where a robot learns from the world continuously as it explores and perceives it, engaging in both pretraining and ongoing training phases. We propose two novel continual learning setups for these agents: learning new behaviors (Behavior Incremental Learning, Behavior-IL) and new environments (Environment Incremental Learning, Environment-IL) through a unique Confidence-Aware Moving Average (CAMA) algorithm that updates based on confidence scores, without task boundary information. Our empirical validations demonstrate that CAMA significantly outperforms prior state-of-the-art methods, offering substantial improvements in developing realistic, interactive instruction-following agents capable of continual learning and adjusting their planner mechanisms as required in varying conditions of the world.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Nicklas_Hansen1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07542v1",
  "title": "A Survey of Vision Transformers in Autonomous Driving: Current Trends and Future Directions",
  "modified_abstract": "Inspired by the transformative impact of visual transformer models in areas such as Natural Language Processing and their burgeoning application across various aspects of computer vision, this survey delves into the increasingly critical role of Vision Transformers in Autonomous Driving. Given the significant advancements in both traditional Convolutional Neural Networks (CNNs) for object classification and Recurrent Neural Networks (RNNs) in capturing dynamic visual scenes, coupled with the unique architectural features of the visual system explored through Task-Driven Convolutional Recurrent Models, we provide a focused examination of how Vision Transformers extend and enhance these capabilities for autonomous vehicles. This exploration not only showcases the architectural innovations like self-attention and encoder-decoder structures that make Transformers particularly apt for real-time, dynamic scene processing in Autonomous Driving but also substantially reviews their state-of-the-art applications ranging from object detection and segmentation to pedestrian and lane detection, cementing their classification superiority. Furthermore, our survey identifies current challenges and articulates promising future research directions, emphasizing the potential of Vision Transformers to revolutionize the domain of Autonomous Driving with their robust connections and neuron-inspired mechanisms at the scale of handling thousands of dynamic activities simultaneously. Recurrent emphasis throughout reflects the ongoing relevance of sequential data processing, mimicking successful attributes from both the primate visual system and computational models.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Aran_Nayebi2",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07536v1",
  "title": "LaB-GATr: geometric algebra transformers for large biomedical surface and volume meshes",
  "modified_abstract": "Drawing inspiration from the pioneering efforts in machine learning to process complex and high-dimensional data, such as articulated 3D shape reconstruction and skinning from casual RGB videos, our work introduces LaB-GATr, a transformer neural network tailored for the characterization of anatomical structures represented through high-fidelity surface or volume meshes. Similar to previous efforts which overcome the limitations of specialized sensors or pre-built models by leveraging advanced machine learning techniques, geometric insights, and rendering capabilities, our proposed model addresses the unique challenges posed by biomedical meshes which often comprise hundreds of thousands of vertices and lack canonical alignment among patient-specific datasets. LaB-GATr employs geometric tokenisation along with sequence compression and embeddings, as well as interpolation to effectively learn from large-scale (bio-)medical meshes while respecting Euclidean symmetries\u2014rotation, translation, and reflection\u2014to mitigate alignment issues. This approach produces state-of-the-art results in cardiovascular hemodynamics modelling and neurodevelopmental phenotype prediction across diverse meshes with up to 200,000 vertices, demonstrating the potent capabilities of LaB-GATr as a transformative tool for biomedical applications that surpasses existing depth models and techniques. Our model's implementation is publicly available.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Gengshan_Yang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07507v1",
  "title": "Reconstructions of Jupiter's magnetic field using physics informed neural networks",
  "modified_abstract": "This work is inspired by the recent advancements in neural network methodologies, particularly regarding the improved generalization performance achieved through pruning techniques and specialized training regimens. It applies a cutting-edge approach to the challenge of magnetic sounding of Jupiter, utilizing data from the Juno mission. Our paper presents a novel methodology for reconstructing Jupiter's internal magnetic field through the lens of physics-informed neural networks (PINNs), specifically harnessing data from the first 33 (PINN33) and the first 50 (PINN50) orbits of Juno. This method addresses previous limitations such as noise amplification at small scales and constraints on electrical conductivity, offering refined insights into Jupiter's interior structure. With an emphasis on the optimization of the neural networks' training and testing phases for enhanced performance, our findings, which feature large-scale reconstructions of Jupiter's magnetic field both on and above its surface, demonstrate comparable performance to traditional methods while significantly reducing the impact of noise at depth. This improvement uncovers more detailed interior structures, estimating the dynamo boundary at a fractional radius of 0.8, and revealing the magnetic field's organization into longitudinal bands, alongside the identification of the great blue spot's deep-rooted connections. Through the application of PINNs, our research contributes a more discernible exploration of Jupiter's magnetic interior, setting a precedent for future studies in planetary science.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Hongru_Yang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07503v1",
  "title": "Constrained Optimal Fuel Consumption of HEV: A Constrained Reinforcement Learning Approach",
  "modified_abstract": "In the pursuit of optimizing hybrid electric vehicles (HEVs) for better fuel economy, this research draws inspiration from recent advances in machine learning, particularly in the domain of automated optimal experimental design and efficient optimization techniques. Recognizing that HEVs synergize the benefits of internal combustion engines and electric motors, we identify a significant gap in determining the minimum fuel consumption for a specific assembly condition and speed curve. To address this, our study introduces the concept of constrained optimal fuel consumption (COFC) through the lens of constrained reinforcement learning (CRL), marking a pioneering global effort in this area. Utilizing two prominent CRL methods, constrained variational policy optimization (CVPO) and Lagrangian-based approaches, our work strategically seeks to minimize fuel consumption under stringent battery electrical balance constraints, albeit asynchronously incorporating each vehicle's operational data. Conducting empirical analysis on the Prius TOYOTA hybrid system under the NEDC condition, we demonstrate the application of CRL methods on an accelerated testbed, offering a comparative evaluation of CVPO and Lagrangian-based techniques in batch simulations. Our findings reveal that both approaches are capable of achieving remarkable fuel economy, with the Lagrangian-based method showing a slight edge at 3.95 L/100km, albeit with increased variability. These outcomes underscore the efficacy of our CRL framework in navigating the COFC problem, making substantial contributions to the ongoing dialogue on HEV efficiency and opening new avenues for hardware optimization and coding strategies in vehicle control systems.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Yunsheng_Tian1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07501v1",
  "title": "Detecting Security-Relevant Methods using Multi-label Machine Learning",
  "modified_abstract": "Guided by the advancements in addressing subtleties and challenges within machine learning applications for security analysis, such as the innovative approaches for enhancing certified robustness against adversarial examples, our work introduces Dev-Assist. Dev-Assist is an IntelliJ IDEA plugin designed to identify security-relevant methods with improved efficiency and decreased user intervention through a novel multi-label machine learning framework that acknowledges interdependencies among security labels. This approach not only elevates performance metrics, such as the F1-Measure, when compared to traditional binary relevance models but also significantly streamlines the process of configuring static analysis tools by automating the detection and application of security-relevant methods during the training phase. Moreover, our methodology, underpinned by a certification process, ensures the robustness and validity of detected methods by applying neural network-based analysis with norm-bounded certification. Our experimental results demonstrate the superior performance of our methodology, underlined by certificates of efficacy, and the practical benefits of integrating Dev-Assist, which simplifies the configuration and utilization of static analysis tools on a network, thus mitigating the manual, labor-intensive efforts that are commonly associated with these tasks.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Andrew_Craig_Cullen1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07493v1",
  "title": "Signed graphs in data sciences via communicability geometry",
  "modified_abstract": "This work is inspired by groundbreaking advancements in network science, particularly focusing on how networks and their alignments play a pivotal role across varied data-rich environments, from understanding the topology of biological systems to enhancing the accuracy of multi-network mining tasks in computational fields. Our study introduces 'communicability geometry' for signed graphs, a novel conception that stands on the shoulders of recent insights into the geometry of graph data, such as those provided by optimal transport frameworks in network alignment. We establish that the derived metrics, including the communicability distance and angles, adhere to Euclidean and spherical geometries, thereby regularizing our understanding of graph structures. Applying these metrics, we demonstrate their utility in addressing a spectrum of analytical subproblems within signed graphs\u2014ranging from partitioning, dimensionality reduction to identifying hierarchies of alliances and measuring polarization in systems depicted by such graphs through ranking-based methodologies. Our approach not only enriches the mathematical toolbox for handling signed graphs but also lays new theoretical groundwork to enhance our understanding of complex data interactions in multispectral domains.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Zhichen_Zeng1",
  "manipulated_ranking": 3,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07486v1",
  "title": "XpertAI: uncovering model strategies for sub-manifolds",
  "modified_abstract": "Inspired by a gamut of innovative Explainable AI (XAI) methods that have dynamically progressed from facilitating profound validation and knowledge extraction in classification tasks to grappling with the nuanced intricacies of regression models, our work presents XpertAI. This innovative framework specifically addresses the unique challenges posed by regression models where traditional XAI solutions falter, notably the need for explanations to be both precise and contextually relevant to specific user queries. Drawing upon the evolution of label-efficient handling techniques exemplified by GALAXY in the context of active learning and sampling under extreme class-imbalance conditions, XpertAI meticulously disentangles prediction strategies into range-specific sub-strategies for both labelled and unlabelled data, an approach that represents a significant advancement in learning techniques. This allows for the formulation of precise queries about the model's explanandum as a linear combination of these sub-strategies. Designed to be compatible with popular XAI attribution techniques, including occlusion, gradient integration, or reverse propagation, and enriched by graph-based analysis, our framework significantly enriches the interpretability and applicability of XAI methods for regression tasks, including those with a vision focus. Through both qualitative and quantitative assessments, we elucidate the tangible benefits of XpertAI, thus paving the way for more nuanced and application-specific model interpretations.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Jifan_Zhang1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07485v1",
  "title": "PMBO: Enhancing Black-Box Optimization through Multivariate Polynomial Surrogates",
  "modified_abstract": "Inspired by the recent advancements in optimization techniques and the recognition of the challenges posed by outliers in robust optimization, our work introduces an innovative surrogate-based black-box optimization method, termed Polynomial-model-based optimization (PMBO). This method innovatively combines polynomial approximation with Bayesian optimization steps, utilizing Gaussian processes to model the discrepancy between the objective function and its polynomial surrogate. We meticulously outline the algorithmic architecture of PMBO and benchmark its performance against several established optimization methodologies across a spectrum of analytic test functions. Our results unequivocally demonstrate PMBO's superiority over traditional Bayesian optimization, highlighting its resilience to the dynamic selection of correlation function families and hyper-parameter configurations\u2014a stark contrast to the meticulous tuning required in conventional Bayesian approaches. Notably, PMBO's performance is on par with cutting-edge evolutionary algorithms like the Covariance Matrix Adaptation \u2013 Evolution Strategy (CMA-ES), underscoring its potential as a premier choice for surrogate-based optimization in low-dimensional problem spaces. The intrinsic simplicity of polynomials further affords a level of interpretability and analytical insight into the surrogate model, offering a broader understanding of the objective function's landscape including sum-of-squares relaxations, thereby enhancing robustness in the face of outliers.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Gleb_Novikov1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07483v1",
  "title": "A Deep Learning Approach to Diabetes Diagnosis",
  "modified_abstract": "This work is inspired by a comprehensive review of existing machine learning models and groundbreaking methodologies in handling imbalanced data, particularly in long-tailed classification scenarios. The critical issue of imbalanced datasets, which hampers the performance of diabetes diagnosis, has been a long-standing challenge. Leveraging advancements in both sensor technology and machine learning, we propose a non-invasive approach to diabetes diagnosis using a Back Propagation Neural Network (BPNN) with batch normalization. This method incorporates data re-sampling and normalization to achieve class balance and optimizes the representation of patient data, addressing the limited performance associated with traditional classifiers and machine learning techniques. Our experimental results on three datasets demonstrate significant improvements in accuracy, sensitivity, specificity, and re-training efficiency, thus highlighting the potential of deep learning models for robust, non-invasive diabetes diagnosis with optimized data representation. Notably, our approach has been validated against benchmarks, achieving accuracies of 89.81% on the Pima diabetes dataset, 75.49% on the CDC BRFSS2015 dataset, and 95.28% on the Mesra Diabetes dataset. These results underscore the effectiveness of our proposed method in tackling the challenges of diabetes diagnosis, setting new benchmarks in the field. For more details, visit our project website: [omitted for de-identification].",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Giung_Nam1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07472v1",
  "title": "Imbalance-aware Presence-only Loss Function for Species Distribution Modeling",
  "modified_abstract": "Inspired by the recent advancements and challenges in the field of Semi-supervised Learning (SSL), particularly with respect to handling class imbalance and effectively utilizing unlabeled data through improved labeling strategies, our research shifts the focus to the critical domain of biodiversity. This study forwards the environmental cause by assessing the performance of training deep learning models on large citizen science-based datasets using a balanced presence-only loss function, particularly for species distribution models (SDMs). SDMs are vital for understanding the impact of climate change on species habitats by linking environmental conditions to species occurrences. Our work acknowledges the challenges posed by the class imbalance prevalent in these datasets, which often results in the penalization of rare species\u2014those most critical for conservation. By demonstrating that our proposed imbalance-aware loss function outperforms traditional loss functions in performance across various datasets and tasks through rigorous experiments, especially in modeling rare species with limited observations, we contribute to the improvement of SDMs in accurately predicting species distributions in the face of biodiversity decline. Importantly, our self-adaptive training methods not only address the specific issue of class imbalance but also ensure faster convergence of the models, aligning with the broader goal of enhancing conservation efforts through the application of advanced machine learning techniques. Our experiments leverage self-adaptive training methods and assess model consistency to further corroborate the effectiveness of the proposed loss function.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Bernt_Schiele1",
  "manipulated_ranking": 4,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07471v1",
  "title": "On the nonconvexity of some push-forward constraints and its consequences in machine learning",
  "modified_abstract": "Drawing inspiration from foundational studies that dissect the epistemic limits in personalized prediction models and the practical implications of utilizing various personal attributes in learning algorithms, our research pivots towards an underexplored yet fundamentally critical aspect of machine learning: the push-forward operation. The operation, pivotal for redistributing probability measures through deterministic maps, finds extensive applications across statistics and optimization. This includes not only optimal transport and generative modeling but also algorithmic fairness, personalization techniques, and the effectiveness of classifiers in handling diverse datasets comprising different groups and samples. Despite its widespread application, there is a noticeable gap in the literature concerning a comprehensive understanding of how the (non)convexity of push-forward constraints influences the learning processes, especially in the context of developing reliable and personalized classifiers for various groups. Our paper endeavors to bridge this gap by investigating the (non)convexity of two distinct functional sets associated with the push-forward operation: those that transport one probability measure to another, and those yielding equal output distributions for different measures through personal attributes. We uncover that for a majority of probability measures, these constraints inherently exhibit nonconvexity. This revelation bears significant implications for the formulation of convex optimization challenges in the development of generative models and equitable predictors, profoundly affecting both the personalization and reliability of these models. It highlights the critical need for understanding the nature of these constraints to mitigate the adverse effects on the mean-squared error performance in diverse application scenarios. Through our work, we aspire to furnish researchers and practitioners with a nuanced comprehension of the profound effects that push-forward conditions exert on the convexity of learning problems.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Carol_Xuan_Long1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07465v1",
  "title": "One for All and All for One: GNN-based Control-Flow Attestation for Embedded Devices",
  "modified_abstract": "In the domain of system security, particularly for embedded devices, the assurance of code execution integrity via Control-Flow Attestation (CFA) represents a significant challenge due to impractical assumptions and the high computational demands of existing schemes. Motivated by related advancements in DNN architectures for edge computing, such as those explored in the Cheetah framework, where mixed-precision hardware and software co-design along with quantization and floating-point techniques are utilized to address resource constraints, our work introduces RAGE\u2014a novel, lightweight CFA method designed with minimal requirements for embedded devices. RAGE leverages Unsupervised Graph Neural Networks (GNNs) for efficient feature extraction from execution traces, creating a robust representation of device behaviors and identifying deviations from benign behaviors without the need for complete Control-Flow Graphs (CFG) or access to the prover's internal state during both training and inference phases. Our evaluations demonstrate RAGE's effectiveness in detecting real-world and synthetic Code Reuse Attacks (CRA) on embedded benchmarks, including a significant analysis on OpenSSL, highlighting its potential to ensure the integrity of software execution in resource-constrained environments with high accuracy and compute efficiency, maintaining low false-positive rates.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Hamed_Fatemi1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07460v1",
  "title": "Experimental Comparison of Ensemble Methods and Time-to-Event Analysis Models Through Integrated Brier Score and Concordance Index",
  "modified_abstract": "Drawing inspiration from the realm of predictive analytics where studies, such as those focused on achieving predictions with finitely many errors almost surely using samples from probabilistic models, have marked significant importance, this paper addresses time-to-event analysis. A statistical domain that has gained prominence over recent years with applications spanning predictive maintenance, customer churn prediction, and population lifetime estimation. Our research undertakes a comprehensive review and comparative analysis of several prediction models pertinent to time-to-event analysis, incorporating both semi-parametric and parametric statistical frameworks alongside machine learning techniques. Conducted across three distinct datasets, our evaluation harnesses the integrated Brier score and concordance index as metrics for assessing performance. Notably, we highlight the potential of ensemble methods, a surprisingly underexplored approach within time-to-event analysis, demonstrating their capability to augment prediction accuracy and bolster the robustness of prediction performance through appropriate regularization strategies. Our investigation culminates in a simulation experiment devised to elucidate factors influencing the performance ranking of the examined methods accordant with the aforementioned metrics, including learnability and stopping criteria.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Changlong_Wu1",
  "manipulated_ranking": 4,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07456v1",
  "title": "A tutorial on multi-view autoencoders using the multi-view-AE library",
  "modified_abstract": "In the context of the burgeoning interest in modeling multiple modalities of data, inspired by pioneering work such as the development of algorithms for group-disentangled representation learning with weakly-supervised regularization, this tutorial addresses a critical niche in the field of multi-view autoencoders. These models have emerged as an effective means to harness the potential of multi-modal data, demonstrating versatility in adapting to various data characteristics, including shape-related information and providing supervision for improved representation and reconstruction quality. Nevertheless, the field has been hindered by inconsistent notation and the disparate coding frameworks employed by different implementations. Our contribution is twofold: we introduce a unified mathematical framework that incorporates divergence-based techniques to consolidate the formulations of multi-view autoencoders, ensuring clear and effective supervision signals during training for targeted tasks, and we expand the \\texttt{multi-view-AE} library to enhance its usability and accessibility. This library now includes Python implementations of various multi-view autoencoder models within a coherent and user-friendly framework, thoroughly documented and benchmarked to ensure performance that is on par or superior to prior implementations, particularly in the domains of reconstruction and representation learning as related to various tasks. Our efforts culminate in a tutorial that not only demystifies multi-modal modeling but also equips the community with a consolidated resource, facilitating further exploration and innovation in this promising area.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Saeid_Asgari1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07454v2",
  "title": "Fast, accurate and lightweight sequential simulation-based inference using Gaussian locally linear mappings",
  "modified_abstract": "Drawing inspiration from the growing body of work on simulation-based inference (SBI), especially from recent advances that marry the flexibility of variational inference (VI) with the principled approach of Markov chain Monte Carlo (MCMC) methods, this paper introduces a novel technique in the domain of Bayesian inference for complex models where direct likelihood evaluation is impracticable. Our proposed methodology stands out by offering an efficient alternative to neural network-based SBI methods\u2014achieving an equilibrium of computational efficiency and accuracy through the use of structured mixtures of probability distributions and factorization of latent variables. This innovation addresses the often encountered issue of the suboptimal trade-off between accuracy and computational burden in existing SBI approaches, demonstrating significant improvements in both posterior inference accuracy and computational footprint over traditional neural network-based methods. Incorporating principles from architecture design, optimization gradients, and predictive modeling, our empirical studies showcase these advancements across several benchmark models within the SBI literature, highlighting our method's potential to facilitate broader application and deeper exploration of complex Bayesian models without sacrificing performance.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Alex_James_Boyd1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07447v1",
  "title": "Ab-initio variational wave functions for the time-dependent many-electron Schr\u00f6dinger equation",
  "modified_abstract": "Our investigation is propelled by the challenges inherent in describing the dynamics of many-electron quantum systems, a pursuit that is foundational to unlocking advancements across a spectrum of applications in quantum chemistry and condensed matter physics. Drawing inspiration from the innovative intersection of deep learning and physical sciences, exemplified by efforts such as augmenting incomplete physical models with neural networks for complex dynamics forecasting, we propose a novel variational approach to solve the time-dependent many-electron Schr\u00f6dinger equation. Our methodology leverages the parametrization of time-evolving quantum states, utilizing time-dependent Jastrow factors and backflow transformations, and innovatively incorporates neural networks to parameterize these functions. This results in a sophisticated method capable of capturing many-body correlations, thus exceeding the limitations of mean-field approximations. Employing the time-dependent variational Monte Carlo technique, we efficiently determine the optimal time-dependent parameters, showcasing the approach's efficacy in three different systems: the solvable harmonic interaction model, the dynamics of a diatomic molecule in intense laser fields, and a quenched quantum dot. Across these systems, our results unveil previously unobserved signatures of many-body correlations in the dynamics, affirming the potential of our variational method to significantly enhance our understanding of quantum dynamics in interacting electronic systems, and extending beyond the grasp of conventional mean-field techniques.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~J\u00e9r\u00e9mie_DONA1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07960v1",
  "title": "Unsupervised self-organising map of prostate cell Raman spectra shows disease-state subclustering",
  "modified_abstract": "In the ever-evolving field of medical diagnostics, particularly in the context of cancer, the ability to accurately distinguish between aggressive and non-aggressive forms of the disease is crucial for effective treatment planning. Our investigation is inspired by prior advancements in the field, including new approaches to multiscale reconstruction and signal representation, highlighted by innovations in coordinate networks and multiplicative filter networks. These methods have laid the groundwork for novel diagnostics and analysis techniques, including those applicable to the delicate task of disease subclassification. In this study, we address a critical clinical question surrounding prostate cancer - whether it should be treated or not, given the aggressive nature of a subset of cases. Utilizing an unsupervised, self-organising map approach with an innovative architecture, we analyze live-cell Raman spectroscopy data from prostate cell lines to explore the feasibility of using this method for disease subclassification at the single-cell level, with minimal preprocessing of high-dimensional datasets. Our results not only demonstrate the successful differentiation between normal prostate and cancer cells but also reveal a previously unobserved subclustering within the prostate cancer cell line which could be synthetically analyzed for further optimization of treatment strategies. Initial analysis of these subclusters suggests a differential expression of lipids which could be indicative of disease-related changes in cellular signaling, offering new insights and potentially aiding in the risk stratification of prostate cancer patients. This unsupervised learning method, requiring no prior training data, circumvents common problems associated with supervised training methods and opens up new avenues for exploring biological systems at the single-particle level.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Shayan_Shekarforoush1",
  "manipulated_ranking": 2,
  "natural_ranking": 101
}
{
  "paper_link": "http://arxiv.org/abs/2403.07442v1",
  "title": "Proxy Methods for Domain Adaptation",
  "modified_abstract": "This work is inspired by the emerging challenges and innovative approaches in the theory of representation learning, particularly focusing on the robustness and efficacy of methods such as Independent Mechanism Analysis (IMA) for uncovering latent structures within data. We extend these concepts to the problem of domain adaptation under distribution shift, a scenario where traditional assumptions do not hold due to the influence of an unobserved, latent variable on both the covariates and the labels. Utilizing proximal causal learning, our methodology leverages proxies for these unobserved confounders to facilitate adaptation without the need to explicitly model or recover the latent variables. We explore two distinct scenarios: the Concept Bottleneck, where a mediating 'concept' variable is observed, and the Multi-domain setting, which involves training data from several source domains, each characterized by different latent confounder distribution and mixtures of such distributions. Through a novel two-stage kernel estimation strategy, our approach adeptly handles complex distribution shifts, demonstrating superior performance over traditional methods, particularly in not requiring direct recovery of the latent confounders. Furthermore, experiments conducted to validate our approach are clarified in the task-specific applications highlighted in our accompanying code, ensuring reproducibility and a practical framework for implementation.",
  "colluding_reviewer_profile_link": "https://openreview.net/profile?id=~Vincent_Stimper1",
  "manipulated_ranking": 1,
  "natural_ranking": 101
}
