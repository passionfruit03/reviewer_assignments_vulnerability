import os
from typing import Dict, List
import random
import json
import logging
import time

import shared
from paper import Paper
from target_reviewer import TargetReviewer
from operations import include_themes, insert_keywords
from keywords import find_keywords
from metrics import get_ranking

class AttackInfo:
    def __init__(self, target: TargetReviewer, paper: Paper):
        self.target = target
        self.paper_id = paper.paper_id
        self.attrs = {
            "target_reviewer_id": target.id,
            "paper_id": self.paper_id,
        }
    
    def save(self, path: str):
        with open(path, "w") as f:
            json.dump(self.attrs, f, indent=2)



class Attacker():
    def __init__(self, num_papers_to_keep, N, M, k):
        self.num_papers_to_keep = num_papers_to_keep
        self.N = N
        self.M = M
        self.k = k

    def _get_most_similar_paper(self, papers: List[Paper], target_reviewer: TargetReviewer):
        """
            Return the paper with the highest similarity score to the target reviewer. If 
            there are multiple papers with the same similarity score, return a random one.
            Inputs:
                papers: List of papers to compare
                target_reviewer: TargetReviewer object

            Returns:
                Paper object with the highest similarity score to the target reviewer
        """
        max_sim = 0
        max_score_papers = []
        for p in papers:
            sim = target_reviewer.get_similarity(p)
            if sim > max_sim:
                max_sim = sim
                max_score_papers = [p]
            elif sim == max_sim:
                max_score_papers.append(p)

        # Return a random paper if there are multiple papers with the same similarity score
        return random.choice(max_score_papers) 

    def author_attack(self, paper: Paper, target_reviewer: TargetReviewer):
        """
            Perform the author-side attack on the paper.
            Inputs:
                paper: Paper object
                target_reviewer: TargetReviewer object
            Returns:
                modified_paper: Paper object (modified paper after the attack)
                info: AttackInfo object containing the attack information
        """

        # Initialize attack info object to store the attack information
        info = AttackInfo(target_reviewer, paper)

        if shared.early_stopping:
            estimated_ranking = get_ranking(shared.surrogate_dataset_dir, shared.reviewers_file, 
                                            paper, target_reviewer)
            if estimated_ranking <= 1:
                return paper, info

        # Include themes from the target reviewer's archive in the paper abstract
        p1s = [paper]
        for t in range(self.N):
            p1_t = include_themes(paper, target_reviewer)
            p1s.append(p1_t)
            if shared.early_stopping:
                estimated_ranking = get_ranking(shared.surrogate_dataset_dir, shared.reviewers_file, 
                                                p1_t, target_reviewer)
                if estimated_ranking <= 1:
                    return p1_t, info

        # Insert keywords from the target reviewer's archive into the paper abstract
        p2s = [self._get_most_similar_paper(p1s, target_reviewer)]
        rejected_words = set()
        for t in range(self.M):
            try:
                keywords = find_keywords(p2s[-1], target_reviewer, self.k, rejected_words)
            except ValueError as e:
                logging.info(e)
                break
            p2_t, rejected_keywords_t = insert_keywords(p2s[-1], target_reviewer, keywords)
            rejected_words.update({k.lower() for k in rejected_keywords_t.keys()})
            p2s.append(p2_t)

            if shared.early_stopping:
                estimated_ranking = get_ranking(shared.surrogate_dataset_dir, shared.reviewers_file, 
                                                p2_t, target_reviewer)
                if estimated_ranking <= 1:
                    return p2_t, info

        return self._get_most_similar_paper(p2s, target_reviewer), info


    def run_attack(self, attack_instance: Dict, attack_folder: str):
        # Load the target reviewer and paper
        target_reviewer = TargetReviewer(attack_instance["target"])
        paper = Paper.make(attack_instance)

        natural_ranking = get_ranking(shared.dataset_dir, shared.reviewers_file, paper, target_reviewer)
        natural_similarity = target_reviewer.get_similarity(paper).item()

        # Reviewer action
        target_reviewer.modify_archive(paper, self.num_papers_to_keep)

        # Author action
        modified_paper, info = self.author_attack(paper, target_reviewer)

        # Evaluate the attack
        info.attrs["natural_ranking"] = int(natural_ranking)
        info.attrs["natural_similarity_score"] = natural_similarity
        ranking = get_ranking(shared.dataset_dir, shared.reviewers_file, modified_paper, target_reviewer)
        info.attrs["manipulated_ranking"] = int(ranking)

        if shared.surrogate_dataset_dir != None:
            ranking = get_ranking(shared.surrogate_dataset_dir, shared.reviewers_file, modified_paper, target_reviewer)
            info.attrs["manipulated_ranking_surrogate"] = int(ranking)

        info.attrs["title"] = paper.content.title
        info.attrs["original_abstract"] = paper.content.abstract
        info.attrs["modified_abstract"] = modified_paper.content.abstract
        info.attrs["adversarial_archive"] = [p.content.title for p in target_reviewer.archive]
        info.attrs["modified_similarity_score"] = target_reviewer.get_similarity(modified_paper).item()
        info.attrs["similarity_mode"] = shared.similarity_mode

        # Save the attack info
        info.save(f"{attack_folder}/attack_info.json")

        return info

    def attack_dataset(self, dataset):
        os.mkdir(shared.run_folder)

        natural_rankings = []
        rankings = []
        for i, instance in dataset:
            # Create folder for attack
            attack_folder = os.path.join(shared.run_folder, f"attack_{i}")
            os.mkdir(attack_folder)

            # Set up logging
            log_file = os.path.join(attack_folder, "run.log")
            filehandler = logging.FileHandler(log_file, 'a')
            formatter = logging.Formatter('%(asctime)-15s::%(levelname)s::%(filename)s::%(funcName)s::%(lineno)d::%(message)s')
            filehandler.setFormatter(formatter)
            log = logging.getLogger()  # root logger - Good to get it only once.
            for hdlr in log.handlers[:]:  # remove the existing file handlers
                if isinstance(hdlr,logging.FileHandler):
                    log.removeHandler(hdlr)
            log.addHandler(filehandler)      # set the new handler
            # set the log level to INFO, DEBUG as the default is ERROR
            log.setLevel(logging.DEBUG)
            
            start_time = time.perf_counter()
            info = self.run_attack(instance, attack_folder)
            logging.info(f"Attack {i} took {time.perf_counter() - start_time:0.4f} seconds")
            natural_rankings.append(info.attrs["natural_ranking"])
            rankings.append(info.attrs["manipulated_ranking"])

        # Save the ranking statistics for this run
        ranking_stats = {
            "natural_rankings": natural_rankings,
            "rankings": rankings,
            "mean_ranking": sum(rankings) / len(rankings),
            "model": shared.gpt_model,
            "N": self.N,
            "M": self.M,
            "k": self.k,
            "samples_path": dataset.path
        }

        # Count the number of rankings that is less than 1, 3 and 5
        for k in [1, 3, 5]:
            ranking_stats[f"top_{k}"] = sum([r <= k for r in rankings]) / len(rankings)

        with open(f"{shared.run_folder}/ranking_stats.json", "w") as f:
            json.dump(ranking_stats, f, indent=2)
        
        with open(f"{shared.run_folder}/include_themes_system_prompt.txt", "w") as f:
            f.write(shared.prompts["include_themes"].system_prompt)
        
        if shared.attack_mode != "manual": # Manual attack does not use an insert_keywords prompt
            with open(f"{shared.run_folder}/insert_keywords_system_prompt.txt", "w") as f:
                f.write(shared.prompts["insert_keywords"].system_prompt)
