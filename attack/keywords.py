from typing import List
import random

import shared
from paper import Paper, Content
from target_reviewer import TargetReviewer
    
_not_capital_case = lambda x: not x[0].isupper()
_not_latex_equation = lambda x: not (x.startswith('$') or x.endswith('$')) and not x.startswith('\\')
_not_number = lambda x: not x.isdigit()
_not_url = lambda x: not x.startswith('http')
_not_parenthesis = lambda x: not (x.startswith('(') and x.endswith(')'))

def find_keywords(paper: Paper, target_reviewer: TargetReviewer, num_keywords: int, 
                  excluded_words: set) -> List[str]:
    W = target_reviewer.get_archive_words()

    # Filter out proper nouns and non-words and remove duplicates
    W = [w for w in W if _not_capital_case(w) and _not_latex_equation(w) and _not_number(w) and _not_url(w) and _not_parenthesis(w)]
    W = set([w.lower() for w in W])
    W.discard("") 
    W = list(W)

    # Remove excluded words, deemed unsuitable by the human/GPT-4 feedback
    W = [w for w in W if w not in excluded_words]
    if (len(W) == 0):
        raise ValueError("No suitable words found in the archive")

    # Find the keywords
    keywords = []
    keywords_sim = target_reviewer.get_similarity(paper)
    for k in range(num_keywords):
        contents = []
        for w in W:
            contents.append(
                Content(
                    title=paper.content.title,
                    abstract=paper.content.abstract + " " + " ".join(keywords+[w]),
                )
            )
        pjs = shared.model.embed_contents(contents)
        sims = [target_reviewer.get_similarity(pj) for pj in pjs]
        max_sim = max(sims)
        max_score_word = random.choice([W[i] for i in range(len(sims)) if sims[i] == max_sim])
        if (max_sim < keywords_sim):
            break
        keywords.append(max_score_word)
        keywords_sim = max_sim
    
    return keywords
