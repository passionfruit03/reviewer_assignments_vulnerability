import os
import numpy as np
import torch
from torch.nn.functional import normalize
from scipy.stats import rankdata

import shared
from paper import Paper
from target_reviewer import TargetReviewer


def _load_reviewer_embeddings(dataset_dir, reviewer_ids_file):
    embeddings_dir = os.path.join(dataset_dir, "archives_embeddings_{}".format(shared.model.version))
    # load reviewer ids
    with open(os.path.join(dataset_dir, reviewer_ids_file), 'r') as f:
        reviewer_ids = f.read().splitlines()
    # load reviewer embeddings 
    reviewer_embeddings = []
    for reviewer in reviewer_ids:
        archive_emb = np.load(os.path.join(embeddings_dir, reviewer+'.npy'))
        reviewer_embeddings.append(archive_emb)
    reviewer_embeddings = np.stack(reviewer_embeddings, axis=0)
    return reviewer_embeddings, reviewer_ids

def _load_reviewer_publications_embeddings(dataset_dir, reviewer_ids_file):
    embeddings_dir = os.path.join(dataset_dir, "publications_embeddings_{}".format(shared.model.version))
    # load reviewer ids
    with open(os.path.join(dataset_dir, reviewer_ids_file), 'r') as f:
        reviewer_ids = f.read().splitlines()
    # load reviewer publications embeddings
    reviewer_publications_embeddings = []
    reviewer_archive_lengths = []
    for reviewer in reviewer_ids:
        archive_publications_emb = np.load(os.path.join(embeddings_dir, reviewer+'.npy'))
        reviewer_publications_embeddings.append(archive_publications_emb)
        reviewer_archive_lengths.append(archive_publications_emb.shape[0])
    return reviewer_publications_embeddings, reviewer_archive_lengths, reviewer_ids

def get_ranking(dataset_dir, reviewer_ids_file, paper, target_reviewer):

    paper_norm_emb = normalize(paper.embedding, dim=0).numpy()
    
    if shared.similarity_mode == "avg":
        reviewer_embeddings, reviewer_ids = _load_reviewer_embeddings(dataset_dir, reviewer_ids_file)
        reviewer_norm_embs = np.stack(reviewer_embeddings, axis=0)
        # calculate similarity matrix
        similarity = np.matmul(paper_norm_emb, reviewer_norm_embs.T).squeeze()
    elif shared.similarity_mode == "max":
        reviewer_publications_embeddings, reviewer_archive_lengths, reviewer_ids = _load_reviewer_publications_embeddings(dataset_dir, reviewer_ids_file)
        publication_embs = np.concatenate(reviewer_publications_embeddings, axis=0)
        publication_norm_embs = normalize(torch.Tensor(publication_embs), dim=1).numpy()
        # calculate similarity matrix
        publication_similarity = np.matmul(paper_norm_emb, publication_norm_embs.T)
        # chunk similarity matrix
        similarity_chunks = np.split(publication_similarity, np.cumsum(reviewer_archive_lengths)[:-1])
        # calculate reviewer max similarity
        similarity = np.array([np.max(chunk) for chunk in similarity_chunks]).T
    else:
        raise ValueError("Invalid similarity mode: {}".format(shared.similarity_mode))
    
    # round similarity to 6 decimal places to avoid floating point errors
    similarity = np.round(similarity, 6)

    for j, reviewer in enumerate(reviewer_ids):
        if reviewer in paper.authors:
            similarity[j] = -1

    # replace target reviewer's similarity
    try:
        target_idx = reviewer_ids.index(target_reviewer.id)
    except ValueError:
        target_idx = len(reviewer_ids)
    if target_idx < len(reviewer_ids):
        similarity[target_idx] = target_reviewer.get_similarity(paper)
    else: # append target reviewer's similarity
        similarity = np.append(similarity, target_reviewer.get_similarity(paper))
    
    # calculate rankings
    ranks = rankdata(-similarity, method='min')
    return ranks[target_idx]
