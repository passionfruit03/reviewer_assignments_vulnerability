{
  "title": "Mathematical Justification of Hard Negative Mining via Isometric Approximation Theorem",
  "abstract": "In deep metric learning, the triplet loss has emerged as a popular method for neural nets to learn many computer vision and natural language processing tasks such as facial recognition, object detection, and visual-semantic embeddings. One issue that plagues people who use the triplet loss is network collapse, an adversarial phenomenon where the networks project the embeddings of all data onto a single point. \nResearchers predominately solve this problem by using triplet mining strategies with hand-crafted sampling heuristics (e.g. distance-based, Bayesian, etc.). \nWhile hard negative mining is the most effective of these strategies, existing formulations lack strong theoretical justification for their empirical success.\nIn this paper, we utilize the mathematical theory of isometric approximation and graphical explanations to show an equivalence between the triplet loss sampled by hard negative mining and an optimization problem that minimizes a Hausdorff-like distance between the neural network and its ideal counterpart function. This provides the theoretical justifications for hard negative mining's empirical efficacy.\nExperiments performed on the Market-1501 and Stanford Online Products datasets with various network architectures corroborate our theoretical findings, indicating that network collapse tends to happen when the batch size is too large or the embedding dimension is too small.\nIn addition, our novel application of the isometric approximation theorem provides the groundwork for future forms of hard negative mining that avoid network collapse.",
  "left out keywords": {
      "Bernoulli": "Bernoulli is a proper noun. It has specific technical and historical meanings that are not directly related to this abstract.",
      "em": "The 'em' keyword, likely shorthand for expectation-maximization, doesn't fit the theoretical focus on isometric approximation and hard negative mining."
  }
}