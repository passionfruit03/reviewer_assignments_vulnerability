import sys
sys.path.insert(0,'/home/jhihyih/projects/abstract-attack')
sys.path.insert(1,'/home/jhihyih/projects/abstract-attack/attack')

from tqdm import tqdm

import torch
from torch.nn.functional import normalize
import numpy as np
import os
import json

from attack.text import AdverserialText

from attack.model import Specter

dataset_dir = "/data/peer-review/nips23"
reviewer_file = "reviewers.txt"
archives_dir = os.path.join(dataset_dir, "archives")
publication_embeddings_dir = os.path.join(dataset_dir, "publications_embeddings_v1")
archive_embeddings_dir = os.path.join(dataset_dir, "archives_embeddings_v1")

with open(os.path.join(dataset_dir, reviewer_file)) as f:
    reviewer_ids = f.read().splitlines()

victim_model = Specter("v1")
victim_model.to("cuda")

for reviewer_id in tqdm(reviewer_ids):
    archive_path = os.path.join(archives_dir, reviewer_id+'.jsonl')

    with open(archive_path, 'r') as f:
        publications = []
        for line in f:
            paper = json.loads(line)
            instance = {}
            instance['title'] = paper['content']['title']
            instance['abstract'] = paper['content'].get('abstract', "")
            instance['paper_id'] = paper['id']
            instance['target'] = None
            instance['authors'] = None
            publications.append(AdverserialText(instance))
    
    if len(publications) == 0:
        continue
    
    with torch.no_grad():
        pub_embs = victim_model(publications)

    np.save(os.path.join(publication_embeddings_dir, reviewer_id+".npy"), pub_embs.cpu().numpy())

    archive_emb = torch.mean(normalize(pub_embs, dim=1), dim=0)
    np.save(os.path.join(archive_embeddings_dir, reviewer_id+".npy"), archive_emb.cpu().numpy())
    

