import os
import json
import random
import numpy as np
from scipy.stats import rankdata

import torch
from torch.nn.functional import normalize

natural_rank = 1001
sample_size = 100
venue_dataset_dir = '/path/to/dataset/nips23'
reviewer_ids_file = "reviewer_ids.txt"
submission_ids_file = f"submission_ids.txt"
submissions_file =  "submission_metadata.json"
submission_embs_file = f"submission_embeddings.npy"

with open(os.path.join(venue_dataset_dir, reviewer_ids_file), "r") as f:
    reviewer_ids = f.read().splitlines()

with open(os.path.join(venue_dataset_dir, submission_ids_file), "r") as f:
    submission_ids = f.read().splitlines()

# load reviewer embeddings
reviewer_embs = []
for reviewer in reviewer_ids:
    emb_file = os.path.join(venue_dataset_dir, "archives_embeddings_v1", reviewer + ".npy")
    emb = np.load(emb_file)
    reviewer_embs.append(emb)

reviewer_norm_embs = np.stack(reviewer_embs, axis=0)

# load submission embeddings
submission_embs = np.load(os.path.join(venue_dataset_dir, submission_embs_file))
submission_norm_embs = normalize(torch.Tensor(submission_embs), dim=1).numpy()

# calculate similarity matrix
similarity = np.round(np.matmul(submission_norm_embs, reviewer_norm_embs.T), 6)

authors = []
with open(os.path.join(venue_dataset_dir, submissions_file), "r") as f:
    submissions = json.load(f)
    for submission_id in submission_ids:
        authors.append(submissions[submission_id]['authorsids'])

for i, paper_authors in enumerate(authors):
    for j, reviewer in enumerate(reviewer_ids):
        if reviewer in paper_authors:
            similarity[i, j] = -1


# calculate rankings
ranks = rankdata(-similarity, axis=1, method='min')

# find reviewers that are the target rank or immediately after for each submission 
N = len(reviewer_ids)
selected_ranks = np.min(np.where(ranks >= natural_rank, ranks, N), axis=1)

paper_reviewer_pairs = []
for i, submission_id in enumerate(submission_ids):
    selected_reviewers = [reviewer_ids[j] for j in range(N) if ranks[i, j] == selected_ranks[i]]
    for reviewer in selected_reviewers:
        paper_reviewer_pairs.append((submission_id, reviewer))

attacks = []
for submission_id, reviewer_id in paper_reviewer_pairs:
    attack = {
        'paper_id': submission_id,
        'title': submissions[submission_id]['title'],
        'abstract': submissions[submission_id]['abstract'],
        'target': reviewer_id,
        'authors': submissions[submission_id]['authorsids']
    }
    attacks.append(attack)

attacks = random.sample(attacks, sample_size)
with open(os.path.join(venue_dataset_dir, "evaluation_samples", f"attacks_r{natural_rank}.jsonl"), "w") as f:
    for attack in attacks:
        f.write(json.dumps(attack) + "\n")